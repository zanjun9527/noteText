
一些设计思路
	这本书 文字多，不够简洁
	太技术化，在HSF的阶段，出于情怀，在有一个版本里投入了非常大的精力去引进OSGi以及去做动态化，这个后来事实证明是个非常非常错误的决定，从这个点我才真正明白在设计系统时一定要想清楚目标，而目标很重要的是和公司发展阶段结合；

	思维导图，查看体系知识更清晰
	
	新技术:官方的文档，官网的demo。后面再横向对比，看源码(从依赖性最小的模块看起)。
	
	不是全面的掌握，每个东西都有关键点，抓住关键点是核心，往原理上问，抓重点记忆
	
	不仅仅是细节化的落实了，要整体的设计，主要核心点，以前是从点研究，后面要从面入点。请求数量 性能指标，aqs等
	
	系统化 由面入线重点内容再到点

	
开发经验
	1? Don’t Repeat Yourself ：这是软件开发的 础原 ，即不要做重复性劳动，也是现在所说的“极客文化”的 代码重复 作重复在软件开发中都是不合理的存在，利用各种手段消除这些重复是软件开发的 个核心工作准

	2? Keep it simple stupid 在做软件设计的工作中，很多时候都不要想得过于复杂，也不要过度设计和过早优 ，用最简单且行之有效的方案 就避免了复杂方案带来的各种额外成本 这样既有利于后续的维护 也有利于进一步的扩展

	3? You Ain’t Gonna Need It ：即 YAGNI 只需要将应用程序必需的功能包含进来，而不要试图添加任何其 你认为可能需要的功能 因为在 个软件中，往往请求都花费在 的功能上

	4? Done is better than perfect ：在面对 个开发任务时，最佳的思路就是先把东西做出来，再去迭代优化 如果 开始就面面俱到 考虑到各种细节，那么很容易钻牛角尖而延误项目进度

	5? Choose the most suitable things 这是在做方案选择 技术选型时候的 个很重要的原则 在面对许多技术方案、开源实现的时候，务必做到不能盲目求新，要选择最合适的而非被吹得天花乱坠的

	
	
	
	
	
1.sql的预编译机制，这个重要的规则是避免使用字符串串联起多个sql查询。

2.悲观锁是避免冲突，遇到就等；乐观锁是提交时才判断冲突。select ... for update ，容易死锁，因为会一直等到锁可用，两个用户都需要a，b资源，这时候会出现互相等待的场景
   在处理跨系统的事务时，等待锁是没意义的，这时候需要涉及超时控制。只需让锁管理对象在锁不可用时抛出异常就行。
	可以给锁增加时间戳，定期清除超时的锁即可
	
	在冲突率很高的并发场景下适合用悲观锁（应该是作为乐观锁的一个补充）

	悲观锁开销在于加锁本身，一般来说悲观锁需要很复杂的同步与调度机制，即使没有真正发生冲突也增加额外开销，
	而乐观锁通常来说根本没有真正的锁，而只是检查了一个标记决定是否真的进行提交，开销要小得多，但是一旦发生冲突需要撤销整个transaction重做。所以冲突相对少的时候乐观锁有优势。
	其实使用update的 SET account_active=(account_active+#{amount}),version=version+1，这里条记录是不会冲突的， 实际起到的作用是保证useraccount 和actran记录的一致性
		
3.跨域多个请求的事务称为长事务，还有就是使用延迟事务，尽可能晚打开事务

4.客户端的会话状态保存，客户端存储数据，：url参数(常用来传递sessionId)，表单隐藏域(可以再传给server端)，cookie(基于域名传递的)，
	服务端的会话状态是存在内存中的，即session，对应的key是sessionid存放内存映射表中，
	
	
5.try 在for之外，那么异常是会终止循环的，若在之内，捕获后可继续循环

6、spring的循环依赖问题：在注入@Autowired 下加@Lazy 注解即可(两边都加比较保险)
   原因是spring中Bean构造函数入参引用的对象必须已经准备就绪，那么两个相互依赖的bean就有可能出现问题
   
   还有一个解决方式是：将相互依赖的两个Bean中的其中一个Bean采用Setter注入(也就是属性注入)的方式即可。
   
   spring对象初始化三个步骤：
	（1）createBeanInstance：实例化，其实也就是调用对象的构造方法实例化对象
	（2）populateBean：填充属性，这一步主要是多bean的依赖属性进行填充
	（3）initializeBean：调用spring xml中的init 方法。
	从上面讲述的单例bean初始化步骤我们可以知道，循环依赖主要发生在第一、第二部。也就是构造器循环依赖和field循环依赖。
	
	@Bean(initMethod = "start", destroyMethod = "destroy")
	指定对应bean中的初始化方法相关

	
	Spring容器会将每一个正在创建的Bean 标识符放在一个“当前创建Bean池”中，Bean标识符在创建过程中将一直保持在这个池中。
	因此如果在创建Bean过程中发现自己已经在“当前创建Bean池”里时将抛出BeanCurrentlyInCreationException异常表示循环依赖；而对于创建完毕的Bean将从“当前创建Bean池”中清除掉。
	初始化完的Bean会从池中移除
	
	setter是实例化结束的对象放到一个Map，可以获取，构造是放在池中，池不能重复创建同一对象
   
	初始化方法，
		初始化就使用初始化的东西，别整些什么servlet的启动啥的非主流

		直接在 Bean 的构造方法里做初始化工作
		***
			使用＠PostConstruct 注解，指明在 Bean 造器方法执行后执行的方法
			Bean实现InitializingBean 接口，在 afterPropertiesSet 中做初始化工作
		***
		XML 中使用 init-method 指定 Bean 构造完成后调用的方法

		***
			销毁对应
				1.@PreDestroy注释，注释回调方法上，销毁Bean之前调用；
				2.实现DisposableBean接口，调用destroy(...)，销毁Bean之前调用；
				3.Bean定义中包含destroy-method（在XML中标签<bean>的属性）或@Bean(destroyMethod="...")指定的方法，销毁Bean之前调用；	
		***
		
		@DependsOn  是有条件的初始化 
		也可使用 BeanFactoryPostProcessor 和 BeanPostProcessor 来做一些更前置的初始化工作，典型的应用场景就是实现自己的注解
		BeanFactoryPostProcessor> Constructor > BeanPostProcessor.postProcessBeforelnitialization > @PostConstruct > InitializingBean > init-method
		想在所有 Bean 都初始化完毕后做一次初始化工作，可以使用 ApplicationListener ，监昕 ContextRefreshedEvent:

	在销毁 Bean 之前做一些收尾工作
	使用＠PreDestroy 注解， 指明在容器关闭后执行的方法
	实现 DisposableBean 接口，在 destroy 方法中做销毁工作
	XML 中使用 destroy method 指定 Bean 销毁时调用的方法
	＠PreDestroy > DisposableBean > destroy-method
   
   注解和xml配置
	@PostContruct 和 init-method
	@PreDesroy 和 destroy-method
   
	我们一般常用的依赖注入方式是setter方法注入
   
   
7、	https相关
		生产上的tomcat是没有配置ssl的，都是nginx端配置的https，所以请求实际上都是在http上存入缓存

		https请求过程
			1.客户端向服务器端发送请求，将客户端的功能和首选项传送给服务器端，包括客端支持的 SSL 本、加密组件列表等
			2.服务器端发送选择的连接参数（从客户端加密组件中筛选出的加密组件内容和压缩方法）以及证书（包含公钥等信息、）给客户端
			3.客户端读取证书中的所有人、有效期等信息并进行校验，然后通过预置的 CA 验证证书合法性，有问题 提示
			4.客户端生成用于数据加密的对称密钥，然后用服务器的公钥进行加密井发送给服务器端
			5.服务器端使用自己的私钥解密数据， 获得用于数据加密的对称密钥
			6.安全的通道建立完毕，后续基于对称加密算法传输数据


		ssl网址		https://myssl.com/   可以查看xiaoyuer.com的https的证书相关

		
8、default-autowire="byName" 在配置mybatis数据源的地方去掉，否则读取属性文件找不到

	<parent>
	<relativePath>作用

	<!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是../pom.xml。Maven首先在构建当前项目的地方寻找父项 
	目的pom，其次在文件系统的这个位置（relativePath位置），然后在本地仓库，最后在远程仓库寻找父项目的pom。 -->
	<relativePath />
	
	ContextLoaderListener默认去WEB-INF下加载applicationContext.xml配置。
	默认的applicationContext.xml和x-servlet.xml文件
	
	netpay中不能吧openservice也依赖进来，这样就不是远程的服务了
	rpc的公用接口jar一定要和service分离开来
	
	直接拖动相关java文件，对应的会更新引用	

	编译ok 运行异常，肯能是tomcat的运行配置中少了环境
	
	
9、jdk版本切换要点
	cmd  echo %path% 输出系统的环境变量
	更换java_home
	删除path中的变量C:\Program Files (x86)\Common Files\Oracle\Java\javapath;
	删除C:\ProgramData\Oracle\Java，将Java文件直接删除
	然后更换system32中的三个java文件对应版本即可
	最多还要修改下注册表	HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment
				HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit
	
	
	
10、boot使用数据源或者jndi
	属性文件中debug=true  就可以查看详细的加载配置
	
	boot的启动方法SpringApplication.run(),是应用主程序开始运行的方法。
	首先开启一个springapplicationrunlisteners监听器，然后创建一个应用上下文configurableapplicationicontext,
	通过这个上下文加载应用所需的类和各种环境配置，最后启动一个应用实例。
	创建上下文过程中(this.annotatedReader.register(new Class[]{source},将@SpringBootApplication加载进配置);)
	
	约定优先原理:自动配置加载一个类的配置时，首先读取项目中的配置，项目中没有才启用默认配置
	enableautoconfiguration最终导入一个自动配置的类列表，引入相关的包，就会被启用
	positive matches 就是这个应用所需加载的一些配置类
	
	数据源加载
		本质上boot中的属性文件也是自动加载完成的，@configurationproperties{prefix="spring.datasource"}
		DataSourceBuilder 和 JndiObjectFactoryBean
	
	
	
11、分页插件
	目前的分页插件PagePlugin，也是网上随便copy的一份下来用的，对mapper有代码侵入，无法封装总页数
	分页插件的原理  物理和逻辑分页
	逻辑分页利用游标分页，好处是所有数据库都统一，坏处就是效率低。 
	物理分页就是数据库本身提供了分页方式，如mysql的limit，好处是效率高

	mybatis-plus 插件	https://www.cnblogs.com/leeego-123/p/10734330.html
		常用的bootmaven依赖 
		<!-- Spring Boot热部署 -->
				<dependency>
					<groupId>org.springframework.boot</groupId>
					<artifactId>spring-boot-devtools</artifactId>
					<optional>true</optional>
				</dependency>
			</dependencies>

			<build>
				<plugins>
					<plugin>
						<groupId>org.springframework.boot</groupId>
						<artifactId>spring-boot-maven-plugin</artifactId>
					</plugin>
				</plugins>
			</build>

			
		boot原本配置
		mybatis:
			mapper-locations: classpath:mappers/*.xml
			 虽然可以配置这项来进行pojo包扫描，但我更倾向于在mapper.xml写全类名
			 type-aliases-package: com.rhine.blog.po     配置之后好像可以省略xml中的包名，<resultMap id="userMap" type="UserBean">

		plus插件的配置，但是需要排斥其他的mybatis依赖，在boot中
			<!-- 
			<dependency>
					<groupId>org.mybatis.spring.boot</groupId>
					<artifactId>mybatis-spring-boot-starter</artifactId>
					<version>1.3.2</version>
				</dependency>
			-->
		mybatis-plus:
		  mapper-locations: classpath:mappers/*.xml
		  type-aliases-package: com.rhine.blog.po
		  
		
	pagehelper
		PageHelper.startPage(1,3);
        List<UserBean> byEmail = userMapper.findByEmail("100");//实际查询返回的是page对象，也是实现list接口的额，
        PageInfo page = new PageInfo(byEmail);
        long total = page.getTotal();

			<dependency>
				<groupId>com.github.pagehelper</groupId>
				<artifactId>pagehelper-spring-boot-starter</artifactId>
				<version>1.2.5</version>
				<exclusions>
					<exclusion>
						<groupId>org.mybatis.spring.boot</groupId>
						<artifactId>mybatis-spring-boot-starter</artifactId>
					</exclusion>
				</exclusions>
			</dependency>

		//分页时，实际返回的结果list类型是Page<E>，如果想取出分页信息，需要强制转换为Page<E>，
		//或者使用PageInfo类对结果进行包装，可以拿到多有的page属性
		PageInfo page = new PageInfo(list);
		Page<UserBean> byEmail = (Page)userMapper.findByEmail("100");
	
	总结：PageHelper首先将前端传递的参数保存到page这个对象中，接着将page的副本存放入ThreadLoacl中，这样可以保证分页的时候，参数互不影响，接着利用了mybatis提供的拦截器，取得ThreadLocal的值，重新拼装分页SQL，完成分页。

	在threadlocal中设置分页参数，之后在查询的时候，获取当前线程中的分页参数，执行查询的时候通过拦截器再sql中添加分页参数，之后实现分页查询，查询结束后在finally语句中清除threadlocal中的查询参数

	原理：使用ThreadLocal来传递和保存Page对象，每次查询，都需要单独设置PageHelper.startPage()方法

	PageHelper.startPage()和查询方法连着用，实际就是拦截器再查询方法的时候，从线程变量中拿到分页信息组装的结果。

	这个类实现了org.apache.ibatis.plugin.Interceptor接口。在com.github.pagehelper.PageInterceptor.intercept(Invocation)方法的最后finally块中调用了afterAll:
	在你要使用分页查询的时候，先使用PageHelper.startPage这样的语句在当前线程上下文中设置一个ThreadLocal变量，再利用mybatis提供的拦截器（插件）实现一个com.github.pagehelper.PageInterceptor接口，这个分页拦截器拦截到后会从ThreadLocal中拿到分页的信息，如果有分页信息，这进行分页查询，最后再把ThreadLocal中的东西清除掉。
	最后实在finally中清除的


	使用pageinfo后面的关联问题
	实际山查询出来的是一个page(list)信息，后面封装了pageinfo的参数

	继承arraylist后，数据都是放在elementData数组中的
	返回的page<e>就是一个数组，元素在elementData中（list中的数据组），

	mybatis 分页插件
		MyBatis 3.4.1或者其以上版本(使用MyBatis 3.4.1(不包含)以下没有Integer.class)
		@Intercepts({ @Signature(type = StatementHandler.class, method = "prepare", args = { Connection.class，Interger.class}) })
		
		
	boot 2.0引入pagehelper
		1.pom引入pagehelper-spring-boot-starter
		2.属性文件配置
			#pagehelper
			pagehelper.helper-dialect=mysql
			pagehelper.reasonable=true							这个属性要注意，默认是false(超限返回空数据)，设置为true，页数超限就显示最后一页的内容
			pagehelper.support-methods-arguments=true
			pagehelper.params=count=countSql
		
		
	*********
	dao中引入了tk-mybatis，其中使用了3.4.5的mybatis版本。然后admin中引入的pagehelper和mybatis对应的是mybatis是3.5几的版本， 
	jar冲突，导致了spring-dubbo.xml中的${zookeeper.address}读取失败，占位符无效，少见的错误。排除方法，针对简单ok的dubbo.xml demo 逐步删除admin无效代码，缩小排查范围。

	查询结果是page转对象问题
		使用pagehelper查询出的对象是page对象，然后放入json中，如果取的时候是getString,然后用arrayList转，会报错。除非放入限定好的对象中或者直接list<E>强转
		就是pagehelper查询出的page对象，放json中，然后用getString反转为对象就要注意，直接强转
	
	
	开闭原则，对扩展开放，对修改关闭
			
			mybatis主要完成两件事，1.根据jdbc规范建立与数据库的连接 2.通过反射打通java对象和数据库参数的交互之间相互转化的关系
			
			PreparedStatement 接口是 Statement 的子接口，它表示一条预编译过的 SQL 语句，可以添加参数
			效率高.安全性好,使用PreparedStatement可以防止SQL注入,参数设置非常方便
			PreparedStatement perstmt?=?con.prepareStatement(sql);  // ？作为参数占位符
			perstmt.setString(1,var1);
			perstmt.setString(2,var2);
			perstmt.executeUpdate()
			
			原始的ibatis的调用实例
				Class.forName("com.mysql.cj.jdbc.Driver");
				Connection connection = DriverManager.getConnection("url", "user", "password");
				PreparedStatement ps = connection.prepareStatement("sql");
				ps.setInt(1, 10);
				ps.execute();
				ResultSet resultSet = ps.getResultSet();
				while(resultSet.next()) {
					String value = resultSet.getString("columName");
				}
	
	
		
12、Java泛型中的标记符含义： 

	任意一个大写字母都可以。他们的意义是完全相同的，但为了提高可读性，使用哪个字母是没有特定意义的！只是为了提高可读性

	 E - Element (在集合中使用，因为集合中存放的是元素)
	 T - Type（Java 类）
	 K - Key（键）
	 V - Value（值）
	 N - Number（数值类型）
	
	
	菱形语法（泛型推断）：从JDK 7 开始，Java允许在构造器后不需要带完整的泛型信息，只要给出一对尖括号<>即可，Java可以推断出尖括号里面应该是什么类型。
　　比如：List<String> list = new ArrayList<>();
	
	
	public static <T> T get(String key, Class<T> clazz) {}
	T是代表任意一种类型，这是泛型里的问题，<T>是一种形式，表示你用的是泛型编程，不受类型的约束
	
	private <T> T getListFisrt(List<T> data) {}			可以接受任意类型List参数。这个是不限定类型
	private T getListFisrt(List<T> data) {}				初始化的时候已经限定了T的类型，所以getListFirst方法只能接受List<T>类型的参数。这个是限定类型。是编译就要指定的特定的类型
		public static void copyList(List<T> list1, List<T> list2, Class<T> classObj) {}
	
	
	1. <? extends T>表示该通配符所代表的类型是 T 类型的子类。
	2. <? super T>表示该通配符所代表的类型是 T 类型的父类。
	
	首先，大家可以看到Point<T>，即在类名后面加一个尖括号，括号里是一个大写字母。这里写的是T，其实这个字母可以是任何大写字母，大家这里先记着，可以是任何大写字母，意义是相同的。

	泛型函数定义及使用
	
		public class StaticFans {  
			//静态函数  
			public static  <T> void StaticMethod(T a){  
				Log.d("harvic","StaticMethod: "+a.toString());  
			}  
			//普通函数  
			public  <T> void OtherMethod(T a){  
				Log.d("harvic","OtherMethod: "+a.toString());  
			}  
		}  
		静态泛型函数和常规泛型函数的定义方法，与以往方法的唯一不同点就是在返回值前加上<T>来表示泛型变量。其它没什么区别
		
			
		*****T 和 <T>的区别*****
			方法返回前的<T> 是告诉编译器，当前方法的值传入类型可以和类初始化的泛型类型不同，也是就是该方法的泛型类可以自定义，不需要跟类初始化的泛型类相同.
			主要是看是否受Class<T>的影响
			要么自定义<T> T，要么跟着类的<E>走。必须要有泛型说明(1.自定义，2.跟类走)。
		
		
		
			/** 有的方法返回值为 <T> T ，有的方法返回值为 T ,区别在那里 ？ */
			public class Request<E> {
				/**
				 * tClass 		方法入参
				 * <T>  		声明此方法拥有一个类型T,也声明此方法是一个泛型方法
				 * T    		指明该方法返回值为类型T
				 * Class<T>  	指明泛型T的具体类型
				 */
				public <T> T getObject(Class<T> tClass) throws IllegalAccessException, InstantiationException {
					T t = tClass.newInstance();
					return t;
				}
				/**
				 * 方法返回前的 <T> 是告诉编译器，当前方法的值传入类型可以和类初始化的泛型类型不同(也是就是该方法的泛型类可以自定义，不需要跟类初始化的泛型类相同)
				 * 
				 *  参数 T 第一个 表示是泛型，第二个 表示是返回是T类型的数据，第三个 表示限制参数类型为T
				 * @param data
				 * @param <T>
				 */
				private <T> T getListFirst(List<T> data) {
					if (data == null || data.size() == 0) {return null;}
					return data.get(0);
				}
				/**
				 * 这个只能传E类型的数据
				 * *********这里不能写成T了，要么自定义<T>，要么跟着类的<E>走。*********
				 */
				private E getListFirst2(List<E> data) {
					if (data == null || data.size() == 0) {return null;}
					return data.get(0);
				}

				public static void main(String[] args) {
					List<Integer> data = new ArrayList<>();
					List<String> data2 = new ArrayList<>();
					// 入参由List<T>的T 决定，因为返回值为<T> T ,所以入参不受 Request<T> 影响
					Integer a = new Request<String>().getListFirst(data);

					// 编译出错，入参由Request<T> T的决定，受Request<T>影响
					//new Request<String>().getListFirst2(data);

					// 没什么区别
					String aa = new Request<String>().getListFirst(data2);
					String bb = new Request<String>().getListFirst2(data2);
				}
			}
	
13、ThreadPoolTaskExecutor可以配置线程池相关，是spring的线程池技术，可以配置相关参数
	调用demo
	SpringThread t = new SpringThread(i);
	executor.execute(t);

	
14、日期的时间变化，注意时分秒的精度
	SELECT DATE_SUB('2019-07-02 17:21:08', INTERVAL 1 DAY)
	SELECT DATE('2019-07-02 17:21:08'-INTERVAL 1 DAY)
	
15、移动端的日志插件
	 <script type="text/javascript" src="https://www.w3cways.com/demo/vconsole/vconsole.min.js?v=2.2.0">
	 <script>
			var vConsole = new VConsole();
	 </script>
		
		
	后面直接使用日志打印即可，移动端会有vconsole的显示
	
	try{
						
	}catch(err){
	   console.log(err)
	   console.log(err.message);
	}

16、boot、dubbo
	提供者注册在zookeeper的地址是，192.168.6.85:20881，这样就是通过20881端口对外提供dubbo服务
	jar 启动时候可以用java -jar app.jar --spring.profiles.active=dev  来指定运行的环境，目前采用的是pom中指定profile加载的属性文件
	
	注册服务
		spring.dubbo.application.name=controller-consumer
		spring.dubbo.registry.address=zookeeper://172.17.0.2:2181
		spring.dubbo.scan=com.gaoxi						扫描dubbo的注解，使用注解
		
		import com.alibaba.dubbo.config.annotation.Service;
		@Service(version = "1.0.0")
		@org.springframework.stereotype.Service

	发现服务
		@Reference(version = "1.0.0")
		spring.dubbo.application.name=controller-consumer # 本服务的名称
		spring.dubbo.registry.address=zookeeper://IP:2182 # zookeeper所在服务器的IP和端口号
		spring.dubbo.scan=com.gaoxi # 引用服务的路径

	其中可以将权限缓存在本地的map中，
	
	
	dubbo的启动方式
	1、使用Servlet容器（Tomcat、Jetty等）运行
	缺点：增加复杂性（端口、管理） 浪费资源（内存）
	服务容器是一个 standalone 的启动程序，因为后台服务不需要 Tomcat 或 JBoss 等 Web 容器的功能，如果硬要用 Web 容器去加载服务提供方，增加复杂性，也浪费资源。

	2、自建main方法类来运行（Spring容器）  		
	缺点： Dobbo本身提供的高级特性(优雅停机等)没用上 自已编写启动类可能会有缺陷
	**********
	注意别和boot的main启动搞混了，这种一般就是ClassPathXmlApplicationContext 加载一个context，然后wait实现服务提供,一般测试用
	**********

	3、使用Dubbo框架提供的Main方法类来运行（Spring容器)
	优点：框架本身提供（com.alibaba.dubbo.container.Main） 可实现优雅关机（ShutdownHook）
	配置配在 java 命令的 -D 参数或者 dubbo.properties 中。其他的在dubbo.xml中配置即可，这两个可以结合下配置，目前项目中是xml主要配置
	服务容器只是一个简单的 Main 方法，并加载一个简单的 Spring 容器，用于暴露服务。
	dubbo.spring.config=classpath*:*.xml   这个相当于通过属性文件去导入了，目前项目是直接引入xml到容器中
	运用：生产上dubbo server可以用这种方式部署。
	搭配boot的时候：new SpringApplicationBuilder(XyeServiceCoreApplication.class).web(false).run(args);要以非web方式进行


	在使用spring-dubbo.xml中，service实现类 注入dubbo服务后，才会去读取配置的zookeeper地址
	
	boot中继承dubbo
		dubbo-spring-boot-starter 使用这个才能在applicationi.properties中自动配置dubbo属性，否则只引入dubbo的maven，就需要添加dubbo.proerties配置

	
17、在继承arraylist后，转为json对象，后面只能解析出list元素中的东西，使用fastjson是这样的
	实际是判断list继承后解析的，只解析了list总的元素值，在eclipse中显示正常，但是在idea中显示缺少
	
	
18、全局异常处理
	使用@ControllerAdvice注解，全局捕获异常类，只要作用在@RequestMapping上，所有的异常都会被捕获
	HandlerExceptionResolver，旧版的是在web.xml中配置错误页面
	
	处理异常：＠ controllerAdvice 和 ExceptionHandler 	@ControllerAdvice是用来定义控制器通知的，＠ExceptionHandler是指定异常发生的处理方法

	
19、shiro相关
		shiro配置的两种方式，代码注入和配置ini文件加载

		配置的几个关键的东西：securityManager,ShiroFilterFactoryBean(配置登录路径,拦截器路径等),自定义Realm（创建时候可能需要加密的方式配置HashedCredentialsMatcher），

		登录之后，存在session中，这样够缓存用户的信息，每次请求都创建一个subject，从缓存中新建的对象
		权限更新后需要更新内存中的权限

		使用shiro的注解需要配置AuthorizationAttributeSourceAdvisor	
		HashedCredentialsMatcher 是密码加密相关

		SecurityUtils.getSubject()是每个请求创建一个Subject, 并保存到ThreadContext的resources（ThreadLocal<Map<Object, Object>>）变量中，也就是一个http请求一个subject,并绑定到当前线程。 


		login的时候，获取一个subject.login(new UsernamePasswordToken(name, userParam.getPassword()));这样在realm中认证的时候，用户的name 和密码信息就会被拿到了
		然后再认证中返回 new SimpleAuthenticationInfo(userName, passwordInDB, ByteSource.Util.bytes(salt),getName());
		最终是通过securitymanager调用AuthenticationInfo info = realm.getAuthenticationInfo(token);实现认证

		可以初始化权限路径，将资源权限加载进来
		  String permission = "perms[" + resources.getResurl()+ "]";
		  filterChainDefinitionMap.put(resources.getResurl(),permission);

		<!-- 过滤链定义，从上向下顺序执行，一般将/**放在最为下边 -->:这是一个坑呢，一不小心代码就不好使了;
		SimpleAuthenticationInfo第一个参数一般传的是userinfo对象，有的也传username，暂时不确定，后续待验证


		首先shiro会先从缓存中获取认证信息(对应getCachedAUthenticationInfo方法),如果没有才会继续从Realm中获取,
		认证的时候 ByteSource credentialsSalt = ByteSource.Util.bytes("vip");
		Object obj = new SimpleHash(hashAlgorithName, password, credentialsSalt, hashIterations);
		会使用SimpleAuthenticationInfo中公用的salt，和token中的密码相对比，一致就校验通过



		除了通用的过滤器外，可以自定义过滤器
		//下面的配置路径 都需要在上面配置 authc 否则访问不到filter
		filterChainDefinitionMap.put("/online","requestURL");


		自定义路径拦截器继承PathMatchingFilter（可以实现用户路径权限的判断）

		filtersMap.put("requestURL", getURLPathMatchingFilter());
		shiroFilterFactoryBean.setFilters(filtersMap);

		相关的配置文件：INI配置文件一般适用于用户少且不需要在运行时动态创建
		INI文件优势 : 简单易懂 , 集成方便.
		INI文件缺点 : 采用硬编码方式把认证授权信息写在INI文件中,可维护性差.
		推荐相关配置放在数据库中动态读取


		常见的realm，我们需要缓存功能,认证功能,授权功能,三大功能 .认证原则:什么样的用户是怎样的角色.拥有什么的权限.
		shiro的默认的拦截器执行是有一定的顺序的，一般全匹配的放最后面，如果匹配到就不继续匹配了，所以把 /放到最前面，则 后面的链接都无法匹配到了。 

		拦截器的通配符的写法
		?：匹配一个字符
		*：匹配零个或多个字符
		**：匹配零个或多个路径

		也可以使用一个过滤器，将每个路径都过滤一遍
		感觉实现自己的路径拦截会更灵活点，在项目中配置注解，维护性差

		Subject.login()登录成功后用户的认证信息实际上是保存在HttpSession中的。如果此时Web应用程序部署了多实例，必须要进行Session同步。


		其实在SecurityManager中设置的CacheManager组中都会给Realm使用，即：真正使用CacheManager的组件是Realm。


		首先需要对session进行同步，因为shiro的认证信息是存放在session中的；其次，当前端操作在某个实例上修改了权限时，需要通知后端服务的多个实例重新获取最新的权限数据。


		当用户登录之后就会被缓存在session中，再次访问会根据sessionid渠道对应的pricipals，

		principal, 就是传入的认证信息的第一个参数,传递的类型就是后面获取的值。UserBean user = (UserBean)subject.getPrincipal()
		  public SimpleAuthenticationInfo(Object principal, Object hashedCredentials, ByteSource credentialsSalt, String realmName) {
				this.principals = new SimplePrincipalCollection(principal, realmName);
				this.credentials = hashedCredentials;
				this.credentialsSalt = credentialsSalt;
			}

	
	
	1.根据页面传的菜单id 查询所有的buttonid，返回所有的button，然后按一定规则展示即可
	2.前台地址先固定，然后后天查出来当前页面可用相关权限，前台直接获取。直接返回按钮或者返回固定的权限列表(页面权限在所属列表中，相当于后台筛选过了，前端直接使用)

	如果按钮配置了路径，就按钮返回显示，没有配，就页面固定，然后根据权限输出

	如果是超级管理员有几百个权限，不可能传到前台去遍历，一定要再查询一遍，(根据模块尽量减小范围，查询后返回)/(遍历所有权限然后过滤？也不可能，都是字符串，没有过滤规则)
	所以目前最好的是更具当前页面级别，范围筛选权限(或者按钮)，前台在判断。		
			
	目录 菜单 按钮
	思路还是后端将权限查出来(全部查出或者根据上级id精细查询，返回给前端判断显示)
		
	根据permssion 判断按钮级别的显示，根据menu_code(归属菜单)判断用户可查看的菜单
	这个应该是查询出所有的权限和路由菜单了，可能数据大点
	如果想做的再精细点，每次查询权限就传入路由菜单或者parentid
			
			
	https://blog.csdn.net/kity9420/article/details/102530950			自定义权限标签？js组件解析权限列表
			
			
	**************权限设计**************
		安全设计(核心就是用户围绕着  角色-权限)
			1.用户的身份认证，即用户登录
			2.用户授权，即操作权限管理。

			权限管理设计一般使用角色来管理(用户-角色-权限)，用户的权限是通过角色来分配的，权限和角色有直接的关系
			登录成功之后，可在登录成功处理器中执行相关自定义操作。

			这里的登录状态，进数据库，保存的是用户名，令牌，和最后登录时间。


			对于页面的按钮显示实现
			后台读取当前用户的所有权限，然后针对当前页面的按钮入新增，newrole,有就返回对应的newrole=true，
			然后页面根据自己的值判断即可，这里传到页面的是对应按钮的判断值。
			传到页面的是按钮的判断值，至于页面，直接写路径和按钮，不必动态读按钮
			系统中使用的，是返回所有可用的button，然后页面根据具体的位置(循环找出合适的)显示按钮
	**************权限设计**************
	
	
20、ruoyi
	获取request的方式，	1.从controller一层层往下传，
						2.RequestContextHolder，直接在需要用的地方使用如下方式取HttpServletRequest即可
						HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();  本质上还是使用了threadlocal变量
						
	job的配置：quartz调度三大核心元素Scheduler(任务调度器)、Trigger(触发器)、Job(任务)
	Spring整合Quartz进行配置遵循下面的步骤：
			
	1：定义工作任务的Job
	，通常也是jobdetail        
	2：定义触发器Trigger，并将触发器与工作任务绑定
	（两种方式，静态和动态，动态的配置文件不用配置）        
	3：定义调度器，并将Trigger注册到Scheduler
		

	触发的方式
	在项目中建一个类继承包中的QuartzJobBean类（这个方法是在时间到来时自动执行的方法）
	是一个抽象类需要重载executeInternal（JobExecutionContext）方法

	在设置jobdetail的时候，可以使用QuartzJobBean完成统一的触发入口，
	也可每个detail都定义实现job统一接口，完成触发也行
	jobDetail.getJobDataMap().增加参数		

	
	quartz这个就是qrtz_cron_triggers  系统默认的存放路径，系统源码存放表名
	
	
	session和缓存相关
			查看唯一登录的情况，登录踢出的功能，就是实现用户信息和session绑定的情况，

			其实在SecurityManager中设置的CacheManager组中都会给Realm使用，即：真正使用CacheManager的组件是Realm。

			踢出的功能，是引入了一个 Deque<Serializable> deque，其中存放的是用户的sessionid，存放在相应的cache中，
			当队列中size大于限制数，踢出对应的session， 将对应的session中设置kickout为true，当用户访问时，只要当前session中
			的kickout为true，就logout并重定向。

			shiro中常用的sessionmanager，DefaultWebSessionManager（可以不依赖容器管理会话）
			cache是缓存的操作，session是会话的操作

			Queue strings = new ArrayDeque<Serializable>();  队列顺序有点像栈
			可以用linkedlist代替，但是list的顺序是按照添加的顺序

			核心的路径匹配就是perm.implies(permission)

			使用自定的redis作为缓存需要自己实现
			CacheManager，主要管理缓存，提供一个缓存实例即可
			Cache，操作具体的实例来完成
			userRealm.setCacheManager(cacheManager);  表示开启了内存缓存
			Cache<Object, AuthorizationInfo>，将权限信息缓存进了cache中，下次可以直接使用
			
			dependencyManagement只是申明依赖，可以实现统一版本，子模块按需自己引入相应的jar

			SnakeYAML可以加载yaml文件，存进map中使用

			动态数据源，在查询的时候拦截选定指定数据源，后面执行数据操作的时候匹配相应的数据源，主要的是继承AbstractRoutingDataSource
			
			动态数据源，在一个事务中，只能管理对应的事务管理器的事务，属于其他事务管理器的事务，不起作用，
			
			
			https://mp.weixin.qq.com/s/X0ZIRj71HUjhpb5xf89YKQ   多数据源方案
			
			
			动态数据源的额数据嵌套(通过代理对象来调用目标对象，而在代理对象中有事务相关的增强处理(begin,do,commit))
	　
			
			
			nested exception is org.springframework.core.NestedIOException: 
			Failed to deserialize object type; nested exception is java.lang.ClassNotFoundException: com.xiaoyuer.pay.web.SessionData
			以redis为缓存中心，当系统需要从redis中获取缓存信息的时候，其中的类需要反序列化出来，系统中需要有相应的类
			
			

21.	可以测试支付调用主站时候的"转码，url中的正常&是不能转义的
	String param = dataForSign.replaceAll("\"", "%22").replaceAll("}", "%7d").replaceAll("\\{", "%7b").replaceAll(" ", "%20");

	单纯的select * from a,b是笛卡尔乘积。
	但是如果对两个表进行关联:select * from a,b where a.id = b.id 意思就变了，此时就等价于：select * from a inner join b on a.id = b.id。即就是内连接。

	nignx先断3秒后关机,留给上一个请求处理时间
	
	数据库备份数据，不要勾选视图和存储过程

	HAVING类似于WHERE（唯一的差别是WHERE过滤行，HAVING过滤组），是跟着group by 使用的，以组为查询的维度
	
	为日志增加变量输出
		MDC.put(STR_USER, accountNo);
		MDC.remove(STR_USER);    这个最终在finally中需要删除掉

		
		
	正则表达式：
		.（点号）也是一个正则表达式，它匹配任何一个字符
		.*	任意字符匹配多次	a*   多个字符匹配多次  依次匹配
		.*?	任意字符匹配一次	a*?
		\s+	匹配任意多个上面的字符。另因为反斜杠在Java里是转义字符，所以在Java里，我们要这么用\\s+
		. 匹配除换行符以外的任意字符

		\w 匹配字母或数字或下划线或汉字 等价于 '[^A-Za-z0-9_]'。
		\s 匹配任意的空白符
		\d 匹配数字
		\b 匹配单词的开始或结束
		^ 匹配字符串的开始
		$ 匹配字符串的结束

		^\d+(\.\d+)?

		1,^ 定义了以什么开始
		2,\d+ 匹配一个或多个数字
		? 设置括号内的选项是可选的
		\. 匹配 "."
		可以匹配的实例："5", "1.5" 和 "2.21"。

		特殊字符必须转义之后才能当做字符串
		java中\\代表一个\

		^匹配输入字符串的开始位置，除非在方括号表达式中使用，此时它表示不接受该字符集合。要匹配 ^ 字符本身，请使用 \^。
		
	正则表达式：
			字符串 String 的 split 方法，传入的分隔字符串是正则表达式！部分关键字（比如.[]()\|等）需要转义. |
			"a.ab.abc".split("\\."); // 结果为["a", "ab", "abc"]
			"a|ab|abc".split("\\|"); // 结果为["a", "ab", "abc"]
		
		
	两端交互的时候，发送端指定编码，接收方使用固定编码接收即可	
		response.setCharacterEncoding("UTF-8");//发送方固定统一编码utf8  然后接收方 的流处理使用统一编码解码即可
		
22.dubbo注解配置相关
			EnableDubboConfiguration 是使用dubbo注解的，spring的扫描注解正常配置,不影响
			
			http://jm.taobao.org/2018/06/13/Provider配置/    		dubbo配置
			http://dubbo.apache.org/zh-cn/blog/dubbo-annotation.html	官方文档

			@Component
			@PropertySource("classpath:application.properties")
			@ConfigurationProperties(prefix = "application.dubbo.demo.server")
			无敌属性类

			代码配置 dubbo配置，后续使用注解配置，@DubboComponentScan("com.xiaoyuer")
			@Configuration
			public class Dubboconfig
			注册
			  @Bean
					public RegistryConfig registryConfig() {
						RegistryConfig registryConfig = new RegistryConfig();
						registryConfig.setAddress(dubboProperties.getAddress());
						registryConfig.setClient(dubboProperties.getClient());
						return registryConfig;
					}

			<dependency>
				<groupId>org.apache.dubbo</groupId>
				<artifactId>dubbo-spring-boot-starter</artifactId>
				<version>2.7.3</version>
			</dependency>
		
		
		boot 中 @ImportResource 是加载xml文件用的，也可以用来加载dubbo.xml文件内容
		
	jdk升级的tomcat问题:
			Tomcat 从7升级到8的时候出现了 java .lang.IllegalArgumentException: An inval id domain [.xxx.com] was specified for this cookie 
			在 tomcat context.xml中配置 <CookieProcessor className="org. apache .tomcat.util. http .LegacyCookieProcessor" />
			
	日志相关
		stdout 输出到控制台，对应linux会输出到catalina.out
		自己配置日志路径	<property name="logbackpath" value="C:/Users/xiaoyuer/Desktop/xye-log"></property>  可行
		C:\Users\xiaoyuer\Desktop\xye-log  不行	
		
		additivity="false"  不继承父logger的输出，
		若是additivity设为false，则子Logger只会在自己的appender里输出，而不会在父Logger的appender里输出。
		
		logback中   appender是打印器  需要绑定到logger中  而root是根logger
		
	<logger name="org.springframework" level="ERROR" />
	这个是单独配置的级别  其中的参数默认覆盖root中的，其他的按照root默认来，会优先匹配logger，匹配不到才使用root的。
	***指定了详细的包的日志级别，root的不管用***
	
	案例：
		<logger name="orderValidate" additivity="false">
			<level value="INFO" />
			<appender-ref ref="orderValidate" />
			<appender-ref ref="STDOUT" />      不加控制台不打印，加上打印，不加orderValidate，控制台也不打印，一个appender也不指定，也不会继承root，因为这里已经匹配了相应的logger了
		</logger>
		<root>
			<level value="INFO" />
			<appender-ref ref="STDOUT" />
			<appender-ref ref="ERROR" />
		</root>

		orderValidate不加STDOUT，控制台是看不到的额，因为指定匹配了logger，没有使用root的
		
		<logger name="ch.qos.logback" level="ERROR" />  和 root 和日志级别关系    additivity="false"这个属性注意下
		root的作用是收集下面所有反馈上来的信息流并根据配置在root中appender进行输出，looger中配置了additivity="false"，就不会反馈到root中。为了不让日志打到root的配置中

		<logger name="com.xiaoyuer.core.dao" level="INFO"  />  单独配置可以覆盖 root的级别，为了单独显示指定的日志
		
	logback MDC机制
			作用：扩展logback内置的日志字段，自定义业务数据
			原理：内部持有一个InheritableThreadLocal实例，用于保存context数据。注意操作完要MDC.clear()方法
			案例：实际使用中可以用一个filter专门定制自定义的日志字段。也可以个别类中单独处理。只要是一个线程请求即可。
			
			代码实例
				MDC.put("sessionId",sessionId);
				logger.info("test23");
				MDC.remove("sessionId");
			
			logback.xml中的配置
				<property name="pattern" value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread|%X{invokeNo}|%X{sessionId}] %-5level %logger{36}-%msg%n"/>
				<appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
				<File>${logbackpath}/xye.log</File>
				<rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
					<FileNamePattern>${logbackpath}/xye.%d{yyyy-MM-dd}-%i.log</FileNamePattern>
					<maxFileSize>500MB</maxFileSize>
				</rollingPolicy>
					<encoder>
						<pattern>${pattern}</pattern>
					</encoder>
				</appender>
				
			
		
23.rpc相关
		zookeeper和服务端建立的是长连接，可以定时进行心跳检测。这肯定是的啊  基础知识

		服务端的底层实现包括服务扫描，服务启动，服务注册等特性。

		定制@rpcservice注解，带有@service注解，这样启动可以被spring扫描到。然后通过反射创建该类的实例，加入容器管理，
		并建立服务名称和服务实例之间的映射关系，便于从后续rpc请求中获取服务名称，拿到实例，反射调用目标实例
		使用ApplicationContextAware获得上下文，初始化的时候，拿到所有标有@rpcservice的类，并将接口名和实例对象绑定
		
	静态内部类，可以实现延迟加载（初始化时候才加载），支付那边的就使用类，第一次调用加载，后续直接用只有在加载内部类的时候才初始化
	
	rpc调用原理相关
	一般的服务区分是，interfacename+version+group 
	version的使用场景在于一个大型的分布式系统中，修改更细某个接口，在过渡状态，1.新增方法（多余，重复）2.version区分
	
	dubbo_version
	当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。用的暂时不多，后面可以再详细看使用场景
	在低压力时间段，先升级一半提供者为新版本,再将所有消费者升级为新版本,然后将剩下的一半提供者升级为新版本
	同一个接口不同version，不同的实现类。
	
	
	服务升级
		接口不变，直接重发即可。同名起名可能重复点，不好命名。其实这种快点
		接口表动
			1.新增接口
			2.接口参数列表变化，
				1.全部消费端一起修改，全部重发，动静大，无关消费者躺枪
				2.版本号，老方法调用原来的，新方法使用新版本服务。对应两个实现类即可，version不一样。新实现类中实现新功能方法即可，老的没动的还是用原来的实现类。
				3.考虑扩展性使用类似map的结构，但不直观，参数校验复杂。

	负载均衡的策略，随机，轮询，权重，其中权重是在服务集群的机器能力不对等的情况下，比较合适。
	
	QPS，可以利用tomcat中的access.log中的出现的日志统计到秒的重复次数。
	cat xx.log |grep 'GET /mvc2'|cut -d ' ' -f4|uniq -c|sort -n -r 
	
	
	服务调用的限流处理
		被拒接的请求，直接返回给调用者，也可以进行排队
		1.0和1开关
		2.设置固定值(每秒请求次数QPS)，超过请求次数，拒绝请求。
		
		限流的维度，
			1.根据服务端自身的接口，方法控制，每个负载不一样。
			2.根据来源做控制，设置不同的限制，根据请求来源不同级别进行不同的流控处理。
					

	序列化
		文本协议，直观、描述性强，容易理解，便于调试，缺点就是冗余数据较多，不适宜传输二进制文件（比如：图片等），解析复杂（需要进行字符串比较）；
		二进制协议，没有冗余字段，传输高效，方便解析（固定长度，并且可以直接比较字节），缺点就是定义的比较死，哪个位置有哪些东西，是什么意义是定义死的，场景单一。
		
		Xstream		把对象转化成xml最好用的专业工具
		微服务框架的主要的数据传输处理方式是二进制序列化。服务要有共用性。
		Java IO 是一种装饰者模式，
		网络模块是通过二进制流的方式进行传输的。通过序列化方式将内存对象转为二进制数据(二进制序列化后的数据传输效率高,适合系统之间传输),对象序列化需要实现Serializable接口
		
		主流的、常用的序列化框架
		相对来说，二进制存储占用体积小。
			1.json序列化，适用与web与控制层的转换。,可视化强,文本化协议
			2.hessian序列化,HTTP协议的，二进制序列化，跨平台，内部会使用HessianSkeleton(HessianOutput和HessianInput)，将传输对象序列化，不需要手工序列化。
			3.java内置序列化，不支持跨平台，性能低，ObjectOutputStream和ObjectInputStream
			4.XStream，使用json居多，数据传输，xml序列化推荐，XStream,可视化强,文本化协议
			5.Thrift用的也不多。
	
		四种常见的异步远程通信方式
			1.oneway，只管发送不关心结果，是单向的通知。
			2.callback回调，执行不在原请求线程中
			3.future能主动控制超时，获取结果，并且执行仍在原请求线程中
			4.可靠异步方式，保证异步请求能在远程被执行，一般用消息中间件完成保证。
			
		**********future的使用场景**********
		底层使用了NIO方式，实现了并行调用。
		需要实现Callable接口
		主线程分别调用服务a,服务b,服务c,最后统一处理三个数据，服务之间没有相互依赖，这种适合futrue模式。
		但是如果服务之间有依赖，那么就必须要等拿到前一个服务结果后再调用下一个服务。
		**********future的使用场景**********
		
		
		providerBean本身并不执行具体服务，只起到调用端代码存根的作用
		服务框架启动时，需要监听服务端口，等着消费端请求进来
		一般来说调用服务一定是在工作线程(非IO线程)，而反序列化工作取决于具体实现。
		
		服务提供者可以对不同的消费者进行分级，确保优先级高的消费者优先拿到服务。保证稳定性。
		服务框架必须做到要模块化和可配置。配置调用的策略。
		
		其中降级限流，还可以停止一些非重要功能的，以便主流程可以继续执行
	
	
	
	
	soa数据库加载使用
			
	在profile中现在dev环境也会默认指定的
		<resources>
			<resource>
				<directory>../vars/${pay_env}/datasource</directory>
				<filtering>true</filtering>
				<targetPath>config</targetPath>
			</resource>
		</resources>
		小秘书中的属性加载需要看resource中得默认指定的
		
		
		具体就是运行文件所在的目录
		C:/Users/xiaoyuer/git/xye-soa-pom/conf/xye-datasource.properties		pc
		/usr/local/java_project/webapps  这是生产jar使用的位置					linux
		
	solr	
		CommitWithin
			简单的说就是告诉solr在多少毫秒内提交，比如如果我指定<add commitWithin=10000>，将会高速solr在10s内提交我的document。用法
			1.可以在add方法设置参数，比如 server.add(mySolrInputDocument, 10000);
				
			solrServer.add(doc, Constants.CREATE_INDEX_MS);
	
	
24.数据库相关，mysql相关
			mysql   
				工具也能操作
					SHOW COLUMNS FROM log_job    
					SHOW CREATE TABLE log_job
				
				sql不区分大小写，
				sql查询一般就联结，并子查询
					
				order by  多列排序， 只有在前一个相同的情况下，才会触发后一个排序，	先order by 后limit
					在union中，只出现在最后一条select之后，并且是针对总结果集排序
				
				between  不仅可以用在时间，也可以用在数字，两端是闭区间
				mysql中也可以使用正则，用的不多
				concat(),拼接，多个参数，逗号分割即可
				用于时间的sql功能用到再说 ，year()等
				
				count(*)  		对表中行计数，所有行，不管表中列是不是null
				count(colum)	对特定列中有值的计数，忽略null
				
				having 用来过滤分组(和group by一起使用)，where是过滤行
				
				select ... from ... where ... group by ... having ... order by ... limit
				
				多条insert,  insert into table(colum) values (),();
				
				insert select		insert into table(colum) select 
				
				对于update或者delete的语句，要先查看select过滤出来数据，确保正确在执行
				对于需要计算和分组的列，建议使用默认值，不要null
				
				存储过程，游标，触发器，都不常用，忽略
				
				
				事务中的保留点，事务处理中设置的临时占位符，可对其发布回退。回退部分事务，回退到某个占位符
				
				start transaction
				...
				commit;  //rollback
				
				事务回滚，create 和 drop语句无法回退
				
				SET autocommit = 0; 可以设置是否自动提交
				
				mysql 的用户权限，增删用户(create/drop)，新创建用户后，要分配权限，grant语句增加权限(可多条权限合并执行)，revoke撤销权限
				
				
				当你删除数据时，mysql并不会回收已删除的数据所占据的存储空间，以及索引位。而是空在那里，而是等待新的数据来弥补这个空缺，这样就有一个缺少，如果一时半会，没有数据来填补这个空缺，那这样就太浪费资源了。
				所以对于写比较频繁的表，要定期进行optimize，一个月一次，看实际情况而定了。
				注意:在optimize table '表名'运行过程中，MySQL会锁定表。
				
				MySQL执行命令delete语句时，如果包括where条件，并不会真正的把数据从表中删除，而是将数据转换成了碎片，通过下面的命令可以查看表中的碎片数量和索引等信息：
				由于命令optimize会进行锁表操作，所以进行优化时要避开表数据操作时间，避免影响正常业务的进行。
				
				当对MySQL进行大量的增删改操作的时候，很容易产生一些碎片，这些碎片占据着空间，所以可能会出现删除很多数据后，数据文件大小变化不大的现象。当然新插入的数据仍然会利用这些碎片。但过多的碎片，对数据的插入操作是有一定影响的，此时，我们可以通过optimize来对表的优化。
				可以减少空间与提高I/O性能
				
				日志方便 主要就是慢查询日志 和binlog(记录更新数据的所有sql语句)，慢查询在优化中很有用
				sql优化   
					不要使用*查询，
					索引
					union性能比or要好得多，同样适用case when.并集查询，优化使用union
				

				mysql4.0版本以下，varchar(20)，指的是20字节，如果存放UTF8汉字时，只能存6个（每个汉字3字节） 
				mysql5.0版本以上，varchar(20)，指的是20字符，无论存放的是数字、字母还是UTF8汉字（每个汉字3字节），都可以存放20个，最大大小是65532字节 
				
				char是一种固定长度的类型，varchar则是一种可变长度的类型，它们的区别是： char(M)类型的数据列里，每个值都占用M个字节，
				在varchar(M)类型的数据列里，每个值只占用刚好够用的字节再加上一个用来记录其长度的字节（即总长度为L+1字节）． 
				
				一般情况下货币是decimal(8,2),不严格的用double也够用
				

		mysql中的主从复制是基于Binary log实现的
		
		
		伸缩性三个方面：功能分割，复制及分片，可以实现数据层的水平伸缩
		mysql的伸缩性
			还有一个是备份
			复制(两台服务器之间同步数据)，增加备份
				读写分离，主从备份
				主从节点，多个节点同一数据多个备份    针对请求主服务器binlog操作，写入从服务器relay日志中，执行操作，异步复制
			
			分片  水平伸缩
				目前最简单的映射分片方法是  取模
				将数据切分多个小份，分布在各自的服务器上，不共享信息，没有数据重叠
				数据分片,水平切分。多个机器之间通过路由查询，
				缺点，无法联合查询，必须在各分片上查询，然后应用层合并起来
						单个分片容易实现acid，但是多个分片从整体很难实现acid，可能某个分片会失败
				
				核心是分片键(根据算法分，比如取模和奇偶)，用来定位分片，访问特定的服务器
				
				取模是要基于服务器数量来排序的，添加服务器后就会很麻烦。再分片问题
				重点，对于第一取模分片之后，后再添加服务器怎么处理数据的迁移。
					*******
					将所有的映射关系保存在一个独立的数据库中。好处是不用分片计算
					*******
					*****
					还有一张方法是 预算最大的服务器数量,比如是32  先搞两台物理机，2*16个db,之后扩容每次增加一倍，这样通过扩容逐渐减小服务器压力
					*****
				
				唯一id 可以使用redis的incr命令
				
				这里的分片不单单指数据库，也可在应用层实现，扩展到缓存，队列等，如redis分片
						
		任何写入主服务器的数据，需要在一秒后才能在从服务器中读到。正产复制延迟不会超过500ms
		降低复制延迟的方法：1.写的缓存在客户端侧，2.重要读转发到主服务器上，3.减少延迟
		将慢查询和备份查询隔离到不同的服务器上	
		
		分表能够解决单表数据量过大带来的查询效率下降的问题，但是，却无法给数据库的并发处理能力带来质的提升。
		
		分表策略，比如citicorder过大   	比如 水平分表的时候，可以使用取模分表。user_id%8  这样就会分成8个表，分库分表  cobar 和mycat
		不到万不得已不用轻易使用分库分表这个大招，避免"过度设计"和"过早优化"。分库分表之前，不要为分而分，先尽力去做力所能及的事情，

		分库分表
			分库分表的取模运算
				分库分表的策略比前面的仅分库或者仅分表的策略要更为复杂，一种分库分表的路由策略
				如下：
				? 	中间变量=user_id%（库数量×每个库的表数量）； ? 
					库=取整（中间变量/每个库的表数量）； ? 
					表=中间变量%每个库的表数量。

		数据库行锁住之后，是可以查询的 ，更新不行
		
		分表通常分为：垂直划分、水平划分；
		垂直划分通常是根据业务场景将一个多字段大表拆分成多个多个少字段小表。那么带来的问题会有，1.分布式事务，2.查询表关联问题
		水平划分是根据某个分片策略将同一个表的数据分开存到多个相同结构表中。

		分布式数据库是网站数据库拆分的最后手段，只有在单表数据规模非常庞大的时候才使用。网站更常用的数据库拆分手段是业务分库，将不同业务的数据部署在不同的物理服务器上。
			拆分：不同的多台服务器上面部署不同的服务模块，模块之间通过RPC通信和调用，用于拆分业务功能，独立部署，多个服务器共同组成一个整体对外提供服务。
			集群：不同的多台服务器上面部署相同的服务模块，通过分布式调度软件进行统一的调度，用于分流容灾，降低单个服务器的访问压力。
			
		CAP三进二
			在分布式系统中，讲究C:Consistency（强一致性）、A:Availability（可用性）、P:Partition tolerance（分区容错性）
			CAP理论就是说在分布式存储系统中，最多只能实现上面的两点。 而由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡，没有任何分布式系统能同时保证这三点。
			分布式架构的时候必须做出取舍。 一致性和可用性之间取一个平衡。多余大多数web应用，其实并不需要强一致性。因此牺牲C换取P，这是目前分布式数据库产品的方向
			
			CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。
			CP - 满足一致性，分区容忍必的系统，通常性能不是特别高。
			AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。
			
			最终一致和强一致性  一般选cap中的可用性和分区容忍性，一致性用最终一致即可。

		
		
		
		
		
		
		
		

		乐观和悲观锁选择的标准是：冲突的频率和严重性，若冲突很少或者冲突后果不严重，选乐观
		但是若冲突结果是严重的痛苦的，那么就需要使用悲观锁，使用乐观在检测到冲突的时候还是需要面临合并冲突处理的情况，但是一般业务是很难自动合并的，只能扔掉从头开始
		
		任何读取的数据都需要跟共享数据进行版本标记比较，任何不同都意味着冲突的发生
		
		*************悲观锁场景使用**************
			EXPLAIN
			SELECT * FROM user_account WHERE user_id=1366 FOR UPDATE
			这是通过service的事务id绑定数据库的事务id，实现一个事务中的锁控制（即当前同一个事务中可多次操作该行数据）
			被for udpate锁住的数据，另一个线程select也会阻塞
			  
			  
			使用update  count=count+1  这种事数据库的级别更新是线程安全的。for update  主要也是针对的对象的更新控制，select 对象，然后set变化后的对象属性
			  
			解释的type 可能是ref 也可能是const
			其中user_id 加上索引 最好是唯一索引就是行锁，不加索引是表锁
			 
			******
			  之前的是同时锁住需求者和服务者，两个线程相反锁顺序就会造成交叉的死锁了,原先的互锁会出现问题，ab 和 ba
			******
			
			考虑支付余额的并发问题(之前是锁住，当前线程可以拿到，其他线程拿不到)，select for update
		*************悲观锁场景使用**************
		
		
		between '2019-09-18' AND '2019-09-19' 相当于 '2019-09-18 00:00:00' AND '2019-09-19 00:00:00'
		
		mysql多表联合更新
		UPDATE  t1,t2	set  ... 	WHERE ...
		
		分库之后，夸库业务，由原来的联合查询转变为多次查询，写入类的如要保证事务，可能要引入分布式事务
		
		mysql有很多字符串函数 find_in_set(str1,str2)函数是返回str2中str1所在的位置索引，str2必须以","分割开。
		
		db_xiaoyuer.log_user_trade  在sqlyog中是可以夸库查询的

		
		CASE WHEN ro.status = 6 THEN
		CASE ro.status WHEN 6 THEN
		
		mysql---同服务器下跨数据库更新
			update 
				A数据库.表名,B数据库.表名 
			set 
				B数据库.表名.字段名 = A数据库.表名.字段名 
			where 
			条件（A数据库.表名.id = B数据库.表名.join_shop_id）;
			
		-- 批量插入
		<insert id="batchInsert" parameterType="java.util.List">
			insert into log_user_trade(trade_code,asset_from_code) values
			<foreach collection="logTrades" item="item" separator=",">
			(
				#{item.tradeCode},
				#{item.assetFromCode}
			)
			</foreach>
		</insert>

		
		留个参考
		update require_info
		<trim prefix="set" suffixOverrides=",">
			<trim prefix="Pay_User_Count=case id" suffix="end,">
				<foreach collection="infos" item="item">
					when #{item.id} then Pay_User_Count+#{item.orderCount}
				</foreach>
			</trim>
			<trim prefix="Status=case id"  suffix="end">
				<foreach collection="infos" item="item">
					when #{item.id} then #{item.status}
				</foreach>
			</trim>
		</trim>
		where id in
			(reqIds)
			
		-- 批量更新
		<update id="batchUpdateClose" parameterType="java.util.List">
			update require_apply
			<trim prefix="set" suffixOverrides=",">
				<trim prefix="apply_status=case id" suffix="end,">
					<foreach collection="applys" item="item">
						when #{item.id} then #{item.applyStatus}
					</foreach>
				</trim>
			</trim>
			where id in 
			<foreach collection="applys" item="item" separator="," open="(" close=")">
				#{item.id}
			</foreach>
		</update>
			
			
		上面简化后的sql：
			UPDATE require_apply
			SET
			apply_status= CASE id  
			WHEN 9289 THEN 2
			WHEN 9290 THEN 4
			WHEN 9288 THEN 6
			END
			WHERE id IN 
			(9288,9289,9290)
			

		数据库操作原子性测试
		设置autocommit=false，然后两个事务操作都成功了，才收工commit，否则rollback

		数据库字段在设计非空时候，如果有默认值，代码不插会默认。否则会报错
		可以为空随便，有默认就默认，没默认就是null	
		
25.属性文件相关
		classpath 是xml中和属性文件中用的语言，直接属性文件加载不用，类似classpath*:a/b/c
		
		PropertiesConfiguration 快速读取属性文件，使用apache的PropertiesConfiguration接收
		
		  //先看看Properties
			String propertiesFileName="a.properties";
			Properties props = new Properties();
			props.load(new FileInputStream(propertiesFileName));
			String value =props.getProperties("key");

			//然后是PropertiesConfiguration
			PropertiesConfiguration propsConfig=new PropertiesConfiguration();
			propsConfig.setEncoding("UTF-8") //默认的编码格式是ISO-8859-1，所以才在读取文件之前先设置了编码格式
			propsConfig.load(propertiesFileName);
			String strValue=propsConfig.getString("key");
			String longValue=propsConfig.getLong("longKey");
			String[] strArray=propsConfig.getStringArray(arrayKey);
			//值得一提的是。propsConfig的默认分割符是','，换句话说，如果值使用','分割，使用getString去取的话是会抛出异常的，因为这被认为是个数组，分割符可以使用setListDelimiter设置。
			...
			三、总结
			告别java.util.Properties。


26.springboot 相关
	约定优于配置，使用默认配置，屏蔽Spring内部的细节
		＠springBootApplication 标志这是Spring Boot入门文件
		启动也可以排除某个类的加载，排除冲突
		对于业务类使用注解，对于一些公用的bean使用xml配置。两者结合者使用
		对于项目必须的功能，boot 提供starter的maven依赖(使用默认的自动配置类，将对应的jar包加载到工程中，并将绑定的服务器加载到工程中)
		
		自动配置是通过spring-boot-autoconfigure的jar包实现的，其中有很多的配置类，加载默认配置。通过@EnableConfigurationProperties使得@ConfigurationProperties 注解的类生效
		其中@conditional、@conditionalOnClass、@ConditionalOnMissingBean 和 @EnableConfigurationProperties(开启注解属性配置) 实现条件加载
		这里@Conditional(RedisChooseConfig.class)，其中RedisChooseConfig implements Condition，实现条件装载bean
		
		java -jar spring-1.war  也可以运行boot的war工程，但是一般没啥实际用处
					
		maven编译版本
		*****
			maven-compiler-plugin  在父pom中    一般父pom中编译这个，然后boot单独编译boot的spring-boot-maven-plugin
		*****
			方式1
			<maven.compiler.source>1.8</maven.compiler.source>
			<maven.compiler.target>1.8</maven.compiler.target>
			<maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>
			
			方式2
				在build的插件中指定版本
				<plugin>
					<groupId>org.apache.maven.plugins</groupId>
					<artifactId>maven-compiler-plugin</artifactId>
					<configuration>
						<compilerVersion>1.7</compilerVersion>
						<source>8</source>
						<target>8</target>
					</configuration>
				</plugin>
				
			方式3
			SpringBoot 工程  继承了spring-boot-starter-parent
				<properties>
					<java.version>1.8</java.version>
				</properties>
			实际上是覆盖了parent中的属性
		

		
		boot中的redis
			在引入Redis中一般排除redis的异步客户端lettuce(一般在spring-boot-starter-data-redis 2.x版本默认，用的比较少)，使用jedis。
			项目中是代码形式是封装的jedis启动类，用jedisPool = new JedisPool(JedisPoolConfig, ip, port)， 然后Jedis jedis = jedisPool.getResource(); 用jedis操作对象
			
			
			spring提供了一个redisconnectionfactory接口，生成redisconnection(对应jedis驱动，其实现类jedisconnection)接口对象
			
			使用redistemplate，实际使用中最多的还是stringredistemplate
			
			
			序列化方式	1.stringredisserializer  2.jdkserializationredisserializer(redistemplate默认序列化器)   3.jacksonJsonRedisSerializer 已经过时，不推荐使用
			
			redis事务
				通常的命令组合是watch...multi...exec, 
				watch是监控redis的一些键，
				multi命令是开始事务，
				exe命令意义在于执行事务(队列命令执行前会判断被watch监控的redis键数据是否变化过(赋值相同也算变过)，没变才会执行事务)
				redistemplate.execute((RedisOperations operationns) ->{
					operationns.watch("key1");//监控key1
					operationns.multi();//开启事务，在exec命令执行前，全部命令都只是进入队列,期间的set后随即get value也是null，
					... 	//一些列操作 
					return operationns.exec();//执行exec命令，将先判别keyl是否在监控后被修改过，如果是则不执行事务，否则就执行事务
				})
			
			使用redis流水线技术可以批量执行redis的命令，但是基本不怎么用到
			redis中也可以做发布订阅模式，只能算是简化版的mq
			引入springcache，需要自定义缓存管理器
		
		
		
		
		配置首页addViewControllers
		
		
		
		boot的配置加载顺序优先级
			1.命令行参数
		*2.java:comp/env 的JNDI 属性
			3.Java 系统属性（ System.getProperties()); 
			4.操作系统环境变量
			5.Random ValuePrope1tySource random.*属性值
		*6.jar 包外部的 application-{profie}.properties或pplication.yml (带spring.profile)配置文件：
			7.jar 包内部的 application-{profile}.properties或application.ym(带 spring.profile)配置文件
		*8.jar 包外部的 application.properties或application.yml(不带spring.profile)配置文件：
			9.jar 包内部的 application.properties application.yml(不带spring.profile)配置文件
		*10.Configuration 注解类上的＠PropertySource; 
			11.通过 SpringApplication.setDefaultProperties指定的默认属性。
			
		加载属性文件
			使用@PropertySource，添加自定义的属性文件进来
			spring.profiles.active=${environment} 放在初始的属性文件中，后续添加sit、pre多个环境的配置
			@Value(”$(database.driverName)”),${}代表占位符，会读取上下文的属性值装配到属性中，这是一个最简单的spring表达式
		
		springmvc
			典型的MVC框架如 SpringMVC、Jersey 、国人开发的 JFinal
			
			mvc中的m-model(夹带事务和缓存机制)简单点说就是service和dao的组合获取返回数据
			请求的时候经过handlermapping机制，返回的是一个执行链HandlerExecutionChain(包含handler和拦截器)
			原版的jsp中是将数据模型渲染到视图中，必须要先跳页面然后数据渲染。
			
			boot中WebMvcConfigurer配置web属性，然后boot中配置类 WebMvcAutoConfiguration 自动配置(即属性中的mvc配置)
			
			传参
				其实mvc会只能将json数据装配好，但是如果使用了@requestBody,传递的json串中必须要有对应key。所有一般不实用
				@requestBody注解常用来处理content-type=application/json类型。默认是application/x-www-form-urlcoded类型，这是key-value形式传参
				数组    支持逗号分割的数组参数，用intp[] intArr接收
				json	ajax中的data： JSON.stringfy(param对象)，后台@requestBody接收  $.post({})
						contentType:”application/json”, //此处需要告知传递参数类型JSON,不能缺少
						data:JSON.stringify(params)'	//将JSON转化为字符串传递
						
				接收参数转换，DateTimeFormat(iso＝iSO.DATE)Date date, @NumberFormat(pattern =”#,### .##”) Double number)
							 
			MVC的应用中，如果在控制器方法的参数中使用 ModelAndView、Model或者ModelMap作为参数类型,SpringMVC会自动创建数据模型对象，
			操作pdf    使用itextpdf jar。上传  MultipartFile 用 HttpServletRequest、MultipartFile、Part参数，推荐用MultipartFile
			
			@Modelattribute是个数据模型的注解。它在执行控制器方法前被执行，代码中增加了一个工程名称（project_name）的符串，因此在控制器方法中可以获取它
			
			使用＠RequestHeader 来获取请求头中的参数，前段ajax中就headers:{id:'1')
			在Requestmapping中consumes= MediaType.ALL_VALUE,//接收任意类型的请求体， produces=MediaType.TEXT_PLAIN_VALUE)//限定返回的媒体类型为文本
			使用restTemplate 和 httpClient  两种不同的http调用方式而已
			其中restTemplate使用用 postForEntity 和 getForEntity 比较方便
			
			
			
		springmvc中，handlermapping负责映射用户的url和对应的处理类，handlermapping接口中定义了根据一个url必须返回一个由handlerexecutioinchain代表的处理链，其中可以添加任意的handleradapters实例来处理这个url请求。
		handlermapping初始化的重要工作，1.将url和hanler对应的关系保存在handlerMap集合中，并将所有interceptors保存在adaptedInterceptors数组中，请求来时执行数组中所有对象。
		handlermapping 完成url和handler的映射关系。
		
		control处理逻辑关键，URL匹配handlermapping中的handler，匹配成功返回处理链handlerexecutionchain(包含多个拦截器)。其中的prehandle和posthandle分别在handler执行前后执行，aftercompletion在view渲染完成、在dipatchersrvlet返回之前执行。
			preHandle
				调用时间：Controller方法处理之前
				执行顺序：链式Intercepter情况下，Intercepter按照声明的顺序一个接一个执行
				若返回false，则中断执行，注意：不会进入afterCompletion
				SpringMVC中的Interceptor拦截器是链式的，可以同时存在多个Interceptor，然后SpringMVC会根据声明的前后顺序一个接一个的执行，而且所有的Interceptor中的preHandle方法都会在Controller方法调用之前调用。
				令preHandle的返回值为false，当preHandle的返回值为false的时候整个请求就结束了
				
			postHandle
				调用前提：preHandle返回true
				调用时间：Controller方法处理完之后，DispatcherServlet进行视图的渲染之前，也就是说在这个方法中你可以对ModelAndView进行操作
				执行顺序：链式Intercepter情况下，Intercepter按照声明的顺序倒着执行(先声明的Interceptor拦截器该方法反而会后调用)。
				备注：postHandle虽然post打头，但post、get方法都能处理

			afterCompletion
				调用前提：preHandle返回true
				调用时间：整个请求完成之后,DispatcherServlet进行视图的渲染之后
				多用于清理资源
			
			
			modelandview是链接业务逻辑和view层的桥梁，持有一个modelmap和一个view对象，该map会传递到view对应的viewresolvers中，不同模板(jsp，freemarker等)处理map方式不同
			
			
						
		场景,对于一串操作中的某个耗时的、不影响主流程的，可以异步一个线程单独处理，异步处理
		spring中的异步调用使用，
			1.@EnableAsync开启spring异步功能，  2.实现AsyncConfigurer接口配置线程池 3.@Async 方法声明使用异步调用	 
		
		在spring中定时很简单，1.使用＠EnableScheduling注解启动定时任务，2.使用＠Scheduled配置具体的某个任务
			
		springboot的jar中依赖的jar，用解压打开，可以拿到相关的jar包	
		
		
		
		在1.x低版本的springsession中使用的还是CookieHttpSessionStrategy，但是在2.3.x之后改类就不存在了。
		
		在boot中使用了server.servlet.session.cookie.name=ADMINSESSION来指定cookieName
			
			
27.dubbo相关
		dubbo默认服务提供方的IP为内网IP，生产上需要映射成公网ip
		
		几个soa问题：多级调用的延迟，调试跟踪困难，安全监测，qos支持，高了用，易伸缩

		dubbo启动两次 在tomcat中配置启动参数是false

		dubbo已经是20881端口了   linux上还需要端口暴露

		dubbo jar 启动之后就不提供web支持，web是访问不了的
		
		
	*****既是消费者，也是提供者*****     服务之间，最好不要相互依赖。
			<?xml version="1.0" encoding="UTF-8"?>
			<beans xmlns="http://www.springframework.org/schema/beans"
				   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
				   xmlns:dubbo="http://dubbo.apache.org/schema/dubbo"
				   xsi:schemaLocation="http://www.springframework.org/schema/beans
				   http://www.springframework.org/schema/beans/spring-beans-4.3.xsd
				   http://dubbo.apache.org/schema/dubbo
				   http://dubbo.apache.org/schema/dubbo/dubbo.xsd">

				<!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 -->
				<dubbo:application name="XyeIdsServer"  />

				<!-- 注册dubbo monitor -->
				<dubbo:monitor protocol="registry"/>

				<!-- <dubbo:registry protocol="zookeeper" address="192.168.6.237:2181" /> -->
				<dubbo:registry protocol="zookeeper" address="${zookeeper.address}" />

				<!-- 消费接口 -->
				<dubbo:reference check="false" id="requireDepositServiceSoa" interface="com.xiaoyuer.soa.api.service.IRequireDepositService"  />
				
				<!-- 用dubbo协议在20880端口暴露服务 -->
				<dubbo:protocol name="dubbo" port="20883" />

				<!-- 暴露服务  ids user -->
				<dubbo:service interface="com.xiaoyuer.ids.api.service.IUserService" ref="userService"/>
			</beans>		
		
		
28.maven相关
	maven依赖特性
			1.路径最近者优先
			2.路径相等(引入的层次相同)，先声明者优先(目前看是jar的引入层次，但是在本pom.xml中顺序靠后的优先)
		Maven默认用的是JDK1.5去编译，使用高版本的maven需要在pom中配置
		<build>
		   <plugins>
			 <plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.5.1</version>
				<configuration>
				<source>1.8</source>
				<target>1.8</target>
				</configuration>
			  </plugin>
			</plugins>
		</build>
	依赖中scope即约定依赖范围： 
			compile：默认值，一直可用，最后会被打包 
			provided：编译期间可用，不会被传递依赖，不会被打包。如tomcat相关jar，容器中有，编译期临时用，运行时由web容器提供。
			test：执行单元测试时可用，不会被打包，不会被传递依赖 
			runtime：运行和测试时需要，但编译时不需要 
			system：不推荐使用 
		
		
		依赖状态
			compile									编译成功的。
			omitted for duplicate 					jar包重复依赖(版本一样)。
			3.4.8 omitted for conflict with 3.4.6	jar包版本冲突(此处引3.4.6)。
			2.4 managed from 2.3  				 	使用2.4版本，拒绝使用<dependencyManagement>中声明的2.3版本
	
		父 pom 中的 dependencyManagement 是允许被子模块的 dependencyManagement 覆盖的。dependencies 优先级高于 dependencyManagement。
		spring-boot-dependencies 同级别 不是子父模块关系，不受版本继承影响

	dependencyManagement
		在使用spring-boot-starter-parent(boot的两种方式，还有一个是spring-boot-dependencies) 中传递依赖无效
		Maven 父pom中dependencyManagement版本优先级高于传递依赖版本(导致引一个jar中的传递依赖，即使指定了版本也无效)
		解决方式两个：
			1. 在子模块中<dependencyManagement>定义覆盖父类的申明
			2.先exclude掉子模块间接依赖，再直接添加依赖并声明版本号
				
		*****maven树，查询依赖*****
		dependency:tree -Dverbose -Dincludes=com.xiaoyuer:xye-core-dao
		Dincludes=org.springframework:spring-tx 过滤串使用groupId:artifactId:version的方式进行过滤	
		
	emanager中  spring-boot-dependencies    和dubbo-dependencies-bom 2.77 冲突，这种引入dependencyManagement管理的，会优先使用这里集中限定的版本。将pom工程中的依赖限定导入进来了。优先使用。
	不是父依赖的问题，是jar冲突的问题

		
	maven-setting.xml配置	
		maven profile 的active 是并集   activeProfiles和之前的default的关系：激活两个     目前系统发布用的就是这个
		maven  <relocation>:，构件的重定位信息(更换了group ID和artifact ID),这里会出现一种现象就是，依赖中的，但是pom.xml中搜索不到

		在同一个 <mirrorOf>central</mirrorOf> 下，多个mirror只有第一个生效。针对的是mirrorOf拦截的。(这里是针对匹配上的代理)
		配置多个mirror时，mirrorOf不能配置" * "，" * " 的意思就是（根据mirrorOf和repository的id）匹配所有的仓库
		
		
		maven采用的是最近依赖原则，同样近的第一个优先。是相对于依赖树中。
		maven三个标识缩写，GAV
		
		mvn -Dmaven.test.skip=true  	跳过测试阶段且不编译测试用例类	
		mvn -DskipTests					跳过测试阶段但编译测试用例类
		
		
29.事务相关
		事务的特性   要么全部完成 要么什么都不做
		乐观锁：通过冲突检测和事务回滚来防止并发业务事务中的冲突，提交的时候才有锁冲突，导致其他的无效操作
		悲观锁：每次只允许一个业务事务访问数据以防止并发业务事务中的冲突，一开始就拿到锁
			
		乐观事务锁的缺点是：只能在提交数据时才能发现业务事务将要失败，某些情况下，发现太迟，代价也大

	乐观锁中使用版本号，返回行数如果是0，要将事务系统回滚以防止不一致的数据进入数据库，即业务事务必须要么被取消，要么解决冲突并重试
	关键的修改需要加日志，版本号进行增量操作
	
	使用场景
		乐观并发管理使用业务事务冲突低的情况。因为冲突频繁发生，直到提交数据才通知冲突很不友好，会默认为经常失败。
		悲观锁在冲突率很高或者冲突代价很高更适用

	获取乐观锁动作必须要和提交记录数据在同一个系统事务中完成，才能保证数据一致性。检测到并发冲突因回滚事务，应该在发生任何异常时都将系统事务回滚。

	
	两阶段提交保持一致性
		 采用两阶段提交保证多master数据一致性，
		   1.开启事务
		   2.通知每个master执行某操作
		   3.所有master接到请求后，锁定执行此操作需要的资源，如扣款动作，就冻结相应款项，冻结完毕后返回
		   4.收到所有master的反馈后，如均为可执行此操作，则继续之后的步骤，如有一个master不能执行或者一段时间内无反馈，则通知所有master回滚操作
		   5.通知所有master完成操作

		先冻结，全部冻结ok了，就执行，全部执行ok就成功了，有一个不成功，就反馈回滚
		   
		   
		三阶段提交
		   是在两阶段提交的基础上增加了percommit过程，当所有master收到percommit后，并不执行动作，直到收到commit或超过一定时间后才完成操作
		 
		在实现两阶段或者三阶段提交时，为了避免通知所有master时出现问题，通常会借助消息中间件或让任意一个master接管成为通知者
		
		
		可见性。
			共享变量写入到内存的行为称为“冲刷处理器缓存”。也就是把共享变量从处理器缓存，冲刷到内存中。
			此时，线程 B 刚好运行在 CPU B 上，指令为了获取共享变量，需要从内存中的共享变量进行同步。
			这个缓存同步的过程被称为，“刷新处理器缓存”。也就是从内存中刷新缓存到处理器的寄存器中。
			经过这两个步骤以后，运行在 CPU B 上的线程就能够同步到，CPU A 上线程处理的共享变量来。也保证了共享变量的可见性。
			
			
			外层service没事务  里面有事务require 里面抛出异常
				外层不受事务控制，直接进库，里层受事务控制，需要更新
						
		**********	
		try 事务测试，try事务，循环事务(本次以3次测试)
			try{1.update 2.异常 3.update}catch{}  这种事务1走完会放入事务待提交，2中直接被异常抛出，3不会执行，这样对于别的循环次数没有影响，但是本次，还是1会成功。
			测试1  版本号不对直接回滚 ，内层异常，在同一个事务中，遇到运行时异常，全部回滚
		**********	

						
		mybatis、事务相关
				JdbcTemplate这种方式也不算成功，实际中用的也不多
	
				MyBatis基于SqlSessionFactory(单例)构建的框架。对于SqlSessionFactory它的作用是生成SqlSession接口对象，这个接口对象是MyBatis操作的核心
				
				typeHandlers，mybatis中，用来写入和读取数据库过程中不同类型的数据(java-javaType，数据库-jdbcType)进行自定义转换。
				MyBatis 自动识别 javaType巳和 jdbcType，从而实现各类型转换。
				
				#MyBatis描别名包，和 主解＠Alias联用,一般也不用
				mybatis.type-aliases-package=com.springboot.chapter5.pojo
				<select id=” getUser ” parameterType=”long" resultType=”user” ></select>
				
				这的列名和 POJO 的属性名是保持一致，数据库中的字段名为user name ，而POJO 属名为 userName 这里 SQL 是通过
				段的别名 (userName)来让它们保持一致的 在默认的情况下， MyBatis 会启动自动映射，将 SQL中的列映射到 POJO 上，有时候你也可以启动驼峰映射，这样就可以不启用别名了 
				
				MapperFactoryBean 和 MapperScannerConfigurer 是有区别的， 这两个需要代码开发方式(一般不是首选)。前者单个，后者集中
				MapperFactoryBean 针对一个接口配置，而MapperScannerConfigurer 则是扫描装配，
				实际上@mapperScan更加简便使用建议使用
				
				#MyBatis 配置文件，当你的配置比较复杂的时候，可使用，简单的就不用配。配置一些插件等  当然sqlSessionFactory也可代码配置addPlugin
				mybatis.config-location= 
				＃配置MyBaits插件（拦截器）
				mybatis.configuration.interceptors= 
				＃具体类商要与日MappedJdbcTypes 联合使用
				mybatis.type-handlers-package= 
				
				若项目依赖了mybatis-spring-boot-starter后，SpringBoot会自动在IoC容器中创建名称为 sqlSessionFactory(可以在初始化的时候，将插件添加进其中) 和 sqlSessionTemplate的两个Bean
				
				//启SpringBean生命周期执行方法加入插件
				@PostConstruct
				
				数据库事务也是通过aop实现的，在jdbc中大量的try catch finally，大量的冗余代码，比如打开和关闭数据库连接，以及事回滚代码，用aop都封装在了一起，只关心业务即可，将业务sql操作切入操作即可
				
				按照aop的设计思想，可以吧除执行sql之外的步骤抽取出来单独实现，这就是spring数据库事务编程的思想，关注业务本身，而不是数据库连接资源和事务的功能开发
				
				aop的实现相关
					aop本质还是使用动态代理完成，比如拦截器
						数据库操作的过程：获取数据库事务连接，事务控制， 关闭数据库连接 。
						通过aop将sql执行织入这个过程即可。
						切点的作用就是向spring描述哪些类的方法需要重启aop编程
					
					execution(* com.springboot.impl.UserServiceImpl.printUser(..))
						1.execution 表示在执行的时候 ，拦截里面的正则匹配的方法：
						2.＊表示任意返回类型的方法：
						3.(..)表示任意参数进行匹配。
					
					在@Aspect中，@after最终都会执行
					如果发生了异常,异常通知@afterThrowing会被触发,返回通知 @afterReturning 不会被触发
					
					获取参数
						JoinPoint类型参数对于非环绕通知而言， Spring AOP 会自动地把它传递到通知中：
						对环绕通知而言，可使用 ProceedingJoinPoint类型的参数。
						
					实际中使用@order注解来控制切面的指定顺序。实际就是拦截器的顺序
			
				在aspect中使用@Around，Object obj=pjp.proceed()这个返回的就是被切方法实际返回的对象，这个可以实际用来添加调用日志的记录。
				
				
				@transactional 作用范围，所有public 非静态的方法启用 ，最终生成一个TransactionDefinition
					通过该注解属性配置去设置数据库事务，接着调用业务代码，若没有发生异常，spring数据库拦截器就会帮助我们提交事务（异常有事务定义器判断处理），最终释放数据库连接，
					该注解标识需要启动事务，将业务方法织入约定的流程，其他的操作封装不关心。
					
				@transactional 可以放在接口上，也可以放在实现类上，但是推荐放在实现类上(这样可以兼容cglib动态代理)，因为放接口上使得类基于接口代理时才生效。
				spring中事务管理是由事务管理器来完成的，最常用的是DataSourceTransactionManager，它是实现了PlatfonnTransactionManager()接口的类
				
				Boot中，当依赖于 mybatis-spring-boot-starter之后，它会自动创建一个 DataSourceTransactionManager事务管理器(还有MyBatis sqlSessionFactory、sqlSessionTemplate内容，自动创建，直接注入使用事务管理器即可)		
					
				使用@transactional标注类和方法后，spring的事务拦截器会同时使用事务管理器的方法开启事务，将代码织入spring数据库事务的流程中。(获得数据库连接，修改隔离级别，执行sql，最后自动关闭和提交数据库事务)
					
				*****
				事务机制中最重要的两个配置项，隔离级别 和传播行为
				acid 四个特性
				*****
				
				这样一个事务回滚另外一个事务提交而引发的数据不一致的情况，我们称为第一类丢失更新。(现在的数据库不会再出现，效果是取消第一次事务操作，不动)。
				把这样的多个事务都提交引发的丢失更新称为第二类丢失更新，这个是常见的
				
				串行化是数据库最高的隔离级别，会要求所有的sql按照顺序执行，能完成保证数据的一致性。
				隔离级别越高，性能就越是下降。mysql默认的级别是可重复读(只有幻读的可能)。这里可重复读,使用的应该是行级锁吧，低针对单一条记录
				
				幻读，在一次事务中多查询，数量不对，是统计值
				可重复读是针对数据库的单一条记录，这是数据库存储的值
				
				
				传播行为
					nested: 在当前方法调用字方法时，若字方法发生异常，只回滚子方法执行过的sql。功能，子方法回滚而当前事务不回滚
							是使用保存点（save point）技术完成让子事务回滚而不致使当前事务回滚的工作。(若不支持保存点技术，就new 事务操作)
							sql执行会设置标志位，后面有异常会回滚到这个标志位的数据状态
							
					自调用事务失效：因为自调用是类自身的调用，没有走代理对象调用，不会产生aop，这样就不会将代码织入约定流程中
									方法：1.使用注入的service调用， 2.从springcontext中获取bean(是一个代理对象)，调用也行
						
		＠transactional,这就意味着会启用数据库事务。对于事务Spring自动地根据配置来创建事务管理器.
		
		可行 但并不高效，较为严格
			悲观锁是使用数据库内部的锁对记录进行加锁，从而使得其他事务 待以保证数据的一致。但这样会造成过多的等待和事务上下文的切换导致缓慢，因为悲观锁中资源只能被一个事务锁持有，所以也被称为独占锁或者排他锁。
			
			乐观锁 称为非独占锁或无阻 。也为可重入的锁
			乐观锁是种不使用数据库锁和不阻塞线程并发的方案。更新版本号 只递增不会递减
				若更新数量为0，表示已经被其他线程修改，这里的补偿操作处理也是乐观锁的重点地方。就是更新条数为0后的业务处理问题。
				并且请求数量越大，失败的数量会越多。通过重入的机制将请求失败的概率降低(按时间戳或者限制重入次数的办法).
				样例中隔离级别更改为读写提交？重入时候读取最新的记录。
		
		可重复读会阻塞其他的读取吗，不是阻塞 是多版本的读控制
			可重复读的实现原理，MVCC的控制方式 ，即Mutil-Version Concurrency Control,多版本并发控制
			事务A在读到一条数据之后，此时事务B对该数据进行了修改并提交，那么事务A再读该数据，读到的还是原来的内容
			保证在当前这个事务内读取的结果不会受到其他事务的影响
			
			可重复读:一个事务在执行过程中可以看到其他事务已经提交的新插入的记录，但是不能看到其他事务对已有记录的更新 
						这是针对本行数据而言是可重复，针对查询数量而言可能有幻读
						
			在循环的Propagation.REQUIRES NEW 事务操作中，最好使用try catch 将整个包住，这样不影响别的事务进行
					
						
		sql中的#{},MyBatis创建预处理语句属性并以它为背景设置安全的值,
		$在这里的作用只是做字符串替换，不会修改或转义字符串 因此，能使用＃的地方就不要使用$，除非是像 order by 这种不是参数的地方

		分布式事务相关
			1.需要一个全局协调者，全局事务管理器想所有事务参与者发送准备请求，事务参与者向全局事务管理器回复自己是否准备就绪
			2.全局事务管理器接收到所有事务参与者的回复后判断，如果所有事务参与者都可以提交，则向所有事务提交者发送提交申请，否则进行回滚。事务参与者根据全局事务管理器的指令进行提交或回滚。
			
			实践中最常用的一致性方案是使用带有事务功能的MQ
				在做本地事务之前，先向MQ发送一个prepare消息，然后执行本地事务，执行ok，向MQ发送一个commit消息。
				否则发送一个rollback消息，取消之前的消息。MQ只会在收到commit确认才会将消息投递出去，保证一致性。
				网络超时等原因可能收不到commit，那么MQ就不会吧prepare消息投递出去。MQ会根据策略尝试回调checkcommit
				检查该消息是用还是不用，确认后，MQ处理。
						
			seata中间件，之前叫 Fescar，阿里开源的分布式事务框架
			就是ABC 同事开启事务，执行成功后,ABC同时提交事务(引入一个协调,ABC同时开启事务,执行,然后都不提交，等待都执行完后,收到消息同时提交，最后结果都是同时提交或者同时回滚)
						
		*************重点就是全局的事务管理器，控制多个分支事务。*************
		全局的事务管理器，管理事务的启动，提交和回滚。
		事务管理器在资源的协调事，需要增加相应的日志记录。
			分布式系统中，两机器无法状态一致，需要引入协调器(事务管理器)，控制全局事务，管理事务生命周期，协调资源。
			
		相对于单库中的事务提交，数据操作时候只有一个动作，提交或回滚
		分布式中，有两个动作，准备和提交，所以称为两阶段提交。
		
		分布式id要考虑两个点 1.唯一性  2.连续性
		*****分库后带来的一些join问题*****
			合并查询的一些复杂点的操作，如排序啥的，建议都在应用层，将数据中信组装处理。其中排序分页比较复杂，难点在将足够多的数据返回给应用层。一般来说就是每个数据源都查询需要的条数，然后再筛选。
			1.在应用层将原来的单次操作，分成多次数据库操作然后组装(合并查询)。
			2.表的数据冗余，但会涉及到更新的问题，要注意。
			3.借助外部引擎，比如搜索引擎。				
						
		通常所说的柔性事务分为：两阶段型、补偿型、异步确保型、最大努力通知型几种
		TCC 型事务（Try/Confirm/Cancel）可以归为补偿型。
			而不足之处则在于对应用的侵入性非常强，业务逻辑的每个分支都需要实现try、confirm、cancel三个操作。此外，其实现难度也比较大，需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口还必须实现幂等。
		
		三阶段提交又称3PC，其在两阶段提交的基础上增加了CanCommit阶段，并引入了超时机制。一旦事务参与者迟迟没有收到协调者的Commit请求，就会自动进行本地commit，这样相对有效地解决了协调者单点故障的问题。
		第一阶段，而在实际的场景中参与者节点会对自身逻辑进行事务尝试，其实说白了就是检查下自身状态的健康性，看有没有能力进行事务操作。
		第二阶段，进入PreCommit阶段进行事务预提交。此时分布式事务协调者会向所有的参与者节点发送PreCommit请求，参与者收到后开始执行事务操作，并将Undo和Redo信息记录到事务日志中。参与者执行完事务操作后（此时属于未提交事务的状态），就会向协调者反馈“Ack”表示我已经准备好提交了，并等待协调者的下一步指令。
		
		
		相比较2PC而言，3PC对于协调者（Coordinator）和参与者（Partcipant）都设置了超时时间，而2PC只有协调者才拥有超时机制。
		这个优化点，主要是避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源。而这种机制也侧面降低了整个事务的阻塞时间和范围。
		另外，通过CanCommit、PreCommit、DoCommit三个阶段的设计，相较于2PC而言，多设置了一个缓冲阶段保证了在最后提交阶段之前各参与节点的状态是一致的。
		以上就是3PC相对于2PC的一个提高（相对缓解了2PC中的前两个问题），但是3PC依然没有完全解决数据不一致的问题。				
						
						
		
30.其他
		每次request请求完成后，下一次是重新的request，不会保留上次请求的信息，这叫无状态
		集群就是水平扩展，做负载均衡

		int i=0;
		int i=i++;  最后值为0			
					
		JVM 在处理 i = i++; 时 , 会建立一个临时变量来接收 i++ 的值 , 然后返回这个临时变量的值 ,
		返回的值再被等号左边的变量接收了 , 这样就是说 i 虽然自增了但是又被赋值了0 , 这样输出的结果自然就是 0 了

		不妨我们用 temp 临时变量来接收 i++ 的值 , 来看一下结果 :
		int i = 0;
		int temp = i++; //temp的值是 : 0
		
		类是对事物的抽象(抽象了属性和行为)，抽象类是对类的抽象，接口是对抽象类的抽象。
		
		request.getHeader("User-Agent")  是拿到前段浏览器的请求信息
		
		final的方法不能被重写。所以父类中的private方法默认是final的，子类将无法访问、覆盖该方法
		
		Java字符串用\\表示\
		AntPathMatcher是URLs匹配工具类
		
		使用lambada的条件，必须只能是函数式接口（概述:接口中只有一个抽象方法）才适用。可以使用@FunctionalInterface 来提前校验函数式接口，编译时提前发现错误。
		单来看lambda像一个没有名字的方法，它具有一个方法应该有的部分：参数列表int x，方法body　return x+1,和方法相比lambda好像缺少了一个返回值类型、异常抛出和名字
		Runnable a=()->{System.out.println("测试新表达方式");};
		
		countDownLatch 是并发包使用的一个计数器，		调用await()方法的线程会被挂起，它会等待直到count值为0才继续执行
		
		Redis是单线程的，是线程安全的。
		
		当接口有多个实现类时，提供了@order注解实现自定义执行顺序，也可以实现Ordered接口来自定义顺序。(数字越小，优先级越高,)
		注意：，也就是@Order(1)注解的类会在@Order(2)注解的类之前执行
		
		CGLib代理，不受接口的限制，底层采用ASM字节码生成框架，效率高比使用Java反射效率要高。唯一需要注意的是，CGLib不能对声明为final的方法进行代理，因为CGLib原理是动态生成被代理类的子类。
		
			
		java栈总是和线程关联一起，每创建一个线程，jvm就会创建对应一个java栈，这个线程执行的多个方法，每运行一个方法就创建一个栈帧(包含内部变量，操作栈和方法返回值等信息)
		每一个方法执行完成，弹出栈帧作为方法的返回值，并清除该栈帧，java栈的栈顶的栈帧就是房钱正在执行的方，方法的到调用就是创建活动栈帧，后面逐层出栈。
		栈是非线程共享，堆是线程共享的。
		
		方法区就是java堆中的永久区。存放的相关class信息和常量池相关
		运行时常量池是方法区的一部分
		
		其中需要使用 //Enhancer类是CGLib中的一个字节码增强器，它可以方便的对你想要处理的类进行扩展
		jdk使用的invocationhandler接口
		cglib使用的是MethodInterceptor接口
		
		ReentrantLock synchronized 相比 ，更加灵活，且具有等待可中断、可实现公平锁可以绑定多个条件等特性

		解决并发思路：
		1.锁，小粒度，reetrantlock
		2.无锁
			CAS ：即 Compare And Swap ，这是一种类似于乐观锁的机制 每次更新值的时候都使用旧值与变量的当前值做比较，如果相同则进行更新，否则重试直到成功
			Threadlocal ：本地存储变量，这样每 个线程都有一份数据的副本，也就不会存在并发问题了
			不可变对象： 不可变对象自然不会有并发问题
			
			cas重要的缺点是，强迫调用者处理竞争(重试，回退或放弃)，然而在锁被获得之前，却可以通过阻塞自动处理竞争
		
		
31.前后端缓存相关
		客户端状态信息
	前段参数传递，url参数，表单的隐藏域和cookie
	cookie只工作在同一个域名的站点中，若一个站点包含了多个域名，cookie不会在之间传递
	
	在分布式的session中，各个server间的缓存同步是不合适的（tomcat中的session复制，sessionid一样，是非常低效的），
	需要间缓存放在cache server中（以sessionid作为key）后面存在统一的redis中，
		
		
	Spring-session 技术是解决同域名下的多服务器集群 session 共享问题的，不能解决跨域 Session 共享问题
	在使用spring session过程中，发现spring session 往客户端写sessionID的策略要么是cookies要么是header。
	其实在开发中实际上有时候既要支持cookies，也要支持header方式，比如在PC端，
	一般是使用cookies，这样可以实现单点登录（不过还是没有解决跨域，要跨域只能用cas单点登录方案），
	手机app的话，不支持cookies，只能使用header，服务器响应请求的时候，往header里面写sessionId。
	
	cookie编码，sessionId的base64编码  boot中web工程设置cookie相关。
		spring-session-1.3.4(默认false)  和spring-session-core 2.0.2(默认true)  注意各版本中的编码默认值
		DefaultCookieSerializer
		在高版本的spring-session 的jar中，设置cookie相关的时候,cookieSerializer.setUseBase64Encoding(false);//这个要关闭，否则sessionid会在浏览器编码显示。
		cookieSerializer.setUseBase64Encoding(false);//这个要关闭，否则sessionid会在浏览器编码显示，导致跨手机 和pc的sessionid变化
        cookieSerializer.setDomainName(domainName);   //xiaoyuer.com
        cookieSerializer.setCookiePath("/");
	
	base64编码 	NGEwMWQ3ZDQtMWY3OS00OTQzLTg5MDQtMWM1ZjM0MGU1NjQy
	
	注意的是虽然浏览器64编码了，但是传到后台后会解码为正常的sessionid，redis缓存中存的也是正常的缓存值。意思前到后有个自动编解码的过程。
		场景	手机编码  ids未编码
			手机带过去的是64编码后的，ids未解码直接拿来使用，在redis中找不到sessioinid，重新创建(req.getSession()默认是true)了一个session存值，这样存和取不在一个session中。
	
	登录相关
		主流的单点登录还是使用的是耶鲁大学开源的CAS，验票的环节
	
		xxl的单点更加简单，就是一个sso server基于cookie和session中缓存统一管理，没有就返回
		一般登陆ok是有两个cookie，一个是server端的 一个是client端的
		app,非cookie登陆就是从header存储信息

		我们Cookie的domain属性是sso.a.com，在给app1.a.com和app2.a.com发送请求是带不上的。
		sso登录以后，可以将Cookie的域设置为顶域，即.a.com，这样所有子域的系统都可以访问到顶域的Cookie。我们在设置Cookie时，只能设置顶域和自己的域，不能设置其他的域。
		这样可以实现顶域下的跨域处理，但是Cookie顶域的特性这种是属于伪跨域

		单调的简单原理： 		https://yq.aliyun.com/articles/636281
		server验证ok（写sso当前域）后需要ST 返回验票，成功后写app1当前域，
		app2跳转后sso已经登录，直接st返回即可，流程同1

		xxl-sso
		1   cookie 和缓存都没有  跳转server
		2   server成功返回，cookie没有  缓存有，设置cookie
		3   登录后再访问，cookie有直接返回，不用设置cookie

		有状态
			服务端需要记录每次会话的客户端信息，从而识别客户端身份，根据用户身份进行请求的处理，
			类似cookie-session这种会记录用户信息的方式是有状态的
		
		无状态
			微服务集群中的每个服务，对外提供的都使用 RESTful 风格的接口。而 RESTful 风格的一个最重要的规范就是：服务的无状态性，即：
				服务端不保存任何客户端请求者信息
				客户端的每次请求必须具备自描述信息，通过这些信息识别客户端身份
			
		相对常规的session登录，(Android、iOS、小程序等，这些 App 天然的就没有 cookie)
			无状态登录的流程：1.首先客户端发送账户名/密码到服务端进行认证 2.认证通过后，服务端将用户信息加密并且编码成一个 token，返回给客户端
							  以后客户端每次发送请求，都需要携带认证的 token，服务端对客户端发送来的 token 进行解密，判断是否有效，并且获取用户登录信息

验证st（有一定时效性的）的必要性在于：
其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，
是不是业务系统也认为登录了呢？这是很可怕的。所以需要统一server端验证

32.线程相关
	每个cpu（或者多核cpu中的每核cpu）在同一时间只能执行一个线程，默认优先级是5
	线程上下文切换(抢占式)，当io阻塞或者有高优先级线程要执行时，就会切换，切换时要存储目前线程的执行状态，并且恢复要执行线程的状态。
	
	实现异步future+callable 实现返回值线程请求，对执行结果进行监听. 
	final List<Future<String>> resultList = new ArrayList<Future<String>>(); 
		for(Seckill seckill:list){
			resultList.add(executor.submit(new createhtml(seckill)));
		}
	   for (Future<String> fs : resultList) { 
		   try {
				System.out.println(fs.get());//打印各个线任务执行的结果，调用future.get() 阻塞主线程，获取异步任务的返回结果
			} catch (InterruptedException e) {
				e.printStackTrace();
			} catch (ExecutionException e) {
				e.printStackTrace();
			}
	   } 
		return Result.ok();
	}
	
	class createhtml implements Callable<String>  {
		Seckill seckill;

		public createhtml(Seckill seckill) {
			this.seckill = seckill;
		}
		@Override
		public String call() throws Exception {
			system.out.println(seckill.getSeckillId());
			return "success";
		}
	}
	
	main函数内部是无法访问非静态内部类的
	Future是返回各个线程的处理结果，并且多个线程，get的时候如果还没处理完就会阻塞（这是同步的），返回的对个future都是绑定固定的线程号的，线程只要执行完就能拿到结果。
	线程是进行中的，调用future.get()时会阻塞主线程，获取异步任务的返回结果
	
	使用future模式，就是先开线程获取结果，然后继续处理其他事，等到需要结果的时候，再get()出结果处理即可。那优先将长请求使用future前置，后续要用了再get出来。
	或者使用回调的方式处理阻塞的请求
	
	
	/***开启新线程之前，将RequestAttributes对象设置为子线程共享*/
		ServletRequestAttributes sra = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
		RequestContextHolder.setRequestAttributes(sra, true);
		
		//在新的子线程中获取request
		HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();
		
		RequestContextHolder可以实现隐式的request调用
	

33.单例模式，延迟加载，懒加载
		
		构造私有化 + synchronized	实现单例模式的获取（防止多线程同时获取）
		synchronized保证了代码块的串行执行。(内部执行wait 和notify)
	

		饿汉模式、懒汉模式、静态内部类模式、枚举模式

		/**
		 * 这里只要实现的功能就是延迟加载的单例模式。其实使用springbean初始化，也是单例，就是少了延迟
		 * 类级的内部类，也就是静态的成员式内部类，该内部类的实例与外部类的实例
		 * 没有绑定关系，而且只有被调用到才会装载，从而实现了延迟加载（调用时候再加载，而不是一开始就加载）
		 */
		private static class SingletonHolder{
		
			 //静态初始化器，由JVM来保证线程安全,外部类加载时并不需要立即加载内部类，内部类不被加载则不去初始化INSTANCE，故而不占内存，当使用了静态内部类才加载
			private  static AlipayClient alipayClient = new DefaultAlipayClient(Configs.getOpenApiDomain(), Configs.getAppid(),Configs.getPrivateKey(), PARAM_TYPE, CHARSET,
														Configs.getAlipayPublicKey(),"RSA2");
			
			private  static AlipayTradeService tradeService = new AlipayTradeServiceImpl.ClientBuilder().build();
		}


		延迟一次性加载，后续直接使用，这里的懒汉模式会有线程安全问题
		public class CommercePayRegister
		{
			 private static CommercePayRegister instance;
				
				private CommercePayRegister(){}
				 
				public static CommercePayRegister getInstance()
				{
					if (instance == null)
					{
						instance = new CommercePayRegister();
					}
					return instance;
				}
		}
		
		懒汉改进：双重检查（避免对除第一次调用外的所有调用都实行同步），因为创建一次后就不用创建了，这里使用lock也一样
		双重检查锁
		注意一个实例化同步问题，避免创建了两个实例。
		if (instance == null)						1
		  {
			synchronized(Singleton.class) {
			  if (instance == null) 
				instance = new Singleton();			2
			}
		  }
		  
	  指令重排序：
		重排序是一种性能优化
		编译期重排序的典型就是通过调整指令顺序，做到在不改变程序语义的前提下，尽可能减少寄存器的读取、存储次数，充分复用寄存器的存储值。
		单线程中只要重排序不影响最后结果，就不能保证按照程序执行顺序执行，即使这样会对其他线程产生影响
		单线程中，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序
		单线程中，不会对存在数据依赖关系的操作做重排序。对不存在数据依赖的，可能会重排序
		不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变
		单线程程序是按程序的顺序来执行的，其实不是这样的，但是也可以近似这样认为，不影响结果。
		
		
		这里重排序对多线程的影响 感觉和时间片的切换执行有点类似的结果
	  
	  new 对象不是原子操作
	  实际上当程序执行到2处的时候，如果我们没有使用volatile关键字修饰变量singleton，就可能会造成错误。这是因为使用new关键字初始化一个对象的过程并不是一个原子的操作，它分成下面三个步骤进行：
		a. 给 singleton 分配内存
		b. 调用 Singleton 的构造函数来初始化成员变量
		c. 将 singleton 对象指向分配的内存空间（执行完这步 singleton 就为非 null 了），这样会导致第二个线程直接未初始化的变量
		  
		  
		所以要使用volatile修饰 ，关闭指令重排序
		volatile ：此关键 的语义保证了新值能够立即同步到主内存，并且每次使用前都即从主内存刷新 volatile 保证了多线程操作时变量的可见性
			只是保证了同一个变量在多线程中的可见性，更多是用于修饰作为开关状态的变量。
			同一个变量线程间的可见性与多个线程中操作互斥是两件事，操作互斥是提供了操作整体的原子性，不要混淆。
		  
		volatile就会不会有问题了呢？因为volatile禁止了new Singleton() 这个操作的指令重排序  这个暂时不看，先记录下
		指令重排序(不改变程序语义的前提下)，必须考虑到指令之间的数据依赖性.实现执行效率优化
		
		CommercePayRegister使用静态延迟创建一个实例，然后进行属性复制，后面这个静态类可以直接使用获得同一实例对象了。
		使用的是饥汉模式，自己内部定义自己一个实例，使用时创建，单例模式。
		这里使用延迟加载的都private了构造函数，这样这个就是单例的。
		后面维护统一的初始化类，一般通用配置类会考虑使用单例，本来就是一次性加载的东西，单例更好提现

		private static  只能在内部静态调用
		静态内部类的形式去创建单例的，故外部无法传递参数进，静态变量是和类一起加载的，static实现共享，延迟实例化，调用的时候实现共享

		静态内部类实现单例   延迟加载  https://blog.csdn.net/mnb65482/article/details/80458571
		
		实际推荐这个
		public class UserSingleton {
				/** 私有化构造器 */
				private UserSingleton() {}

				/** 对外提供公共的访问方法 */
				public static UserSingleton getInstance() {
					return UserSingletonHolder.INSTANCE;
				}

				/** 写一个静态内部类，里面实例化外部类 */
				private static class UserSingletonHolder {
					private static final UserSingleton INSTANCE = new UserSingleton();
				}
			}
		
		不过spring中的bean默认是单例的，初始化配置UserSingleton的bean，其实也可以，就是不是延迟加载。
		注意静态属性是和类绑定的，和实例无关，这样就实现了实例共享静态属性
		spring的bean是每次注入都是同一个bean，但是还是可以new出来的。上面的方法就new不出来，限制的更严格(防止别的开发人员，new对象然后使用错误。总之就是保证绝对的唯一使用)
		使用@Lazy，springbean也可以实现延迟加载，(减少springIOC容器启动的加载时间,不过一般没必要)
		
		这个特点是：1. 仅在需要使用单例时调用getInstance进行单例的创建。(相当于懒汉吧这个)，静态内部类调用的时候加载
					2. JVM虚拟机保证了静态内部类SingletonHolder的类初始化只执行一次(单例特性，不用null判断之类的了)，不需要我们手动保证并发的同步。相当于将实例化的过程交给了jvm
					3. 用静态的内部类实现单例模式的原理：静态内部类可以不依赖外部类的实例而被实例化(延迟特性)。

					
					
					
34.多线程和锁相关
			线程直接run就是普通的方法，并没有开线程。必须new Thread(Runnable target).start 才开新线程
			
			
				使用两种锁锁方法
		   public static void lock(int i){
				lock.lock();
				num1 ++;
				lock.unlock();
			}
			public static synchronized void sync(int i){
				num2 ++;
			}
				
			事物提交是在整个方法执行完才会提交。
			select... for update  和update都可以实现悲观锁(一般伴随事务一起使用，数据锁定时间可能会很长)
			String nativeSql = "UPDATE seckill  SET number=number-?,version=version+1 WHERE seckill_id=? AND version = ?";这个是乐观锁的实现方式之一，是具体的案例，
			乐观锁这种场景，	并发高的时可能会出现失败次数多的情况
		
			select * from table_xxx where id='xxx' for update; 
			注意：id字段一定是主键或者唯一索引，不然是锁表
			
			两者本质上是一样的 只不过是封装了下，这个和理解的是一致的
			Executors是线程池的工厂类
			ExecutorService executorService = Executors.newFixedThreadPool(11);
			new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>()); 这样默认的是最大的队列值
			
			ListenableFuture是guava中的多线程能得到结果的方法,可得到多线程调用的结果  这个知道概念就行了
			
			
			
35.分布式锁相关

		ReentrantLock是基于aqs实现的。

		使用private static  Lock lock = new ReentrantLock(true);/  系统是放在@service中，因为容器中的实例能保证是单例的，这样并发下lock只有一个实例
		
		redisson获取锁，不成功则订阅释放锁的消息，获得消息前阻塞。得到释放通知后再去循环获取锁。后续删除key后会发送消息，所以上文提到获取锁失败后，阻塞订阅此消息。
		和redis的setnx的方法差不多
		
		
		分布式锁的使用场景：
			Java提供的原生锁机制在多机部署场景下失效了
			因为两台机器加的锁不是同一个锁(两个锁在不同的JVM里面)。需保证两台机器加的锁是同一个锁。 Lock或synchronize只能解决单个jvm线程安全问题
			场景1	即同一请求多次操作一个资源 ，也肯能执行定时任务时就会遇到同一任务可能执行多次的情况，
			场景2	不同请求操作统一资源
			常见方案
				1、数据库实现（效率低，不推荐），线程出现问题，容易出现死锁
				2、redis实现（使用redission实现，但是需要考虑思索，释放问题。繁琐一些）   锁的失效时间难控制、容易产生死锁、非阻塞式、不可重入    这个怎么理解呢,待定
				3、Zookeeper实现   （使用临时节点，效率高，失效时间可以控制）
				4、Spring Cloud 实现全局锁（内置的）
				
				
				
			Redis 实现为去插入一条占位数据，而 ZK 实现为去注册一个临时节点。
			遇到宕机情况时，Redis 需要等到过期时间到了后自动释放锁，而 ZK 因为是临时节点，在宕机时候已经是删除了节点去释放锁	
				
				
			Zookeeper实现原理	
					zk节点唯一的！ 不能重复！节点类型为临时节点
					jvm1创建成功时候，jvm2和jvm3创建节点时候会报错，该节点已经存在。这时候 jvm2和jvm3进行等待。
					jvm1的程序现在执行完毕，执行释放锁。关闭当前会话。临时节点不复存在了并且事件通知Watcher，jvm2和jvm3继续创建。
					设置有效时间，超过时间就删除节点
			
		zookeeper相关
		
				zookeeper中的数据是按照“树”结构进行存储的
				zookeeper=文件系统+监听通知机制。

				zookeeper通常有2n+1台server组成。用作投票机制

				主要的开源框架	两款开源框架ZKClient和Curator。可以用来操作zookeeper
				
				ZooKeeper目录树中每一个节点对应一个Znode。每个Znode维护着一个属性结构，它包含着版本号(dataVersion)，时间戳(ctime,mtime)等状态信息。
				ZooKeeper正是使用节点的这些特性来实现它的某些特定功能。每当Znode的数据改变时，他相应的版本号将会增加。
				
				ZooKeeper的临时节点不允许拥有子节点
									
			四种类型的znode：  持久 临时 有序
				-持久化目录节点：客户端与zookeeper断开连接后，该节点依旧存在
				-持久化顺序编号目录节点：断开后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号
				-临时目录节点：断开后，该节点被删除
				-临时顺序编号目录节点：断开后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号
		
			监听通知机制
				客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。
		
			ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是保持独占，另一个是控制时序。

			使用监听器时间同步等待的过程
					@Override
					void waitLock() {
					   IZkDataListener iZkDataListener = new IZkDataListener() {
						  // 节点被删除
							public void handleDataDeleted(String arg0) throws Exception {
								if (countDownLatch != null) {
									countDownLatch.countDown(); // 计数器为0的情况，await 后面的继续执行
								}
						 }
							// 节点被修改
							public void handleDataChange(String arg0, Object arg1) throws Exception {

							}
						};
						// 监听事件通知
						zkClient.subscribeDataChanges(lockPath, iZkDataListener);
						// 控制程序的等待
						if (zkClient.exists(lockPath)) {  //如果 检查出 已经被创建了 就new 然后进行等待
							countDownLatch = new CountDownLatch(1);
							try {
								countDownLatch.wait(); //等待时候 就不往下走了   当为0 时候 后面的继续执行
							} catch (Exception e) {
								// TODO: handle exception
							}
						}
						//后面代码继续执行
						//为了不影响程序的执行 建议删除该事件监听 监听完了就删除掉
						zkClient.unsubscribeDataChanges(lockPath, iZkDataListener);

					}
				}
			使用TimeUnit类来实现线程的sleep也可以
			
			
			windows 下查看zookeeper节点，dubbo节点
				1.zkCli.cmd -server 127.0.0.1:2181 
				2.然后ls /   查看当前目录

		
				可视化客户端 ZooInspector(Zookeeper Inspector的IP+端口要一次性写对，不然要关掉重新再写才行)
				
				实现层次命名空间的数据模型，就是一个精简的文件系统。基于watcher机制，监听znode节点(包括子节点)变化，通知给订阅状态的客户端。
							
				server内部控制时间逻辑的最小单位是tickTime(ms)  
				
				常常遇到session expire异常，这时需要重新连接，重新建立session。
				
				new ZooKeeper(url,sessionTimeOut,watcher)  三个重要的参数
				
				*****所有的增删改查操作都是针对znode节点进行的*****
				
				zookeeper的watcher是一次性的，每次处理完状态变化时间后，需要重新注册watcher。
				
				zkclient其中状态变化主要包括：1.子节点的变化 2.数据变化 3.连接及状态的变化
				
				
				利用znode的特点和watcher机制，统一管理服务名称和对应的服务器列表信息。
				 zookeeper		容错特性			通过事务日志和数据快照来避免因为服务器故障导致的数据丢失，leader和follower服务器都会记录事务日志
								leader选举特性		leader 负责数据的读写，而follower只负责数据的读
							
				zookeeper使用了ZAB(Zookeeper Atomic Broadcast)协议，保证了leader,follower的一致性，
				当leader宕机的话，使用 Fast Leader Election 快速选举出新的leader,节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。
				
				临时节点:EPHEMERAL
				
				dubbo注册zookeeper节点相关
				
					三层节点 ： 根节点，服务名称节点和服务提供者的地址节点
						其中根节点，服务名称节点是持久节点(persistent)，地址节点是临时节点(ephemeral)
				
				
				根节点->服务名称节点->type节点->地址节点
				/dubbo->com.soa.api.IUserService->provider->dubbo://192.168.6.222:20881/com.....
				
				consumers->consumer://192.168.6.222/com...
				
				其中provider、consumers 后续属于新增的一层type节点
					1.dubbo节点，dubbo下一级接口，接口下的provider、consumers、configurators、routers都是持久化节点
					2.provider和consumers下的url子节点都是临时节点。
					
					*******消费者从ZK获取provider地址列表后，会在本地缓存一份。后续都是对这个服务列表的本地存储进行更新*******
					
					*******
						ZK的本质就是为两端提供服务地址的发布/订阅服务，让消费者及时感知最新的服务列表，consumer真正调用provider是通过某种通信协议直接调用，并不依赖ZK。
						即使zk宕机，不影响两端调用，只是让本地缓存的服务列表有可能过时的。
					*******
					
				调用的消费端，从注册中心获取到的是服务提供者的地址列表(负载均衡算法选出一台服务器地址调用)。
				并且服务名称对应节点上有IZKCildListener监听，节点变化，对应的handleChildChange方法执行，更新节点，更新serverlist
					
				只有配置信息更新时，服务消费者才会去zookeeper上获取最新服务地址列表，其他时候使用本地缓存即可。这样服务消费者在服务信息没有变更时，几乎不依赖配置中心，降低配置中心压力。
					
			znode的结构
			主要属性	
				zxid：		每次的变化都会产生一个唯一的事务zxid。通过zxid，可以确定更新操作的先后顺序(如果zxid1小于zxid2，说明zxid1操作先于zxid2发生)。zxid对于整个zk都是唯一的，即使操作的是不同的znode。
				version:	节点的每次修改，都将使得版本号增加1
				data:		每个znode默认能够存储1M数据。

			dubbo中主要用了zookeeper的服务动态上下线的功能
			
			zk 基本锁 原理：利用临时节点与 watch 机制。每个锁占用一个普通节点 /lock，当需要获取锁时在 /lock 目录下创建一个临时节点，创建成功则表示获取锁成功，失败则 watch /lock 节点，有删除操作后再去争锁。
			临时节点好处在于当进程挂掉后能自动上锁的节点自动删除即取消锁。 缺点：所有取锁失败的进程都监听父节点，很容易发生羊群效应，即当释放锁后所有等待进程一起来创建节点，并发量很大。
			
			zk 锁优化 原理：上锁改为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。
			只有序号最小的可以拥有锁，如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。 步骤：在 /lock 节点下创建一个有序临时节点 (EPHEMERAL_SEQUENTIAL)。判断创建的节点序号是否最小，如果是最小则获取锁成功。不是则取锁失败，然后 watch 序号比本身小的前一个节点。（避免很多线程watch同一个node，导致羊群效应）当取锁失败，设置 watch 后则等待 watch 事件到来后，再次判断是否序号最小。取锁成功则执行代码，最后释放锁（删除该节点）。
			
			优缺点 	
					优点： 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 
					缺点： 性能上可能并没有缓存服务那么高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。
				
			Zookeeper 是一个分布式协调服务，可用于服务发现，分布式锁，分布式领导选举，配置管理等。
			Zookeeper 提供了一个类似于 Linux 文件系统的树形结构（可认为是轻量级的内存文件系统，但只适合存少量信息，完全不适合存储大量文件或者大文件），同时提供了对于每个节点的监控与通知机制。
			Zookeeper 集群是一个基于主从复制的高可用集群，每个服务器承担如下三种角色中的一种
			1. Leader
				1. 一个 Zookeeper 集群同一时间只会有一个实际工作的 Leader，它会发起并维护与各 Follwer及 Observer 间的心跳。
				2. 所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器。只要有超过半数节点（不包括 observeer 节点）写入成功，该写请求就会被提交（类 2PC 协议）。
			2. Follower 
				1. 一个 Zookeeper 集群可能同时存在多个 Follower，它会响应 Leader 的心跳，
				2. Follower 可直接处理并返回客户端的读请求，同时会将写请求转发给 Leader 处理，
				3. 并且负责在 Leader 处理写请求时对请求进行投票。
			3. Observer 
				角色与 Follower 类似，但是无投票权。Zookeeper 需保证高可用和强一致性，为了支持更多的客户端，需要增加更多 Server；Server 增多，投票阶段延迟增大，影响性能；引入 Observer，Observer 不参与投票； Observers 接受客户端的连接，并将写请求转发给 leader 节点； 加入更多 Observer 节点，提高伸缩性，同时不影响吞吐率
			
			Zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式和广播模式
			当 leader 崩溃或者 leader 失去大多数的 follower，这时候 zk 进入恢复模式，恢复模式需要重新选举出一个新的 leader，让所有的 server 都恢复到一个正确的状态。
			一旦 leader 已经和多数的 follower 进行了状态同步后，他就可以开始广播消息了，即进入广播状态
	
			
			
			
			
			
36.回调的思想是:
		类A的a()方法调用类B的b()方法
		类B的b()方法执行完毕主动调用类A的callback()方法

		同步回调和异步回调, 主要体现在其是否需要等待. 同步调用, 如果被调用一方的APi(第三方API), 
		处理问题需要花很长时间, 我们需要等待, 那就是同步回调, 如果调用完之后不需要理解得到结果, 
		我们调完就走, 去做其他事情, 那就是异步调用, 异步调用需要在我们调用第三方API处, 开启一个新的线程即可, 而同步调用和平常的调用没有任何区别.

		目前就是a调用b方法，并把实现callback接口的实例传过去，b方法调用完直接用实例回调方法即可

		a调用了b的方法，b开始执行，b执行完了，再调用a的方法，这就是回调。


37.多个微服务的dubbo的架构问题， 可以同时为提供者和消费者，但是最好不要相互依赖。

	Spring  Cloud Sleuth，PinPoint	   微服务架构中分布式服务链路跟踪
	
	合理的拆分系统，一般相互之间没有直接的相互调用，都是单独的业务系统。

	微服务解决，1代码复用，2.应用解耦，需求变更快，各模块独立，不影响  敏捷交付

	其中的 Hessian 是基于 HTTP协议的 
	Dubbo 是基于 TCP 协议的，
	而thrift则同时支持两种协议
	tcp三次握手的
		服务器监听请求，客户端发起连接请求（第一次连接），
		请求在路上可能存在丢失的风险，所以当请求到了服务器后如果服务器同意建立连接会给客户端一个回信（第二次连接），告诉它：我已经收到请求，可以连接。
		但是回信也存在一个问题，那就是回信能不能到客户端？它需要客户端给他一个回信说我已经收到批准通知了，如果客户端一直不回复的话意味着客户端没有收到批准通知。因此客户端一收到批准通知就立马回复（第三次握手）：OK老铁我收到你的批准通知了。至此，三次握手结束
		
		TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态；
		1.client发送server，TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号
		2.server接收数据，如果同意连接，则发出确认报文，这个报文也不能携带数据，但是同样要消耗一个序号
		3.client收到确认后，还要向服务器给出确认。此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。
		
		tcp关闭的时候是四次握手，这个不深究了。

	不使用事务就是直接提交 没法控制
	
	信息的加密
		java加密
		单向加密算法
			只能加密数据，不能解密回原来的明文。
			常会拼接一个salt字符串。密文＝ MDS(MDS(明文) + salt)
			常用：MD5,SHA,HMAC
		对称加密算法
			又称为单秘钥加密，同一个密钥既可以加密，也可以解密。
			常用：DES,AES,和PBE
		非对称加密算法
			非对称加密需要两个密钥，一个公钥，一个私钥。
			公钥加密数据，只有私钥才能解密。
			私钥用来签名，公钥用来验证签名。判断公私钥的正确性，就是ca证书做的事。
			常用： RSA 和 DH，重点RSA
	
		可分为对称加密：	加密后的信息可以解密成原值	des(适用数据库密码的加密)
		非对称加密：		无法解密还原为原值			比如 md5
		
		自定义md5加密工具类
			private static final String hexDigits[] = { "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "a", "b", "c", "d", "e", "f" };
			系统中使用的是array[16]作为自定义，然后处理MD5加密后的byte[]。
			private static String byteToHexString(byte b) {
				int n = b;
				if (n < 0)
					n += 256;
				int d1 = n / 16;
				int d2 = n % 16;
				return hexDigits[d1] + hexDigits[d2];
			}

	常用安全算法
		彩虹表破解hash算法
			一一穷举存储明文和密文的所有组合。

		md5 和 sha 都属于hash算法

			一个Byte由8 bits组成，是数据存储的基础单位，1Byte又称为一个字节
			bit（比特）是表示信息的最小单位，是二进制数的一位包含的信息。一个bit是一个0或1，中文叫做一个二进制位

		16进制编码
			每4位二进制对应一位16进制数据
			一个字节包含8位二进制(8bits)，转为2位的16进制
		
		base64编码	
			任何人只要得到base64编码的内容，便可通过固定的方法，逆向得出编码之前的信息，
			只是一种编码算法，将一组二进制信息编码成可打印的字符，在网上传输展现。
			字节数组，一个字节是8个二进制信息，所以字节数组是二进制信息
		md5 
			md5信息摘要之后(摘要长度是128位),返回一个16个字节的字节数组，将字节数组转成16进制格式的数据(32位)。
			而1个byte转成16进制正好是2位（16进制使用4个bit，一个byte有8个bit），所以MD5算法返回的128bit 转成16进制就正好是 32位

		SHA
			摘要信息长度是160位，常用16进制编码，和base64编码

		数字证书
			要获得数字证书，首先要使用数字证书管理工具，如keytool、openSSL等，然后构建CSR,提交给数字证书认证机构，最终形成数字证书
			
		摘要认证	
			*****数据传输安全，简单说就是验签的过程。(加秘钥验签)*****
				参数排序，加上secret拼接待加签的明文，使用md5等摘要算法，生成密文，传输
			
			使用treeset去重，复杂对象重写compareto方法实现排序
			TreeSet的底层是用TreeMap来组织数据的，
			
		签名认证
			使用公私钥
			客户端
				1.排序拼接待加密串，
				2.使用共同的md5等摘要算法生成摘要串，
				3.然后在使用私钥加密生成密文，
				4.然后16进制编码为字符串传输
			服务端
				1.排序拼接待加密串，
				2.使用共同的md5等摘要算法生成摘要串(字节转16进制字符串)，
				3.使用公钥解密传来的密文得到明文串(字节转16进制字符串)
				4.两个字符串一致，即可

		中信证书加密解密
			加密过程
				发送方公钥证书		PTNRtest.cer				这个给中信报备了，接收方解密用
				发送方私钥证书		PTNRtest.key				配合密码加载
				发送方私钥密码文件	PTNRtest.pwd				一般内容是字符串
				中信公钥证书		中信公钥.cer				这个和1类似，报备后，用来解密用

				sign加密的，使用了PTNRtest.pwd，PTNRtest.key，PTNRtest.cer加密   PKCS7Signature.sign()
				
			解密过程
				中信返回结果信息，接收方解密验签，使用了中信公钥证书.cer，这个和之前加密流程是相对应的，双方都需要报备证书.cer。		
				
				
		
38.浏览器敲get  对应的sessionid放哪里了
	第一次访问服务器，浏览器会带回一个sessionid，
	并set-cookie操作：Set-Cookie: SESSION=7004b0d6-dfb0-4756-898e-60379fb8b884; Domain=xiaoyuer.com; Path=/; HttpOnly
	当再次访问会将sessionid待入服务器，Cookie: SESSION=7004b0d6-dfb0-4756-898e-60379fb8b884; UM_distinctid=170a97fef53902-05e6115cbcea8b-4446062d-1fa400-170a97fef54e3; CNZZDATA1255347938=2105466369-1583388160-%7C1583388160

	查询  map 和list<map>  一行就是一条记录，对应的是一个resultType，如果类型是map，多条记录就是list<map>
	
	
39.做一个回退的merge测试，目前回退  更新   pick
	1.A->B  同源
	2.A回退，改动提交
	3.B cherry pick A,这时会有冲突，因为改动已经不是基于B一开始的源头了,B的源头已经改变了。

	同源测试更新是可以的，同源就是在原文件上，没有改动，单一改动，merge过来。

	
40.spring的启动后创建自动代理类，
	aspectj支持编译期织入且不需生成代理类。spring 集成了aspectj,但其不属于spring aop的范围。
	实现了pojo级别的代理实现
	aspectj中  切点直接声明在增强方法出，叫做匿名切点。想要复用就使用@Pointcut命名一个(目前系统再用的方式)。


	aop和aspectj是建立在动态代理的基础上实现的
	aop:advisor 与 aop:aspect都可以配置aop
	advisor只持有一个Pointcut和一个advice，而aspect可以多个pointcut和多个advice
	
	mybatis构造方法必须要有无参的构造方法
	
	
	
	ioc的依赖注入机制
			spring的三个核心的组件  bean，core，context
				bean组件(在org.springframework.beans包下)
					bean的定义	BeanDefinition 		完整描述了配置文件中<bean>节点的所有信息，当spring成功解析<bean>节点后就会转化为一个beanDefinition对象，给后续操作
					bean的创建	典型的工厂模式，顶级接口是BeanFactory
					bean的解析	对spring的配置文件的解析
					
					
				context组件(在org.springframework.context包下)
					作用就是给spring提供一个运行时的环境。作为spring的ioc容器，基本上整合了spring的大部分功能
					ApplicationContext是Context的顶级父类。继承了BeanFactory，resourceloader接口。
					
				core组件

				ioc的一些扩展点
					主要有BeanFactoryPostProcessor 和 BeanPostProcessor    分别在构建BeanFactory 和Bean时调用
					InitializingBean 和DisposableBean					   分别在bean实例创建和销毁时调用
					FactoryBean
					
		aop
			jdk的动态代理    public static Object newProxyInstance(ClassLoader loader,Class<?>[] interfaces,InvocationHandler h)
					通常参数1和被代理的类是同一个loader，这里被代理类被封装进了InvocationHandler实现类中
			从代理的原理可知，代理的目的是调用目标方法时可以转而执行InvocationHandler中的invoke方法。就是对调用方法的一种增强实现
	
	
	
	
	
41.查看进程 和端口号    kill进程
	ps -ef | grep java	ps 静态的进程统计信息    e：所有   f:完整格式显示   grep  搜索
	netstat -ano | findstr "8080"
	kill -9  pid
	
42.针对post url中需要转义的， 转义param中的value即可，对整个param转义 会将&也转义了，这样request接收到的只有作为整个key="",
	method=”post”:这是传递大量数据时用的，传递之前会先将数据打包，数据能正确解析，传中文不会有乱码。 
	method=”get”：以URL传递的，地址栏长度有限，对数据量是有限制的，因此，传中文会有乱码，需特殊处理。

43.jar包中日志logger需要和项目中的log.xml匹配，统一logback，不然没有日志	
	每次重启solr后，admin控制台的日志会清除
	
	成熟的日志框架就 log4j 和logback。   apache的commons logging、slf4j没有日志框架的具体实现，可看做是日志接口框架。
	可以认为 Logback 是slf4j默认的实现

	logback-core,logback-classic,slf4j-api

	logger没有分配级别，那么将从被分配级别的最近的祖先那继承级别。rootlogger默认是debug
	logback中的logger有继承机制，配置文件中的additivity为了不继承rootlogger的配置，从而避免输出多分日志
	
	e.printStackTrace()   logger不打印日志,控制台打印
	
	
44.maven配置,jenkins，多环境

	实际使用的时候，一般配置一个私有库，然后用阿里云镜像代理中央库
	远程仓库：中央仓库+私服+其他公共库，maven使用的默认是中央仓库，私服和阿里云镜像可以同时使用
	依赖的优先级：本地仓库 > 私服（profile）> 远程仓库（repository）。全局和pom中的优先，局部配置优先于全局配置

	mirror：拥有远程仓库的所有 jar，包括远程仓库没有的 jar，定义了两个Repository之间的镜像关系（大部分jar包都可以在阿里镜像中找到，部分jar包在阿里镜像中没有，需要单独配置镜像）
	配置多个mirror，默认只有第一生效（只有当第一个无法连接的时候才会接着往下匹配，但是注意第一个中a.jar没有是不会接着往下面的mirror中找的），可以都放着不影响，择优第一个就行。
	mirror相当于一个拦截器，将远程库的地址重定向到mirror里配置的地址。每个仓库只能使用一个镜像。
	mirrorOf配置了*，相当于代理所有的远程仓库，就只能去该镜像中下载，其他配置的多个库就失效了（因为拦截机制）。可以用来私服下载特定的jar

	全局多仓库设置，是通过修改maven的setting文件实现的。可以将常用的公共库，私服库配置进去。不建议在项目pom配置
	  在setting文件中添加多个profile（也可以在一个profile中包含很多个仓库），并激活（即使是只有一个可用的profile，也需要激活）。
		<activeProfiles>
		<activeProfile>myRepository1</activeProfile>
		<activeProfile>myRepository2</activeProfile>
	  </activeProfiles>

	setting.xml中的<server>，是pom中<distributionManagement>上传的认证配置，server元素的id必须与pom.xml中需要认证的repository元素的id完全一致。正是这个id将认证信息与仓库配置联系在了一起。
	配置多个远程仓库时，如果在一个远程找不到，依次从下一个仓库里找，在activeProfiles设置启用的仓库。默认会有一个ID是central的官方远程仓库。
	
	发布机的sit仓库引用，发布机只配置了251的public(默认激活该配置)，实际-p 又激活了setting.xml对应环境的jar仓库，这样就就会去这两个私服库去拉取jar。(实测正确)
	
	
	Invalid keystore format   因为maven中开启了filter过滤  证书相关不能开启  filter=false
	
	<resource>同目录下还是可以覆盖的，但是webresource不是同一目录所以不能覆盖。webresource只针对tomcat部署，boot还是用的target/classes下的
	
	
	这个是可以将zx下的目录打到cer下的，并且target和war目录一致
	<resource>
		<!-- 元配置文件的目录，相对于pom.xml文件的路径 -针对WEB-INF下面文件内容按照环境区分需要单独配置到该目录 -->
		<directory>../vars/${pay_env}/zx</directory>
		<!-- 目标路径 -->
		<targetPath>WEB-INF/classes/cer</targetPath>
	</resource>
	
	targetPath:指定build资源到哪个目录，默认是base directory
	directory:指定属性文件的目录，build的过程需要找到它，并且将其放到targetPath下，默认的directory是${basedir}/src/main/resources
	
	
	
	jenkins插件配置使用的是jenkins的maven_setting.xml文件，
	pipeline使用了sh脚本。sh 'mvn clean install -P sit -Dmaven.test.skip=true -Dmaven.javadoc.skip=true'   这里使用的系统默认的setting.xml,有点不一样。

	多环境
		sit环境真正执行的也就是打成war的不是resource下的，因为有webresouce在
		以webresouce为优先覆盖，如果resource下有就会覆盖。resource有，webresource没有,resource中的这样也会打包到web-inf下，这样就解释通了，最好不要用webresouce，直接用resource即可
		默认使用resource，如果webresource有重的就优先覆盖。	
				

				
		*******************************************之前理解是不全的,最好是不要冲突*******************************************************
		对于同一application.properties，顺序在前的优先使用，所有一般动态目录要放在前面
		<resources>
			<resource>
				<directory>src/main/resources/${pay_env}</directory>
			</resource> 
			
			<resource>
				<directory>src/main/resources</directory>
				<includes>
					<include>**.*properties</include>
				</includes>
			</resource> 
		</resources>
		*******************************************之前理解是不全的，最好是不要冲突******************************************************************
		1.动态加载指定文的文件，2.然后通用文件resources单独指定include下
				
		<optional>true</optional>   maven不会依赖传递
		
		api 打包对应的pom要打包吗，一般建议，不将api继承父pom，这样避免pom打包的问题
					
		was cached in the local repository, resolution will not be reattempted until the update interval of nexus has elapsed or updates are forced and
		 删除发布机中的maven库下的文件.(基本jar确认没问题，就是拉取不下来，也是这样处理)			
				
				
		updatePolicy    setting.xml中的配置
			该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。
						
						
		-D skipTests    这个更快点
		-Dmaven.test.skip=true		


		<profiles>
				<profile>
					<id>sit</id>
					<distributionManagement>
						<snapshotRepository>
							<id>my-nexus-snapshot</id>
							<url>http://192.168.6.251:8087/nexus/content/repositories/sit/</url>
						</snapshotRepository>
					</distributionManagement>
				</profile>
		</profiles>		


		这是以前的，这样看貌似 本地jar环境已经没有了
			<distributionManagement>
				<repository>
					<id>my-nexus-releases</id>
					<url>http://192.168.6.251:8087/nexus/content/repositories/releases/</url>
				</repository>
				<snapshotRepository>
					<id>my-nexus-snapshot</id>
					<url>http://192.168.6.251:8087/nexus/content/repositories/snapshots/</url>
				</snapshotRepository>
			</distributionManagement>
				
						
						
45.cmd 不识别mysql命令的时候，需要在环境变量中的path中添加mysql对应的bin目录

46.	git
		fork：在github页面，点击fork按钮，将别人的仓库复制一份到自己的仓库。
		clone：直接将github中的仓库克隆到自己本地电脑中						clone别人是无法改动提交的，因为没有权限
		pull request的作用
		在A的仓库中fork项目B （此时我们自己的github就有一个一模一样的仓库B，但是URL不同）将我们修改的代码push到自己github中的仓库B中pull request ，源头url仓库就会收到请求，并决定要不要接受你的代码
		
		两个分支针对同源都做了更改，合并的时候就会出问题
		冲突merge之后，就会标记没有变动，重复merge之后没变化
		
		
		git cherry-pick <commit id＞可以选择某个分支中的一个或几个commit(s）来进行操作

		merge可以提现时间线，rebase是重新设基会覆盖时间线。
		永远不要在所有人都在的公共开发分支上做 rebase 操作。一般情况下在临时分支上是需要 rebase 主分支代码的，而 merge 则主要用在主分支上将临时分支的代码合并过来，然后就可以删除临时分支了。暂时也不使用，先过。

	
47.js中的多个ajax异步请求的执行时并行，不会等待操作。执行的快与慢，要看响应的数据量的大小及后台逻辑的复杂程度。
	cookie.setMaxAge();pc中设置的是-1。默认值是-1，表示关闭浏览器，cookie就会消失。如果是正数，表示从现在开始，即将过期的seconds。
	
	Cookie newCookie=new Cookie("SESSION",null); 
	newCookie.setMaxAge(0);    
	ids页面刷新sessionid，当前request带进的session，response时候删除对应session，那么这过程中的设值无效
		
	就是不同的domain域对应不同的cookie，比如http 和https,重定向和浏览器直接敲，会把相应的sessionid带到后台，这样是共用一个sessionid
	但是http后台请求每次都会产生一个新的sessionid
		
		
		
	
48.	<filter-name>userInfoFilter</filter-name>  这个对应的是spring中的bean
	<filter-class>org.springframework.web.filter.DelegatingFilterProxy</filter-class>
	是对一个filter的代理，通过Spring容器来管理servlet filter的生命周期。为什么不用原始的filter？
	***是因为filter的类里面使用了Spring的注解，所以也必须也使用Spring的DelegatingFilterProxy
	
	filter中不想执行的程序直接return;掉即可
	
	
48.*拦截器和servlet
	Filter -> Servlet -> Interceptor
	过滤前-拦截前-action执行-拦截后-过滤后

	filter是Servlet规定规范规定的(针对URL地址)，只能用于web程序中，
	过滤器的运行是依赖于servlet容器，拦截器是依赖Spring框架(针对action,可使用Spring的依赖注入,无法处理静态资源)

	任何Spring Web的entry point，都是servlet(用于处理请求(service方法),请求给与响应)。Springmvc的核心是一个DispatcherServlet
	filter 是预处理，与Servlet的区别在于：它不能直接向用户生成响应
	spring mvc 是对于Servlet的再包装，单纯的使用Servlet，你需要考虑线程安全(java的内存模型是线程安全问题产生的根本)
	
	
	webapp 下的文件直接访问  mvc放过
	
	
	
	java的内存模型：java的主内存+线程私有内存的模型
		java程序中，所有线程都共享主内存，但是每一个线程都有自己的工作内存。工作内存和主内存通过一些规定的操作来交互同步数据，但线程只能访问自己的工作内存。
		因此在多线程环境下，很容易造成共存内存数据不一致引起的并发问题

	拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，这个情况就需要使用filter代理	
	
	需要对拦截器进行bean处理才能使用springbean
	
	
	Filter和servlet都可以对URL进行处理，Filter是一个链式处理，只要你想继续处理就可以传递下去；而Servlet则是一次处理并返回！适合简单逻辑处理。
	
	********************
	先走filter,然后走servlet,然后回到filter，一个filter可以用chain.doFilter()分成前后两部分。 
	
	两个filter 和一个servlet测试
		输出结果是：
		filter_1_before
		filter_2_before
		来到servlet了
		filter_2_after
		filter_1_after
	

	拦截器的执行顺序
		? 执行preHandle方法，返回一个布尔值。如果为false ，则结束所有流程：如果为 true则执行下一步。
		? 执行处理器逻辑它包含控制器的功能 (controller执行内容)
		. 执行 postHandle方法。
		? 执行视图解析和视图渲染
		. 执行 afterCompletion 方法。
		
		多个拦截器
			责任链模式的规则，对于处理器前方法采用先注册先执行，而处理后法和完成方法是先注册后执行的规则 。
			
		处理器前(preHandle)方法会执行，一旦返回 false ，则后续的拦截器、处理器和所有拦截器的处理器后(postHandle法都不会被执行。
		完成方法 aftercompletion不一样，它只会执行返回 true 的拦截器的完成方法，而且顺序是先注册后执行
	
	
49.	redis相关
	可以直接使用redisTemplate，也可以bean中创建一个jedispool操作。pc中是重复了一个redis操作
	redis一般使用的是 Spring的RedisTemplate，也可以使用 Jedis自己的封装
	redistemplate
		没有设置序列化方式的时候redistemplate使用的序列化方式为JdkSerializationRedisSerializer，所以我们存入key前面会带上一串东西，而StringRedisTemplate使用的是 StringRedisSerializer，序列化的方式不一样，所以使用的时候key就不会出现一串字符串
		
		RedisTemplate使用的是 JdkSerializationRedisSerializer 序列化对象
		StringRedisTemplate使用的是 StringRedisSerializer 序列化String
		
		RedisTemplate可以用来存储对象，但是要实现Serializable接口，以二进制数组方式存储，内容没有可读性

		StringRedisTemplate和RedisTemplate。
		两者的数据是不共通的；
		StringRedisTemplate默认采用的是String的序列化策略，保存的key和value都是采用此策略序列化保存的。
		RedisTemplate默认采用的是JDK的序列化策略，保存的key和value都是采用此策略序列化保存的。

		
		StringRedisTemplate，这个是官方建议的，也是最方便的，直接导入即用，无需多余配置！ 只要属性文件中配置spring.redis.pool等属性即可
		如果k-v是Object类型，则需要自定义 RedisTemplate。
		
		Jackson2JsonRedisSerializer(redis客户端jason显示) 和JdkSerializationRedisSerializer() 
		JdkSerializationRedisSerializer:    \xAC\xED\x00\x05t\x00\x0Ezheshizhendeba  取出来还是zheshizhendeba
		
		
		支付的redis使用template，需要调整
		session延长的 请求路径延长
		Duration是在Java8中新增的
			server.servlet.session.timeout=PT60S				P2DT3M5S
			开头,紧接着’P’,下面所有字母都不区分大小写: D C 天 	H C 小时	M C 分钟 	S C 秒 
			字符’T’是紧跟在时分秒之前的，每个单位都必须由数字开始,且时分秒顺序不能乱			
		
		没有s计算，最短60秒  以分钟分计算单位，		server.servlet.session.timeout=PT5M20S   这种就是5分钟
		int interval = request.getSession().getMaxInactiveInterval();				查看最大过期时间
		每次请求会延长session的过期时间
		
		boot2.0整合redisTemplate
			需要引入spring-data-redis，使用默认版本
			引入jedis 2.9.0
			这样版本才能统一对应上。
			系统中是<exclusion>了*,所有，使用了boot默认的
		
		尽量使用RedisTemplate操作，正规点
		redis 常用的数据结构 string hash list set等		其中String结构和hash结构是最为常用的结构
		hashset   hash结构  (key,(field,value))  field是hash结构的内部键
			hset  相当于是给hash存储结构添加key-value数据，类似map
			hgetall是取出hash结构的所有数据
			redis.hset("url","google","www.google.cn");
			redis.hset("url","baidu","www.baidu.com");
			
			就相当于是	Map<String,String> map = new hashMap()；
						map.put("google","www.google.cn");
						map.put("baidu","www.baidu.com");
						redis.hmset("url",map)；
						
			在数据量很大的hash结构，取出的是要要考虑性能

			redis的list
				是一个链表机构，类似栈或者队列，对元素的push和pop，可分别选择在首尾部添加和删除，lrange可以获取list在指定区间的元素
		
			redis的set
				相对来说就是增加了一个去重的功能，用的暂时不多
				zset还可以实现排序功能
	
				*****redis 中的incr方法，如果键不存在，会将默认重新计数，并且，过期时间为-1，需要判断ttl的时间，看情况重置过期时间*****
				
			
		redis快的原因
			1.纯内存操作
			2.单线程操作，避免了频繁的上下文切换
			3.采用了非阻塞I/O多路复用机制
			
			
		redis的数据类型，以及每种数据类型的使用场景
			(一)String 这个其实没啥好说的，最常规的set/get操作，value可以是String也可以是数字。一般做一些复杂的计数功能的缓存。
			(二)hash 这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。
			(三)list 使用List的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用lrange命令，做基于redis的分页功能，性能极佳，用户体验好。
			(四)set 因为set堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。 另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的**喜好等功能**。 
			(五)sorted set sorted set多了一个权重参数score,集合中的元素能够按score进行排列。可以做排行榜应用，取TOP N操作。
			
		redis采用的是定期删除+惰性删除策略
			 定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。
			 因此，如果只采用定期删除策略，会导致很多key到时间没有删除。 于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。
			 
			 
		内存淘汰机制	 
			如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。 在redis.conf中有一行配置	 
			# maxmemory-policy volatile-lru
				1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。 
				2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。推荐使用，目前项目在用这种。 
				3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。应该也没人用吧，你不删最少使用Key,去随机删。 
				4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。不推荐 
				5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。依然不推荐 
				6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐 ps：如果没有设置 expire 的key, 不满足先决条件(prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。
				 
		缓存穿透
			概念访问一个不存在的key，缓存不起作用，请求会穿透到DB，流量大时DB会挂掉。
			解决方案：
			采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的key，不存在的key直接被过滤；
			访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。
		缓存雪崩
			大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。
			解决方案
				可以给缓存设置过期时间时加上一个随机值时间，使得每个key的过期时间分布开来，不会集中在同一时刻失效；
				采用限流算法，限制流量；
				采用分布式锁，加锁访问	
				
		秒杀架构思想
			限流
				由于活动库存量一般都是很少，对应的只有少部分用户才能秒杀成功。所以我们需要限制大部分用户流量，只准少量用户流量进入后端服务器。
			削峰
				秒杀开始的那一瞬间，会有大量用户冲击进来，所以在开始时候会有一个瞬间流量峰值。如何把瞬间的流量峰值变得更平缓，是能否成功设计好秒杀系统的关键因素。实现流量削峰填谷，一般的采用缓存和 MQ 中间件来解决。
			异步
				秒杀其实可以当做高并发系统来处理，在这个时候，可以考虑从业务上做兼容，将同步的业务，设计成异步处理的任务，提高网站的整体可用性。
			缓存
				秒杀系统的瓶颈主要体现在下订单、扣减库存流程中。在这些流程中主要用到 OLTP 的数据库，类似 MySQL、SQLServer、Oracle。由于数据库底层采用 B+ 树的储存结构，对应我们随机写入与读取的效率，相对较低。如果我们把部分业务逻辑迁移到内存的缓存或者 Redis 中，会极大的提高并发效率。
			
			异常用户
				限制异常用户	针对同一个用户（ Userid 维度），做页面级别缓存，单元时间内的请求，统一走缓存，返回同一个页面
								大量请求同时间段查询同一个商品时，可以做页面级别缓存，不管下回是谁来访问，只要是这个页面就直接返回。
				
			正常多用户
				对于后端系统的控制可以通过消息队列、异步处理、提高并发等方式解决。对于超过系统水位线的请求，直接采取「Fail-Fast」原则，拒绝掉
				
			流程，用户抢购->是否是异常用户->缓存中的库存是否足够->扔给mq去慢慢操作
			
			总结流程
				秒杀系统核心在于层层过滤，逐渐递减瞬时访问压力，减少最终对数据库的冲击。通过上面流程图就会发现压力最大的地方在哪里？
				MQ 排队服务，只要 MQ 排队服务顶住，后面下订单与扣减库存的压力都是自己能控制的，根据数据库的压力，可以定制化创建订单消费者的数量，避免出现消费者数据量过多，导致数据库压力过大或者直接宕机。
				库存服务专门为秒杀的商品提供库存管理，实现提前锁定库存，避免超卖的现象。同时，通过超时处理任务发现已抢到商品，但未付款的订单，并在规定付款时间后，处理这些订单，将恢复订单商品对应的库存量。
			总结
				核心思想：层层过滤
				尽量将请求拦截在上游，降低下游的压力
				充分利用缓存与消息队列，提高请求处理速度以及削峰填谷的作用
				
				针对扣库存的地方，利用redis实现分布式锁(一般是集群，用分布式锁),锁住针对库存的减操作。可以使用redisson已经有对 Reentrantlock的封装操作。
				
				普通锁和分布式锁区别
					如果是集群中，普通锁是相对线程来说的，不同的服务器之间内存不同，自然锁失效。分布式中只能使用分布式锁。将所有锁资源统一到比如redis中
						普通锁是针对单机多线程中方法调用冲突的问题，可以在单独一块内存中进行解决。
							可以通过lock和synchronized进行解决
							Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现

						分布式锁是针对分布式系统中多系统多进程间方法调用冲突的问题，不能在单独的一块内存中进行解决。
							分布式锁现在有三种解决思路：数据库锁、redis分布式锁、zookeeper分布式锁
							
							使用redis删除锁的时候，可能存在10s时间，执行15秒的情况，这是删除锁需要判断是不是当前线程用户锁，不要误删别的线程的锁了。尽量保证时间内完成，别整别的事。
 		
		缓存雪崩
		缓存雪崩我们可以简单的理解为：由于原有缓存失效，新缓存未到期间所有原本应该访问缓存的请求都去查询数据库了，而对数据库 CPU 和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。一般有三种处理办法：
		1. 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。
		2. 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。
		3. 为 key 设置不同的缓存失效时间	
		缓存预热
			缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！
		
		缓存更新
			缓存更新除了缓存服务器自带的缓存失效策略之外（Redis 默认的有 6 中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：
			(1)定时去清理过期的缓存；
			(2)当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。
		
		缓存降级
			当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。
			系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）		
					
	
50.重定向
	RedirectView
	return  new ModelAndView("redirect:/my_requires.htm");

	return "redirect:/user/show?id="+user.getId() ;
	mv.setViewName("redirect:/user/show?id="+user.getId())
			
	RedirectAttributes  这个可以传递一个对象给重定向的数据(其实是存入了session中)
	
	
51.注解相关
	这种注解其实没有必要，分层太细了
		@Repository
		public class ShopRepository {
			@Autowired
			ShopDao shopDao;
			public Shop findById(long id) {
				return shopDao.findById(id);
			}
		}
		
	Annatation(注解)是一个接口，程序可以通过反射来获取指定程序中元素的 Annotation对象，然后通过该 Annotation 对象来获取注解中的元数据信息。
	注解中的@Target 常用的类型 指定用在什么上面，根据实际需求定
		TYPE,		 /**用于描述类、接口(包括注解类型) 或enum声明 Class, interface (including annotation type), or enum declaration */
		FIELD,		 /** 用于描述域 Field declaration (includes enum constants) */
		METHOD,		 /**用于描述方法 Method declaration */
		PARAMETER,	 /**用于描述参数 Formal parameter declaration */
		
	一般@Retention(RetentionPolicy.RUNTIME)都是运行期
	
52.一个dubbo的服务启动需要一个端口，同一台机器上
dubbo.port=2881

53.我们看到InnoDB默认的行锁可以使得操作不同行时不会产生相互影响、不会阻塞，从而很好的解决了多事务和并发的问题。但是，那得基于一个前提，即 Where 条件中使用上了索引；反之，如果没有使用上索引，则是全表扫描、全部阻塞

54.main方法而言，虽然写在类中，它是游离于任何类之外的，因此某类的非静态内部类对它而言是不直接可见的，也就无法直接访问 。

55.  Executor(实现线程池的功能)
	 Executor, ExecutorService 都是接口，ExecutorService继承于Executor，Executors是工具类(创建线程池)，他提供对ThreadPoolExecutor的封装产生ExecutorService的具体实现类。
	 
	 1.ExecutorService 接口继承了Executor 接口，是Executor 的子接口。
　　 2.Executor定义的execute()方法，用来接收一个Runnable接口的对象，无返回结果；而ExecutorService中定义的submit()方法可以接收Runnable和Callable接口对象，通过一个 Future 对象返回运算结果。
　　 3.Executor和ExecutorService除了允许客户端提交一个任务，ExecutorService 还提供用来控制线程池的方法。
　　   　　比如：调用 shutDown() 方法终止线程池。
	 
	 ***ThreadPoolExecutor实现了ExecutorService接口，所以Executors创建的线程池对象是ExecutorService的实现类
	 
	 3个组成部分
		1.工作任务：Runnable/Callable 接口
		2.任务的执行。包括执行机制的核心接口Executor(ExecutorService接口是其子接口)。Executor框架有两个关键类，ThreadPoolExecutor和ScheduledThreadPoolExecutor。
		3.异步计算的结果。包括接口Future和实现Future接口的FutureTask类。
	
		线程池的任务是异步执行的，只要提交完成就能快速返回，可以提高应用响应性
		Executor框架把工作任务与执行机制分离开来：工作任务包括Runnable接口和Callable接口，而执行机制由Executor接口提供。
	
		ForkJoinPool是一个并发执行框架
		异步计算的结果：Future接口
		实现Future接口的FutureTask类，代表异步计算的结果，持有计算结果
		
		原始的new Thread(new RunnableTask())).start()，但在Executor中，可以使用Executor而不用显示地创建线程：executor.execute(new RunnableTask());
		
		Executor：			execute()方法用来接收一个Runnable接口的对象，不返回任何结果
		ExecutorService(Executor的子接口)：submit()可以接受Runnable和Callable接口的对象，返回Future 对象运算结果,真正的线程池接口，Executors创建线程池后的对象
			当将Callable的对象传递给ExecutorService的submit方法，则该call方法自动在一个线程上执行，并且会返回执行结果Future对象。
			当将Runnable的对象传递给ExecutorService的submit方法，则该run方法自动在一个线程上执行，并且会返回执行结果Future对象，但是在该Future对象上调用get方法，将返回null。

		
		 
		ExecutorService executorService = Executors.newCachedThreadPool(); 
		Future<String> future = executorService.submit(new TaskWithResult(i)); 

		Executors 类提供工厂方法创建不同类型的线程池(本质上使用了ThreadPoolExecutor的构造方法，内部实际使用了ThreadPoolExecutor创建线程池)。
		对比我们使用层面在Executors就可以了。比如:
		1.newSingleThreadExecutor()?创建一个只有一个线程的线程池，
		2.newFixedThreadPool(int numOfThreads)来创建固定线程数的线程池，
		3.newCachedThreadPool()   缓存线程池，线程数量不限制，可能oom
		4.Executors.newScheduledThreadPool(int corePoolSize) 创建一个线程池(任务调度的线程池实现)，以在给定的延迟后运行命令，或者定期执行命令(它比Timer更灵活)。不常用
		
  
		ThreadPoolExecutor(是最核心的线程池实现，用来执行被提交的任务，是ExecutorService的实现类),可以使用execute和submit两个方法向线程池提交任务,前者不带返回值(参数是runable)，后者带返回值(参数是futuretask，实现callable接口)
		
		ExecutorService
			shutdown调用后，不可以再submit新的task，已经submit的将继续执行。
			shutdownNow试图停止当前正执行的task，并返回尚未执行的task的list
		
		不足
			1）newFixedThreadPool 和 newSingleThreadExecutor: 	主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至 OOM。
			2）newCachedThreadPool 和 newScheduledThreadPool:   主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。
			
		CountDownLatch (CountDownLatch是一个同步工具类,是一个原子操作的计数器，是一次性的，计算器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当CountDownLatch使用完毕后，它不能再次被使用。)
			CountDownLatch latch = new CountDownLatch(3);  
			Worker w1 = new Worker(latch,"张三");  
			Worker w2 = new Worker(latch,"李四");  		this.downLatch.countDown();
			Boss boss = new Boss(latch);  				this.downLatch.await();
			executor.execute(w2);  
			executor.execute(w1);  
			executor.execute(boss);  		
			executor.shutdown();  
		共用一个CountDownLatch，调用await，一直阻塞等待，直到这个CountDownLatch对象的计数值减到0为止
		
		Future的扩展增强，有点是监听任务完成，不用手动获取
			ListenableFuture顾名思义就是可以监听的Future，它是对java原生Future的扩展增强。我们知道Future表示一个异步计算任务，当任务完成时可以得到计算结果。
			如果我们希望一旦计算完成就拿到结果展示给用户或者做另外的计算，就必须使用另一个线程不断的查询计算状态。这样做，代码复杂，而且效率低下。
			使用ListenableFuture Guava帮我们检测Future是否完成了，如果完成就自动调用回调函数，这样可以减少并发程序的复杂度。
		
		可见虽然主线程中的多个任务是异步执行，但是无法确定任务什么时候执行完成，只能通过不断去监听以获取结果，所以这里是阻塞的。这样，可能某一个任务执行时间很长会拖累整个主任务的执行。
		一个任务的执行结果:Future接口中的isDone()方法来判断任务是否执行完，如果执行完成则可获取结果，如果没有完成则需要等待。 
		
		这里使用while(true)实现,对某一个futuretask完成后才可以get，否则阻塞没有意义
		 while (true) {
            if (booleanTask.isDone() && !booleanTask.isCancelled()) {
                Boolean result = booleanTask.get();
                System.err.println("任务1-10s： " + result);
                break;
            }
        }

		那么多个while (true) 的get，如果第一个阻塞很久，那么后面的也卡主了，相当于阻塞了主线程的流程
		因为我们一开始用 Thread1.get() 获取第一个线程的结果时，是阻塞的，而且我们假定任务1执行了10s钟，导致了线程2（3s就执行完任务）和线程3（2s就执行完任务）都执行完了任务，也不打印出来。那在实际业务中，这种方法肯定是不可取的。
		所以接下来我们引入 Guava Future
		
		使用限流  使用Guava 的 RateLimiter
			RateLimiter是单机的，也就是说它无法跨JVM使用,
			使用的是令牌桶算法是最常用的限流算法，它最大的特点就是容许一定程度的突发流量。
			以固定的频率向桶中放入令牌，例如一秒钟10枚令牌，实际业务在每次响应请求之前都从桶中获取令牌，只有取到令牌的请求才会被成功响应，获取的方式有两种：阻塞等待令牌或者取不到立即返回失败，
			RateLimiter 允许某次请求拿走超出剩余令牌数的令牌(马上许可)，但是下一次请求将为此付出代价，一直等到令牌亏空补上，并且桶中有足够本次请求使用的令牌为止.(可以预支，后续延迟)

		解决的是监听future返回结果阻塞问题，引入Guava Future
		大致意思是手工Future.get会一直阻塞在那边，这时可以使用ListeningExecutorService executorService = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(5));装饰下原线程池，
		使用ListenableFuture监听future返回结果，Futures.addCallback(listenableFuture, new FutureCallback<Integer>() {}即可，这里这个方法是异步的，主线程会顺序先执行，主线程不会阻塞，监听再依次输出
		
		
		基本所有的 ExecutorService 对任务的异常都做了捕获，当你的任务代码抛出异常时，你是拿不到错误的 所以需要你在自己的任务中（ Runnable 或者 Callable ）捕获异常并处理

		
		
56.MDC.put   	MDC.remove
结合类java.lang.ThreadLocal<T>及Thread类可以知道，
MDC中的put方法其实就是讲键值对放入一个Hashtable对象中，然后赋值给当前线程的ThreadLocal.ThreadLocalMap对象，即threadLocals，这保证了各个线程的在MDC键值对的独立性。
是的单线程的日志输出具有共享变量



57.hashmap相关
	https://blog.csdn.net/vking_wang/article/details/14166593  map介绍
	在链表头部插入数据，意思是数组+链表的结构
	hashmap的链表设计，实际上HashMap存放的对象是Entry对象，Entry相当于HashMap中的实体，Entry有key,value,hash,next属性，key和value都保存在Entry里
	如果Entry数组的bucketIndex上已经有Entry存在，就把新的Entry放在bucketIndex位置，原Entry移到新Entry的next指向的位置―――在链表头部插入数据，如果bucketIndex位置上没有Entry存在，新插入的Entry的next指向null。
	系统总是将新的Entry对象添加到bucketIndex处。如果bucketIndex处已经有了对象，那么新添加的Entry对象将指向原有的Entry对象，形成一条Entry链，但是若bucketIndex处没有Entry对象，也就是e==null,那么新添加的Entry对象指向null，也就不会产生Entry链了。
			
	数组中每个元素存储的是一个链表的头结点，数组中存储的是最后插入的元素
	a-b-c依次存储，B.next = A,Entry[0] = B,如果又进来C,index也等于0,那么C.next = B,Entry[0] = C

	hashmap扩容，源码解析：
		hash表这里是头插入，插入第一个元素，自然要将当前的next指向当前的hash数组元素，默认第一是null
		*****扩容的源码，总结就是采用头插入法，数组中存储的是entry链表的头节点*****
		这里的e 相当于table[i]赋值，(是当前数组位置的头结点)
		void transfer(Entry[] newTable, boolean rehash) {
			int newCapacity = newTable.length;									4
			for (Entry<K,V> e : table) {
			while(null != e) {
				Entry<K,V> next = e.next;   获取7									
				if (rehash) {e.hash = null == e.key ? 0 : hash(e.key);}
				int i = indexFor(e.hash, newCapacity);
				e.next = newTable[i];  重新设置3.next=null
				newTable[i] = e;       新表位置设置3
				e = next;              将而3变成7
			 }
			}
		}

		去掉了一些冗余的代码， 层次结构更加清晰了。
		第一行：记录odl hash表中e.next
		第二行：rehash计算出数组的位置(hash表中桶的位置)
		第三行：e要插入链表的头部， 所以要先将e.next指向new hash表中的第一个元素
		第四行：将e放入到new hash表的头部
		第五行： 转移e到下一个节点， 继续循环下去
		

		首先HashMap是线程不安全的，其主要体现：
		#1.在jdk1.7中，在多线程环境下，扩容时会造成环形链或数据丢失。一个线程已经完成，另一个线程中途挂起了，3-7  7-3。容易形成环链(了解就行)
		#2.在jdk1.8中，在多线程环境下，会发生数据覆盖的情况。
			
	
		这是操作同一个对象，会有并发问题
		count=1
		worktest worktest1 = new worktest(count);
		newFixedThreadPool.execute(worktest1);
		newFixedThreadPool.execute(worktest1);

		这是操作不是同一对象，实际操作不是外部的count.没有并发问题
		newFixedThreadPool.execute(new worktest(count));
		newFixedThreadPool.execute(new worktest(count));

		处理同一个资源的时候才会出现并发问题，一般操作的是数据库，有副本
		3-7-5
		处理map的时候也是一样的，线程1 刚走完3-7那步挂起，线程2全部走完，那么线程1执行7的时候，7的next就不对了，又变成了7-3了，这样就出现了问题

		直接初始化map
		Map kk = new HashMap<String, String>(){
			{
				put("name", "123");
				put("age", "222");
			}
		};
			
	HashMap，Hashtable，LinkedHashMap，TreeMap
		HashMap，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。
		
		1.7中头插法出现的环形链表,暂不细究
		自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升.while执行		
				
	
		
concurrentHashMap和HashMap相关
	HashMap相关
		HashMap的线程不安全主要体现在下面两个方面：
			1.在JDK1.7中，当并发执行扩容操作时会造成环形链和数据丢失的情况。				使用头插法
			2.在JDK1.8中，在并发执行put操作时会发生数据覆盖的情况。							使用尾插法，因此不会出现环形链表的情况
			
			真正的map中的entry链表结构如下：存在于hashmap中的内部类
			 static class Node<K,V> implements Map.Entry<K,V> {
					final int hash;
					final K key;
					V value;
					Node<K,V> next;
				}
			这里数组中存的是一个链表，其实只要取第一个头结点就可以了
			
			改变数组或者list中的对象的引用，不改变数组的值。就是说list或者数组中存入的是引用地址，存入后地址就不变了。
			数组和list中存的引用对象，存入的是引用对象的地址，后续
			person1=person2;是将两个对象都指向了person2的地址
			
			多线程的内存模型是针对变量来的，并不是针对整个方法，这里和数据库不一样，entry对象被另一个线程修改后就会回写到主内存中
			
			核心就是头插法导致的顺序反转。
			原3-7
			1.线程a在读取了next后挂起，当前3 next指向7
			2.b线程执行完后变成了7-3 
			3.a恢复执行，引用到了b更新后的7指向3，形成闭环。
			
			这里线程在用到变量的时候才会取，这样当第一次取e暂停后面处理正常，转到next第二次取e时会从主内存中重新获取（主内存中已经被更新）。
			这种工具类中没有类似事务操作的，之后内存之间的快速处理。用时取快速回写。用到的时候再从主存中获取。
			另一个线程已经更新完了，当前线程去操作获取，不存在冲突。
			
			既然出发点就没有考虑多线程环境，代码里面没有任何同步手段，在多线程下使用就可能出现太多的错误，不仅仅是什么覆盖、死循环。 
			没有对get put操作同步不就完了吗
			
			java8中对HashMap进行了优化，如果链表中元素超过8个时，就将链表转化为红黑树，以减少查询的复杂度，将时间复杂度降低为O(logN)。
			Java 8 中使用 Node 模型来代表每个 HashMap 中的数据节点，都是 key，value，hash 和 next 这四个属性。Node 用于链表，红黑树用 TreeNode。
			
			HashTable 那样不管是 put 还是 get 操作都需要做synchronized同步处理，效率低。线程安全
			concurrentHashMap相对使用的不是方法锁，允许多个并发操作（分布在不同的段上），效率高，内部使用了段segment，每个小段就是hashtable
			
			总结，无论是 1.7 还是 1.8 其实都能看出 JDK 没有对它做任何的同步操作，所以并发会出问题，甚至出现死循环导致系统不可用。
		
	concurrentHashMap相关	
		concurrentHashMap使用了锁分段，避免竞争同一个资源阻塞。
			核心思想就是尽量只锁住需要操作的散列表段。
				将资源分段存储
				jdk1.7版本使用
				结构：segment[]+hashEntry[]   数组结构  *****这是jdk7版本使用，在jdk8版本基本不使用了，只是做了相应的兼容****
				
				Segment数组的意义就是将一个大的table分割成多个小的table来进行加锁，也就是上面的提到的锁分离技术，
				而每一个Segment元素存储的是HashEntry数组+链表，这个和HashMap的数据存储结构一样。本身segment继承ReentrantLock，会在put操作时加锁保证线程安全。

				通过两次Hash定位到元素位置，第一次是定位segment，第二次是定位hashEntry。
				
				jdk1.7版本	使用了segment中继承的ReentrantLock实现锁控制
				jdk1.8版本	使用synchronized+cas实现锁控制
					
				锁分段技术
					首先将数据分成一段一段地存 储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数 据也能被其他线程访问
					一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元 素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时， 必须首先获得与它对应的Segment锁，
					输入参数initialCapacity是ConcurrentHashMap的初始化容量(默认16)，loadfactor是每个segment的负载因子(默认0.75)
					
		
			Segment的优缺点
				段Segment继承了重入锁ReentrantLock，有了锁的功能，每个锁控制的是一段，当每个Segment越来越大时，锁的粒度就变得有些大了。
				分段锁的优势在于保证在操作不同段 map 的时候可以并发执行，操作同段 map 的时候，进行锁的竞争和等待。这相对于直接对整个map同步synchronized是有优势的。
				缺点在于分成很多段时会比较浪费内存空间(不连续，碎片化); 操作map时竞争同一个分段锁的概率非常小时，分段锁反而会造成更新等操作的长时间等待; 当某个段很大时，分段锁的性能会下降。

				***
				JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap。
				虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本。
				
				JDK1.8为什么不用ReentrantLock而用synchronized ?
				减少内存开销:如果使用ReentrantLock则需要节点继承AQS来获得同步支持，增加内存开销，而1.8中只有头节点需要进行同步。
				内部优化:synchronized则是JVM直接支持的，JVM能够在运行时作出相应的优化措施：锁粗化、锁消除、锁自旋等等。
				***
				
				自旋锁是指当一个线程尝试获取某个锁时，如果该锁已被其他线程占用，就一直循环检测锁是否被释放，而不是进入线程挂起或睡眠状态。
				
				线程问题在于 多个线程同时resize时候，但是有的执行完 有的正在resize，这样数量就会出现问题。所以要加锁控制
				
				当数组大小已经超过64并且链表中的元素个数超过默认设定（8个）时，将链表转化为红黑树。
				
				***
				CAS(Compare and Swap)比较并替换。这就是乐观锁的思想.但是更严谨的应该是使用版本号更新
				CAS机制中使用了3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。
				更新一个变量的时候，只有当变量的预期值A和内存地址V当中的实际值相同时，才会将内存地址V对应的值修改为B。
			
				什么事ABA问题？怎么解决？
					当一个值从A变成B，又更新回A，普通CAS机制会误判通过检测。		利用版本号比较可以有效解决ABA问题。

				缺点；	
					1） CPU开销过大		在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，循环往复，会给CPU带来很到的压力。
					2） 不能保证代码块的原子性	CAS机制所保证的知识一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证3个变量共同进行原子性的更新，就不得不使用synchronized了。

				synchronized属于悲观锁，悲观的认为程序中的并发情况严重，所以严防死守，CAS属于乐观锁，乐观地认为程序中的并发情况不那么严重，所以让线程不断去重试更新。
				***
						
				JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点）
				在大量的数据操作下，对于JVM的内存压力，基于API的ReentrantLock会开销更多的内存，虽然不是瓶颈，
				因为粒度降低了，在相对而言的低粒度加锁方式，synchronized并不比ReentrantLock差，在粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了

				concurrentHashMap的安全性在于put的操作，而不是外部包裹了一段的代码块
				
				JDK1.8中concurrentHashMap不安全性
					两个线程A、B都在进行put操作，且hash函数计算出的插入下标是相同的，在hash碰撞判断后，a挂起(时间片耗尽)，b走完，随后线程A获得时间片，直接进行插入数据，会将b的覆盖。
			
			
58. 装饰器模式可以动态的把新的职责添加到对象上。这里关键点是“动态”，也就是运行时；而继承在编译的时候已经确定了。没啥意思   io是装饰模式

59. mybatis-plus  tkmybatis不同的快捷框架吧
	sitemesh的设计思想是装饰者(decorator)设计模式。SiteMesh使用一个Servlet过滤器，它可以拦截返回的Web浏览器的HTML，提取相关内容，并将其合并到被称为装饰器（Decorator）的模板。

	
60.hessian 只要匹配项目目录后的路径即可
	service工程中接口，war工程中实现类.bean的注入问题
	测试步骤：结论，普通接口可以，hessian的暴露类需要引在注入的service工程中
	1.service中放普通接口，war中实现类，在service中可以注入
	2.service中放hessian接口，war中配置代理类，在service中无法注入
	
	在boot 的配置中client 和server的配置文件尽量要分开，放一起会冲突

	之前netpay的hessian接口代理是remoting/aaa    发送到netpay匹配剩下的aaa即可。
	
	

61.filter，servlet，interceptor
	filter
		简单的用@webfilter，需要springbean的就需要使用代理filter
		filter的执行顺序，FilterRegistrationBean注册时，filter顺序与@Bean注解实例顺序一致
		filter 中 response sendRedirect之后要 return;
		
		Filter的优先级大于Servlet，而springMVC又是基于Servlet来进行注入bean的，所以这就导致了Filter无法注入bean
		在Spring中，web应用启动的顺序是：listener ->filter -> servlet，先初始化listener，然后再来就filter的初始化，再接着才到我们的 dispathServlet 的初始化，因此，当我们需要在filter里注入一个注解的bean时，就会注入失败，因为filter初始化时，注解的bean还没初始化，不能注入 。
		 （1）容器在启动的时候，会先加载filter，然后再加载Spring中的Bean。所以如果是直接在Filter 中进行SpringBean的注入，那么无法成功进行注入，因为要注入的Bean还没有进行初始化，是null。
		 （2）DelegatingFilterrProxy是一个Filter。容器在启动的时候会加载这个Filter，对这个类的操作将会委托到 targetBeanName对应的Bean进行处理(Spring容器管理)，因为TargetBean是Spring的一个Bean，所以可以进行SpringBean的注入。
		
		如果不配置DelegatingFilterProxy，则由于filter比bean先加载，也就是容器或者Tomcat会先加载filter指定的类到container中，
		这样filter中注入的spring bean就为null了，
		或者也可以在filter中使用SpringContextUtil读取bean
		
		filter中没有		chain.doFilter(httpRequest, httpResponse);//go   没有就不会进controller

	servlet
	*************
		路径匹配（以“/”字符开头，并以“/*”结尾），
		扩展名匹配（以“*.”开头），
		精确匹配，三种匹配方法不能进行组合，不要想当然使用通配符或正则规则。
		 
		servlet的执行顺序由匹配顺序决定，1.优先精确路径匹配 2.次之最长路径匹配， 3.最后后缀匹配
		如果前面三条规则都没有找到一个servlet，容器会根据url选择对应的请求资源。如果应用定义了一个default servlet，则容器会将请求丢给default servlet
		 
		比如servletA的url-pattern为/test/*，而servletB的url-pattern为/test/a/*，此时访问http://localhost/test/a时，容器会选择路径最长的servlet来匹配，也就是这里的servletB。 
		 
		sevlet拦截之后就不会进相关的controller了(而且当有一个servlet匹配成功以后，就不会去理会剩下的servlet了)
		自定义的servlet 可以使用路径匹配的，但是自定义dispatcherServlet，无法使用路径匹配，只能使用后缀和全路径，这个暂定
	*************
		servlet的本质其实也是一个java bean，controller是对servlet的封装，底层依旧是servlet。
		Spring MVC是基于servlet的，DispatherServlet，负责处理请求，调用了你的controller
		多DispatcherServlet的情况下，是registration.setName("rest")，默认为“dispatcherServlet”，这个语句很重要，因为name相同的ServletRegistrationBean只有一个会生效，也就是说，后注册的会覆盖掉name相同的ServletRegistrationBean。
		
		spring使用mvc时会产生两个context上下文，一个是ContextLoaderListener产生的，一个是由DispatcherServlet产生的(以spring的上下文为父容器)，它们俩是父子关系。
		父子级别的上下文。context，多个dispatcherservlet   的 context
		ContextLoadListener监听器在tomcat容器初始化的时候监听tomcat的servlet上下文，在这个监听器中，servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context);
		
		@webservlet 注册到web容器中作为一个servlet(只适用自定义的servlet，DispatcherServlet是框架提供的servlet,需要ServletRegistrationBean定义)
		
		配置对应的DispatcherServlet，是需要加载对应的context的。
		那么支付项目中的，就是全部交给了mvc，通过.htm拦截。
		也可以继续加载自身的remoting.xml，交给单独的DispatcherServlet(ServletRegistrationBean.addUrlMappings       murlappings不支持通配符，需为具体的路径    配置*.htm后缀是可以的 ,普通的没影响)。
			
		在原始的mvc的xml中配置中，dispatcher中的url(/)映射，当request匹配不到其他servlet，就会进入该Servlet，包括静态资源请求。

				
		在 web.xml 中使用 Listener 监听器来加载 Spring 的配置，Spring 会创建一个全局的 WebApplicationContext 上下文，其被称为根上下文，保存在 ServletContext 中，
		可以使用工具类取出上下文 WebApplicationContextUtils.getWebApplicationContext(ServletContext）
		从 WebApplicationContext 中可以获得ServletContext 的引用，整个Web 应用上下文对象将作为属性放置到 ServletContext 中，以便 Web 应用环境可以访问 Spring 应用上下文。getServletContext()

		多个dispatcherServlet，每个 DispatcherServlet都有自己的 WebApplicationContext 上下文 这个上下文是私有的,继承了根上下文中的所有东西

		
	拦截器：
		定义了ABC拦截器，且在SpringMVC定义的顺序为A、B、C，preHandle是顺序执行的，postHandle与afterCompletion方法倒序执行的。
		
		
		且在SpringMVC定义的顺序为A、B、C。且B拦截器的preHandle方法返回false。则拦截器的执行行为如下：
		A.preHandle
		B.preHandle
		A.afterCompletion
		即B拦截器之前（包括B拦截器）的preHandle被执行及afterCompletion方法被执行（不包括B拦截器）
		
		
		只要有一个拦截器不放行，postHandle不会执行。
		所以当拦截器非正常执行完成时，会直接跳过所有拦截器的postHandle()函数，然后再逆向的执行preHandle()函数返回为true时的afterCompletion()方法

		boot中和普通的mvc中，执行顺序就是配置添加的顺序
		
		
		
		前置方法preHandle->处理器handler->后置方法postHandle->视图解析和渲染视图->完成方法afterCompletion
		
	
62.spring中的子父容器
		WebApplicationContext
			ApplicationContext是spring的核心，spring把bean放在这个容器中，在需要的时候，用getBean()方法取出，在web应用中，会用到webApplicationContext，继承自ApplicationContext
 
		ServletContext
			是Servlet与Servlet容器之间直接通信的接口，Servlet容器(tomcat等)启动创建一个ServletContext对 象，每个web应用有唯一的ServletContext对象，所有Servlet对象共享一个 ServletContext，Servlet对象可以通过它来访问容器中的各种资源
			在web框架中，每个DispatcherServlet有它自己的WebApplicationContext，WebApplicationContext被绑定在ServletContext上，


63.jdk8
	中使用default,即可使用默认的方法，不满足需求的时候可以覆盖，static可以直接调用
	Lambda 表达式由参数、-> 和实现主体 三大部分组成。
	
	
	*************************
		函数式接口(Functional Interface，@FunctionalInterface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。
		函数式接口可以被隐式转换为 lambda 表达式。以前是采用匿名实现类实现的
		
		
		1.一个函数式接口有且只有一个抽象方法。
		2.默认方法不是抽象方法，因为它们已经实现了。
		3.重写了超类Object类中任意一个public方法的方法并不算接口中的抽象方法。
		
		因为任何接口的实现都会从其父类Object或其它地方获得这些方法的实现。
		
		所以Comparator中的equals不算是接口中的抽象方法
	*************************
	
	List<person> plist = ...;
	//根据超时天数倒序排序
	Comparator<person> comparator = (t1, t2) -> t1.getAge().compareTo(t2.getAge());
	plist.sort(comparator.reversed());	
	实际就是list.sort(Comparator)操作
	
	List.sort((o1, o2) -> flag ?a.compareTo(b) : b.compareTo(a));

	

	
64.linux 命令
		top查看系统的cpu
		
		执行当前文件夹下的文件是    ./version.sh

		more 可以分页查看，
		less 支持内容查找，高亮显示
		grep  aa xye.log	字符串查找	找出xye.log中的aa内容
		find  -name xye.log  文件查找 find  -name "*.log"				查询速度慢
		which   				查看系统命令是否存在，以及执行的到底是哪一个位置的命令。
		whereis 				只能用于可执行文件的搜索，只能搜索二进制文件，说明文件，源代码文件。比which范围广
		curl 					页面请求返回	

		locale			

		linux中的系统发布，优雅停机
		应用启动后，小校验从而完成服务器上的应用发布。(一般通过检测脚本或者页面。有返回结果即可。)
		优雅停机，在负载均衡上做文章，关闭应用前，把应用从负载均衡中心上移去，然后再优雅关闭应用（结束当前所有请求后关闭），然后进行新应用的启动及检查，检查通过后再把应用加到负载均衡上，并对外提供应用。
		灰度发布，指针对新应用，在用户体验上完全感知不到的更新，可能持续时间长，重要的状态需要记录。
		
		elk日志也一般是采集的异常信息，或者一段时间内的总调用次数和响应相关
		之前通过短信方式来报警，现在通过手机应用来接收报警也不错。
		降级多是大量请求且不能扩容进行的功能限制，可能针对某功能，也可能根据不同使用者。
		切换更多是依赖的下层出现故障，需要手工切换。

		linux权限，文件权限，用户权限
			Linux的文件基本上分为三个属性：可读（r），可写（w），可执行（x）。
			类型后面紧接着的3*3个字符分3组，各指示此文件的读、写、执行权限，对于owner、group、others而言

			| d | rwx | r-x | r-x |
			|文件类型 | 所有者权限|用户组权限 |其他用户权限|

			drwxr-xr-x   2 root root 48 2013-11-27 16:34 test/
			第一个栏位，表示文件的属性，
			第二个栏位，表示文件个数。
			第三个栏位，表示该文件或目录的拥有者
			第四个栏位，表示所属的组（group），每一个使用者都可以拥有一个以上的组，不过大部分的使用者应该都只属于一个组
			第五栏位，表示文件大小
			第六个栏位，表示最后一次修改时间
			第七个栏位，表示文件名

			（以-rwxr-xr-x为例）：　　  
			rwx(Owner)r-x(Group)r-x(Other)　
			这个例子表示的权限是：使用者自己可读，可写，可执行；同一组的用户可读，不可写，可执行；其它用户可读，不可写，可执行。
			另外，有一些程序属性的执行部分不是X,而是S,这表示执行这个程序的使用者，临时可以有和拥有者一样权力的身份来执行该程序。

			egj是yunwei创建，egjjjj是开发创建，在kaifa用户下查看权限，yunwei和kaifa都是同一个组的
			哪个用户创建就属于哪个owner
			drwxrwxr-x.   3 prd_yunwei prd_yunwei  4096 May 12 10:48 egj		yunwei建目录，所有者属于运维，同组有写操作
			drwxr-xr-x.   2 kaifa      prd_yunwei  4096 May 12 10:59 egjjjj		kaifa建目录，所有者属于kaifa，yunwei用户不能写操作(因为同组没有写操作)

			创建权限
			who 
			u 表示“用户（user）”，即文件或目录的所有者。
			g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。
			o 表示“其他（others）用户”。
			a 表示“所有（all）用户”。它是系统默认值。

			+ 添加某个权限。
			C 取消某个权限。
			= 赋予给定权限并取消其他所有权限（如果有的话）。

			chmod ［who］ ［+ | C | =］ ［mode］ 文件名     有对应的数字简化版
			例如：chmod g+r，o+r example使同组和其他用户对文件example 有读权限
			******在配置文件写权限的时候，必须要拥有上层目录的读权限******
			
			groups 查看当前用户所属组
			chmod +rwx file	给file的所有用户增加读写执行权限     	不写用户就默认所有
			
	linux和windows文件传输
		1.使用winscp工具
		2.安装xftp后自动集成到xshell的插件中
		3.	rz和sz
			sz  下载(命令格式：  sz filename   下载文件filename || sz file1 file2   下载多个文件 || sz dir/*　　　下载dir目录下所有文件)	　
			rz 上传	(命令格式： 	rz)
			注意：
			1.如果机器上没有安装过 lrzsz 安装包，则无法使用rz和sz命令。
			　可使用yum命令安装：yum install -y lrzsz  或者下载源码进行安装。下载地址：https://ohse.de/uwe/software/lrzsz.html	
			
	shell脚本
		Bash 也是大多数Linux系统默认的Shell。
		在一般情况下，人们并不区分 Bourne Shell 和 Bourne Again Shell，所以，像 #!/bin/sh，它同样也可以改为 #!/bin/bash。
		#!/bin/bash					这里#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 程序。
		echo 命令用于向窗口输出文本，还有个类似的功能printf。

		运行 Shell 脚本有两种方法：
			1、作为可执行程序，常用方法
				chmod +x ./test.sh  #使脚本具有执行权限
				./test.sh  #执行脚本

			2、作为解释器参数
				/bin/sh test.sh
				这种方式运行的脚本，不需要在第一行指定解释器信息，写了也没用。
			
			执行当前目录下的sh脚本，一定要写成 ./test.sh，而不是 test.sh，运行其它二进制的程序也一样，
			直接写 test.sh，linux 系统会去 PATH 里寻找有没有叫 test.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 test.sh 是会找不到命令的，要用 ./test.sh 告诉系统说，就在当前目录找。
			
			以 # 开头的行就是注释，会被解释器忽略。
			
		 shell变量
			your_name="runoob.com"		变量赋值，字符串可用单引号，也可用双引号，也可以不用引号。()
										单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；
										单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用
										双引号里可以有变量,双引号里可以出现转义字符
			echo ${your_name}         	#输出展示变量   可省略{}，但是建议加上
			readonly myUrl				设置为只读
			unset variable_name 		删除变量，删除后不能再用，只读不能删
			echo ${#variable_name} 		输出长度
			数组名=(值1 值2 ... 值n)	数组
			
		shell运算
			val=`expr 2 + 2`
			echo "两数之和为 : $val"
			表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。
			完整的表达式要被 ` ` 包含，注意这个字符不是常用的单引号，在 Esc 键下边。
			
			val=`expr $a + $b`
			echo "a + b : $val"
			
			代码中的 [] 执行基本的算数运算，如：
			result=$[a+b] # 注意等号两边不能有空格
			echo "result 为： $result"
				
			关系运算符
				-eq	等于则为真
				-ne	不等于则为真
				-gt	大于则为真
				-ge	大于等于则为真
				-lt	小于则为真
				-le	小于等于则为真
				
			文件运算符
				-b file	检测文件是否是块设备文件，如果是，则返回 true。	[ -b $file ] 返回 false。
				-c file	检测文件是否是字符设备文件，如果是，则返回 true。	[ -c $file ] 返回 false。
				-d file	检测文件是否是目录，如果是，则返回 true。	[ -d $file ] 返回 false。
				-f file	检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。	[ -f $file ] 返回 true。
				-g file	检测文件是否设置了 SGID 位，如果是，则返回 true。	[ -g $file ] 返回 false。
				-k file	检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。	[ -k $file ] 返回 false。
				-p file	检测文件是否是有名管道，如果是，则返回 true。	[ -p $file ] 返回 false。
				-u file	检测文件是否设置了 SUID 位，如果是，则返回 true。	[ -u $file ] 返回 false。
				-r file	检测文件是否可读，如果是，则返回 true。	[ -r $file ] 返回 true。
				-w file	检测文件是否可写，如果是，则返回 true。	[ -w $file ] 返回 true。
				-x file	检测文件是否可执行，如果是，则返回 true。	[ -x $file ] 返回 true。
				-s file	检测文件是否为空（文件大小是否大于0），不为空返回 true。	[ -s $file ] 返回 true。
				-e file	检测文件（包括目录）是否存在，如果是，则返回 true。	[ -e $file ] 返回 true。
				其他检查符：

			-- if esls
				if condition1
				then
					command1
				elif condition2 
				then 
					command2
				else
					commandN
				fi
			-- for循环
				for var in item1 item2 ... itemN; do command1; command2… done;
			-- whlie	
				while condition
				do
					command
				done
				
			-- case
			case ... esac
			case $aNum in
				1)  echo '你选择了 1'
				;;
				*)  echo '你没有输入 1 到 4 之间的数字'
				;;
			esac

				#使用 . 号来引用test1.sh 文件
				. ./test1.sh

				# 或者使用以下包含文件代码
				# source ./test1.sh

				echo "菜鸟教程官网地址：$url"	
				
		当前目录执行，不加找不到(不在执行程序默认的搜索路径之列)
		./a.sh会用你脚本中第一行的那个#!XXX的shell来执行语句,而sh a.sh则是用sh来执行语句
		sh或是执行脚本，或是切换到sh这个bash里，默认的shell是bash	
		执行脚本的时候是用sh + 脚本名的方式来执行，其实，大部分的时候，简单脚本只要权限设置正确，可以直接执行，不需要sh命令的
				
		以sh执行，那么，不必设定执行权限，也不用写shell文件中的第一行（指定bash路径）。因为方法三 是将hello.sh作为参数传给sh(bash)命令来执行的。这时不是hello.sh自己来执行，而是被人家调用执行。		
				
				
				
				
		awk是一个强大的文本分析工具，简单来说awk就是把文件逐行读入，（空格，制表符）为默认分隔符将每行切片，切开的部分再进行各种分析处理		
				
		pid =`ps -ef| grep tomcat | grep -v grep | awk '{print $2}'`			#这里命令中的反引号的作用就是先执行命令然后把结果返回作为参数
		grep -v  			这里是查找，排除的意思
		awk '{print $2}'	就是截取第二个字段输出
		
		linux 命令中的 |
			利用Linux所提供的管道符“|”将两个命令隔开，管道符左边命令的输出就会作为管道符右边命令的输入。依次向后传递。
						
		实际自己测试的shell脚本
			简单的kill进程，然后重启
				#!/bin/bash
				tomcatpid=`ps -ef| grep tomcat | grep -v grep | awk '{print $2}'`
				echo "now pid is ${tomcatpid},trying to stop"
				kill -9 ${tomcatpid}
				echo "trying to reload"
				sh /home/zhanjun/test/tomcats/apache-tomcat-01/bin/startup.sh
				tailf -f /home/zhanjun/test/tomcats/apache-tomcat-01/logs/catalina.out
			
			
65.http 和 htps 的cookie  ， 没有区分，主要是满足domain中的值

66.	 java.lang.Error: Unresolved compilation problem
	 java compile 和installed jres 版本一致
	
67.	属性文件相关
	<directory>src/main/resources/${pay_env}</directory>
	目录中的会覆盖resource下的同名文件
	value注入值 $[xiaoyuer.domain]没有就显示$[xiaoyuer.domain]
	

68.try....finnally 的用法主要是为了释放资源，不进行异常捕获，将异常交由上层调用者处理  没有catch	

69. nginx配置
	******************************
	https://m.yu.com   和  https://192.168.6.222:8083  存的cookie是不一样的，
	https://m.yu.com/  nginx 转发后是8443 ，然后登录中的两个域名不一样    RedirectFilter 中 loginUrl
	pre 和 sit  手机有没有路径转发,有集群和转发
	域名转发ip相关
	nginx
	proxy_set_header Host $proxy_host;
	配置。当Host设置为$http_host时，则不改变请求头的值，所以当要转发到bbb.example.com的时候，请求头还是aaa.example.com的Host信息；
	当Host设置为$proxy_host时，则会重新设置请求头为bbb.example.com的Host信息。
	
	
	设置 http 请求 header 传给后端服务器节点，如：可实现让代理后端的服务器节点获取访问客户端的真实ip
	
	******************************	
	
	
	windows下nginx用nginx.exe -s stop关
	转发的两台tomcat需要session共享才能相互访问，不然登录会丢失session
	
	普通的负载均衡软件，如 LVS，其实现的功能只是对请求数据包的转发、传递，从负载均衡下的节点服务器来看，接收到的请求还是来自访问负载均衡器的客户端的真实用户；
	而反向代理就不一样了，反向代理服务器在接收访问用户请求后，会代理用户 重新发起请求代理下的节点服务器，最后把数据返回给客户端用户。在节点服务器看来，访问的节点服务器的客户端用户就是反向代理服务器，而非真实的网站访问用户
	ngx_http_upstream_module是负载均衡模块，可以实现网站的负载均衡功能即节点的健康检查，upstream 模块允许 Nginx 定义一组或多组节点服务器组，使用时可通过 proxy_pass 代理方式把网站的请求发送到事先定义好的对应 Upstream 组 的名字上。
	proxy_pass 指令属于ngx_http_proxy_module模块，此模块可以将请求转发到另一台服务器，在实际的反向代理工作中，会通过 location 功能匹配指定的URI，然后把接收到服务匹配URI的请求通过proyx_pass抛给定义好的 upstream 节点池。
	
	
	nginx中存放静态资源，实现静态资源分离
	localhost:80/image/ 时会访问本机的/usr/local/myImage/image/ 
	location /image/ {
				root   /usr/local/myImage/;
				autoindex on;
			}
	server {
		   listen 88;
		   server_name localhost;
		   root     /home/ubuntu/static/;
	}

	#localhost:80/image/1.jpg
	#也可以移除掉root 和 autoindex 配置，直接在html目录下的image目录下新建一张图片1.jpg。
	http://localhost:8087/image/zxnew.png  实际访问成功
	

	server {
			listen       8087;
			server_name  localhost;
			location / {
				root   html;											
				index  index.html index.htm;
			}
			location /image/ {
				root   C:/Users/xiaoyuer/Desktop/图片/;
				autoindex on;
			}
		}

	
	
	

70.mysql
	梯度漏斗   
		select *from t where a = 1 and b = 2 and c = 3; 
		就等于在满足 a = 1 的结果集中过滤掉b = 2 的 再从 a = 1 and b = 2 结果集中过滤掉 c = 3 的，得到最终的结果集越多查询越高效
	
	最左匹配
		索引文件以B－Tree格式保存 重点是联合索引的最左边字段(没有就匹配失败，不走该索引)
		index(a,b,c)		相当于创建了多个索引：key(a)、key(a,b)、key(a,b,c)，只能从左到右顺序连贯组合
		where a=3 and c=4    仅使用了a
			
		最左匹配原则都是针对联合索引来说的，in 和 = 都可以乱序，MySQL优化器会将其优化成索引可以匹配的形式
		
		范围查询(>、<、between、like)就会停止匹配。联合索引中使用范围查询(>、<、between、like)的字段后的索引在该条 SQL 中都不会起作用。
		比如
			某表现有索引(a,b,c),现在你有如下语句
			select * from t where a=1 and b>1 and c =1;     #这样a,b可以用到（a,b,c），c不可以
			
			
			
		EXPLAIN
		SELECT * FROM pp_trade_ass_confirm_paid WHERE item_order_ext_id IN (
			SELECT id FROM `pp_trade_item_order_extention`  WHERE order_no IN (
				SELECT trace FROM ac_pay_acct_txn WHERE account='0000000000000888811'  AND trace LIKE '%XQDD%' AND create_time BETWEEN '2019-05-06 00:33:15' AND '2019-08-01 00:00:00'
			) 
		)
		order_no  需要加索引  这种关联查询很慢的，对应的各sql的最左的都应该加上索引	
					
			
	查看表作为其他表的外键
			SELECT
			ke.referenced_table_name parent,
			ke.table_name child,
			ke.REFERENCED_COLUMN_NAME parent_column,
			ke.column_name child_column,
			ke.constraint_name
			FROM
			information_schema.KEY_COLUMN_USAGE ke
			WHERE
			ke.referenced_table_name IS NOT NULL
			AND ke.referenced_table_name = 'Base_Skill_Appli;'
			AND ke.REFERENCED_COLUMN_NAME = 'id'
			AND ke.REFERENCED_TABLE_SCHEMA = 'db_xiaoyuer'
			ORDER BY
			ke.referenced_table_name;
			
		COUNT(*)  COUNT(1)  COUNT(id)比较
			结论是优先使用count(*)
			1、执行速度上：针对一般情况（SQL语句中没有where条件）执行速度上
			count(*)=count(1)>count(主键)>count(其他列)，在没有特殊查询要求推荐使用count(*)来代替其他的count。
			2、执行结果上，count(*)与count(1)以及count(主键)的结果完全相同，即返回表中的所有行数，包含null值；count(其他列)会排除掉该列值为null的记录，返回的值小于或者等于总行数。
			
		格式化
			时间 	DATE_FORMAT(si.date_insert,'%Y-%m-%d %T') AS releaseTime
			数字 	CAST( 120.6 AS DECIMAL(15,2))		
					FORMAT(13625.265,2)	13,625.27     
			
			当天时间
				Calendar calendar = Calendar.getInstance();
				calendar.setTime(new Date());
				calendar.set(Calendar.HOUR_OF_DAY, 0);
				calendar.set(Calendar.MINUTE, 0);
				calendar.set(Calendar.SECOND, 0);
				Date zero = calendar.getTime();
					
				Calendar.HOUR_OF_DAY是24小时制
				Calendar.HOUR是12小时制
				所以下面方法是结果是不同的
				calendar.set(Calendar.HOUR_OF_DAY, 23); ?输出日期?2017-04-13 23:07:02
				calendar.set(Calendar.HOUR, 23); ?输出日期2017-04-13 11:07:02
			
			
			
	
		sql xml中 foreach 中的空list，直接跳过，类似代码中的一样
	
		4.0版本以下，varchar(20)，指的是20字节，如果存放UTF8汉字时，只能存6个（每个汉字3字节） 
		5.0版本以上，varchar(20)，指的是20字符，无论存放的是数字、字母还是UTF8汉字（每个汉字3字节），都可以存放20个，最大大小是65532字节 
	
		 GROUP BY 合并 取结果集的第一条显示
		 
		 mysql 中double  字段长度 20,2
		
		utf8_general_ci: UTF8 的默认校对规则
		InnoDB	支持行锁,事务,外键
		默认的字符集就是，utf8_general_ci
			mysql中的btree索引使用的是
				其数据文件本身就是索引文件,相比MyISAM ，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data
				保存了完整的数据记录 这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引 这被称为聚簇素’寻 （也叫聚集索引） 而其余的索引都为辅助
				索引 ，辅助索引 data 域存储相应记录主键的值而不是地址，这也是和 MyISAM同的地方 在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取汁 主键的值，再走一遍主索引 因此，在设计表的时候，
				不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂

		使用explain解释，关注三个参数
		type			显示那种类别，有无使用索引。如果为ALL,说明进行全表扫描
		key				显示mysql实际决定使用的键(索引)。如果没有命中索引，键是NULL
		EXtra			特殊的备注信息
		
		联合索引中，最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，该列就可以用作索引
			
		避免where子句中对字段施加函数，如 to_date(create_time) ＞xxxxx，这会造成无法命中索引
		
		将打算加索引的列设置为 NOT NULL ，否则将导致引擎放弃使用索引而进行全表扫描
		
		联表关联时，确保ON和USING中的列上有索引。有一个表的列有就行
		确保任何的GROUP 和 ORDER BY 中的表达式只涉及一个表中的列，这样mysql才可能使用索引来优化
			
		几种常见的mysql未命中索引的情况
			1.条件中有or，即使有条件带索引也不会使用，除非or条件中的每个列都加上索引
			2.like查询以%开头，索引不会命中。但是以%结尾，索引可以使用。
			3.使用正确的字段类型，强制转换会无法命中索引。(若列类型是字符串，使用引号引用起来，否则不使用索引。)
			4.采用 not in, not exist
			5.链表时候，两个字段的编码不同
			
		binlog是用来做point-in-point的恢复和主从复制的，由数据库上层生成，是sql执行的逻辑日志，事务提交完成后进行一次写入。
		
		事务隔离的实现基于锁机制和并发调度 其中并发调度使用的 MVCC （多版本并发控制），通过保存修改行的旧版本信息来支持并发 致性读和回滚等特性
			MVCC(多版本并发控制), 通过版本号来减少锁的争用
			
		
		共享锁：又称读锁，获取共享锁的事务，只能读不能改。
		
		排他锁：又称为写锁，事务对一个数据加上排他锁后，其他事务不能再对此数据加任何其他类型的锁。
				能读能写，innoDB的增删改会默认加排他锁。典型的是select for update加排他锁
		
		InnoDB 行级锁是基于索引实现的，如果查询语句未命中任何索引，那么 InnoDB 会使用表级锁
		InnoDB 行级锁是针对索引加的锁，不针对数据记录，因此即使访问不同行的记录，如果使用到了相同的索引键仍然会现锁冲突
		
		行级锁是一种排他锁，防止其他事务修改此行；在使用以下语句时，Oracle 会自动应用行级锁：
		1.INSERT、UPDATE、DELETE、SELECT … FOR UPDATE [OF columns] [WAIT n | NOWAIT];
		2.SELECT … FOR UPDATE 语句允许用户一次锁定多条记录进行更新
		3.使用COMMIT 或 ROLLBACK 语句释放锁。
		
		
		
		mysql 索引列不能参与计算：带函数的查询不参与索引。
		尽量的扩展索引，不要新建索引
		
		mysql的一些建议
			避免死锁，多个程序尽量约定以相同的顺序访问表
			
			垂直分区的缺点在于主键会出现冗余，需要管理冗余列，并会引起 Join 操作，可以通
			过在应用层进行 Join 来解决 此外，垂直分区会让事务管理变得复杂

			水平分区最好分库
			水平分区能够支持非常大的数据量存储，应用端的改造也较少，但分片事务难以解决，
			跨节点 Join 性能差，逻辑复杂

			为null的列是无法参与到查询中的
			id使用bigint即可，足够使用
			
			count(column)统计column不为null的记录数， null不在统计范围内
			避免多表join 尽量使用冗余策略解决联表问题
			批量插入代替单挑插入
				
				
			数据库的访问规则
			注意分表后的表名问题
				固定hash规则
					分库分表中的常用方法就是取模,分x张表就mod数值x
					
				映射表法
					将id和对应的表和库建立一个关系表，每次查询关系表，确定数据库和表。这种方法，一般用来以上面的规则为基础，作为配合使用。
					
				自定义规则，
					自定义函数，确定数据库的访问规则
					
					
		当我们使用读写分离、缓存后，数据库的压力还是很大的时候，这就需要使用到数据库拆分了。
		垂直水平拆分，类似业务拆分和数据拆分
			按照合理拆分规则拆分，join操作基本避免跨库。
			分片事务一致性难以解决。
			3. 数据多次扩展难度跟维护量极大。
			4. 跨库join性能较差
		
		拆分单额问题
			两张方式共同缺点
			1. 引入分布式事务的问题。
			2. 跨节点Join 的问题。
			3. 跨节点合并排序分页问题。
			
		拆分原则
			1. 尽量不拆分，架构是进化而来，不是一蹴而就。(SOA)
			2. 最大可能的找到最合适的切分维度。
			3. 由于数据库中间件对数据Join 实现的优劣难以把握，而且实现高性能难度极大，业务读取 尽量少使用多表Join -尽量通过数据冗余，分组避免数据垮库多表join。
			4. 尽量避免分布式事务。
			5. 单表拆分到数据1000万以内。
			切分方范围、枚举、时间、取模、哈希、指定等
		
		分表的取模扩容处理
			方案1：按照用户取模，
			带来的问题：后续扩容困难
		方案2：按用户ID范围分片（1-1000万=分片1，xxx）
			带来的问题：用户活跃度无法掌握，可能存在热点问题
		方案3：按省份区县地区枚举，这里也可能是年月啥的划分。
			数据分配不一定均匀		
					
					
					
					
			*****数据库的迁移，关键一点是要关心迁移过程中可能出现的数据变化。*****
			当然最稳就是先停机，再迁移，最后切换。
			参考的方案，总的原则就是，1.正常迁移，2.记录增量 3.暂停待迁移的数据写操作，4.处理完增量，切换规则。
					这里好处是停止写操作的时间只是在处理增量的操作中。减少了停机时间。
					这里就是用日志，记录增量变动，针对变动停机修复。
						1.数据迁移时，记录增量日志，迁移技术后，对增量变化进行处理。
						2.最后可以吧要被迁移的数据写暂停，保证增量日志都处理完毕后，再切换规则，放开所有的写，完成迁移工作。
					
					
		
		mysql-connector  mysql时区
			serverTimezone=UTC   在mysql-connector高版本8.x中会有时区的问题，方案要么配置serverTimezone=Asia/Shanghai,要么换用低版本的mysql-connector
		
			com.mysql.jdbc.Driver和mysql-connector-java 5一起用。
			com.mysql.cj.jdbc.Driver和mysql-connector-java 8 一起用。	需要配置市区serverTimezone=GMT%2B8
				
		TIMESTAMPDIFF(MINUTE,'2020-08-27 10:38:00','2020-08-27 11:38:00')  			计算两个时间差
		

		mysql扩展
			服务化的增加一个优点，单个系统复杂性降低，减轻了多分支开发带来的分支合并冲突解决的麻烦。
			分库分表  这里需要考虑到扩容的情况，这个是避不开的问题
				拆表的数量一般是2的n次方
				分表解决的是单表数据量过大带来的查询效率的下降问题
				单纯的分库和分表 都是可以通过单独的取模进行分
				比如根据user_id划分256张表，那么就是user_id%256  这0-255共256张表
				
			如果同时进行分库和分(这种数据量就很大了，分库还能缓解单库的写操作压力)
				一种分库分表的路由策略
					1.中间变量=user_id%(库数量*每个库的表数量)
					2.库=取整(中间变量/每个库的表数量)
					3.表=中间变量%每个库的表数量
					
			带来的几个问题，1.引入多表关联查询，2.必须执行路由字段，3.扩容问题，4.分布式事务
			
			
			分布式事务，落地实现，目前原理居多
			分布式事务（Distributed Transaction）包括事务管理器（Transaction Manager）和一个或多个支持 XA 协议的资源管理器 ( Resource Manager )。
			我们可以将资源管理器看做任意类型的持久化数据存储；事务管理器承担着所有事务参与单元的协调与控制
			
			二阶段提交(Two-phaseCommit)，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)的反馈情报决定各参与者是否要提交操作还是中止操作。
				协调者很重要
				1. 准备阶段(这里写日志不提交事务)
				事务协调者(事务管理器)给每个参与者(资源管理器)发送 Prepare 消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的 redo 和 undo 日志，但不提交，到达一种“万事俱备，只欠东风”的状态。
				2. 提交阶段
				如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)
			
			
			失败情况
				在XA的第一阶段，如果某个事务参与者反馈失败消息，说明该节点的本地事务执行不成功，必须回滚。于是在第二阶段，事务协调节点向所有的事务参与者发送Abort请求。接收到Abort请求之后，各个事务参与者节点需要在本地进行事务的回滚操作，回滚操作依照Undo Log来进行。
			
			二阶段无法解决的问题（数据状态不确定）
				4、协调者再发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。
			
			2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。
			
			三阶段提交（Three-phase commit），是二阶段提交（2PC）的改进版本。
				与两阶段提交不同的是，三阶段提交有两个改动点。引入超时机制。同时在协调者和参与者中都引入超时机制。在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。
				也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。
							
						
				PC最关键要解决的就是协调者和参与者同时挂掉的问题，所以3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。在第一阶段，只是询问所有参与者是否可可以执行事务操作，并不在本阶段执行事务操作。当协调者收到所有的参与者都返回YES时，在第二阶段才执行事务操作，然后在第三阶段在执行commit或者rollback
			
				XA三阶段提交在两阶段提交的基础上增加了CanCommit阶段，并且引入了超时机制。一旦事物参与者迟迟没有接
				到协调者的commit请求，会自动进行本地commit。这样有效解决了协调者单点故障的问题。但是性能问题和不一致的问题仍然没有根本解决。
			
			
			1.协调者询问事务是否可以执行，这一步不会锁定资源
			2.参与者反馈，协调者接收到所有YES指令
			3.协调者发送事务执行指令，这一步锁住资源
			4.参与者执行事务操作，反馈状态，协调者收到所有参与者的ACK响应，通知所有参与者执行事务的commit
			5.执行commit操作，反馈状态
			
			
			简单概括一下就是，如果挂掉的那台机器已经执行了commit，那么协调者可以从所有未挂掉的参与者的状态中分析出来，并执行commit。如果挂掉的那个参与者执行了rollback，那么协调者和其他的参与者执行的肯定也是rollback操作。
			所以，再多引入一个阶段之后，3PC解决了2PC中存在的那种由于协调者和参与者同时挂掉有可能导致的数据一致性问题。
			
			tcc
				逻辑类似2pc，但实现方式是代码层面人为实现
			mq
		
			
			
			
			最大努力通知型（多次尝试）  事务
			场景充值和账户
			最大努力通知方案是分布式事务中对一致性要求最低的一种，适用于一些最终一致性时间敏感度低的业务。
			最大努力通知方案需要实现如下功能：
				1、消息重复通知机制。   账户系统没收到就一直发送，收到返回ack
				2、消息校对机制。		账户系统先查询充值结果，确认ok了再进行更新
			建次级事务交给mq去消费，也没啥，
		
			1.2PC/3PC：依赖于数据库，能够很好的提供强一致性和强事务性，但相对来说延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。
			2.TCC：适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。
			3.本地消息表/MQ 事务：都适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账/校验系统兜底。
			4.Saga事务：由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。 Saga 相比缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。Saga 事务较适用于补偿动作容易处理的场景

			

		mysql 可以开启慢查询，定位比较低效的sql语句
			性能优化的几个方法
				单例模式的使用场景，对于io处理，文件配置解析等一些消耗资源的操作，需要限制创建，公用一个。
				future模式，发送处理，然后执行其他任务，后取结果
				使用线程池
				
				结果缓存  springcache
				
				mysql  explain中的type=const  表示查询结果最多匹配一行(只有在使用了主键和唯一索引情况下，进行常数值比较时查询的type才为const)
				
				索引失效的场景
					1.当查询的列不是独立的，而是表达式或者函数一部分时，将无法使用该列的索引。
					2.当查询的列需要进行模糊匹配时，索引失效，全表扫描
				
				索引的最左原则
				
				密码安全
					用户密码虽然经过单向的hash,如md5加密了。但是还是可以彩虹表破解，这是hash+salt就相对安全了
				
				上传文件
					注意	文件类型白名单，显示上传大小，上传后重命名
					
		java.sql.Date		这个是数据库中的类型date，指精确到年月日
		java.util.Date		这个是数据库中的类型dateTime，精确到时分秒，一般开发用这个
		
		
		
		DISTINCT 和 group by
		DISTINCT 简单用来去重列值，而group by更多是用来统计，以某个维度来统计数据
		DISTINCT 用在所有的检测列之前，并且 它是作用于 所有列，不能部分使用
		distinct简单来说就是用来去重的，而group by的设计目的则是用来聚合统计的.
		单纯的去重操作使用distinct(可单列或者多列，多列相当于联合唯一)，速度是快于group by的。
		group by 必须在查询结果中包含一个聚集函数，而distinct不用。 常用来统计数据
		聚合函数 ：AVG、MAX、MIN、SUM、COUNT
		
		
		mysql优化
			SELECT * FROM mytable t
			WHERE (CASE aPARAM WHEN 'a' THEN t.a = aID WHEN 'b' THEN t.b = aID WHEN 'c' THEN t.c = aID ELSE TRUE END)
			等价于
			SELECT * FROM mytable t
			WHERE ( t.a = aID AND aPARAM = 'a') OR (t.b = aID AND aPARAM = 'b') OR (t.c = aID AND aPARAM = 'c') OR (aPARAM NOT IN ('a','b','c'))

			这里CASE WHEN 可以和OR  或者union all 相互转换，当case when影响到索引使用的使用，可以使用union all 来替换，兼容索引使用
			应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描
			尽量避免在WHERE子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描
			启慢查询日志来找出较慢的SQL
			
			
			
			Lock wait timeout exceeded; try restarting transaction场景
			在5.5中，information_schema 库中增加了三个关于锁的表（MEMORY引擎）；
				innodb_trx ## 当前运行的所有事务
				innodb_locks ## 当前出现的锁
				innodb_lock_waits ## 锁等待的对应关系
			
				SELECT * FROM information_schema.innodb_trx
				SELECT * FROM information_schema.innodb_locks
				SELECT * FROM information_schema.innodb_lock_waits
						
				SHOW ENGINE INNODB STATUS			查看mysql事务处理列表，可查询死锁和事务相关的信息，
				SHOW FULL PROCESSLIST				查看所有mysql进程id
			
			kill掉有锁表的进程   kill trx_mysql_thread_id         innodb_trx中的trx_mysql_thread_id字段，这里就是对应的上面的进程id 
				对应的就是mysql的Connection is closed
			

			库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止拼音英文混用
			不要用外键，影响性能
			禁止使用TEXT、BLOB类型解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能
			必须使用varchar(20)存储手机号解读
			建立组合索引，必须把区分度高的字段放在前面解读：能够更加有效的过滤数据
			禁止使用SELECT *    会增加无意义的字段消耗，不能有效的利用覆盖索引
			禁止使用属性隐式转换解读：SELECT uid FROM t_user WHERE phone=13800000000 会导致全表扫描，而不能命中phone索引，猜猜为什么？（这个线上问题不止出现过一次）
			禁止在WHERE条件的属性上使用函数或者表达式解读
			禁止负向查询，以及%开头的模糊查询解读：
				a）负向查询条件：NOT、!=、<>、!<、!>、NOT IN、NOT LIKE等，会导致全表扫描
				b）%开头的模糊查询，会导致全表扫描	
					like 模糊查询中，右模糊查询（321%）会使用索引，而%321 和%321%会放弃索引而使用全局扫描。
				
			禁止使用OR条件，必须改为IN查询解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费更多的CPU帮助实施查询优化呢
				where 字句有 or 出现还是会遍历全表。
			
			
			聚集索引和非聚集索引的根本区别是表中记录的物理顺序和索引的排列顺序是否一致。
			
			最左前缀匹配原则，mysql 会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，
			理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒：效果一样
			mysql在处理where条件是，优化查询条件一批匹配索引的位置，直到遇到范围查询(>、<、between、like)就停止匹配，也就是说mysql会自动优化查询条件来匹配索引，来满足使用索引的条件。
			where a,b  和 where b，a是一样的
			
			*****
			这里可以看explain中的key_len长度， 个位数小差别是mysql的平衡量
				select * from table_name where a = '1' and c = '3' 
				如果不连续时，只用到了a列的索引，b列和c列都没有用到 
				结果显示是，使用到了(a,b,c)索引，但是key_len是一个字段的长度，说明，只有a列走了索引，c没走，走了几个字段的索引，看key_len大概。
			*****
			
			RR级别下防止幻读
				快照读：使用MVCC防止幻读
				当前读：使用间隙所防止幻读
				
				
			
			
			
			
			
			
			mysql索引结构，sql优化，limit优化，回表，索引覆盖
				参看链接
				https://blog.csdn.net/mu_wind/article/details/110128016
				https://blog.csdn.net/sinat_14913533/article/details/115537106  
				https://www.cnblogs.com/myseries/p/11265849.html 

				叶子节点(深度为0的节点)才会有data，其他都是索引(附带指针指向了下个节点)
				聚集索引 和 普通索引的区别
					本质区别：表记录的排列顺序和与索引的排列顺序是否一致
					聚集（clustered）索引，也叫聚簇索引，聚集索引clustered index(id), 非聚集索引index(username)
					类似字典中的拼音就是聚集索引，偏旁部首查汉字，就是非聚集索引。
					
					1.聚集索引：索引顺序和表中数据排序一致(聚集索引的顺序就是数据的物理存储顺序) ,一个表只能有一个聚族索引(因为数据在物理存放时只能有一种排列方式)。 
						
						聚集索引的叶子节点就是对应的数据节点，叶子节点中保存存了索引列和具体的数据
						InnoDB中,把数据data存放在B+树中的叶子节点上(存储了表中行数据)，而B+树的键值就是主键(索引和数据行保存在同一个B-Tree中)。命中叶子节点可以直接取出数据，相比非聚簇索引需要第二次查询
						
						InnoDB默认对主键建立聚簇索引。
							如果你不指定主键，InnoDB会用一个具有唯一且非空值的索引来代替。
							如果不存在这样的索引，InnoDB会定义一个隐藏的主键，然后对其建立聚簇索引。
						
					2.非聚集索引(辅助索引，普通索引，二级索引)： 索引顺序与物理存储顺序不同
						除了聚集索引以外都是非聚集索引，细分：普通索引，唯一索引，全文索引
						
						叶节
						B+Tree的叶子节点仍然是索引节点，其上的data，不是数据本身，而是数据存放的地址（不是指向行的物理指针，而是行的主键值）。b+中非叶子节点只有键值，叶子节点包含完成数据。
						回表二次查询
							查找时，存储引擎需要在二级索引中找到相应的叶子节点，获得行的主键值，然后使用主键去聚簇索引中查找数据行，这需要两次B-Tree查找，检索两次索引
						
						查询过程	扫描两次索引 
							要查询name = C 的数据，其搜索过程如下：
							1.辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。
							2.使用主键在主索引B+树种再检索一次，最终到达叶子节点定位到行数据
							
				回表查询	先从普通索引定位主键值，再从聚集索引定位行数据，一般是查询列不在索引覆盖范围情况下				
				覆盖索引
					定义：如果一个索引覆盖所有需要查询的字段的值。即索引的叶子节点中已经包含要查询的数据。
					只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表。
					从非主聚簇索引中就能查到的想要数据，而不需要通过回表从主键索引中查询其他列
					尽量不要使用select *
					实现方法是：将被查询的字段，建立到联合索引里去。    使用联合索引  一定要使用索引并且带上查询列
					使用场景：Limit分页查询
					
					Using where' 在非索引的列上可能仍然会做全表扫描，where相对影响会大点
						EXPLAIN
						SELECT req_code FROM require_info WHERE req_title='测试鱼食――先发后托6'
						这种使用了where 后面没有索引，但是查询列是单个索引列，这种不走索引覆盖，走的全表

						EXPLAIN
						SELECT user_id FROM require_info 
						这种不加where的直接查询，user_id会走索引覆盖
						索引覆盖，必须要使用索引

						EXPLAIN
						SELECT req_code,req_title FROM require_info
						这种没有使用索引，不走索引覆盖
					
				mysql中的extra(有没有使用索引要看type)
					using idex						使用了覆盖索引				避免访问了表的数据行，效率不错
					using where 					只表示发生了where过滤		过滤条件字段无索引；如果type是all 就是扫全表， 常见的优化方法为，在where过滤属性上添加索引
					using where using index			发生了where过滤，索引覆盖了查询
					extra为null， type为ref，		表明虽然用到了索引，但是没有索引覆盖，产生了回表，常见于，where后面使用索引，但是查询的列回表了
						有点类似Using index condition
						
				升级联合索引(name, sex)，联合索引也是一棵B+树，不同的是联合索引的键值数量不是1，而是大于等于2.相当于创建了多个索引
					select id,name … where name=‘shenjian’;
					select id,name,sex … where name=‘shenjian’;
					都能够命中索引覆盖，无需回表。

				索引失效场景
					sql查询中使用like %a 时候索引会失效，因为%表示全匹配，如果已经全匹配就不需要索引，还不如直接全表扫描。a%不确定
					IN可以使用索引， NOT IN 无法使用索引
					查询条件中使用函数，索引将会失效

				B+ 树
					B+ 树非叶子节点上是不存储数据的，仅存储键值，不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，
					B+ 树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的。


				索引覆盖来优化SQL？
					全表count查询优化
					列查询回表优化				单列索引(name)升级为联合索引(name, sex)，即可避免回表
					分页查询					将单列索引(name)升级为联合索引(name, sex)，也可以避免回表

				索引覆盖


				SELECT * FROM require_info   是按照id asc
				SELECT id FROM require_info		排序就乱了

				大数据量的limit优化思路
					1.尽可能的减少筛选出来的数据，不要扫描大量无用行。想过办法通过索引覆盖查询来查出必要的需要扫描的行，然后再去扫描实际的数据行
					2.避免回表(常发生在查询的列不在索引覆盖范围内)
					3.除了使用覆盖索引，优化查询速度外，我们还可以使用 Redis 缓存，将热点数据进行缓存储存。
					
					方法：1.采取的是限制分页(只查看前几千条数据)和增加缓存(记录上次查询的最大id，带到下次查询中筛选)。
						  2.子查询的分页方式或者JOIN分页方式,用到的是索引覆盖，这两种逻辑一样，功能也一样
							想过办法通过索引覆盖查询来查出必要的需要扫描的行，然后再去扫描实际的数据行
							select orderNo,ctime from `order` order by orderNo limit 2000000,20
							select orderNo,ctime from `order` inner join ( select orderNo from `order` order by orderNo limit 2000000, 20 ) as o using(orderNo);    //using等价于join操作中的on
							分析：inner join的表通过索引覆盖查询直接通过索引找到了需要返回的数据行，然后order表通过与这个派生表进行关联，只扫描20行数据就可以了(第一种跨越大量数据块并取出)
							子查询是在索引上完成的，而普通的查询时在数据文件上完成的
							*****对limit的优化，不是直接使用limit，而是首先获取到offset的id，然后直接使用limit size来获取数据。*****
							  
							SELECT * FROM tableName ORDER BY id LIMIT 500000 , 2
							
							-- 基于排序做条件过滤，有一定限制，主键id必须有序，同样适用创建时间等其他字段，但是必须要设置索引
							SELECT * FROM tableName
							WHERE id >= (SELECT id FROM tableName ORDER BY id LIMIT 500000 , 1)
							LIMIT 2;
							时间: 0.274s
							
							SELECT * FROM tableName AS t1
							JOIN (SELECT id FROM tableName ORDER BY id desc LIMIT 500000, 1) AS t2
							WHERE t1.id >= t2.id ORDER BY t1.id  LIMIT 2;
							时间: 0.278s
						
							-- 基于索引覆盖，这种更为高效
							SELECT * FROM tableName AS t1
							JOIN (SELECT id FROM tableName ORDER BY id desc LIMIT 500000, 2) AS t2
							WHERE t1.id = t2.id;
			
			 
			子查询		将子查询语句包装成外查询的条件		将select * from a where id>(select id from a where id=1)
			自联结		自己关联自己的表结构				实际使用场景是省市区   自联结比子查询快


			子查询和join 目前看可以转换
			内连接：(INNER) JOIN，返回两张表都匹配上的行
			join = inner join = from a, b where 过滤条件
			
	
	
71.nexus 搭建
	nexus仓库类型  
		Group：这是一个仓库聚合的概念，访问顺序取决于配置顺序
		Hosted:私有仓库，专门用来存储我们自己生成的jar文件  
		   Snapshots：本地项目的快照仓库  
		   Releases： 本地项目发布的正式版本  

		Proxy:公网上发布的jar 例如：spring
		  Central：中央仓库

		1.下载版本，cd目录下 nexus.exe/run  执行 访问
		2.创建相应的maven-proxy库，相应的group库。maven的setting.xml中配置对应的私服即可。
		当项目去私服下载的时候，私服没有会自动远程更新到私服下，然后拉取(实测正确)
			
			
		配置远程库时nexus的aliyun_proxy，总结就是nexus会远程更新
		1.本地mirror配置了central的代理库  aliyun，私服是远程aliyun代理，那么被本地的拦截了,nexus不会更新jar
		2.本地没有配置mirror，nexus下会更新jar
		3.本地配置mirror关于远程私服aliyun，所有请求都会到私服上，nexus会更新
		
		
		jar存放路径
		它的默认路径在\nexus-3.2.1-01-win64\sonatype-work\nexus3\blobs\default\content下面
		
		
		
		https://blog.csdn.net/m0_38001814/article/details/89494078   nexus 介绍
		
		
		发布机远程没有的会优先使用本地的jar
		
		子项目在maven库中依赖缺少父pom，上传一个父的pom.xml到nexus中即可。选择pom上传
		nexus中的可以把别的库中的jar文件夹直接复制过去，然后需要发布下或者退出nexus后台重新进入，然后就刷新了

		nexus proxy仓库可用的远程地址，https://repo1.maven.org/maven2/
		
		
		
		****
			Nexus私服的Release仓库不允许上传SNAPSHOT版本，会报错，而SNAPSHOT仓库压根就不提供Web界面上传功能。
			手工上传特定的快照jar,SNAPSHOT,到nexus库中
			光copyjar到nexus没用，还需要相关的xml中配置了相关的version等信息
			其中repositoryId是maven的setting.xml中配置的server节点的serverId(server中的配置的是nexus admin的登录账号和密码,获得上传的用户权限)
			mvn deploy:deploy-file -DgroupId=com.xiaoyuer.op -DartifactId=xye-open-dmo -Dversion=1.1.5-SNAPSHOT -Dpackaging=jar -Dfile=D:\k8sjar\xye-open-dmo-1.1.5-SNAPSHOT.jar -Durl=http://192.168.6.251:8087/nexus/content/repositories/K8sJar/ -DrepositoryId=my-nexus-snapshot
		****
		
		调k8s遇到的jar问题
		
			dubbo 2.8.4原因是没哟同时上传jar，这种关联的jar尽量不要手工上传，update官方的标准jar
			调试k8s的容器，其中dubbo 2.8.4x 手工上传后总是有问题，原先的mavenjar alibaba文件夹可以。替换251也不行，这种很可能是不能单独一个jar 还有其他相关的jar需要关联的。
			所以后来改添加snapshot进发布库，这是可行的，然后251nexus后台目录中删除jar和复制jar到k8sjar,不能立刻nexus页面显示，需要发布一次后才能更新
			
			netpay中的hessian接口也是，使用的是xye-open-dmo的手工上传的copy 的jar，导致报错，应该使用最新的jar，因为自己copy过来的肯定没有install后的实时全面，手工上传容易出错。
			因为可能open-dmo是从打包好的lib下直接拖过来的，但是install本身会传递一个依赖关系。到了lib下，jar已经update完了，不一定存在这种依赖关系。所以copy，不一定存在这种依赖关系。

		maven deploy，上传，部署
			-e 	打印完成的错误日志
			-U 	强制检查快照版本更新，没有就默认是天为检查
				老版本
				<distributionManagement>
					<repository>
						<id>my-nexus-releases</id>	//maven得setting.xml中配置的server验证信息
						<url>http://192.168.6.251:8087/nexus/content/repositories/releases/</url>
					</repository>
					<snapshotRepository>
						<id>my-nexus-snapshot</id>
						<url>http://192.168.6.251:8087/nexus/content/repositories/snapshots/</url>
					</snapshotRepository>
				</distributionManagement>
				分环境版本
				 <profiles>
					 <profile>
						<id>pre</id>
						<distributionManagement>
							<snapshotRepository>
								<id>my-nexus-snapshot</id>
								<url>http://192.168.6.251:8087/nexus/content/repositories/pre/</url>
							</snapshotRepository>
						</distributionManagement>
					</profile>
				</profiles>
	
			maven deploy 错误
				1、部署的仓库类型错误
					nexus的repository分三种类型：Hosted、 Proxy和Virtual，另外还有一个repository group(仓库组)用于对多个仓库进行组合，部署的时候只能部署到Hosted类型的仓库中。
				2、部署的仓库部署策略为禁止部署
					releases仓库的部署策略默认为禁止部署，如果要部署到这个仓库中需要修改部署策略为Allow Redeploy
				3、仓库发布版本与部署的项目发布版本不相符
					项目的发布版本如果为<version>1.0-SNAPSHOT</version>，则不能部署到发布版本为Release的仓库中
					
		
72.跨域		https://blog.csdn.net/itcats_cn/article/details/82318092
	跨域问题是针对JS和ajax的，html本身没有跨域问题
	JavaScript的"同源策略"，即只有 协议+主机名+端口号 (如存在)相同
	请注意：localhost和127.0.0.1虽然都指向本机，但也属于跨域。
	1、响应头添加Header允许访问											类似文件 CorsFilter，后端filter中设置设置res.setHeader("Access-Control-Allow-Origin", "*");
	2、jsonp 只支持get请求不支持post请求
	3、httpClient内部转发
	4、使用接口网关――nginx、springcloud zuul   (互联网公司常规解决方案)     同域名访问，用nginx转发请求
	
	
	
	
	在前后分离的架构下，跨域问题难免会遇见比如，站点 http://domain-a.com 的某 HTML 页面通过  的 src 请求 http://domain-b.com/image.jpg。网络上的许多页面都会加载来自不同域的CSS样式表，图像和脚本等资源。

出于安全原因，浏览器限制从脚本内发起的跨源HTTP请求。 例如，XMLHttpRequest和Fetch API遵循同源策略。 这意味着使用这些API的Web应用程序只能从加载应用程序的同一个域请求HTTP资源，除非使用CORS头文件。
	
	@crossorigin可以针对单个的路径实现，要求Spring4.2及以上的版本


	公司在几年前就采用了前后端分离的开发模式，前端所有请求都使用ajax
	传统结构项目中，shiro从cookie中读取sessionId以此来维持会话，在前后端分离的项目中（也可在移动APP项目使用），我们选择在ajax的请求头中传递sessionId，

	
	
73.Websocket 电签  上传图片
	Websocket之前， long poll 和 ajax轮询。  服务端能主动调用客户端
程序设计中，这种设计叫做回调
1.前段开启连接，需要带上userid。前端会将session传过去，通过参数绑定唯一的userid
2.后端配置WebSocketServer，通过onOpen将session和userid绑定。WebSocketServer中奖session和userid绑定。
3.发送消息就调用对应的server发消息

打开websocket连接开启,生成二维码，扫码跳转地址，打开另一个websocket连接(和第一个同参数，但是不同session)，签名发送消息，前端接收。

引入jquery.qrcode.min.js插件，然后直接调用即可
$('#qrcode').qrcode({
		render: 'canvas', //table,canvas方式
		width: 120, //宽度
		height:120, //高度
		text: "http://www.runoob.com" //二维码内容
	});

  手写功能：
  jSignature.js  然后var $sigdiv = $("#signature").jSignature({'UndoButton':true});初始化插件即可
  
  
  函数放script中可以直接执行，比如alert 和 bind等
  $(document).ready(function() {}

List tempList = upload.parseRequest(request);
InputStream is = item.getInputStream();
然后outstream输出即可

		
		
	指令重排也是有限制的，即不会出现下面的顺序，进行重排时候，必须考虑到指令之间的数据依赖性
	
	编译期重排序的典型就是通过调整指令顺序，做到在不改变程序语义的前提下，尽可能减少寄存器的读取、存储次数，充分复用寄存器的存储值。
		只要程序的最终结果等同于他在严格的顺序化环境中执行的结果那么上诉的所有行为就是被允许的。
	
74.list 转逗号分割
	StringUtils.join(orderPayStrs.toArray(), ",");
	
	function aa(){alert("1")}; window.setTimeout(aa,0);   js 定时执行
	
	只有静态内部类能被static修饰
	在Dao层中,可以使用conn.setAutoCommit(false)   默认为true,可以阻止自动提交
	
	Java集合类HashMap内部就有一个静态内部类Entry。Entry是HashMap存放元素的抽象，HashMap 内部维护 Entry 数组用了存放元素，但是 Entry 对使用者是透明的。像这种和外部类关系密切的，且不依赖外部类实例的，都可以使用静态内部类。
	
	定义在方法中的类，就是局部类。如果一个类只在某个方法中使用，则可以考虑使用局部类。
	
75.treadlocal 是一次线程的操作，存一个对象 key-value，可以存一个map，这样可以多存几个值，用完记得移除
	线程变量会有线程挂起？(避免资源的浪费)
		挂起之后，线程的挂起操作实质上就是线程进入"非可执行"状态下，在这个状态下CPU不会分给线程时间片，进入这个状态可以用来暂停一个线程的运行。
		线程挂起后(阻塞状态 cpu不分配运行时间)，可以通过重新唤醒线程来使之恢复运行。

	ThreadLocal无法在父子线程之间传递--内容保存在线程对象中，子线程无法继承父线程的内容
	父线程的概念(创建线程,类似main中开线程)，只是一种逻辑称呼，创建线程的当前线程就是新线程的父线程，新线程的一些资源来自于这个父线程
	
	每个线程中都有一个自己的 ThreadLocalMap 类对象(t.inheritableThreadLocals)，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。
	2. 将一个共用的 ThreadLocal 静态实例(static final)作为 key，将不同对象的引用保存到不同线程的ThreadLocalMap 中，然后在线程执行的各处通过这个静态 ThreadLocal 实例的 get()方法取得自己线程保存的那个对象，避免了将这个对象作为参数传递的麻烦。
	3. ThreadLocalMap 其实就是线程里面的一个属性，它在 Thread 类中定义ThreadLocal.ThreadLocalMap threadLocals = null;
	
	
	//这里是将value存在了每个线程的ThreadLocalMap中，key是共用的 ThreadLocal静态实例，value是当前线程的值
	//private static final ThreadLocal<ApplicationContext> opContextHolder = new ThreadLocal<ApplicationContext>();
	//ThreadLocalMap，当前线程为key，存放value
	//ThreadLocal
	public void set(T value) {
        Thread t = Thread.currentThread();
        ThreadLocalMap map = getMap(t);
        if (map != null)
            map.set(this, value);
        else
            createMap(t, value);
    }
	
	 void createMap(Thread t, T firstValue) {
        t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue);
    }
	
	op系统gateway传值
		重定向redirect，服务器端在响应第一次请求的时候，让浏览器再向另外一个URL发出请求，从而达到转发的目的。它本质上是两次HTTP请求，对应两个request对象。
		转发forward，客户端浏览器只发出一次请求，Servlet把请求转发给Servlet、HTML、JSP或其它信息资源，由第2个信息资源响应该请求，两个信息资源共享同一个request对象。	
	
		1.将参数放进threadlocal中
		2.在接下来的另一个方法中，取出threadlocal中存的data信息，放进session中
		3.重定向后从session中取出data信息。
		那么这里threadlocal的使用，只不过是为了再后面的方法中不带参数的传递信息，后面方法直接取线程变量，不需要由前传到后。本质上还是从session中取数据。
		重定向后，因为是同一个域名，www.xiaoyuer.com,所以是同一个sessionid
		这其实可以直接存session，然后重定向后获取。

			1.存入线程变量
			private void setContext(Map<String, Object> parameters, OpPartner opPartner, OpService opService,OpPartnerService ops)
			{
				ApplicationContext.getContext().setParameters(parameters);
				ApplicationContext.getContext().setPartnerService(ops);
				...
			}

			2.存入session
			private GenericResult<Object> invokePageService(OpService opService)throws IOException
			{
				String uuid = UUID.randomUUID().toString();
				ApplicationContext context = ApplicationContext.getContext();
				HttpServletRequest request =((ServletRequestAttributes)RequestContextHolder.getRequestAttributes()).getRequest();
				request.getSession().setAttribute(uuid, context);
				request.getSession().setAttribute(CommonConstant.PAGE_SERVICE_SESSION_KEY, uuid);
				return result;
			}
			
			3.
			重定向之后  重定向之后，
			uuid = String.valueOf(request.getSession().getAttribute(CommonConstant.PAGE_SERVICE_SESSION_KEY));
			context= (ApplicationContext)request.getSession().getAttribute(uuid);
			

76.freemarker静态化生成html
	// 当前系统绝对路径
	String ftlPath = session.getServletContext().getRealPath("/WEB-INF/ftl");
	config.setDirectoryForTemplateLoading(new File(templatePath));这个使用的是绝对的路径
	 
	to  只要指定文件名即可
	generateHtml  中奖常用的系统变量放进入，生成静态化文件html，使用的是原先的ftl文件
	Template template = config.getTemplate(templateName, "UTF-8");
	// 合并数据模型与模板
	FileOutputStream fos = new FileOutputStream(fileName);
	Writer out = new OutputStreamWriter(fos, "UTF-8");
	template.process(root, out);  //第一个是map参数，第二个是需要输出的文件路径，可以不存在会默认写一个出来

	静态网页化之提高速度  不管是asp、php、jsp、.net等动态程序，都需要读取调用数据库内容，才能显示数据，
	相对于流量比较大，就增加了数据库的读取次数，占用很大的服务器资源，影响网站速度。
	而采用网站做成静态的，直接除去了读取数据库的操作，减少了环节，提高了网站反映速度。
	 
	然后项目中是在BaseInfoInitServlet中初始化完成的静态页面生成
	 
	想法就是通过使用freemaker使用预先的ftl模板，然后生成静态的html文件，后面页面直接引用
	
	spring.freemarker.template-loader-path  只是freemarker将静态文件加载的配置，配置对应目录，其他前后缀正常加。目前不必关注，配置正常添加即可

	
77.mybatis
	mybatis的逆向在github的官方文档有相关的xml配置说明。
	List<Map<String, Object>> list = ppCalendarMapper.selectCalendar();    可以对象map化  xml用map接收

	@MapperScan扫描后无需添加@Mapper、@Repository等注解
		@Select("select * from bank_resc where id=#{id}")
		@Results({@Result(column = "bank_code",property = "bankCode"),@Result(column = "bank_name",property = "bankName")})
		BankResc selectByIdd(Integer id);	
			
		这种映射需要自己写，只适合简单的表，复杂的表不适用
		
		mybatis dmo使用自定义构造函数后，需要加一个默认的构造函数，否则会报错
		
		MyBatis 里面的 XML 文件中,使用 in 来查询时,不能直接将值塞过去,需要遍历放进去,然后在遍历取出 list 的数据时,不能使用 "#", 而是用 "$"(目前分页结合循环出现)

		xml中加减的时候，ua.account_fix = (ua.account_fix + #{param.freezeAmt}) 可以是一个负数
		
		requireInfoMapper.insertSelective(requireInfo);  直接requireInfo中返回id    useGeneratedKeys="true" keyProperty="id"  返回主键
		
		mybatis.mapper-locations   不是 mybatis.mapping-locations  cao
	
		dao，idea中的dao中的xml需要单独配置加载
		<resources>
				<resource>
					<directory>src/main/java</directory>
					<includes>
						<include>**/*.java</include>
						<include>**/*.xml</include>
					</includes>
					<filtering>false</filtering>
				</resource>
		</resources>

		数组：<if test="object!=null and object.length>0">
		参数为集合List：<if test="object!=null and object.size()>0">
		Ljava/lang/String;"就是表示类型String；
		[Ljava/lang/Object;"就是表示Object[]
				
				
		特别简单的项目，可以@Select直接写在一起
			@Select("select user_name from user_base_info where id=#{id}")
			String getuserInfobyid(Integer id);
				
			@Select("select * from user_base_info where 1=1 and id>1625")
			@Results({@Result(column = "user_name",property = "userName"),@Result(column = "nick_name",property = "nickName")})
			List<UserInfo> getuserinfos();

		一般也不传map类型
		MyBatis的传入参数parameterType类型分两种
		 1.基本数据类型：int,string,long,Date;
		 2.复杂数据类型：类和Map
		如何获取参数中的值:
		 1.基本数据类型：#{参数} 获取参数中的值
		 2.复杂数据类型：#{属性名}  ，map中则是#{key}		
						
				
				
				
78.设计思想  DelegatingFilterProxy   当不改变这个类本身又需要改变部分功能的，可以使用子类继承，然后覆盖实现


79.servlet API不支持“排除”URL模式 典型的就是filter等url的配置，是不支持正则的表达的

80.配置数据库连接的时候只有在xml中&才需要转义成&amp;,在properties中不需要转义
	配置数据源
	com.mysql.jdbc.Driver 是 mysql-connector-java 5中的，
	com.mysql.cj.jdbc.Driver 是 mysql-connector-java 6以后的

	使用springboot 整合spring-redis  ERROR redis.clients.jedis.HostAndPort-cant resolve localhost address    需要将hostname加到hosts文件中

	
81. 类加载路径
	Test.class.getClassLoader().getResource("")=Test.class.getResource("/")
	ClassLoader.getResource的path中不能以/开头，path是默认是从ClassPath根目录下进行读取的
	否则读取为null
	
	getResourceAsStream  这个读取的是流文件
	
	Class.getResource(String path)
	path不以’/'开头时，默认是从此类所在的包下取资源；
	path  以’/'开头时，则是从ClassPath根下获取；
	
	
	boot本地运行
		ROOTPATH = System.getProperty("user.dir");
		C:\Users\xiaoyuer\git\xye-netpay\xye-netpay-pom\xye-netpay-boot
		之前是获取类加载路径
		
		
	双亲委派机制(防止重复加载同一个.class)
		类加载器一般是三层的classLoader，是上下子父级关系，不是继承关系，只是调用逻辑关系
			BootstrapclassLoader:主要负责加载核心的类库(java.lang.*等)，$JAVA_HOME$/jre/lib，		负责加载 JAVA_HOME\lib 目录中的，
			ExtClassLoader：主要负责加载jre/lib/ext目录下的一些扩展的jar。		 					负责加载 JAVA_HOME\lib\ext 目录中的，
			AppClassLoader：主要负责加载应用程序的主函数类，										负责加载用户路径（classpath）上的类库

		*****
			向上递归检查，向下递归加载
			三层的classLoader负责的范围也不一样，这样依次检查，依次加载(在自己的范围内)。
		*****

		自底向上，挨个检查是否已经加载了指定类，如果已经载入，那么直接返回该类的实例的引用
		如果bootstapclassloader也未加载成功该类，那么会抛出异常。然后自顶向下挨个尝试加载。直到customclassloader,如果还未能加载，就抛出ClassNotFoundException给调用者。
		原则：1.自底向上检查类是否已经装载  2.自顶向下尝试加载类

		类请求递归委派到顶层，放父加载无法完成，子类才会去加载。这里双亲就是指的是父类。
		特殊场景：jdbc的spi加载，因为BootstrapclassLoader加载范围，只能委托子类加载实现。
		
		这种设计有个好处是，如果有人想替换系统级别的类：String.java。篡改它的实现，但是在这种机制下这些系统的类已经被Bootstrap classLoader加载过了，所以并不会再去加载，从一定程度上防止了危险代码的植入
	
		双亲委派机制，就是一个类只能被一个类加载器加载。
			逐层往上(接待就返回，没有就抛给上层，直到有一级接待或者上级返回没有接待且不应由上级接待，那么本层就会加载)
			
				jvm显式加载class文件到内存
					this.getClass.getClassLoader.loadClass()
					Class.forName()
					
			jvm主要提供三个Classloader
			
			实际测试 可以遍历this.getClass.getClassLoader 的parent即可，就能看出链
				Bootstrap ClassLoader 
					加载jvm自身工作需要的类，没有字符加载器，jvm控制，别人访问不到
				ExtClassLoader
					除了System.getProperty("java.ext.dirs")目录下是由其加载，其他都由AppClassLoader来加载
					
				AppClassLoader
					是上个加载器的子类，加载classpath目录下的类。
					是自实现类加载器的父加载器
	
			jvm表示一个类是否是同一个类的条件 1.完整类名是否一致，2.加载该类的classloader实例是否是同一个

	

82.普通文件的读取
	从classpath下加载
	InputStream resourceAsStream = this.getClass().getClassLoader().getResourceAsStream("cer/nihao_dev.txt");
		      InputStreamReader isr = new InputStreamReader(resourceAsStream);
		      BufferedReader br = new BufferedReader(isr);
		      String lineTxt = null;
		      while ((lineTxt = br.readLine()) != null) {
		        System.out.println(lineTxt);
		      }
		      br.close();
	}
	
	File in = new FileInputStream(file);  这个是直接路径加载。类似d盘某个路径
	
	/**获取文件路径*/
	private static String getRootPath() {
		String oriPath = DSGJ0001.class.getClassLoader().getResource("").getFile();
		if ("\\".equals(File.separator)) {
			oriPath = oriPath.substring(1, oriPath.length());
		} else if ("/".equals(File.separator)) {
			// linux
		}
		oriPath=oriPath.replace("%20", " ");
		return oriPath;
	}
	

83.日志路径
	这里是工程中以war工程为目录起点，后面可以单独测试下
	<property name="logbackpath" value="../logs/"></property>  作为一个jar包执行的话就是  jar的上级目录
	String path = System.getProperty("user.dir").replace("\\", "/");//获取当前应用所在目录
	path = path.substring(0, path.lastIndexOf("/"));			这里jar目录截取到d盘
	path = path + "/conf/xye-datasource.properties";
	logger.info("datasource-path:{}",path);
		
84.	Collections.sort(obj,new Comparator(){})实现排序


85.	@RunWith(SpringRunner.class)
	@springbootTest，这样能识别依赖注入的特性，有一些测试属性可以配置
	

86.	shiro
		*****
		主要的就是shirofilter  securitymanager，和realm(AuthorizingRealm )。
		*****

		https://www.cnblogs.com/yoohot/p/6085830.html   讲解的是shiro的各filter用法
			 
		一般通过继承EnterpriseCacheSessionDAO实现sessiondao的操作，这个是用来实现session的持久化。用来自定义session处理的，也可以用默认的
		有doCreate”、“doReadSession”、“doUpdate”和“doDelete”。其中只有doCreate是实现的，其它的都是没有实现的方法。
		
		单体的建议使用ecache的，redis适用于分布式的架构
		

		
		不过作为前后端分离项目,用户的信息及过期权限等信息依然是靠后端存储,以上依然涉及session,只不过是将产生的jsessionid当作token使用,使用redis存储而已.可以考虑使用jwt,彻底是后端无状态化;

		ThreadPoolExecutor是jdk中的线程池类
		ThreadPoolTaskExecutor这个类则是spring包下的，是sring为我们提供的线程池类
		
		

		@Override
		protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException {
		…
		SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo(loginUser,password,salt,getName());
		return authenticationInfo;  //这里返回的就是shiro中的principal

		也可以使用filterChainDefinitionMap配置权限
		
		shiro注解
			ShiroFilterFactoryBean 中配置相关的拦截配置(但如果接口多，每个接口url都要写一遍太麻烦。所以采用注解的方式，在每个controller方法上加注解。)

		
			/**
			 * 开启Shiro的注解(如@RequiresRoles,@RequiresPermissions),需借助SpringAOP扫描使用Shiro注解的类,并在必要时进行安全逻辑验证
			 * 配置以下两个bean(DefaultAdvisorAutoProxyCreator(可选 depends-on="lifecycleBeanPostProcessor" )和AuthorizationAttributeSourceAdvisor)即可实现此功能
			 * @return

			 
			 开启注解 DefaultAdvisorAutoProxyCreator 和 AuthorizationAttributeSourceAdvisor

		shiro――rememberme	 
			 
			shiro中使用remenberme中 boolen类型，true会在页面写一个cookie(remenberme)，value是经过加密存放的是用户的信息
			cookieRememberMeManager.setCipherKey(Base64.decode("fCq+/xW488hMTCD+cmJ3aQ=="));这个是内部加密用的key，会用这个key加密principal(SimpleAuthenticationInfo),回写到cookie中


			*****shiro会在每一次访问时都会创建一个subject，这是源码中有，并且绑定了线程变量*****
			DefaultSecurityManager.createSubject在创建subejct的时候就会调用resolvrPrincipals方法，
			这个当前方法内部会先从当前的session中获得的subject的principal，如果没有找到再从cookie找，调用的方法是getRememberedInedtity

	 
		 
		必须全部符合（默认不写或者在后面添加logical = Logical.AND）
		@RequiresPermissions(value={“stuMan:find_record_list”,“tea:find_record_list”})
		上面这种情况是默认当前对象必须同时全部拥有指定权限
		符合其中一个即可(logical = Logical.OR)    
		 
		 
		doGetAuthorizationInfo方法(封装了对应的权限内容)
		System.out.println("经试验：并不是每次调用接口就会执行，而是调用需要操作码（permission）的接口就会执行");
		
		shiro的菜单通配部分
		shiro的注解配置

		Subject currentUser = SecurityUtils.getSubject();
		
		currentUser.getprincipal实际上就是当时 SimpleAuthenticationInfo info = new SimpleAuthenticationInfo(user, password, getName());  中的这个存入的user对象
		
		暂时看是从session中拿到DelegatingSubject.RUN_AS_PRINCIPALS_SESSION_KEY作为key的value，那么还是从session中拿到的缓存的principal信息
		
		
		try {
			//在调用了login方法后,SecurityManager会收到AuthenticationToken,并将其发送给已配置的Realm执行必须的认证检查
			//每个Realm都能在必要时对提交的AuthenticationTokens作出反应
			//所以这一步在调用login(token)方法时,它会走到MyRealm.doGetAuthenticationInfo()
		
		
		shiro强大的自定义访问控制拦截器：AccessControlFilter，
		isAccessAllowed：表示是否允许访问；mappedValue就是[urls]配置中拦截器参数部分，如果允许访问返回true，否则false；
		onAccessDenied：表示当访问拒绝时是否已经处理了；如果返回true表示需要继续处理；如果返回false表示该拦截器实例已经处理了，将直接返回即可。基本上到这就重定向
		onPreHandle：会自动调用这两个方法决定是否继续处理；

		isAccessAllowed和onAccessDenied方法会影响到onPreHandle方法，而onPreHandle方法会影响到preHandle方法，而preHandle方法会达到控制filter链是否执行下去的效果。
		
		如果使用PathMatchingFilter就是接是onPreHandle方法走。
		
		WebUtils.issueRedirect(request, response, loginUrl)
		
		onlineSessionfilter 可以用来判断是否满足登录，syncOnlineSessionfilter用来接着同步操作
		
	 
		AuthorizingRealm 中的ispermitted方法可以debug一下
	 
	 
		 @RequiresPermissions  会拦截标签，分发到对应的PermissionAnnotationHandler。
		 
		 shiro:hasPermission  是好像直接访问到了AuthorizingRealm中的isPermitted，没有走controller的拦截handler
		 
	 真正的拦截规则是AuthorizingRealm.isPermitted>>>>>>>>>>org.apache.shiro.authz.permission.WildcardPermission#implies
	 
	 
	*****shiro的核心匹配机制*****
	public boolean implies(Permission p) {
			if (!(p instanceof WildcardPermission)) {
				return false;
			} else {
				WildcardPermission wp = (WildcardPermission)p;
				List<Set<String>> otherParts = wp.getParts();
				int i = 0;

				for(Iterator var5 = otherParts.iterator(); var5.hasNext(); ++i) {
				同级别比完，拥有的权限短路径，后面全匹配    用的的是a/c  两级权限，访问的是的三级路径,直接过？不是直接过，前面需要都匹配上才行
					Set<String> otherPart = (Set)var5.next();
					if (this.getParts().size() - 1 < i) {
						return true;
					}

					//同级别比较
					Set<String> part = (Set)this.getParts().get(i);
					if (!part.contains("*") && !part.containsAll(otherPart)) {
						return false;
					}
				}

				比完之后，拥有的路径长，待匹配的短，那么后一位需要时*
				while(i < this.getParts().size()) {
					Set<String> part = (Set)this.getParts().get(i);
					if (!part.contains("*")) {
						return false;
					}
					++i;
				}
				return true;
			}
		}
	 
			权限鉴权
				 
				只有第一次会将信息存入缓存，通过权限过滤，
				 
				其中的路径匹配可以再看看  在org.apache.shiro.authz.permission.WildcardPermission#implies 方法中
				 
				 权限的分隔划分，以:划分一个权限的匹配，源码操作
				 
	 
	shiro-页面标签

		使用shiro注解，会还是会使用securityManager.isPermitted方法，最终进入AuthorizingRealm.isPermitted方法，执行同controller进入的匹配方法

		延伸到时html页面
			 thymeleaf中使用shiro:hasPermission标签控制页面显示需要：
			 thymeleaf-extras-shiro
			 或者命名空间  xmlns:shiro="http://www.thymeleaf.org/thymeleaf-extras-shiro"
			 
			 配置一个ShiroDialect的bean
		在freemarker中
			引入shiro-freemarker-tags，页面 使用<@shiro.hasPermission name="权限添加">  
		 
		在jsp中

			在页面顶部引用<%@taglib prefix="shiro" uri="http://shiro.apache.org/tags" %> 标签库，
			页面使用<shiro:hasPermission name="1111">  
			 
	 
		shiro-session
			sessionFactory是创建会话的工厂，根据相应的Subject上下文信息来创建会话；默认提供了SimpleSessionFactory用来创建SimpleSession会话。
			更具需要也可以自钉子新的OnlineSession，搭配自定义 OnlineSessionFactory。实现session的内容增加

			其实这两个都可以使用默认的配置，即不持久化session也不自定义session
			SessionFactory中创建的session实体，Sessiondao操作中是创建了一个sessionid并赋值给当前的session
			 
			顺序是先实例化自定义的session，然后通过sessiondao创建sessionid赋值给对应的session

	ehcache(单体) 和redis(分布式)
		ehcache直接在jvm虚拟机中缓存，速度快，效率高；但是缓存共享麻烦，集群分布式应用不方便。是可以做集群缓存共享的，但是做服务话不适用，这个是跟着java内存走的
		redis是通过socket访问到缓存服务，效率比ecache低，比数据库要快很多，处理集群和分布式缓存方便，有成熟的方案。

		如果是单个应用或者对缓存访问要求很高的应用，用ehcache。
		如果是大型系统，存在缓存共享、分布式部署、缓存内容很大的，建议用redis。
		
		主站关闭浏览器后的cookie还在吗  rememberme的操作，测试主站的记住我
		
		kickout使用了cache和Deque队列实现，返回subject.logout退出，然后重定向登录
		WebUtils.issueRedirect(request, response, kickoutUrl);
		deque.push(sessionId);
					// 将用户的sessionId队列缓存
					cache.put(loginName, deque);
					
		kickoutSession.setAttribute("kickout", true);设置踢出属性，然后判断当前的session，重定向			
		
		这个也可以
		   public RedisCacheManager cacheManager() {
			RedisCacheManager redisCacheManager = new RedisCacheManager();
			redisCacheManager.setRedisManager(redisManager());
			return redisCacheManager;
		}		
		
		使用冒号分隔的权限表达式是org.apache.shiro.authz.permission.WildcardPermission 默认支持的实现方式。这里分别代表了 资源类型：操作：资源ID  


87.redis持久化 
		方案：相对来说使用aof的sec足够了，先重点，细节用到再说
		rdb:在指定的时间间隔内将内存中的数据集快照写入磁盘
			数据集快照dump到dump.rdb中，可以修改dump的频率。快照文件总是完整可用的
			频率比如:300秒内，如果超过10个key被修改，则发起快照保存 ；
			
			fork一个子进程来进行持久化，不影响主进程的io操作
			快照备份，是备份当前瞬间 Redis 在内存中的数据记录。备份慢一点，恢复快
			
			生成rdb文件的命令
			save	阻塞服务器进程，知道rdb创建完成为止，阻塞期间，不接受任何命令
			bgsave	用子线程创建rdb文件，服务器进程继续处理命令请求
			
			多个save命令(用save选项设置的保存条件)，是或关系，满足条件就会执行bgsave命令
			
			
		aof:每次写指令操作，进入aof记录文件。追加命令进文件保存
			追加文件，备份快，恢复慢，备份文件可能大
			其作用就是当 Redis 执行写命令后，在一定的条件下将执行过的写命令依次保存在 Redis中， 将来就可以依次执行那些保存的命令恢复 Redis 的数据了
		
			三种策略：每次修改，每秒同步(常用)，从不同步
		数据恢复时按照丛前到后的顺序再将指令执行一遍
		
		其中包含重写机制，一条 incr*100，最后可以合成一条incr100
		
		默认是rdb模式
		
		redis提供了string,hash(hset),list,set(类似队列操作) 这几种数据类型可供存取
		
		redis
		最简单的 String ，可以作为 Memcached 替代品，用作缓存系统
		使用 SetNx 可以实现简单的分布式锁
		使用 list Pop Push 功能可以作为阻塞队列／非阻塞队列
		使用 SUBSCRIBE PUBLISH 可以实现发布／订阅模型对数据进行实时分析，如可以累加统计等
		使用 Set 做去重的计数统计
		使用 SortedSet 可以做排行榜等排序场景
		以上场景基本上涵盖了 Redis 支持的各种存储结构
		? Key 可以是任意类型，但最终都会存储为 byte ［］
		? String 简单的（key,value ）存储结构，支持数据的自增、支持 BitSet 结构
		? Hash 哈希表数据结构，支持对 field 的自增等操作章数据存储 189
		? list 列表，支持按照索引、索引范围获取元素以及 Pop Push 等堆桔操作
		? Set 集合，去重的列
		? SortedSet 有序集合
			
		如果想要保证数据的安全性，建议同时开启 AOF RDB ，此时由于 RDB 有可能丢失文件 Redis 重启 优先使用 AOF 进行数据恢复

		需要注意 ，如果通过 kill -9 或者 Ctrl+C 关闭 Redis ，那么 RDB AOF 都不会被触发，这样会造成数据丢失，建议使用 redis-cli shutdown 或者 kill 优雅关闭 Redis
				
				
		
88.//外部tomcat――web.xml 默认 <welcome-file>index.htm</welcome-file>,boot 默认是/

89.前端相关
	thymeleaf
		th:text中的thymeleaf并不会被认为是变量，而是一个字符串
		<h2 th:object="${user}">
			<p>Name: <span th:text="*{name}">Jack</span>.</p>
		</h2>
			
		超链接url表达式。
		thymeleaf使用（,,）的形式解析多个参数,结合${}放置变量十分方便：
		<a th:href="@{/teacherShowMember(class_id=${class.classId}，class_name=${class.className})}"></a>
		传统URL传递多参数使用？&拼接：
		<a th:href="/teacherShowMember?class_id=123&class_name=XXX"></a>	
			
		但是在开发过程中，jsp的缺点是什么呢？
		<% %>等等jsp标签，java代码块与html静态文件元素来回穿插，导致页面可读性差。
		如果在使用each迭代时，更是要使用到c标签库
		<c:forEach var="user" items="${user}"> <c:forEach>
		而thymeleaf则使用th:each="user:${user}",不需要像jsp一样引入很多标签库
		而thymele拥有强大的内置工具，只要使用th:text="#dates.format(date,"yyyy-MM-dd")"即可获取时间
		再比如：jsp页面，无法进行选择操作，而thymeleaf可以使用th:switch="${sex}" th:case="0"
		
		主要功能：
			1.使用功能函数，如日期转换函数
			2.使用编程语句，如条件和循环语句
			3.使用页面框架模板，如布局显示
		
	编码和转义不一样
		编码是前后端的交互，转义是前台的显示
		就是将展示的东西，替换一些常用的特殊字符
		margin是边框外，是盒子外的尺寸，描述div的长宽不计入
		
	css	
		line-heigt 和 height实现垂直居中
		line-heigt=font-size，上下文字是紧贴着
		外边距重合，但是两个外边距之间有间隔内容，就不会合并
		div的高度由height决定，和内容无关

	ftl  模板操作方便，各种遍历，和判断操作直接使用
	html 单纯的html只能展示，遍历操作，一般通过js操作	
	
	css制作三角箭头，可以配置after伪类使用
	用下面连个属性，加上color中的transparent可以做一个箭头，原理是四个方向不同尺寸和颜色会对角线分割，隐藏部分就会是一个箭头.border边框不会重合
	border-width
	border-color

		
90.为什么有了@Compent,还需要@Bean呢？?
	如果想将第三方的类变成组件，你又没有没有源代码，也就没办法使用@Component进行自动配置，这种时候使用@Bean就比较合适了。不过同样的也可以通过xml方式来定义。
	Spring的Starter机制，就是通过@Bean注解来定义bean。
	可以搭配@ConditionalOnMissingBean注解?@ConditionalOnMissingClass注解，如果本项目中没有定义该类型的bean则会生效。避免在某个项目中定义或者通过congfig注解来声明大量重复的bean。

	@Component (”user")
	@bean(name="testBean"),如果没有配置name属性，那么将方法名作为bean的名称
	内部时使用AnnotationConfigApplicationContext来构建ioc容器(基于注解,这个一般是测试用)
		
	
	
91.	常用的工具类，工具接口
		Kaptcha  一个可配置的实用验证码生成工具
		联系我们中的地图   使用的是百度地图的开放api
		Pattern类  url路径匹配工具类
		
92. 在notepad++中勾选正则表达式，替换首尾字符，     ^/$->'/',
	主站加载很慢直至超时，除了看console，还要看network中的加载情况

93.实现job接口或者继承QuartzJobBean，系统job自动执行。
		context.getMergedJobDataMap().get(ScheduleConstants.TASK_PROPERTIES));
		可以使用这个属性将当前系统job绑定一个自定义的job，当系统执行的时候，然后通过绑定的job信息，执行对应的逻辑。支付中是直接将路劲放在库的trigger_name中的
		意思是通过绑定参数，封装执行的方法
		
93.不从controller传request，RequestContextHolder的使用
	HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();
	但是，如果service层的函数是异步的话，是获取不到request的。
	通常RequestContextHolder.getRequestAttributes()无法在子线程等异步情况下使用，
	
94.	return null;  和	return;
	一个是有返回值的return  还有一个是void的返回结束
	
95.	栈溢出场景	
	public String getJsonString(){return JSON.toJSONString(this);}
	JSON.toJSONString(userInfo)
				
96.总结：对比RPC和http的区别
	1.RPC要求服务提供方和服务调用方都需要使用相同的技术，要么都hessian，要么都dubbo，而http无需关注语言的实现，只需要遵循rest规范
	2.RPC主要是基于TCP/IP协议的，而HTTP服务主要是基于HTTP协议的，HTTP协议是在传输层协议TCP之上的，所以效率来看的话，RPC当然是要更胜一筹啦
	4.rpc框架，有服务治理，自带负载均衡(http需借助nginx)，传输效率rpc要高，使用接口开发
	
	TCP 是传输层协议，HTTP 是应用层协议，而传输层较应用层更加底层，在数据传输方面，越底层越快，因此，在一般情况下，TCP 一定比 HTTP 快

	几种常用的序列化方式，java,hessian,thrift，kryo,protobuf
	需要长连接获取高性能，选基于tcp的thrift和dubbo
	需要跨网段，跨防火墙，选基于http协议的hessian
	
	接口访问控制，接口安全，接口控制
		sign验签，基于时间戳防止重放攻击，
		限流 熔断
		RPC 主要指内部服务之间的调用
		
	微服务熔断器
	Hystrix 断路器机制
		会强迫其以后的多个调用快速失败，不再访问远程服务器，从而防止应用程序不断地尝试执行可能会失败的操作
		
		Hystrix Command 请求后端服务失败数量超过一定比例(默认 50%), 断路器会切换到开路状态(Open). 这时所有请求会直接失败而不会发送到后端服务。
		一旦后端服务不可用, 断路器会直接切断请求链, 避免发送大量无效请求影响系统吞吐量, 并且断路器有自我检测并恢复的能力
		
		
		控制同一个ip和设备id的请求频率，记录在redis中的，使用incr 和 expire实现

		返回给ajax的信息，关键信息，手机号等要*加密处理,敏感数据 userinfo一定要严格返回

	
97.dubbo的创建
	使用自定义的schema，实现命名空间自定义配置
		1.设计配置属性和JavaBean 
		2.编写XSD文件 													
		3.编写NamespaceHandler和BeanDefinitionParser完成解析工作，串联所有部件 		(Spring提供了默认实现类NamespaceHandlerSupport和AbstractSingleBeanDefinitionParser)
		4.编写spring.handlers和spring.schemas串联起所有部件 						META-INF/spring.handlers和META-INF/spring.schemas  分别定义处理类 和xsd文件    spring默认载入

		<xsi:schemaLocation=" http://blog.csdn.net/cutesource/schema/people  http://blog.csdn.net/cutesource/schema/people.xsd">  分别对应handler处理类和xsd文件
		是通过统一的前者命名关联了handler和xsd解析
		
		Spring是怎么解析<dubbo:.../>配置的。如上

		dubbo直接使用了BeanDefinitionParser，没有继承AbstractSingleBeanDefinitionParser,将xml中的注解内容parse成bean的实例到容器中
		dubbo:service     其中service就是其中的一个配置类，对应<xsd:element name="service">对应着配置项节点的名称，因此在应用中会用 service 作为节点名来引用这个配置
		
		Spring容器启动的过程中，会将Bean解析成Spring内部的BeanDefinition结构，
		
		dubbo也是实现了InvocationHandler ，最后invoker.invoke(new RpcInvocation(method, args)).recreate(); 这里就开始进入调用远程的服务
		
		spi机制，配置文件发现实现类机制，优点实现三方解耦，缺点会一次性加载全付实现类
		
		/默认情况下如果本地有服务暴露，则引用本地服务
		
		// 用户指定URL，指定的URL可能是对点对直连地址，也可能是注册中心URL
		
		按 key=menthodName/value=invoker 缓存起来  
		
		注册模块dubbo-register：
			1.构造器利用客户端创建了对zookeeper的连接，并且添加了自动回复连接的监听器。
			2.注册url就是利用客户端在服务器端创建url的节点，默认为临时节点，客户端与服务端断开，几点自动删除
			3.取消注册的url，就是利用zookeeper客户端删除url节点
			4.订阅url， 功能是服务消费端订阅服务提供方在zookeeper上注册地址.
			5 取消订阅url， 只是去掉url上的注册的监听器
			
			使用了zookeeper的注册中心，ZookeeperRegistryFactory,是操作zookeeper的客户端的工厂类，用来创建zookeeper客户端，ZookeeperClient
			
		dubbo://192.168.6.222:20881/com.xiaoyuer.soa.api.service.IRequireService?anyhost=true&application=xye-soa-core-require&default.retries=0&default.service.filter=xyeProviderExceptionFilter&default.timeout=20000&default.token=true&dispatcher=message&dubbo=2.8.4x-SNAPSHOT&generic=false&interface=com.xiaoyuer.soa.api.service.IRequireService&methods=testQueryParams,saveRequireInfo,saveRequire,getRequireInfo&organization=dubbox&owner=programmer&pid=60516&side=provider&threads=300&timestamp=1590393669112
		/dubbo/com.xiaoyuer.soa.api.service.IRequireService/providers/dubbo%3A%2F%2F192.168.6.222%3A20881%2Fcom.xiaoyuer.soa.api.service.IRequireService%3Fanyhost%3Dtrue%26application%3Dxye-soa-core-require%26default.retries%3D0%26default.service.filter%3DxyeProviderExceptionFilter%26default.timeout%3D20000%26default.token%3Dtrue%26dispatcher%3Dmessage%26dubbo%3D2.8.4x-SNAPSHOT%26generic%3Dfalse%26interface%3Dcom.xiaoyuer.soa.api.service.IRequireService%26methods%3DtestQueryParams%2CsaveRequireInfo%2CsaveRequire%2CgetRequireInfo%26organization%3Ddubbox%26owner%3Dprogrammer%26pid%3D60516%26side%3Dprovider%26threads%3D300%26timestamp%3D1590393669112	
		
		
		在zookeeper的服务端创建临时的目录节点，每一级都是节点目录 	使用zkcli客户端可以查看目录节点
		[zk: localhost:2181(CONNECTED) 8] ls /dubbo/com.xiaoyuer.soa.api.service.IRequireChooseService ->[consumers, configurators, routers, providers]
		[zk: localhost:2181(CONNECTED) 10] ls /dubbo/com.xiaoyuer.soa.api.service.IRequireChooseService/providers-> 提供者列表
		
		
		dubbo的container模块
		默认只会启动dubbo-container-spring的这个container，主要负责jar启动，优雅停机
			因为服务通常不需要Tomcat/JBoss等Web容器的特性，没必要用Web容器去加载服务，一般main加载spring启动即可
			Dubbo是通过JDK的ShutdownHook来完成优雅停机的，所以如果用户使用”kill -9 PID”等强制关闭指令，是不会执行优雅停机的，只有通过”kill PID”时，才会执行。
			
		dubbo的remoting
			dubbo底层通信模块的实现。实现对请求/应答的各种逻辑处理，包括同步，异步，心跳等逻辑，最底层的通信借助netty或者mina实现
			
		zkClient.createPersistent(path, true);	zookeeper递归创建目录
		zookeeper 存储的只是目录节点，每个节点都会有自己对应的值,维护的就是目录节点
		
		启动开启netty服务，绑定ip和port，客户端调用使用netty访问对应的地址，这样会进入netty的对应handler
		netty绑定地址，添加处理handler， client端使用netty访问地址，服务端接收到转给了handler处理
		
		netty入门学习
			Netty封装了JDK的NIO(nio，主要使用selector多路复用器来实现)
			netty是封装java socket nio的。 类似的功能是 apache的mina。
			建立在客户端和服务端之间的,服务端建立相应的规则，然后运行起来，等待客户端访问或者发送”消息“
			
			一个socket对应一个channel？
			服务端
				服务端用ServerBootstrap(netty服务端应用开发的入口)，有两个NioEventLoopGroup；
				有两种通道需要处理， 一种是ServerSocketChannel：用于处理用户连接的accept操作， 另一种是SocketChannel，表示对应客户端连接。
				分别用来用来接收进来的连接和用来处理已经被接收的连接，一旦‘boss’接收到连接，就会把连接信息注册到‘worker’上。
				
				.childHandler(),设置子通道的处理器，也就是SocketChannel的处理器，内部是实际业务开发的”主战场”，具体的业务实现。
				
				Channel都有且仅有一个ChannelPipeline与之对应，Channel包含了ChannelPipeline，ChannelPipeline内部包含了N个handler，每一个handler都是由一个线程去执行；
				channel.pipeline().addLast添加处理的handler
				
				b.option(),.childOption()分别配置ServerSocketChannel的选项 和 子通道也就是SocketChannel选项 
				
				ChannelFuture f = b.bind(port).sync();//绑定端口并启动去接收进来的连接
				f.channel().closeFuture().sync();//这里会一直等待，直到socket被关闭
				
			客户端
				client用Bootstrap，只有一个NioEventLoopGroup。只有一种channel，也就是SocketChannel
				
			网络传输的载体是byte，所有框架都是这个规定，JAVA的NIO提供了ByteBuffer，用来完成这项任务， Netty也提供了叫做ByteBuf
			通过合理的切分微服务变价可结局大缤纷分布式的事务问题
			
			IO多路复用技术通过把多个 IO 的阻塞复用到同一个 select 的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程模型比，I/O 多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源。
			由于 Netty 采用了异步通信模式，一个 IO 线程可以并发处理 N 个客户端连接和读写操作，这从根本上解决了传统同步阻塞 IO 一连接一线程模型，架构的性能、弹性伸缩能力和可靠性都得到了极大的提升
	
			
			
			
98.redis hget 实例
	hget pay:bankcard_acct 1366          pay:bankcard_acct       key-1366 value-5
	
	type key	查询redis的key类型
	
	
99.	静态资源加载
		
		这个解释的很清楚了
		**********
		
			一般是建议资源加载(spring.resources.static-locations)和映射(addResourceHandler.addResourceLocations)相统一，一致。就是都写上，不写也行，不写使用boot默认。
			资源加载，重名的相对而言 static优先。都是统一加载到一起作为资源，不会重复
			********映射的优先级高于资源加载********
			静态资源 js等要配置映射，否则会被拦截，这个不一定 两种选择  1映射 2默认资源路径

			http://192.168.6.222:8086/static/js/runtime.6a3e6d1f.js 	走的资源映射
			如果是 http://192.168.6.222:8086/js/runtime.6a3e6d1f.js，   走的boot静态资源加载  

			对于静态映射的文件，无需资源加载
			http://192.168.6.222:8086/templates/index.html 需配置资源映射后直接访问(因为不能直接访问WEB-INF下的文件)，不用指定加载目录，映射的优先级高。
			
		**********
		
		
		HandlerInterceptorAdapter  在拦截器中赋值全局的静态资源路径


100.-- js相关	
		var smsCodeValiate;  变量要初始化 不然就是undefined
		var smsCodeValiate = false;
		var tip='tips';  

		动态添加一个到form中
			var cardForm=$('#cardForm'); //得到form对象
			var options=$("#cardArea option:selected");//拿到选中的select
			var bankNameInput=$("<input type='hidden' name='bankName'/>");
			bankNameInput.attr("value", options.text());
			cardForm.append(bankNameInput);
			
			var data = $.param({'bankName':options.text()})+'&'+$('#cardForm').serialize();  表单之后的追加  这个是&拼接 

		<link rel="stylesheet" href="/css/openPayIndex.css">
		实际测试是域名根目录下，不包括项目名，netpay测试就调到域名的根路径下。实际测试写绝对路径，全路径即可
		url: '${ctx}/open-api/pay/withdrawSubmit/',  直接放里面
		 
		 
		ajax是json串交互，无法返回页面
		 
		form表单序列化之后追加字段
			var data = $.param({'state': state}) + '&' + $('#desProForm').serialize();     这里是&拼接的参数串

		jquery中表单序列化方法：
			1.serialize()  (将表单内容序列化成一个字符串。)
			　　格式：var data = $("form").serialize();
			　　这样在ajax提交表单数据时，就不用一一列举出每一个参数。只需将data参数设置为 $("form").serialize() 即可。

			2.serializeArray()  (将页面表单序列化成一个JSON结构的对象。注意不是JSON字符串)
			　　格式：var jsonData = $("form").serializeArray();
			　　比如，[{"name":"lihui", "age":"20"},{...}] 获取数据为 jsonData[0].name；


		正则校验
			/^(?!([a-zA-Z]+|\d+)$)[a-zA-Z\d]{6,20}$/.test('123123a')
			var rex = new RegExp(/^(?!([a-zA-Z]+|\d+)$)[a-zA-Z\d]{6,20}$/);
			rex.test()


			
		实现倒数计时，这是异步执行的，不会影响到下一个语句的执行,clearInterval是停止执行，和setInterval搭配使用
			   function timeCode(num) {
				var textHtml = 180;
				var timeCode = setInterval(function () {
					textHtml--;
					if (textHtml == 0) {
						clearInterval(timeCode);
						document.getElementById("codeBtn").innerHTML = '重新获取验证码';
						getCodeFlag = true;
					} else {
						document.getElementById("codeBtn").innerHTML = textHtml+`s后重新发送`;
					}
				}, 1000)
			}
			
		dl dd dt    常规用法   解释型列表
			<dl>
				<dd><img src=”图片路径” alt=”" /></dd>
				<dt>图片标题</dt>
			</dl>
			<dl>
				<dt>电影标题</dt>
				<dd>主要演员：刘德华，周润发</dd>
			</dl>
			
		span 元素为行内元素，没有width属性，需要转换为块级元素才可以设置width；
					
			
101.BeanFactoryPostProcessor和BeanPostProcessor。两个后置处理器的区别
		**********
			BeanFactoryPostProcessor在bean实例化之前执行，
			之后实例化bean（调用构造函数，并调用set方法注入属性值），
			然后在调用两个初始化方法前后，执行了BeanPostProcessor。
			初始化方法的执行顺序是，先执行afterPropertiesSet，再执行init-method。
		**********

		BeanPostProcessor(bean级别的处理)	
			执行时机:	实例化之后,操作的是具体的bean
						spring容器实例化bean之后，在执行bean的初始化方法(InitializingBean,init-method)前后
			
			使用案例:	@Autowired,ApplicationContextAwareProcessor	
			
			BeanPostProcessor的执行顺序是在BeanFactoryPostProcessor之后
			
		BeanFactoryPostProcessor(BeanFactory级别的处理)	,多个按优先级处理
			
			执行时机：	在beanDefinition加载完成，bean实例化之前执行
			操作对象：	BeanDefinition	针对整个Bean的工厂进行处理,不能实例化操作
			使用案例:	PropertyPlaceholderConfigurer
			
		java对象的创建过程往往包括 类初始化 和 类实例化 ，
			初始化：静态的（变量，方法，代码块）会被执行，只在类加载的时候执行一次
			实例化：创建一个类的实例对象。可多次实例化，堆中开内存
			在Java对象初始化过程中，主要涉及三种执行对象初始化的结构，分别是 1.实例变量初始化、2.实例代码块初始化 以及 3.构造函数初始化。
			Java要求在实例化类之前，必须先实例化其超类，以保证所创建实例的完整性			
					
			xml中定义的bean标签，Spring会解析成一个BeanDefinition(存储bean标签的信息，用来生成bean实例)，这个BeanDefinition就是bean标签对应的javabean。
			
102.open 和 netpay在同一个tomcat中 ,然后数据源出问题
	spring.jmx.enabled=false
	
103.缓存的选择  ehcache   redis  memcached
	memcached:	服务器端是c编写的，客户端多语言实现，相对下面，效率低
	Ehcache:	纯java编写的,相对上面效率高，
	综合比较，两者会选ehcache。redis另算
	
	ehcache看了下,就是一个框架，提供了一些缓存方案而已，底层基于jvm内部缓存开发的，大白话就是，基于jvm提前加了一些功能方便你开发，没啥太大区别。

	ehcache直接在jvm虚拟机中缓存，速度快，效率高；但是缓存共享麻烦，集群分布式应用不方便。
	redis是通过socket访问到缓存服务，效率比ecache低，比数据库要快很多，处理集群和分布式缓存方便，有成熟的方案。 

	如果是单个应用或者对缓存访问要求很高的应用，用ehcache。 如果是大型系统，存在缓存共享、分布式部署、缓存内容很大的，建议用redis。
	补充下：ehcache也有缓存共享方案，不过是通过RMI或者Jgroup多播方式进行广播缓存通知更新，缓存共享复杂，维护不方便；简单的共享可以，但是涉及到缓存恢复，大数据缓存，则不合适。
		
	
104.手动获取spirngContext
	
	***************
	静态方法中调用spring bean   无法注入static bean
		原因是Spring容器的依赖注入是依赖set方法，而set方法是实例对象的方法，而静态变量属于类，因此注入依赖时无法注入静态成员变量，在调用的时候依赖的Bean才会为null。
				
	@Override这里个别jdk版本不一致导致的原因，一般都要加上，该类需要加入spring 容器
	定义static变量ApplicationContext，是为了静态共享，利用容器的getBean方法获得依赖对象。是面向开发者使用的，面向开发应用
	@Component
	public class SpringContextUtil implements ApplicationContextAware{
		private static ApplicationContext context;
		
		public static ApplicationContext getApplicationContext(){return context;}

		public void setApplicationContext(ApplicationContext ctx) throws BeansException {context = ctx;}
		
		public static Object getBean(String beanName){return context.getBean(beanName);}
	}

	Redisclient中 初始化方法    加入spring 容器,初始化后  然后加载到静态属性中
	public static JedisPool jedisPool;
	
	RedisUtil redisUtil = (RedisUtil)SpringContextUtil.getBean("redisUtil");//手工获取
	
	SpringIoC容器是一个管理springbean的容器,IoC容器都需要实现接口BeaFactory ，
	ApplicationContext继承beanfactory接口，大部分springioc是实现applicationcontext接口的实现类。
	
	private static ApplicationContext applicationContext; 这个是类的属性
	上述代码中出现了ApplicationContextAware接口的方法 setApplicationContext(),初始化的时候该方法就会被调用,从而获取 SpringIoC的上下文(applicationContext)，
	
	
105.有import选项 但是类一致导不进来， 查看jdk 重新install
	maven 版本  编译 运行 不一致
	spring-data-redis在xye-project-util中是1.8.3版本，支付中已经排除了引用，但还是方法找不到，
			这里的原因是1.8.3版本是void delete() 编译好了,最终使用的是2.05版本是Boolean delete(),两个不是同一个方法，所以找不到方法
			这样就是 编译是找自身的jar  运行才会找最终的依赖jar。尽量版本统一
			方法定义了  会就近寻找jar依赖   
			
	runtime 是运行的意思。指的是直接在运行时所需要的包，而非在编译时等时候需要的包。  mysql-connector可以配置，但是编译需要的jar不能使用

			
			
106.job梯度时间发送
		时间梯度，总的来说就是定30,60,100规则穿，记录次数，10job一次，然后每次判断下次的时间点到没到，漏发的再补发一次就可以了

107.日常小问题记录
		git did not exit cleanly (exit code 1)   最终重装git客户端
		
		References to interface static methods are allowed only at source level 1.8
		在project facets 中修改jdk的版本
		
108.属性文件导入，属性文件加载
		@PropertySource(value= "classpath:test.properties")  	添加自定义的属性文件进来,将属性文件加入到容器，后续可以直接用@value注入
		@ConfigurationProperties(prefix = "na")					这是写在类上，省略前缀，匹配后面的名字，但是注意要有set方法
			默认从全局配置文件中获取值,如果想加载指定属性文件，就得使用@PropertySource进行加载后再使用@ConfigurationProperties。
			
		使用 Configuration configs = new PropertiesConfiguration(filePath);直接临时读取属性文件，初始化
		
	@Value(”$(database.driverName)”),${}代表占位符，会读取上下文的属性值装配到属性中，这是一个最简单的spring表达式

	
	从属性文件中加载list或者map
		blog-top-links={"home":"/home"}
		blog-list=1,2,3

		@Value("#{'${blog-list}'.split(',')}")
		private List<String> pList;

		@Value("#{${blog-top-links}}")
		private Map<String, String> topLinks;

		
		@Value("#{'${key}'.split(',')}")
			
	*****静态代码块，static初始化*****
			静态static代码块在首次调用这个类时才触发，而不是一开始就加载。且只加载一次

			
			PropertiesUtil   自动状态属性类 	  这里就可以在static初始化中加载属性文件，后面直接静态方法调用即可
					InputStream is = PropertiesUtil.class.getClassLoader().getResourceAsStream(propName);
					prop.load(is);
			
			
	@Bean 和  CommandLineRunner
		@Order注解并不能改变Bean加载优先级，对多个CommandLineRunner是有效的，加在普通的方法上或者类上没有作用
		CommandLineRunner 和 @bean 加载顺序，启动mq先消费，CommandLineRunner的常量尚未加载就消费，导致初始化client异常。
		用户扩展CommandLineRunner，进行启动项目完毕之后一些业务的预处理。
		InitializingBean，项目启动时，初始化bean的时候都会执行该方法
		
		
109.iframe 路径编码
		iframe嵌入的地址时候，相当于get请求，其中包含url的连接&等符号需要编码后传输
		本地测试是表单提交，带有的&已经被编码过了
		
110.bean条件加载，实例加载
		@Conditional(RedisChooseConfig.class)
		实现Condition接口，获取环境变量，动态加载
		Environment environment = context.getEnvironment();
		String property = environment.getProperty("resRoot");
		
111.spring cache

	@EnableCaching 表示SpringIoC容器启动了缓存机制
	 
	 一般而言，对于查询，我们会考虑使用＠Cacheable；对于插入和修改，我们会考虑使用＠CachePut；对于删除操作，我们会考虑使用＠CacheEvict。
	 
	 
	常规配置: 失效时间，序列化方式(redis服务端易读)，目录的层级(默认是::)

	每次调用需要缓存功能的方法时，Spring会检查检查指定参数的指定的目标方法是否已经被调用过(依据是value+key,相同会覆盖，一般key中追加参数信息)；
	如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法 并缓存结果后返回给用户。下次调用直接从缓存中获取。

	先执行@Cacheable----->再执行service层的方法，基于注解()，实际就是个拦截器(就是使用注解省去了判断的过程。原先是判断没有缓存，就从数据库查询，这里讲判断隐藏到了注解去实现)
	
	缓存的目的是：通过Cache来缓存不经常改变的数据以提高系统性能和增加系统吞吐量，避免直接访问数据库等低速的存储系统。经常变的数据，不适合用缓存
	Spring Cache对Cache进行抽象，提供了@Cacheable、@CachePut、@CacheEvict等注解。
	可用于单体应用系统，也可集成Redis(默认使用rediscache，最终存在Redis中，可跨系统)等缓存服务器用于大型系统或者分布式系统。(每次重启也不会去service，会花点时间从redis取，后面再取就快多了)

	通常情况下，直接使用SpEL表达式来指定Key比自定义KeyGenerator更简单。
	key = "'USER:'+#id"
	空参数使用默认的key就是calendar::SimpleKey []，多次调用会覆盖同名key
	可以使用单独的字符串作为key="'zhaoyun'"

	注解使用在mapper上，需要使用p绑定：	@Cacheable(value=ConstantsRedis.CACHE_PAY,key="'withdraw:config:'+#p0+'-'+#p1")
	在service上：	@Cacheable(value="cache",key="'letterBank-'+#firstLetter")  
	若不希望返回值为null时进行缓存，则使用unless="#result == null",排除掉返回值为null的结果	
	若不希望参数为空的时候进行缓存，则需要使用condition = "#i==null",这时函数还没执行，排除掉参数为空的情况
	
	
	key="‘wangyun’+#userinfo.id"
	
	cache 和redis的区别
		1.Spring cache是代码级的缓存，一般是使用一个ConcurrentMap，也就是说实际上还是是使用JVM的内存来缓存对象的，这势必会造成大量的内存消耗。但好处是显然的：使用方便。
		2.Redis 作为一个缓存服务器，是内存级的缓存。它是使用单纯的内存来进行缓存。
		3.集群环境下，每台服务器的spring cache是不同步的，这样会出问题的，spring cache只适合单机环境。
		4.Redis是设置单独的缓存服务器，所有集群服务器统一访问redis，不会出现缓存不同步的情况。
	
	使用spring cace 注意点，高查询  低改动
		感觉cache和redis比，
	

	
	  
	@Cacheable
		根据方法对其返回结果进行缓存，下次请求时，如果缓存存在，则直接读取缓存数据返回；如果缓存不存在，则执行方法，并把返回的结果存入缓存中。一般用在查询方法上。
	@CachePut
		使用该注解标志的方法，每次都会执行，并将结果存入指定的缓存中。其他方法可以直接从响应的缓存中读取缓存数据，而不需要再去查询数据库。一般用在新增方法上。
		需要返回待缓存的对象，存入cache
	@CacheEvict
		使用该注解标志的方法，会清空指定的缓存。一般用在更新或者删除方法上
		无需返回对象
	
	
	
	
	这个做springcache用的，查询结果缓存使用
	@Bean
    public CacheManager redisCacheManager(RedisConnectionFactory redisConnectionFactory) {
    	//1.设置key 和value的序列化方式
        RedisSerializer<String> redisSerializer = new StringRedisSerializer();
        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);
		
        ObjectMapper om = new ObjectMapper();
        om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
        om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);
        jackson2JsonRedisSerializer.setObjectMapper(om);

//		定义缓存前缀,默认是两个冒号
		  CacheKeyPrefix keyPrefix = new CacheKeyPrefix() {
		            @Override
		            public String compute(String cacheName) {return cacheName + ":";}
		        };
		        
        //2.链式调用(返回new的config对象)设置config
        RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig()
                .entryTtl(Duration.ofMinutes(30))
                .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(redisSerializer))
                .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(jackson2JsonRedisSerializer))
                .disableCachingNullValues().computePrefixWith(keyPrefix);
        return RedisCacheManager.builder(redisConnectionFactory).cacheDefaults(config).build();
    }
	
	
112.dubbo相关
		dubbo 容错，降级 限流
	
		容错机制	共六种集群容错机制
			默认的容错机制就是：失败自动切换，重试其它服务器。通常用于读操作。默认2次，可通过 retries="2" 来设置重试次数。
			
		服务降级 代码配置或者dubbo admin配置    目的是为了保证核心服务可用。
			1.在xml中配置mock属性：在远程调用异常时，服务端直接返回一个固定的字符串(也就是写死的字符串)
			2.创建一个单独的mock类：在远程调用异常时，服务端根据自定义mock业务处理类进行返回)，接口名要注意命名规范：接口名+Mock后缀，mock实现需要保证有无参的构造方法。
			
			使用场景
			不可用或者响应时间太长时，快速返回错误的响应信息
				1) 多个服务之间可能由于服务没有启动或者网络不通，调用中会出现远程调用失败;
				2) 服务请求过大，需要停止部分服务以保证核心业务的正常运行；
				   服务降级，当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。舍车保帅
				
			对一些非核心服务进行人工降级，在大促之前通过降级 开关关闭哪些推荐内容、评价等对主流程没有影响的功能?
			故障降级，比如调用的远程服务挂了，网络故障、或者 RPC服务返回异常。 那么可以直接降级，降级的方案比 如设置默认值、采用兜底数据（系统推荐的行为广告挂 了，可以提前准备静态页面做返回）等等?
			限流降级，在秒杀这种流量比较集中并且流量特别大的 情况下，因为突发访问量特别大可能会导致系统支撑不 了。这个时候可以采用限流来限制访问量。当达到阀值 时，后续的请求被降级，比如进入排队页面，比如跳转 到错误页（活动太火爆，稍后重试等）

		服务限流
			<dubbo:service interface="com.foo.BarService" executes="10" /> 也可以精确到方法上

		负载均衡策略
			dubbo默认的负载均默认是随机调用法。一共有4种负载均衡策略：
			RandomLoadBalance? ?随机调用负载均衡；
			RoundRobinLoadBlance 轮询调用；
			LeastActiveLoadBlance? 最少活跃数调用法，使慢的提供者收到更少请求；
			ConsistentHashLoadBalance? 一致性Hash算法，相同参数的请求总是发到同一提供者；
			
			
		多个版本的并存(一般用来实现灰度发布)
			就是一个权限接口类名，对应两个实现类，调用的时候指定对应的版本号即可
			低压力时段，让部分消费者先调用新的提供者实现类，其余的仍然调用老的实现类，在新的实现类运行没有问题的情况下，逐步让所有消费者全部调用成新的实现类。
			
			提供者
				<dubbo:service interface="com.xxxx.rent.service.IDemoService" ref="iDemoService4" version="2.4.4"/>
				<dubbo:service interface="com.xxxx.rent.service.IDemoService" ref="iDemoService5" version="2.4.5"/>
			
			消费者
				<dubbo:reference id="iDemoService5" interface="com.xxxx.rent.service.IDemoService" version="*"/>
				
		
		*******dubbo的同步异步调用*******
			Dubbo底层网络通信采用Netty
			DubboInvoker中代码有三种调用方法，分别是：忽略返回值调用、异步调用和同步调用。
			
			同步调用是相对单个线程的等待而言的，默认是同步等待结果阻塞的，支持异步调用
			***dubbo自身底层调用是使用netty异步实现的，消费端默认同步调用返回结果***
			
			在dubbo-demo-consumer.xml中配置调用服务信息，设置为异步调用async="true" ，无需要返回结果，就return="false"，
				<dubbo:reference id="asyncDemoService" check="false" interface="com.alibaba.dubbo.demo.AsyncDemoService">
					<dubbo:method name="sayHello" async="true"/>
				</dubbo:reference>
			
			需要返回结果，就Future<String> future =  RpcContext.getContext().getFuture();
			String hello = future.get(1, TimeUnit.SECONDS);//这个步骤是阻塞的
			
			Dubbo Consumer 端发起调用后，同时通过RpcContext.getContext().getFuture()获取跟返回结果关联的Future对象，然后就可以开始处理其他任务；
			当需要这次异步调用的结果时，可以在任意时刻通过future.get(timeout)来获取。
			
			
			同步调用过程： 阻塞的过程在于先调用了future的get方法
				1.将请求封装为Request对象，并构建DefaultFuture对象，请求ID和Future对应。
				2.通过Netty发送Request对象，并返回DefaultFuture对象。
				3.调用DefaultFuture.get()等待数据回传完成。
				4.服务端处理完成，Netty处理器接收到返回数据，通知到DefaultFuture对象。
				5.get方法返回，获取到返回值。

			
			异步使用案例：将future 返回到 RpcContext.getContext()    future的get方法后置
				fooService.findFoo(fooId);// 此调用会立即返回null
				Future<Foo> fooFuture = RpcContext.getContext().getFuture();// 拿到调用的Future引用，当结果返回后，会被通知和设置到此Future 
				
				barService.findBar(barId);// 此调用会立即返回null
				Future<Bar> barFuture = RpcContext.getContext().getFuture(); // 拿到调用的Future引用，当结果返回后，会被通知和设置到此Future
				 
				// 此时findFoo和findBar的请求同时在执行，客户端不需要启动多线程来支持并行，而是借助NIO的非阻塞完成
				// 如果foo已返回，直接拿到返回值，否则线程wait住，等待foo返回后，线程会被notify唤醒
				Foo foo = fooFuture.get(); 
				Bar bar = barFuture.get(); // 同理等待bar返回
				 
				// 如果foo需要5秒返回，bar需要6秒返回，实际只需等6秒，即可获取到foo和bar，进行接下来的处理。
				
				RpcContext的一次调用生命周期。
					RpcContext.getContext().setAttachment   这个传递参数
					
					因此每发起RPC调用，上下文状态会变化。
					消费端在执行Rpc调用之前，经过Filter处理, 会将信息写入RpcContext.见ConsumerContextFilter，消费者在执行调用之前(AbstractInvoker)，会将RpcContext中的内容写入到invocation，实现参数传递
					服务端在执行调用之前，也会经过Filter处理，将信息写入RpcContext. 见ContextFilter类
	
					会判断ASYNC_KEY是否为true,如果是，则会向context中写入future对象：
					
			dubbo直连(绕过注册中心，点对点直连，以服务接口为单位，忽略注册中心的提供者列表)
				3.dubbo 直连，开发环境可不使用zookeeper,节省启动注册时间，soa关闭再启动不影响消费端使用，步骤如下：
				服务端口SOA   1.取消注册 添加 register="false"   2.关闭令牌验证   token="false"

				<dubbo:registry protocol="zookeeper" address="${zookeeper.address}" register="false"/>
				<dubbo:provider token="false" timeout="20000" retries="0"/>
				消费端   替换 spring-dubbo.xml 表达式
				eclipse:   find:interface=\"(.*)\"    replace with:  interface=\"$1\" url="dubbo://localhost:20881"  勾选 regular expressions  

			dubbo  qos  qos-server端口冲突
				dubbo的重启多次启动导致的端口冲突  qos-server 端口冲突，
				经过网络查找，结果是Root WebApplicationContext 启动了两次，第二次报错，dubbo端口被占用
				qos主要用来对服务动态上下线，目前用不到
				解决办法：在tomcat的server.xml中设置 <Host appBase="webapps" autoDeploy="false" deployOnStartup="false" name="localhost" unpackWARs="false">

			dubbo服务超时调用
				默认是服务不成功，重试两次。如果服务端处理时间超过了设定的超时时间，就会有重复请求
					1.对于核心的服务中心，去除dubbo超时重试机制，并重新评估设置超时时间。
					2.业务处理代码必须放在服务端，客户端只做参数验证和服务调用，不涉及业务流程处理

				<dubbo:provider delay="-1" timeout="6000" retries="0"/>  加大超时时间(接口配置>全局配置)

				一个是provide提供的超时参数(推荐服务端配置)，还有一个是consumer提供的超时参数
				服务消费者端设置超时时间，如果在消费者端设置了超时时间，以消 费者端为主，即优先级更高。因为服务调用方设置超时时间控制性更灵活。如果消费方超时，服务端线程不会定制，会产生警告。
				
				
				消费端配置1s的超时时长，注意这里不能说默认就是1s你还显示声明干嘛。你要不显示声明服务端显示声明就会按照服务端的配置时长操作的咯，注意这一点

				出现超时异常的时候服务端可不会停下来，你客户端为了保证高可用一顿重试最后结果就是重复请求。当然如果是查询没问题，可以避免一些网络波动造成的超时异常，可以提升服务的可用性。但是如果是数据插入，如扣款、订单等操作，重复操作可不是一件好事情
				
			
			zookeeper关闭调用 和服务关闭调用
			zookeeper关闭 不影响调用，服务关闭当然失败，新版的dubbo2.7.7 提供者断开后，消费者调用失败，过段时间提供者重启，消费者可以连上

			
			dubbo超时
				针对服务端重启，消费者超时，这个在duboo的源码中有个配置，对应的duboo中的源码： com.alibaba.dubbo.common.Constants DEFAULT_SESSION_TIMEOUT = 60 * 1000; 这个时间配置了，需要重新打jar才行
			

113.多接口实现
		class a 实现 接口b(继承接口c)，那么a需要实现所有方法(包括c中的)，但是若a继承了一个实现c接口的类，就不用都实现(只认implements)，系统中是baseSericeImpl。

		接口中方法注释可以上层显示，实现类中也能看到
		
114.重定向
		/withdraw 抽空测试下同级的重定向  像/pay/withdraw这种两级路径：
			http://www.yu.com/xye-open/open-api/pay/test/cache.htm
			response.sendRedirect("withdrawSubmit");
			http://www.yu.com/xye-open/open-api/pay/test/withdrawSubmit
			http://www.yu.com/xye-open/open-api/pay/test/kk/withdrawSubmit
			
		实际测试，同一个controller，重定向是在最后一个路径后面追加路径
		
		registry.addViewController("/").setViewName("forward:/index.html");//这里直接是相当于浏览器发送新请求
		
115.在bean中可以添加一个缓存的map，这样实现复用
	常用的工具类很方便ResourceUtils，IOUtils
	
	
116.UrlBase64字符编码相关
		//new String(UrlBase64.decode(value.getBytes(chartSet)), chartSet)     //拿到UrlBase64编码后的字符串
        String aa="我是革命一块砖";
        byte[] utf8s = UrlBase64.encode(aa.getBytes(Charset.defaultCharset()));
        String s = new String(utf8s, Charset.defaultCharset());
        byte[] decode = UrlBase64.decode(s.getBytes(Charset.defaultCharset()));
        String s1 = new String(decode, Charset.defaultCharset());
        System.out.println(s1);//最终s1是"我是革命一块砖"
		
		
117.javap签名
	javap -s a.class 这里常出现的问题就是jar版本不一致导致的方法变更
	查看类的编译签名
	
	其实仔细看看发现就是对应java类型的首字母拉, Boolean 比较特殊, 对应的是 Z ， Long 对应J
　　引用数据类型：比较麻烦点，以“L”开头，以“；”结束，中间对应的是该类型的路径

       如：	String ： Ljava/lang/String；
			Object： Ljava/lang/Object；
       自定义类 Cat  对应  package com.duicky;
              Cat ： Lcom/duicky/Cat；

　　数组表示：  数组表示的时候以“[” 为标志，一个“[”表示一维数组
       如：int [] ：[I
           Long[][]  ： [[J
           Object[][][] ： [[[Ljava/lang/Object；
		   
	传输带有特殊字符的编码过就可以传后端
		[{"amount":"200","orderId":"1"},{"amount":"300","orderId":"2"}]
		%5B%7B%22amount%22%3A%22200%22%2C%22orderId%22%3A%221%22%7D%2C%7B%22amount%22%3A%22300%22%2C%22orderId%22%3A%222%22%7D%5D
		
	Map<String, String[]> parameterMap = request.getParameterMap();
	也可 Enumeration<?> paramNames = request.getParameterNames()  遍历名称获取
	
	
	
118.上传文件，
		单文件上传 MultipartFile
			1.既然是文件上传，form 中	enctype="multipart/form-data"  method="post"
			2.commons-fileupload和commons-io这两个文件上传的依赖包
			3.配置commonsMultipartResolver
			<form action="/talents/importTalents"  method="post" enctype="multipart/form-data">

			选择文件 <input type="file" name="excel">

				<input type="text" name="eUserId" value="123">
				<input type="text" name="groupId" value="1">
				<button type="submit">提交</button>
			</form>
		
119.杂知识
		https://www.cnblogs.com/z00377750/p/9136385.html  	JavaWeb架构发展  其中也涉及了前后分离的介绍
		
		********************
		重点问题记录
			k8s部署dubbo-admin问题
				bbo.xsd 识别错误--->找dubbo.jar---->找pom.xml依赖--->确定发布分支-->至于sit能发布的原因是，war包名称不一致，导致去/data/apps下使用原dev的包。
		
			升级dubbo的问题，版本升级一定要参考官网的，不能随意使用版本，标准就是官方文档。任何时候都不要用最新的版本试验
				dubbo升级2.7.x,  不要使用最新版，按照官方文档中的推荐2.7.7即可	
				
				dubbo官方文档						http://dubbo.apache.org/    
				springboot-dubbo官方文档			https://github.com/apache/dubbo-spring-boot-project/

		********************		

		双循环的终止问题，continue 和break都只影响本身的循环，不会影响上层循环体
		
		AxureRP文件将resource下的crx文件rar格式解压，然后对应插件中开发者模式加载目录，即可访问index页面
		
		但如果是开发api接口，前后端分离，最好使用token，为什么这么说呢，因为session+cookies是基于web的。但是针对 api接口，可能会考虑到移动端，app是没有cookies和session的。

		使用迭代器 list 的remove操作
			使用list操作会有索引左移和并发修改问题，建议使用迭代器删除
			Iterator<Integer> it=list.iterator();
			while(it.hasNext()){
					if(it.next()==3){it.remove();}
				}
			System.out.println(list);
			
			iterator.next() 这里的每次next就会迭到下一个元素，最好用变量接收iterator.next()
			
		servlet-api版本导致的filter加载错误排查
		
		web项目的标准结构：java+resource+webapp.这个默认的结构，一般不会去自己定义

	 总结
		boot中
			1. No MyBatis mapper was found in '[com.xiaoyuer.emanager.pay]' package  启动类下放一个取消警告，不影响使用。所有的waring都不影响使用
			2.mybatis-config.xml 可以不用
			3.boot 工程下 无需加web.xml(SpringBootServletInitializer初始化servlet代替了web.xml)
			4.日志看全，没有明显错误的时候，
			5.javax.servlet-api   4.0.1 provided     上线使用的tomcat中的，旧版本servlet-api.jar，接口中是没有方法体的
			
			*****
				当我们运行Tomcat的时候，肯定把Tomcat依赖的jar包都导入了，而<scope>provided</scope>的作用就是让servlet-api依赖只在编译的时候起作用，
				运行的时候不起作用，避免和Tomcat自带的依赖产生冲突，所以我们引入servlet-api依赖
			*****
			
			mysql-cj-abandoned-connection-cleanup没作用 干扰项
			 
			艹   这是最初的错误，没有看 后来东扯西扯，无效查询(然后可能就是一直没有释放mysql的连接相关，报了以上错误)，警告没啥，严重一定是错误
			10-Sep-2020 17:04:41.866 严重 org.apache.catalina.core.StandardContext.startInternal One or more Filters failed to start. Full details will be found in the appropriate container log file

			对应
			严重 org.apache.catalina.core.StandardContext.filterStart Exception starting filter [adminPayFilter]
			java.lang.AbstractMethodError
						
		对象调用方法 其中可以不static,静态方法调用，属性就要static
			
			
		js前段编码
			encodeURI()，用来encode整个URL，不会对下列字符进行编码：+ : / ; ?&。它只会对汉语等特殊字符进行编码。针对访问路径，
			url = 'www.xxx.com/aaa/bbb.do?parm1=罗'
				
			encodeURIComponent ()，用来enode URL中想要传输的字符串，它会对所有url敏感字符进行encode。针对参数param
			url = 'www.xxx.com/aaa/bbb.do?parm1=www.xxx.com/ccc/ddd?param=abcd'	
				
			
				curl "http://www.baidu.com"  如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地  get
				curl -d "username=user1&password=123" "www.test.com/login"	post
				
			curl模拟的访问请求一般直接在控制台显示，适用结果内容比较少
			而wget则把结果保存为一个文件。适用结果比较多
			wget是个专职的下载利器，简单，专一，极致；而curl可以下载，但是长项不在于下载，而在于模拟提交web数据，POST/GET请求，调试网页等。
			
			引用复制
				会出现问题，这种引用传递要注意
				UserInfo userInfo = new UserInfo();		userInfo.setNickName("ceshi1");
				UserInfo userInfo2 = new UserInfo();	userInfo2=userInfo;
				userInfo2.setNickName("ceshi2");
				这里最终会将userInfo的信息也变更了
				
				obj.sonObj    这种类中包含属性类的情况，也属于引用复制，改变新的，也会改变老的子属性 要注意
				
				list<obj>对象的复制，一般是foreach中，新建copy对象。
		
		double的保留位数和四舍五入处理,BigDecimal处理金额计算
				public static double setDifScare(double arg) {
				BigDecimal bl = new BigDecimal(arg).setScale(2, BigDecimal.ROUND_HALF_UP);
				return Double.parseDouble(bl.toString());}
				
		网银支付，网银网页跳转
			后端生成自动跳转的Html表单串(包含验签信息),将生成的html写到浏览器中完成自动跳转打开银联支付页面；
		
		静态类外部访问必须是public。
		debug调试时，类中的静态属性查看不到。
	
		lombok
			1.引入lombokjar 2.ide中安装lombok插件	3.使用@data注解
			不过一般不建议用，需要别人强制安装插件。
			
		druid 可以开启监控功能。(开启监控后，localhost可能访问不了数据库)
		
		sentinel与hystrix对比，微服务的熔断器，限流熔断   
		熔断机制是应对雪崩效应的一种微服务链路保护机制。

		熔断的机制(断路器，故障容错)，是微服务中，调用的服务过载或者出现故障时(后续不断请求阻塞，造成系统down)，自动阻断对服务的访问和调用，转而调用备用方法
			
		暂停执行等待响应的模式是阻塞
		
		应用操作尽可能使用细粒度的锁。
		避免过度设计，别自high，相对于一开始的大而全，更加好的方式是，快速完成一个基础的架构，后期逐渐添加新功能，版本快速迭代新功能，这样实现功能有效开发
		简洁易扩展是比较好的方式。适合简单运行而不是复杂崩溃。
		方法尽量private 减少其他类的使用耦合
		
		复制粘贴代码的后续问题是，遇到一个改动可能要改两个地方，重复的越多，改动就越多，比如主站和job的相同逻辑
		
		设计的时候有三种哦架构图特别的实用，用例图，类图和模块图
		
		开闭原则，尽量考虑的是扩展，而不是重复的修改。
		
		当用户访问网站时候，第一个交互的组件是dns，将域名解析成ip
			
		异步一般是一种发射后不用管的模型，但是异步调用的结果也可以采用回调机制被调用者消费
		回调是一种异步处理技术，调用者等待执行结果时不会阻塞，而是提供一种操作完成后被通知的机制
		
		针对消息消费的无序性，可以由应用流程保证，就是确认满足消息的发送条件之后再发送。
		
		想要高效区分任务优先级，需要知道成本和价值，然后就有成本价值划分优先级（任务优先级=任务价值/任务成本），不要做无用功
			
		spring security	
			1.引入spring-boot-starter-security   2.@EnableWebSecurity
		
		比较常用的工具类
			apache commons ，google guava, joda time，fastjson
			HttpClient，工具类现在已 经从 Apache Commons 移到 Apache HttpComponents 中，并且包名被改为 org apache.http

			PropertyUtils 其和 BeanUtils 功能几乎一致 不同的是 BeanUtil 在对Bean 赋值时会进行自动类型转化，只要属性名相同，类型会尝试转换，而 PropertyUtil会报错
				
			httpClient 是使用HttpEntity来现的 常用的几个 httpEntity:UrlEncodedFormEntity,MultipartFormEntity,StringEntiry
					
			
		ajax 返回前台会有数据暴露(可以在network中看到返回的参数),页面跳转没有数据暴露的问题	
		
		www.yu.com/auth  实际就是转到了192.168.6.222:8080/auth(实际从yu过来，带着cookie,访问了具体的ip机器)    当然/ids路径下的cookie拿不到，
		还要就是cookie的编码问题，不要使用编码
		
		主站的绑卡http访问支付，支付从页面中是拿不到cookie对象的，这样支付实际没有校验card bin。跳过了校验
		非cookie页面过来的，systemGateWay.htm路径清除多余session，相当于systemGateWay.htm过来的，根本不使用当前request中的session，不是页面的不共享session，单独http请求的sessionid是不共享的
	
	
		cookie加密？类似之前springsession中session编码处理一样。定位维护加密key
				
			分布式session的实现思路
				重新实现httpsession的操作接口。操作必须要在进入应用之前完成，可以配一个filter拦截。
				
				配置sessionfilter，请求到达mvc之前封装request和response，并创建自己的innerhttpsession，并设置到request和response中，这样request.getHttpSession就是自己创建的session对象。
				用户请求完成后，会将innerhttpsession的所有内容更新到分布式缓存中，这样用户可跨服务器再次访问。
				解决跨域名共享cookie的问题。
					这样要实现sessioin同步，需要一个跳转引用，该应用可以被多域名访问，功能是，从一个域名下取得sessionid，然后同步到另一个域名下。其实就是一个cookie中的sessionid。
					实现两个域名下的session同步，必须要将统一个sessionId作为cllike写到两个域名下。
					
				 盗用cookie的情况
				 增加一个私密加密信息signinfo?
				 除非用户只盗用的sessionid   要是连singinfo一起盗用，那也没辙
				 
				可以压缩cookie来节省带宽流量，就是将多个k-v作为文本压缩
	
		INVALID BOUND STATEMENT (NOT FOUND)
		检查过了，最后随意在xml文件中加一个空格或者空行然后保存，就好了
	
		api网关
		安全认证
		1.基于 Token 的客户端访问控制和安全策略
		2.传输数据和报文加密，到服务端解密，需要在客户端有独立的 SDK 代理包
		3.基于 Https 的传输加密，客户端和服务端数字证书支持
		4.基于 OAuth2.0 的服务安全认证(授权码，客户端，密码模式等）
	
	数据结构
		ArrayList 和linkedlist区别
			ArrayList是实现了基于动态数组的数据结构，LinkedList是基于链表结构。
			对于随机访问的get和set方法，ArrayList要优于LinkedList，因为LinkedList要移动指针。
			对于新增和删除操作add和remove，LinkedList比较占优势，因为ArrayList要移动数据。
		
		初始化
			List<String> names = new ArrayList<>() {{
				add("Tom");
				add("Sally");
			}};
			
			List<String> colors = Stream.of("blue", "red", "yellow").collect(toList());
		
		LinkedList 是链表结构，增删块，查找慢
		HashSet 首先判断两个元素的哈希值，如果哈希值一样，接着会比较equals 方法 如果 equls 结果为 true ，HashSet 就视为同一个元素。如果 equals 为 false 就不是同一个元素
		
		hashmap 在jdk7中 	数组+链表
				在jdk8中 	数组+链表+红黑树 组成	当链表中的元素超过了8个以后，会将链表转换为红黑树
				
		ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证个 Segment 是线程安全的，也就实现了全局的线程安全
				默认16个Segments
				
		队列是一种特殊的线性表，特殊之处在于它只允许在表的前端（front）进行删除操作，而在表的后端（rear）进行插入操作，和栈一样，队列是一种操作受限制的线性表。进行插入操作的端称为队尾，进行删除操作的端称为队头。
		
		一个字节等于8位  1byte = 8bit      int占用4个字节，long 8个字节，
		
		对象引用操作
			前台js中
				var psa={"name":"wangzhan",}
				var obj1=psa;
				psa={"name":"baidu"}
				JSON.stringify(obj1);   -- 出来的是wangzhan，因为操作的不是同一个对象

				var psa={"name":"wangzhan",}
				var obj1=psa;
				psa.name="baidu";
				JSON.stringify(obj1);	-- 出来的是baidu，因为操作的是同一个对象
	
		测试controller 中的return；   遇到return; 直接终止执行
		测试 异常抛出的return		   异常抛出，直接终止

		静态初始化和bean加载的顺序问题
			es的client静态初始化，mq启动消费和常量类的属性加载顺序引起的异常。之前留下的一个记录，先消费消息，然后加载常量类，出现错误

		事务中  try 事务操作的问题,不影响，之后接收到runtimeexception才会回滚，单个try住不影响外层的事务，这也是遍历中常用的操作
		
		spring 事务控制 设置手动回滚 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); 用在serviceA中try住操作常规B的catch中手动设置回滚，throw运行异常等效

		serviceA中 try{ serviceB.do();} catch (Exception e) {}   
			
		如果B是默认事务，那么B中异常后，整体commit是会异常的，因为同一事务已经被标记过回滚了。
		https://blog.csdn.net/f641385712/article/details/80445912
		org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only
		一开始测试无效是因为B跑到了open  两个系统当然是新事物
		但是try的是一个普通操作是没事的，如果内部有B也有事务，那就要留意点了。
		
		支付的批量验收事务异常原因是，GateWayBatchServiceImpl.systemGateWay(List<Map<String, String>>)入口开启了事务，后面又用了事务模板transactionTemplate做了回滚标记
		
		这种外部没有事务，然后循环两个事务操作，两个事务互不影响，上层没有就新开一个
		
		
		外层事务有事务，这里第一个执行成功了
		 try {
				事务A执行正常
				事务B执行异常
              } catch (Exception e) {}
		
		public void ttest() {
			for (int i = 0; i < 2; i++) {try {netpaytranService.ttest(i);} catch (Exception e) {}}
		}
		
		
		试验：serviceA-> service B b中new事务异常 全部回   
				实际验证，@Transactional并没有捕获异常的功能，遇到运行异常自己回滚后会向上抛出。除非上层捕获了不影响上层
				b中开新事物的话，如果a中和b中操作同一个资源，会锁住，因为a中还没有释放。
		

		http://xiaoyuer.jenkins.net/ 域名  后面是域名 兄弟，应该是http://jenkins.xiaoyuer.net/ 域名
		
	
		dubbo+k8s
			k8s+dubbo	
				https://blog.csdn.net/weixin_41715077/article/details/89385413	 		核心点和原理图
				https://blog.csdn.net/xujiamin0022016/article/details/107288208/		实际操作

			幸运的是dubbo提供了自动义指定注册ip的配置，我们可以在部署开发联调服务时指定ip地址为宿主机的ip地址。
			env:
					  - name:  APOLLO_META
						value: http://172.31.205.22:8080
					  - name: DUBBO_IP_TO_REGISTRY
						value: 172.31.205.23
					  - name: DUBBO_PORT_TO_REGISTRY
						value: "30011"
					  - name: DUBBO_PORT_TO_BIND
						value: "30011"

			有考虑过，但后期 K8S 节点扩容后，集群外服务往集群内的一些网络权限变更就得重新搞，一个服务就得开一个策略，假设有一百个服务，扩容一台节点，就得加一百条策略，网络工程师应该会砍死我，。

			针对外部访问不到容器内的服务
				目前是这样解决的，在容器部署模板里的 env 部分定义一个变量，并声明 valueFrom.fieldRef.fieldPath 的值为 status.hostIP，
				这样容器启动后就可以通过环境变量获取到宿主机的 IP 地址，将注册地址改为主机地址，外部 zookeeper 对主机地址可见即可
				env:
						- name: DUBBO_IP_TO_REGISTRY
						  valueFrom:
							fieldRef:
							  fieldPath: status.hostIP
						- name: DUBBO_PORT_TO_REGISTRY
						  value: "20920"	
					  
		rememberMe 记住登录
			if("true".equals(rememberMe)){
				Cookie cookie = new Cookie("cellphone",cellphone);
				cookie.setPath("/");//根目录
				cookie.setMaxAge(-1);//永久
				response.addCookie(cookie);
			}else{
				Cookie cookie = new Cookie("cellphone",null);
				cookie.setMaxAge(0);
				response.addCookie(cookie);
			}
			request.getSession().setAttribute("loginUser",JSON.toJSONString(user));
			
			常规的if判断 用 ==？ : 会简洁点
			
			
			并发等待,redis锁，搭配 synchronized 入口方法
				使用while(判断)+redis实现并发的等待处理  redisUtil.exists(key), redis的setEx，
				
			@RequestParam  传空串是可以的，类似这种?cellPhone=&userName=要小心
			
			
			对于xml解析的实用类，XStream用的比较常用，//把xml为转换为实体对象 对于嵌套的xml内容使用内部类接收
			
		redis相关
		内存读写
		作用：一个是缓存常用的数据，另一个核心的问题是数据一致性和访问控制。
		存命中率高的，读操作多的，小数据量的数据
		
		老版的使用jedis自己加载客户端，后面boot都建议使用集成的StringRedisTemplate,不要自定义了
		不要将整个对象存到redis中，性能差
		
		使用它就能够帮助把对象通过序列化储到 Redis 中，也可以把 Redis 存储的内容转换为 Java 对象，为 Spring 供的 RedisTemplate 还有两 属性。
		? KeySerializer一键序列器
		? ValueSerializer一值序列器。
		
		Redis 还支持一些事务 订阅消息模式、主从复制、持久化作为 Java 开发人员需要知道的功能
		
		发布订阅整个一般也交给mq做，实际一般不使用
		
		这里有一个问题需要讨论 如果 key, 超时了， Redis 会回收 key 的存储空间吗 ？
			答案是不会。
			***** Redis的key 超时不会被其自动回收，它只会标识哪些键值对超时了。*****
			Redis 提供两种方式回收这些超时键值对， 它们是定时回收和惰性回收。
			定时回收，在确定的某个时间触发段代码，回收超时的键值对
			惰性回收，一个超时的键，被再次用get命令访问时将触发 Redis 其从内存中清空。
		
		
		内存回收，主要是针对键值对的回收
			noeviction	内存达到最大的，它就只能读而不能写。
			
			Redis在默认情况下会采用 noeviction 策略。换句话说，如果内存己满，则不再提供入操作，而只提供读取操作。
			显然这往往并不能满足我们的要求，因为对于互联网系统而言常常会涉及数以百万甚至更多的用户，所以往往需要设置回收策略。
			
		主从操作，一般用在数据库和redis这种存储型结构中	
		redis一般做主从，好处是减轻压力和更加稳定
			主写从读，主机是一台，而从机可以是多台
			主服务器在写入数据后，即刻将写入数据的命令发送给从服务器，从而使得主从数据同步。
			从挂了不收影响，主挂了会在从服务器中选举一台来当主服务器
			
			默认采用 Redis当前目录的 dump.rbd 文件进行同步。
			
			*****绝大多数的迁移步骤，先迁移整体，同时开启临时记录区域，同步进行*****
			主从同步的过程重点
				1.同步开始，主会备份数据，主将备份文件发送给从，并同时将新增命令放进缓存区，
				2.从会丢弃所有现有的数据，开始载入发送的快照文件
				3.待从执行完备份文件，主会将缓冲区的也发过去执行，后面就主收到一条写就发给从一条同步写(从在解析完备份文件，接收命令，等待命令写入)
				
			主从切换技术，一般不手动换，费劲
			一主(master)二从(slave)三sentinel的架构模式,  三个哨兵监控三个服务器
			
			哨兵模式
			
			哨兵的配置在sentinel.conf配置文件中配置，一般2个及以上的哨兵投票才能切换主机(默认超时3分钟后才会进行投票切换主机)
			
			哨兵，以独立的进程监控3台服务器Redis是否正常运行,实际使用一般多个哨兵共同监控(除了监控各个Redis服务外，各个哨兵之间还会相互监控)
				这里的哨兵有两个作用
				通过发送命令，让 Redis服务器返回监测其运行状态，包括主服务器和从服务器。
				当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知到其他的从服务器，修改配置文件，让它们切换主机

			redis和数据库一致。
				对于时效性不高的，可以每隔一段时间取更新，但是时效性要求高的，就直接从数据库中查询，并同步写入redis
				redis和数据库的简单同步，不变的不变，微变的间隔变，实时变的直接数据库读后同步redis
				
			redis推荐使用模板操作，RedisTemplate，一般来说简单的string类型的操作使用StringRedisTemplate即可，前者可以操作hash等其他结构
			
			key的类型只能为字符串，value支持五种数据类型：String、list、set、hash、sorted set
			
			字符串定义了key(包括 hash 数据结构)，而值则使用了序列化，这样就能够保存 Java对象了。但是不推荐保存一个java类对象，容易反序列化各系统不兼容	

			命中：可以直接通过缓存获取到需要的数据。
			使用缓存的前提是要高命中率，有效提高性能

			缓存适合“读多写少”的业务场景，反之，使用缓存的意义其实并不大，命中率会很低。
			业务需求决定了对时效性的要求，直接影响到缓存的过期时间和更新策略。时效性要求越低，就越适合缓存。在相同key和相同请求数的情况下，缓存时间越长，命中率会越高。
				
			自调用失败，类似spring事务
				那是因为缓存注解也是基于SpringAOP实现的，对SpringAOP 基础是动态代理技术，也就是只有代理对象的相互调用，AOP 有拦截的功能，才能执行缓存注解提供的功能。
				而这里的自调用是没有代理对象存在的，所以其注解功能也就失效了。
			
			request->防火墙->负载均衡->转发到服务器		
		负载均衡
			请求分发，在关卡处可以通过配置禁止一些无效的请求，比如封禁经常作弊的ip地址，这里是初级过滤
			限流	，负载均衡器有限流的算法，对于请求过多的时刻，可以告知用户系统繁忙，稍后再试，从而保证系统持续可用。
			
			常用负载均衡的策略
				轮循均衡			每一次来自网络的请求轮流分配给内部中的服务器
				权重轮循均衡		根据服务器的不同处理能力，给每个服务器分配不同的权值，使其能够接受相应权值数的服务请求。
				随机均衡
				最少连接数均衡
				处理能力均衡(CPU、内存)
			
		有效请求和无效请求
			1.针对一个用户的无效恶意请求，可以增加图片验证码
			2.短信验证，进一步的限制请求，比如限制用户在单位时间的操作次数，降低请求量， 
			3.一人多账号的，最终集中到实名那步，限制到人信息即可
			4.多人多账号的，僵尸账号排除法，平
			时没交易，只在特殊节日交易
			5.当然还能使IP封禁，尤其是通过同一IP或者网段频繁请求的，但是可能误伤有效请求，需注意
		
		一般系统还是做服务化，偏向按照业务划分
		
		分库主要是一个路由算法，也可以将数据对应数据库信息单独存放，这样路由时直接查询即可。
		
		sql优化，建立索引等优化	这个可以作为一个重点看的
			时刻记住更新是表锁定还是行锁定
			1.有主键和唯一索引，优先使用主键，后者会锁表，前者是行锁定				这个待确认
			2.使用连接查询代替子查询，not in 和 not exist性能低下
		
		cdn,内容分发网络，就近缓存数据到节点，就近分发数据
		
		悲观锁，利用数据库内部机制提供的锁的方法，对更新的数据加锁，并发时一旦一个事务持有了数据库记录的锁，其他的线程将不能再对数据进行更新了，这就是悲观锁的实现方式
			常见操作 select for update				主键查询，所以只会对行加锁。如若是非主键查询，要考虑是否对全表加锁的问题，加锁后可能引发其他查询的阻塞
			当一条线程抢占了资源后，其他的线程将得不到资源，那么这个时候，CPU就会将这些得不到资源的线程挂起，挂起的线程也会消耗的资源，尤其是在高井发的请求中
			只能有一个事务占据资源，其他事务被挂起等待持有资源的事务提交并释放资源
			使用悲观锁就会造成大量的线程被挂起和恢复，CPU频繁切换线程上下文，这将十分消耗资源，性能不佳。
			
			悲观锁一定要注意sql中不能锁表了
			InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。
			还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆
		
			
		乐观锁
			使用乐观锁的弊端在于导致大量的SQL被执行，对于数据库的性能要求较高，容易引起数据库性能的瓶颈，而且对于开发还要考虑重入机制，从而导致开发难度加大。
			
			CAS原理并不排斥并发，也不独占资源，只是在线程开始阶段就读入线程共享数据，保存为旧值。其中可能存在ABA问题
				当处理完逻辑，需要更新数据的时候，会进行一次比较，即比较各个线程当前共享的数据是否和旧值保持一致。不一致考虑重试或者放弃。有时候可重试，这样就是一个可重入锁，
				ABA问题，场景两个线程同时判断一个变量，线程2变成了B后又变成了A,但是线程1中无感知，
					前提1，线程1,2并不是想spring事务一样完成后才提交，外层没事务影响
					前提2，基于单独的业务字段判断，并且能回退
					解决，加入不能回退的非业务字段version，只能增。
					总的来说就是对值变化后还原无感知，导致的更新错误
					解决，加入不能回退的非业务字段version。
				
			解决并发失败数量多的场景
				使用重入机制
					不能重入太多次，限制1.按规定时间内重入，入成功为止，过时退出  2.按规定次数重入 ，while(true){} 实现限制重入
				
		 stream of elements -> filter ->sorted -> map -> collect
				
		 在jdbc的数据库处理中，批量操作，尽量一批一批的整量操作		
		 
		Spring Java 多种 Redis 连接 API 进行了封装，各连接的实现类都继承了抽象类AbstractRedisConnection(实现RedisConnection 接口)。所以对于使用者
		而言，只需要知道 RedisConnection 接口的 API 就可以消除各个 API 的差异了。
		有了这个接口我们自然可以想到 Spring 会提供创建这个接口对象的工厂一RedisConnectionFactory 。它也是类似的一个简易模型，RedisConnectionFactory，通过工厂就可以生成 RedisConnection 以通过各命令操 Redis各命令
					
		所有的key(key和hashKey), 设置为StringRedisSerializer			
		hashValue:? ? ? ?推荐使用? ? ?GenericJackson2JsonRedisSerializer:类似Jackson2JsonRedisSerializer，但使用时构造函数不用特定的类   默认是JdkSerializationRedisSerializer
			
			
		StringRedisSerializer：对String数据进行序列化。序列化后，保存到Redis中的数据，不会有像上面的“\xAC\xED\x00\x05t\x00\x09”多余字符。就是"frequency".	
			
		如果不进行设置的话，默认使用JdkSerializationRedisSerializer进行数据序列化。
		（把任何数据保存到redis中时，都需要进行序列化）
		用视图的形式查了一下，发现实际保存的内容如下：
		key-value：
		  key:\xAC\xED\x00\x05t\x00\x08test_key
		  value:\xAC\xED\x00\x05t\x00\x0Dtest_value111	
		所有的key和value还有hashkey和hashvalue的原始字符前，都加了一串字符。查了一下，这是JdkSerializationRedisSerializer进行序列化时，加上去的。
		原以为只会在value或hashvalue上加，没想到在key和hashkey上也加了，这样的话，用原来的key就取不到我们保存的数据了。所以，我们要针对我们的需求，设置RedisSerializer。  
		  
		1，用StringRedisSerializer进行序列化的值，在Java和Redis中保存的内容是一样的
		2，用Jackson2JsonRedisSerializer进行序列化的值，在Redis中保存的内容，比Java中多了一对双引号。
		3，用JdkSerializationRedisSerializer进行序列化的值，对于Key-Value的Value来说，是在Redis中是不可读的。对于Hash的Value来说，比Java的内容多了一些字符。
					

	redis
		作为缓存系统，Redis还可以限定数据占用的最大内存空间，在数据达到空间限制后可以 按照一定的规则自动淘汰不需要的键
		
		列表中 LPUSH 和 RPUSH 可以想左右两边增加元素，对应还有弹出功能LPOP 和 RPOP
		
		redis事务、队列处理暂不操作
		
		lpush test_redis_list 1 2 4
		查看、获取list  lrange test_redis_list  0 3   
		
		持久化
			rdb	
				进行快照的条件可以由用户在配置文件中自定 义，由两个参数构成：时间和改动的键的个数。当在指定的时间内被更改的键的个数大于指定 的数值时就会进行快照。RDB是Redis默认采用的持久化方式
				Redis默认会将快照文件存储在当前目录的dump.rdb文件中，可以通过配置dir和 dbfilename两个参数分别指定快照文件的存储路径和文件名
				Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。
				通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。
		aof
				开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬 盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认 的文件名是appendonly.aof，可以通过appendfilename参数修改：
				可见AOF文件是纯文本文件，其内容正是Redis客户端向Redis发送的原始通信协议的内容
				实际上Redis也正是 这样做的，每当达到一定条件时Redis就会自动重写AOF文件，重写是为了删除冗余命令，比如多条覆盖，只保留最后一条即可
			
				
				执行命令由于缓存机制，不是直接写入磁盘(现在硬盘缓存，随后写入硬盘,AOF文件中)
				在默认情况下系统每30秒会执行一次同步操作，以便将硬盘缓存中的内容真正地 写入硬盘，在这30秒的过程中如果系统异常退出则会导致硬盘缓存中的数据丢失。
				一般来讲 启用AOF持久化的应用都无法容忍这样的损失，这就需要Redis在写入AOF文件后主动要求系 统将缓存内容同步到硬盘中。
				在Redis中我们可以通过appendfsync参数设置同步的时机：
				默认情况下Redis采用规则
				everysec		即每秒执行一次同步操作。这个推荐使用
				always  		每次执行写入 都会执行同步，这是最安全也是最慢的方式。
				no 				不主动进行同步操作，而是完全交由操作系统来做（即每30秒一次），这是最快但最不安全的方式。
				
		主从复制
			基于快照文件和缓存的命令实现
			当一个从数据库启动后，会向主数据库发送SYNC命令，主数据库接收到SYNC命令后会
			开始在后台保存快照（即RDB持久化的过程），并将保存期间接收到的命令缓存起来。当快照 完成后，Redis会将快照文件和所有缓存的命令发送给从数据库。从数据库收到后，会载入快 照文件并执行收到的缓存的命令。当主从数据库断开重连后会重新执行上述操作，不支持断 点续传
			在复制的过程中，快照无论在主数据库还是从数据库中都起了很大的作用，只要执行复制就会进行快照，即使我们关闭了RDB方式的持久化（通过删除所有save参数）。更进一步，无 论是否启用了RDB方式的持久化，Redis在启动时都会尝试读取dir和dbfilename两个参数指定 的RDB文件来恢复数据库。
			
		redis删除策略
			两个定是主动，惰性是被动。
			redis实际使用是惰性删除+定期删除的组合。
			定时删除：创建一个定时器，当key设置有过期时间，且过期时间到达时，由定时器任务立即执行对键的删除操作   性能不佳
			定期删除：redis默认是每隔100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取
			惰性删除：定期删除可能会导致很多过期key到了时间并没有被删除掉。所以就有了惰性删除。
					  假如你的过期key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个key，才会被redis给删除掉。这就是所谓的惰性删除
				
				
		122
			在创建新的额rdb文件时，会检查键，已经过期的键不会被保存到新建的rdb文件中。在aof重写过程中也是
			当一个过期键被删除后，服务器会追加一条del命令到现有的aof文件末尾，显示删除过期键。
			主服务器删除过期键，会向从发送del命名，统一由主发起删除
			
			Redis 主从复制、哨兵和集群三者区别
				主从复制是为了数据备份，哨兵是为了高可用，Redis主服务器挂了哨兵可以切换，集群则是因为单实例能力有限，搞多个分散压力，简短总结如下：
				主从模式：备份数据、负载均衡，一个Master可以有多个Slaves。
				sentinel发现master挂了后，就会从slave中重新选举一个master。
				cluster是为了解决单机Redis容量有限的问题，将数据按一定的规则分配到多台机器。
				sentinel着眼于高可用，Cluster提高并发量。
				1. 主从模式(数据备份)：读写分离，备份，一个Master可以有多个Slaves。解决数据备份问题
				2. 哨兵sentinel(高可用)：监控，自动转移，哨兵发现主服务器挂了后，就会从slave中重新选举一个主服务器。解决故障切换问题
				3. 集群(数据分片,f分布式存储)：为了解决单机Redis容量有限的问题，将数据按一定的规则分配到多台机器，内存/QPS不受限于单机，可受益于分布式集群高扩展性。解决单机容量问题
		
	
			sentinel高可用性。将数据存储到多个Redis实例中。
			cluster模式的出现就是为了解决单机Redis容量有限的问题（主从模式或sentinel模式就不能满足需求），这时数据进行分片，将Redis的数据根据一定的规则分配到多台机器redis实例中。
			每个集群中至少需要三个主数据库才能正常运行，Redis集群至少需要3个节点，每个节点又需要一个从备份节点，所以Redis集群至少需要6台服务器。
			
			
			因为redis的集群是把内容存储到各个节点上，而哨兵的作用就是监控redis主、从数据库是否正常运行，主出现故障自动将从数据库转换为主数据库
			集群不必另外使用 Redis Sentinel，内置了哨兵功能
			
			主从模式 可以实现读写分离，数据备份。但是并不是「高可用」的
			哨兵模式 可以看做是主从模式的「高可用」版本，其引入了 Sentinel 对整个 Redis 服务集群进行监控。但是由于只有一个主节点，因此仍然有写入瓶颈。
			Cluster 模式 不仅提供了高可用的手段，同时数据是分片保存在各个节点中的，可以支持高并发的写入与读取。
			虚拟槽分区是Redis Cluster采用的分区方式，Redis Cluster的节点之间会共享消息，每个节点都会知道是哪个节点负责哪个范围内的数据槽
			客户端访问任意节点时，对数据key按照CRC16规则进行hash运算，然后对运算结果对16383进行取作，如果余数在当前访问的节点管理的槽范围内，则直接返回对应的数据
			如果不在当前节点负责管理的槽范围内，则会告诉客户端去哪个节点获取数据，由客户端去正确的节点获取数据
			把16384个槽平均分配给节点进行管理，每个节点只能对自己负责的槽进行读写操作
			由于每个节点之间都彼此通信，每个节点都知道另外节点负责管理的槽范围			
			保证高可用，每个主节点都有一个从节点，当主节点故障，Cluster会按照规则实现主备的高可用性
			
			redis的动态扩容操作都是通过redis-trib.rb脚本文件来完成的
			当添加新节点成功以后，新的节点不会有任何数据，因为他没有分配任何的数据Slot(哈希slots),这一步需要手动操作。
			
			
			扩容需要新节点没有slot，需要重新分配
			常用的数据分片的方法有：范围分片，哈希分片，一致性哈希算法，哈希槽等
			分配槽位，可以自定义指定节点的分槽范围，一般要均分。slot 0-5460
			多少个节点，自己针对16384个哈希槽，自己分配
			Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽(Slot)，集群的每个节点负责一部分hash槽
			在Redis Sentinel模式中，每个节点需要保存全量数据，冗余比较多，而在Redis Cluster模式中，每个分片只需要保存一部分的数据，
			Redis Cluster数据分区规则采用虚拟槽方式(16384个槽)，每个节点负责一部分槽和相关数据，实现数据和请求的负载均衡
			通过对Key进行CRC16(key)%16384运算得到对应的槽是哪一个，从而将读写操作转发到该槽所对应的服务节点
			Redis集群使用数据分片(sharding)而非一致性哈希(consistency hashing)来实现：一个Redis集群包含16384个哈希槽(hash slot)，数据库中的每个键都属于这16384个哈希槽的其中一个，集群使用公式CRC16(key)%16384来计算键key属于哪个槽，其中CRC16(key)语句用于计算键key的CRC16校验和。
			Redis官方推荐使用redis-trib.rb工具快速搭建Redis Cluster
			集群自动故障转移过程分为故障发现和节点恢复。节点下线分为主观下线和客观下线，当超过半数节点认为故障节点为主观下线时，标记这个节点为客观下线状态。从节点负责对客观下线的主节点触发故障恢复流程，保证集群的可用性
			
			那么每个节点都和其他N-1个节点保持连接和心跳，节点之间采用Gossip协议进行通信。通信主要确认节点是否存活、节点的数据版本、投票选择新的master等内容。
			
			扩容步骤
				一、添加两个服务节点到集群
					redis-cli --cluster add-node 192.168.8.196:5007 192.168.8.196:5001
					redis-cli --cluster add-node 192.168.8.196:5008 192.168.8.196:5001
				二、为master节点添加分片，重新分配槽
					redis-cli --cluster reshard 192.168.8.196:5007
					指定slot数量和槽来源,选择all，就是全节点指定数量迁移
				三、设置从节点
				
			Redis 集群中内置了 2^14=16384 个哈希槽，当需要在 Redis 集群中放置一个key-value时，redis 先对key使用crc16算法算出一个结果，
			然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。	
			当你往Redis Cluster中加入一个Key时，会根据crc16(key) mod 16384计算这个key应该分布到哪个hash slot中，一个hash slot中会有很多key和value。
			
			
			
			
	排除core-dao后	编译正确，运行异常
	同名路径的依赖问题，注意jar和另一个类。
	1.xye-open-web引入了xye-core-dao      使用的是userInfo类  暂时看手机存的是json字符串，没有序列化问题   主要涉及小程序支付的问题
	  ac-dmo中自建引用的core.dao.dmo   同名覆盖  这样看编译时看上去是引用jar，但实际运行时同名包下的类，可以用get属性测试
			
	注意文件长度是字节长度，不是位长度
	byte[] bytes = string.getBytes("GBK");
	StringBuilder sb = new StringBuilder(minLength);
	for (int i = bytes.length; i < minLength; i++) {sb.append(padChar);}
	
	
		BufferedReader in = new BufferedReader(new InputStreamReader(request.getInputStream()));
		StringBuffer strBuff = new StringBuffer();
		String line;
		while ((line = in.readLine()) != null) {
			strBuff.append(line);
		}
		Utils.log("HTTPS,服务器响应结果是: \n" + strBuff.toString());
    	
	
		简化if  else   return timeFlag ? true:false;

		字符串比较
			String s4 = s3+",world!";  s3+=",world!"; 类似  变量引用会开新地址
			
		MediaType,即是Internet Media Type,互联网媒体类型；也叫做MIME类型， 
		在Http协议消息头中，使用Content-Type来表示具体请求中的媒体类型信息。（推荐使用这个，下二）
		常见的媒体格式类型如下
		text/html:HTML格式
		text/xml:XML格式

		以application开头的媒体格式类型：
			application/xml:XML数据格式
			application/json:JSON数据格式
			application/octet-stream:二进制流数据（常见的文件下载)
			application/x-www-form-urlencoded:表单中默认的encType,表单数据被编码为key/value格式发送到服务器

		另外一种常见的媒体格式是上传文件时使用：
		multipart/form-data:需要在表单中进行文件上传时，就需要使用该格式
		
		
		小鱼儿系统
			每次来一个线程就存一次context，每次覆盖一个新的CommonConstant.PAGE_SERVICE_SESSION_KEY, uuid 来存值setContext中存。相当于将信息存在了session中
			private GenericResult<Object> invokePageService(OpService opService)
				throws IOException
			{
				String uuid = UUID.randomUUID().toString();
				ApplicationContext context = ApplicationContext.getContext();
				HttpServletRequest request =((ServletRequestAttributes)RequestContextHolder.getRequestAttributes()).getRequest();
				request.getSession().setAttribute(uuid, context);
				request.getSession().setAttribute(CommonConstant.PAGE_SERVICE_SESSION_KEY, uuid);
				LOGGER.info("设置session{},UUID是:{}", request.getSession().getId(),uuid);
				GenericResult<Object> result = new GenericResult<Object>();
				result.setObject(opService.getPageTarget());
				return result;
			}
				
		域名解析ip(180.97.15.66)->接收66,防火墙打出到250->转到192.168.8.250(keepalived配置的虚拟ip)内部服务器,指定跳到31->31nginx-机器，转发到相应的服务器。
				
				
		小程序登录相关信息返回
		   String url = "https://api.weixin.qq.com/sns/jscode2session?appid="
                    + appId + "&secret=" + appSecrect + "&js_code=" + wmpcode + "&grant_type=authorization_code";
            resultStr = HttpClientUtil.httpGet(url);		
			
		element ui 有axure原型资源，可以直接套用	
		
		@Deprecated  手动添加  标注废弃而已	
		
		git ls-files -v | grep "^[a-z]"   找出   git 中的 assume-unchanged 的文件
		
		反射相关
			Field field=clazz.getDeclareField("name");
			field.set(this,"Test")；
			如果属于类的静态属性，那么set和get方法的第一个参数就可直接设置为null；因为静态不需要实例，跟着类走的。
			
			
			方法，Method[] method=clazz.getDeclaredMethods();
			属性，Field[] field=clazz.getDeclaredFields();
			构造函数，Constructor[] constructor=clazz.getDeclaredConstructors();
			
			Class 对象的 newInstance()方法来创建该 Class 对象对应类的实例，但是这种方法要求该 Class 对象对应的类有默认的空构造器
			
			
		8位是一个字节，8bit=1byte,一位就是二进制数的一个数字。byte类型的数据是8位带符号的二进制数
		
		加速使用cdn
		目前主要用来缓存静态数据
			其实就是一种网络缓存技术
			cdn作用是把用户需要的内容分发到离用户近的节点，使用户就近获取所需内容。
			cdn系统分为cdn源站，cdn节点。源站提供节点使用的数据源头，而节点部署在距离最终用户比较近的地方。
			
		防止表单重复提交
			请求表单页面创建一个token存入缓存中，带到页面的hidden token中  提交的时候校验
			
		观察者模式，也叫发布-订阅模式，就是时间监听机制。
		代理模式，给某个对象创建一个代理对象，对原对象的调用增加一些额外操作
		
		10M的宽带的带宽是1280KB/s。
		ISP提供的线路带宽使用的单位是比特（bit），而一般下载软件显示的是字节（Byte）（1Byte=8bit），所以要通过换算，才能得实际值。
		1Mb/s=1024Kb/s=1024/8KB/s=128KB/s  
		上行宽带(速度)和下行宽带(速度)是不对称的，一般是下行速度大于上行的速度。我们平时所使用的宽带说多少M，都是指的下行宽带，
	
		前段页面中的network中的doc类型  xhr类型(XMLHttpRequest)
		它依赖的是现有的CSS/HTML/Javascript，而其中最核心的依赖是浏览器提供的XMLHttpRequest对象，是这个对象使得浏览器可以发出HTTP请求与接收HTTP响应。
		所以我用一句话来总结两者的关系：我们使用XMLHttpRequest对象来发送一个Ajax请求
		
120.前后端分离
		前端从后端剥离，形成一个前端工程，前端只利用Json来和后端进行交互，后端不返回页面，只返回Json数据。前后端之间完全通过public API约定
		未分离，先后端取数据后渲染页面。需要等后端返回数据才能处理(页面跳转，值在requrest中)，/分离后，先html后向后台取数据，这个可以自己模拟Jason数据不需要等
	
		前后端分离这样的开发架构下，前后端的交互都是通过 JSON 来进行，没有什么服务端跳转或者客户端跳转之类。

		前后端分离的情况下，基本使用json交互，使用@RestController相对@responseBody更加的方便
		这和mvc模式不一样，原:通过视图名称找到对应视图，将数据模型渲染展示
	
		使用jason交互，是没哟mvc这套视图配置的
		
		rest
		一般还是只是用Get和Post请求，使用接口名字来区分，所以，对于Rest规范，只需要记得传递数据只使用JSON，而不是后端去渲染模板，从而实现前后端的完全分离。
	
		前后端约定接口&数据&参数
		前后端并行开发（无强依赖，可前后端并行开发，如果需求变更，只要接口&参数不变，就不用两边都修改代码，开发效率高）
		
		大多数都是单独请求后台数据，使用json传输数据，而不是一个大而全的HTTP请求把整个页面包括动+静全部返回过来
	
		浏览器发送请求
			1.直接到达html页面（前端控制路由与渲染页面，整个项目开发的权重前移）
			2.html页面负责调用服务端接口产生数据（通过ajax等等，后台返回json格式数据，json数据格式因为简洁高效而取代xml）
			3.填充html，展现动态效果，在页面上进行解析并操作DOM。
	
121.httpclient
		soa调用默认就是传输的80端口
		创建了个SSLClient(基于DefaultHttpClient使用)
		本质都是使用了DefaultHttpClient，支付都是使用的443端口(可以nginx中转到支付的80，但是原请求都是s端口)
		JSONObject result = HttpClientUtil.httpsPost(httpsurl, port, params,timeOut);


		使用 httpPost提交参数	UrlEncodedFormEntity		BasicNameValuePair，必须要使用如下类型，使用json传参会失败
		content-type	application/x-www-form-urlencoded; charset=UTF-8
		//httpPost.addHeader("Content-Type", "application/json");
			  


		
122.HttpRequest中常见的四种ContentType
			客户端发起 HTTP POST/PUT 请求时，可以指定Request Header 的 Content-Type，表示向服务端发送的 Request Body 中的数据的格式
			服务端返回数据时，也可以指定Content-Type，表示返回数据的格式

			 HTTP 请求分为三个部分：状态行、请求头、消息主体。
			 服务端通常是根据请求头（headers）中的 Content-Type 字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。
			 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。
			<method> <request-URL> <version>
			<headers>
			<entity-body>
			===============
			POST http://www.example.com HTTP/1.1
			Content-Type: application/x-www-form-urlencoded;charset=utf-8
			title=test&sub%5B%5D=1&sub%5B%5D=2&sub%5B%5D=3
			
			1.application/x-www-form-urlencoded	
				浏览器的原生 form 表单,不设置 enctype 属性,默认属性
				浏览器原生支持
				
			2.multipart/form-data
				一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 form 的 enctyped 等于这个值
				浏览器原生支持
				
			3.application/json
				低版本 IE 之外的各大浏览器都原生支持 JSON.stringify
				直接提交json串
				POST http://www.example.com HTTP/1.1
				Content-Type: application/json;charset=utf-8
				 
				{"title":"test","sub":[1,2,3]}
				
				当时我是把 JSON 字符串作为 val，仍然放在键值对里，以 x-www-form-urlencoded 方式提交。(这种就有点old了)
				
			4.text/xml
				一般不用，没有json灵活
123.小鱼儿系统的设计相关
		open传参方法，这里更像一个网关，负责将请求转发到具体的业务处理。API Gateway 负责请求转发、合成和协议转换。
		请求地址:	/xye-open/gateWay/systemGateWay.htm|Params:partner=XYE_ADMIN2&query_type=userBankcardsInfo&service=op_common_query_serv...&sign=055e5bb5dbda93f43d82...&sign_type=md5&userId=131718|Spend:8

			 systemGateWay--->1.url参数转map,2.验证op_service,3.验证op_partner，并且验证两者关系,4.验证sign
			 --->在线程变量中存入先关的参数信息（parametersMap,opservice,oppartner 和partner_service）
			 --->调用opservice的执行方法(IOpService ops = (IOpService)SpringContextUtil.getBean(opService.getSystemTarget());if (ops.execute()))
			 -->ApplicationContext.getContext().getParameters();    从线程变量中拿到变量开始执行方法.那么相当于是使用的map传递。然后根据 业务组装各自的参数dto
			 -->层级返回

124.jdk1.8 jdk8 java8
		简化遍历的方式(原 for 循环),但是要确定循环次数就要用for循环
			//1、正常遍历		list.forEach(item->System.out.println(item));
			//2、根据条件遍历	list.forEach(item->{if("b".equals(item)){System.out.println(item);}

			Map.forEach((key, value) -> {System.out.println(key + "：" + value);});  这个遍历比较方便
			
			创建->转换->集合
			List<Integer> ageList = userInfos.stream().map(userInfo -> userInfo.getAge()).collect(Collectors.toList()); // [10, 20, 10]  可以写map(UserInfo::getAge)
			
			String result = list.stream().collect(Collectors.joining(","));  //逗号拼接
			
			map.entrySet().stream().sorted(Comparator.comparing(Person::getKey)).collect(Collectors.toList())  //排序
			
			//使用map方法获取list数据中的name，这里map就是获取一个新的集合，原集合不变
			List<String> names = list.stream().map(Student::getName).collect(Collectors.toList());
			
			
			方法引用又包括实例方法、静态方法。双冒号（::）,语法格式 	类名 :: 方法名。
			[双冒号运算表达式]是个什么类型。答案是：Function<T,R>类型。也是lambda的简写方式
			
			这种[方法引用]或者说[双冒号运算]对应的参数类型是Function<T,R> T表示传入类型，R表示返回类型。
			比如表达式person -> person.getAge(); 传入参数是person，返回值是person.getAge()，那么方法引用Person::getAge就对应着Function<Person,Integer>类型。
			https://www.runoob.com/java/java8-method-references.html
			
			java8的一些新特性
				接口的默认方法：即扩展方法。	允许通过关键字default向接口中加入非抽象方法。（也允许添加静态方法）
					
				函数式接口定义为只具备一个抽象方法的接口。
				Java8 在接口定义上的改进就是引入了默认方法，使得我们可以在接口中对方法提供默认的实现，但是不管存在多少个默认方法，只要具备一个且只有一个抽象方法（default 默认方法、static 静态方法以及继承 Object 的方法都不算抽象方法），那么它就是函数式接口。
					@FunctionalInterface
					public interface Comparator<T> {
						int compare(T o1, T o2);
						
						boolean equals(Object obj);
						
						default Comparator<T> reversed() {
							return Collections.reverseOrder(this);
						}
						//...
					}

				由于JDK1.8中接口支持默认方法，但即使一个接口中有很多默认方法，只要接口只定义了一个抽象方法，它就仍然是一个函数式接口。
				lambda,参数列表+箭头+函数体组成
					*****只有在有函数接口的地方才能使用lambda*****
					********用来代替函数式接口的匿名实现类********
					*****场景：用来代替一些匿名类。或者是赋给一个变量*****
					*****简化函数式接口的匿名类，只是创建了一个匿名的实现类，参数还是要最终传入的*****
					函数接口：仅仅包含一个抽象方法的接口。
					可将任意只包含一个抽象方法的接口用作lambda表达式。使用＠FunctionalInterface,可提前检查合法性。
					也可以使用::来简化，暂不用，后面遇到再说
						・静态方法引用 	Integer::valueOf
						・实例方法引用： System out: println
						・构造方法引用： User :: new
						・某个类型的任意对象的实例方法引用： User::getName
					
				stream
					增加了连续的操作，先做filter再map的原则
				
				map
					list 和map 增加foreach简化的遍历方式
					
					ConcurrentHashMap 的 keySet方法被修改，尽量不要使用  
					
				Date API
					由joda time 作者写的
					
				float,double 只能用来做科学计算或者工程计算，在商业计算中我们要用java.math.BigDecimal 但是如果使用BigDecimal(double val）构造方法， 
				那么由于小数的double底层存储的是个不确定的数字，使得构造的BigDecimal也是个不确定的数字，应该使用 BigDecimal(String val）构造方法做精确	
				
				stream foreach 是开线程的  数量大了这个块，一般的使用foreach即可,复杂的业务操作逻辑中，流的优势就很大了。
				
				流编程就是增加连贯性，简化写法。stream编程挺方便的。主要操作的对象是元素集合。

				*****
					Lambda表达式由参数列表（People people）,箭头  -> 、和Lambda主体  "男".equals(people.getName()) 三部分组成
				*****	

				格式一： 参数列表 -> 表达式 
				格式二： 参数列表 -> {表达式集合}

				传参形式	推荐，省略参数类型，加上括号，主体加上大括号
					1.(People people)-> "男".equals(people.getName())										//完整的参数列表
					2(people) -> "男".equals(people.getName())												//参数类型可以省略
					3.people -> "男".equals(people.getName())												//当传入参数只有一个时，括号可以省略
					4.(People people,People  people1) -> people.getName.equals(people1.getName())			//当传入多个参数，或者不传参数时，括号不能省略
					5.(people, people1) -> people.getName.equals(people1.getName())							//当传入多个参数，或者不传参数时，括号不能省略，多个参数的参数类型也可以省略

				主体形式
					1.（people） -> "男".equals(people.getName())							//当没有花括号时，表达式结果就是返回值,隐含了return
					2.（people） -> {return "男".equals(people.getName())}					//当有花括号时，要显性返回方法要求的返回信息

				Lambda表达式可以用来替换函数式接口(接口里面只定义了一个抽象方法)的实现类。避免了定义新的类或者使用匿名函数。



			Lambda本身不知道对应哪个函数式接口，编译器会根据上下文（context）环境来适配目标类型(target type)。意思是先右后左。那么函数签名相同，Lambda就可以混合使用
			Runnable a = ()->System.out.println("Runnable");


			当函数式接口的抽象方法与lambda表达式表达的函数签名相同的时候我们可以使用


			jdk8会提供一些默认的函数式接口，比如Consumer等 list的forEach就用了这个接口，这个后面可能要使用.默认的函数式接口基本都在jdk下的java.util.function包下

			特殊的void规则  这个暂时不细看，用到再说
				如果一个Lambda的主体是一个语句表达式，他就和一个返回void的函数描述符兼容（当然需要参数列表也兼容）			Consumer
				如果Lambda表达式的主体是一个语句表达式，那么它就和一个返回void的函数描述符兼容，
			
				
			方法引用,复用的地方又不多推荐使用lambda表达式,否则应该使用方法引用.  

			Stream中map()方法可以理解为对集合的每一个元素进行相应的操作,
			创建一个该接口的一个实例，方法的真正执行需要通过该实例去触发执行
		
			只要保证入参和返回结果一致，就可以使用lambda了，所以双冒号也是lambda的一种缩写


			
         * 通过分析可以得知静态方法eat需要输入的参数为Cat实例，无返回结果，
         * 那么它与JDK所提供的函数式接口Consumer<T>所需要的输入和输出是一致的，
         * 因此该静态方法可以使用函数式接口Consumer<T> + 双冒号lambda表达式 的方式进行引用
         * 引用方式如下：
         */
        Cat cat = new Cat();
        Consumer<Cat> eatMethod = Cat::eat;//静态方法的双冒号表达式是类名::方法名
        eatMethod.accept(cat);

		**********选择合适的函数式接口 才是lambda的使用的关键，一定要匹配**********，只要能匹配上，随便用。使用过程中可以按照需要挑选，大部分情况不需要自己定义函数式接口。
		由于方法的输入参数类型和输出参数类型相同,所以可以使用一元函数接口
		只需要分析清楚方法的输入参数类型和参数格式，以及返回结果类型，然后由此选择合适的函数式接口，再配合双冒号（"::"）lambda表达式，就可以完成方法引用了。
			
		https://blog.csdn.net/nrsc272420199/article/details/84718802    方法引用的实际讲解

		lambda是针对某个函数式接口创建的，所以要选好接口后再匹配相应的方法
		Lambda更适合和JDK提供Stream配合使用

		*****:: 双冒号操作符  已经理解了，就是对应函数式接口的那个方法，一致即可用，需要选择好函数接口*****


		双冒号操作符(方法引用)中的方法需要匹配下面的类型再使用

		四大核心函数式接口
			Consumer<T>  	void accept(T t)	消费型接口
			Supplier<T>	 	 T get()			供给型接口
			Function<T, R> 	 R apply(T t)		函数型接口   
			Predicate<T>	 boolean test(T t)	断言型接口

		Lambda局部内使用外部变量必须是final类型的，但是使用类上的属性，貌似没有问题。方法内的属性需要final，类上的属性不需要
			对引用类型(地址传递)来说是引用地址的一致性(地址不变)，对基本类型(值传递)来说就是值的一致性（数据不变）
			*****在JDK8之后，匿名内部类引用外部变量时虽然不用显式的用final修饰，但是这个外部变量必须和final一样，不能被修改（这是一个坑）。*****
			  即可以对对象属性操作，可以对list操作，但是不能new 对象重新赋值。list是引用对象类型
			解决方案：可以通过定义一个相同类型的变量b，然后将该外部变量赋值给b，匿名内部类引用b就行了，然后就可以继续修改外部变量。

		局部内部类和匿名内部类访问的局部变量必须由final修饰，java8开始，可以不加final修饰符，由系统默认添加。java将这个功能称为：Effectively final 功能

        可以操作引用类型的值，但是不能改变地址
		
		主要用的就是list 和string，都能实现
		map foreach 的遍历拼接string， 使用StringBuilder实现;  String 是final，在内部不可更改


		针对基本类型可以
			1）把 limit 变量声明为 static。
			2）把 limit 变量声明为 AtomicInteger。(数字用)
			3）使用数组。




		System.out::println  这个也是对应消费型接口的一种简写
		list.forEach(System.out::println);
		list.forEach
			default void forEach(Consumer<? super T> action) {
				Objects.requireNonNull(action);
				for (T t : this) {
					action.accept(t);
				}
			}
		@FunctionalInterface实际测试，接口的方法继承也不行
		Runnable也是一个函数式接口	



		特殊情况
			重写了超类Object类中任意一个public方法的方法并不算接口中的抽象方法。
			
			比如：Comparator 中就有  int compare(T var1, T var2);  和  boolean equals(Object var1);
			
		lambda访问外部变量的规则，
			1.只能引用标记了 final 的外层局部变量，这就是说不能在 lambda 内部修改定义在域外的局部变量，否则会编译错误。
			2.局部变量可以不用声明为 final，但是必须不可被后面的代码修改（即隐性的具有 final 的语义）
			3.不允许声明一个与局部变量同名的参数或者局部变量。
			
			
			****这个阶段一般在编译阶段就报错了，以为着可以修改内部的值，但是不能改变地址，不能改变引用*****
			****只要在lambda中使用的变量都默认是final特性*******
			Lambda表达式引用的局部变量无论是否声明final，均具有final特性！表达式内仅允许对变量引用（引用内部修改除外，比如list增删），禁止修改！
			以下情况均不允许编译通过：
				情况一：修改外部局部变量
				 int n = 0;
				 src.forEach(item -> {
					 n = 3;
					 dest.add("dest: " + item);
				 });
				
				情况二：Lambda使用外部局部变量，变量隐性final！
				int n = 0;
				src.forEach(item -> {
					dest.add("dest: " + item + n);
				});
				n = 0;
				
				情况三：声明外部局部变量同名参数
				int n = 0;
				src.forEach(n -> {
					dest.add("dest: " + n);
				});
			*********		


			目前看，只要使用lambda中没有提示外部变量，就可以使用

			

		Stream操作
			形式：创建流+中间处理(intermediate)+终端处理（只能有一个terminal操作）
			其中中间处理有很多如，filter(),map()的常用操作
			优点：
				1.“延迟计算”，占用内存很少。
				2.集合类的迭代逻辑是调用者负责，通常是for循环，而Stream的迭代是隐含在对Stream的各种操作中，例如map()
			Stream 是对集合（Collection）对象功能的增强，高效的聚合操作（过滤、排序、分组、聚合等）
			所有 Stream 的操作必须以 lambda 表达式为参数

			终端处理之后，流关闭，无法再进行中间处理。数据收集主要使用 collect 方法，该方法也属于归约操作

		Optional
			适用场景，连环的属性判空处理
			Optional是为了更优雅的判断null而诞生的,但是并不代表有null的地方一定就要用Optional代替

			判空推荐使用ofNullable
			*****每次的map操作会返回一个optional对象，相当于操作的是optional中的value对象*****，
			连续的map操作，只要有一个就数连续null传递，最后一个orElse收尾返回。
			map(Function)：对Optional中保存的值进行函数运算，并返回新的Optional(可以是任何类型)
			其中如果是空，就会返回一个new Optional<>()，value是null
			目前看只能判断null，但是不能判断''这样的空串,可以使用filter过滤


		consumer函数式接口中  void accept(T t); 		
		strings.forEach(a -> System.out.println(a));
		a -> System.out.println(a)		  这里实际上是consumer接口的一个匿名内部类，a是形参，实际list遍历的时候会将实参传入。
		其实定义的就是一个函数接口的匿名实现类，然后真正执行的时候将实参传进去执行
		
		等价的
		//Lambda表达式写法	s -> System.out.println(s);
		//方法引用写法		System.out::println
		
		双冒号 :: 为引用运算符，而它所在的表达式被称为方法引用。如果Lambda要表达的函数方案已经存在于某个方法的实现中，那么则可以通过双冒号来引用该方法作为Lambda的替代者。
		有些情况下，我们用Lambda表达式仅仅是调用一些已经存在的方法，除了调用动作外，没有其他任何多余的动作，在这种情况下，我们倾向于通过方法名来调用它
		类型比较固定，就那几种。
			类型			语法				对应的Lambda表达式
			静态方法引用	类名::staticMethod	(args) -> 类名.staticMethod(args)
			实例方法引用	inst::instMethod	(args) -> inst.instMethod(args)
			对象方法引用	类名::instMethod	(inst,args) -> 类名.instMethod(args)
			构建方法引用	类名::new			(args) -> new 类名(args)
		
		方法引用代替lambda表达式对代码的简化程度远远没有lambda表达式代替匿名类的简化程度大， 有时反而增加了代码的理解难度且使用场景的局限性不利于增加或修改代码，个人认为有时没有必要刻意使用方法引用
		这里场景基本是单独调用已经存在的某个方法，可以简化使用方法引用，一般情况下用，可读性不好。效果不大。
		方法引用，关键词，单独调用方法时，方法已经存在某个实现中，Lambda要表达的一种简写。效果不太好
			

125.配置job
	启动加载进入内存
	统一的service中管理各个job，通过自定义job注解添加配置信息，然后启动加载进入系统即可。老版的是集中在xml中，新版的是集中代码注入
	LinKunJobConfig extends SchedulerFactoryBean  实现其中的registerJobsAndTriggers方法  最终setTriggers(triggers); super.registerJobsAndTriggers();
		//生成每个trigger，主要步骤，jobDetailFactory，cronTriggerFactory
		private CronTrigger registerCronJobTrigger(String methodName,LinKunJob linKunJob) throws Exception {
			String cronExpression = linKunJob.cronExpression();
			int jobType = linKunJob.jobType();
			String[] arguments = linKunJob.arguments();
			MethodInvokingJobDetailFactoryBean jobDetailFactory = new MethodInvokingJobDetailFactoryBean();
			jobDetailFactory.setTargetMethod(methodName);
			jobDetailFactory.setConcurrent(false);
			jobDetailFactory.setArguments(arguments);
			jobDetailFactory.setTargetObject(applicationContext.getBean("manualJobService"));
			jobDetailFactory.setName(methodName + "_detail_" + linKunJob.jobType());
			jobDetailFactory.afterPropertiesSet();
			CronTriggerFactoryBean cronTriggerFactory = new CronTriggerFactoryBean();
			cronTriggerFactory.setJobDetail(jobDetailFactory.getObject());
			cronTriggerFactory.setCronExpression(cronExpression);
			cronTriggerFactory.setName(methodName + "_trigger_" + linKunJob.jobType());
			cronTriggerFactory.afterPropertiesSet();
			return cronTriggerFactory.getObject();
		}
		
 	cron表达式
		0 0/3 20,23 * * ?” 	每天 20点至20:59 和 23点至 23:59分两个时间段内 每3min一次触发
		cron中 	?  	不指定值，用于处理天和星期天配置的冲突
				- 	指定时间区间
				/ 	指定时间间隔执行		
			
		"0 0 */3 * * ? *"      "* * 0/3 * * ? *" 注意* 是匹配任意。这里按时为界线,到了小时点，就按规则执行
		17:12:22
		20:0:0	
			
		
126.xxl-job 
		最重要的两个部分：	1.执行器，一般是bean执行器，分散配置在各自的服务中，在spring的基础上，@XxlJob注入任务(将执行器的name和注册地址作为一对，放入了admin的地址list中) 
							2.统一的调度中心，就是xxl-job-admin。
		当然可以定义一个通用的httpjobhander，然后配置多个任务附带不同的参数就可以了，也不行直接一个http的请求地址也行
		增加执行器后，admin增加配置，会有心跳检测，间隔一段时间就能连上。
		
		
127.restful
		rest:根据三个单词缩写，资源(标识定位资源)+表现(获取资源后json等形式展示出来)+状态转换(增删改查等操作)

		简单参是用@pathvariable,复杂的用@requestBody接收json绑定转换为java对象(多个参数可以用json传递)
		
		一般不建议在uri中有动词，比如/get/user/1   而应该用user/1     呵呵 好吧
		
		REST 都不推荐使用PUT users?userName=user_name&note= note, 	按rest的建议是采用PUT users/{userName}/{note}    呵呵 好吧
		
		
128.jvm,内存溢出
			
	虚拟机枝和本地方法枝溢出
		StackOverflowError ：线程请求的战深度大于虚拟机所允许的最大深度 循环递归会触发这种 OOM   	常见递归操作
		OutOfMemoryError ：虚拟机在扩展棋时无法申请到足够的内存空间，一般可以通过不停地创建线程触发这种 QOM
		Java 堆溢出： 在创建大量对象并且对象生命周期都很长的情况下，会引发OutOfMemoryError
		方法区溢出： 方法区存放 class 等元数据信息，如果产生大量的类（使用CGLIB ），那么就会引发此内存溢出，即 OutOfMemoryError:PermGen space ，在使Hibernate 等动态生成类的框架时会容易引发这种情况
						运行期间，jvm不会在主程序运行期间清理。一般发生在启动阶段
	对象访问（句柄访问，指针访问）
	hotspot采用的是直接指针访问：直接保存的是对象实例的地址，对象实例中保存了类型数据的指针。其中实例在堆中，对象类型数据在方法区中。

	根据对象引用能定位到堆中的对象
	
	
129.系统调优相关
		了解系统总体架构，明确压力方向。具体的接口或者是模块的使用率。
		关键业务数据量分析，一天多少量，缓存数据量
		了解系统响应速度，吞吐量，tps,qps等指标需求。
		
		使用top查看，关键看进程id,虚拟内存分配 和物理内存，cpu占有，一般关心物理内存的占用，虚拟内存不用关心
		ps -aux|grep java 相比 -ef 能看物理内存占用情况
		.
		系统层面影响的只有三个，CPU 、内存和 I/O。
		
		使用top  和 ps  先查个大概 ，然后看具体进程的原因
		
		测试在虚拟机上使用root用户是可以的，jstack 和 jstat 相关命令 
		jstat   	查看类加载，内存管理，gc情况
		jstack  	打印异常进程的堆栈信息 dump线程快照，定位死锁，超时问题
		jmap        查看堆内存使用情况，可生成JVM堆的转储快照，dump文件较大，消耗大量资源
		
		
		jps   		查看进程和运行的主类，cmd下测试可用
		
		jstack
			jstack用于生成java虚拟机当前时刻的线程快照。注意是当前时刻
			线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。
				
			jstack命令的语法格式： jstack  <pid>。可以用jps查看java进程id。
			jstack 31177 > /home/tengfei.fangtf/dump17
			在实际运行中，往往一次 dump的信息，还不足以确认问题。建议产生三次 dump信息，如果每次dump都指向同一个问题，我们才确定问题的典型性。
	
		
		
		**********现在用VisualVM 比较多点，直接线程的客户端，比较好用(直接继承了上面的命令)**********
		目前连接远程，有两种方式，1.服务器上启动jstatd  2.服务器上配置jmx。然后客户端连接
		连接后续有个调优的可以试试
		
		
		
		常见的攻击方式
		xss  	 	由页面输入脚本引起，接收转义即可
		crsf  		利用cookie引起，使用token或者在cookie中设置httpOnly
		sql注入		使用预编译语句，用占位符将参数语句当做普通字符串
		
		释放不必要的引用： ThreadLocal 使用完记得释放以防止内存泄漏，各种 stream用完也记得 close
		
		新生代不能设置过小，过小会经常minor GC，
		
130.rocketmq相关	
		rocketmq producer 发送消息失败，总是连接不上服务器地址
		1. 目前这种写法Rocket默认开启了VIP通道，VIP通道端口为10911-2=10909。若Rocket服务器未启动端口10909，则报connect to <> failed。
		2. 解决方式：增加一行代码producer.setVipChannelEnabled(false);
		
		
		
		消息中间件
		消息存储：消息的Header信息(投递次数等基本信息)，消息的Body(主要内容)，消息的投递对象。
			
		消息的可靠性
		一定要业务处理成功之后，再返回确认消息。否则消息就丢失了。
		分布式系统的三个重要点，服务框架，消息中间件，和数据访问层。
		
		rocketmq  后台可以新增消息发送   topic选项新增消息发送
		rocketmq常见异常 	https://blog.csdn.net/weixin_43439073/article/details/95746775
		
		boot中已经能继承mq了，引入依赖rocketmq-spring-boot-starter，并添加相关配置
			rocketmq.producer.group = producer_bank2
			rocketmq.name-server = 127.0.0.1:9876
			
			@Component
			@Slf4j
			@RocketMQMessageListener(topic="topic_notifymsg",consumerGroup="consumer_group_notifymsg_bank1") 
			public class NotifyMsgListener implements RocketMQListener<AccountPay> {
				@Autowired
				AccountInfoService accountInfoService;
				@Override
				public void onMessage(AccountPay accountPay) {
					log.info("接收到消息:{}", JSON.toJSONString(accountPay)); 
					AccountChangeEvent accountChangeEvent = new AccountChangeEvent();
					accountChangeEvent.setAmount(accountPay.getPayAmount());
					accountChangeEvent.setAccountNo(accountPay.getAccountNo());
					accountChangeEvent.setTxNo(accountPay.getId());
					accountInfoService.updateAccountBalance(accountChangeEvent); 
					log.info("处理消息完成:{}", JSON.toJSONString(accountChangeEvent));
				} 
			}
			
			小鱼儿中是@PostConstruct中初始化mq的监听配置
			defaultMQPushConsumer.registerMessageListener(new MessageListenerConcurrently() {});
			// Consumer对象在使用之前必须要调用start初始化，初始化一次即可<br>
			消费者初始化，defaultMQPushConsumer.start();
			生产者也是同样的初始化， defaultMQProducer.start();
		
131.ELK
	建立 ELK ( Elasticsearch + Logstash + Kibana ）日志集中分析平台，便于快速搜索、定位日志
			***Elasticsearch***
			ELK
			elasticsearch			日志搜索
			logstash				日志收集汇总分析
			kibana					web界面展示
		
	Elasticsearch
		官网 https://www.elastic.co/cn/   
		
		boot中会引入 spring-data-elasticsearch，具体的看官方文档推荐使用的client,We strongly recommend to use the High Level REST Client instead of the TransportClient.
		https://docs.spring.io/spring-data/elasticsearch/docs/4.0.2.RELEASE/reference/html/#reference
		
		Kibana 是用户界面，可对 Elasticsearch 数据进行可视化
	
		kibana中的console控制台查询
			这是交集查询语法
				GET _search
					{
					  "query": {
						"bool":{
						  "must":[{"match":{"_index":"user"}},{"match":{"_id":"1366"}}]
						}
					  }
					}
					
		官网推荐默认使用的是RestHighLevelClient，添加使用的是indexrequest,搜索使用的是SearchRequest,
		多条件查询使用BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery()，然后.must拼接各种query即可
		最终是sourceBuilder.query(boolQueryBuilder)---> request.source(sourceBuilder)---> client.search(searchRequest)
		查询结构SearchHits hits = response.getHits();然后迭代器迭代后getSourceAsString获取jsonString
		
		其中QueryBuilders是用来生成不同功能的query.
		
		matchQuery：模糊查询，会将搜索词分词，再与目标查询字段进行匹配，若分词中的任意一个词与目标字段匹配上，则可查询到。

		termQuery：精确查询，不会对搜索词进行分词处理，而是作为一个整体与目标字段进行匹配，若完全匹配，则可查询到。
		
		**********elasticsearch 里默认的IK分词器是会将每一个中文都进行了分词的切割，所以你直接想查一整个词，或者一整句话是无返回结果的。**********

		elasticsearch 里默认的分词器(Standard Analyzer)是会将每一个中文都进行了分词的切割，所以你直接想查一整个词  加上.keyword
		
		如果想通过term查到数据，那么term查询的字段在索引库中就必须有与term查询条件相同的索引词，否则无法查询到结果。
	
		使用ik分词也不是万能的，维护词库的成本比较高,直接的做法,就是不分词( 7.0版本后使用fieldname.keyword就可以不分词,这里的keyword是=,不是contain)，这里match中的query使用operator.and 也可以
		
		
		**********
			match
				match中，这里的分词是针对搜索输入的字符串进行分词
				
			term
				term本身搜索的参数字符串不分词，属性词分词后包含参数串即可
				
				term检索，如果content分词后含有中国这个token，就会检索到
				curl -XPOST http://192.168.1.101:9200/index/fulltext/_search -d’
				{“query” : { “term” : { “content” : “中国” }}}’
		**********
		
		指定分词三种方式		https://blog.csdn.net/tclzsn7456/article/details/79957221

		elasticsearch 的ik分词器 地址 https://github.com/medcl/elasticsearch-analysis-ik/releases
		
		关于分词
			实我们只需要关注两个就可以了。
			1、standard				elasticsearch默认
			2、ik分词器				ik_max_word和ik_smart模式
			大家常说ElasticSearch中内置的分词器standard，更确切的说是Lucene内置的，ES是Lucene提供支持的。
			
			直接将里ik分词的解压到plugins\ik 即可,
			
		*****match query中查询，operator：表示单个字段如何匹配查询条件分词，默认是or，设置成and就是全部满足*****
		minimum_should_match：当operator参数设置为or时，该参数用来控制应该匹配的分词的最少数量；
		GET _search
				{
				  "query": {
					"match":{
					  "name":{"query":"保家", "operator": "and"}
				  }
				}

		伪代码是：
		"query":{  
					  "match":{  
						 "eventname":"Microsoft Azure Party"
					  }
				
		“Microsoft Azure Party”，被分析器分词之后，产生三个小写的单词：microsoft，azure和party，

		if (doc.eventname contains "microsoft" and doc.eventname contains "azure" and doc.eventname contains "party") 
		return doc
		
		高亮搜索
		
			//千万记得要记得判断是不是为空,不然你匹配的第一个结果没有高亮内容,那么就会报空指针异常
			搜索高亮显示，追加highlight，针对field对应字段拼接pre_tags，post_tags，到html页面显示
			
				 "highlight": {
						"pre_tags" : "<font style=\"color:red;\">",
						"post_tags" : "</font>",
						"fields" : {
							"name" : {}
						}
					}
			
			
			查询结果出来直接高亮显示到html 
					HighlightField name = next.getHighlightFields().get("name");
					String fragments = name.getFragments()[0].toString();
		
		
		分词，使用分词
			CreateIndexRequest 创建索引，确定分词相关，主要是配置index下的settings和mapping结构
			createIndexRequest 搭配XContentBuilder使用可以初始化设置某个字段的分词使用，也是是手工指定字段的分词属性
			
			配置CreateIndexRequest的settings和mapping，其中mapping可以配置相关的field的分词使用，注意是配置某个index下_mapping属性
		
		------es 初始化配置相关代码段------
				String esHost = "192.168.6.153";
				RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(esHost, 9200, "http")));
				
				//1.索引的settings设置
				Builder settings = Settings.builder()
						.put("number_of_shards",1)//分片数
						.put("number_of_replicas",1);//备份数
				
				//2.索引的mapping结构设置
				XContentBuilder mappings = JsonXContent.contentBuilder()
						.startObject()
							.startObject("properties")
								.startObject("req_title")
									.field("type","text")
									.field("analyzer", "ik_max_word")
								.endObject()
							.endObject()
						.endObject();
				
				//3.将settings和mappings封装到一个request对象中
				CreateIndexRequest createIndexRequest = new CreateIndexRequest(Es.INDEX_REQUIRE)
						.settings(settings)
						.mapping(mappings);
				
				//4.通过client对象连接ES并执行创建索引
				client.indices().create(createIndexRequest, RequestOptions.DEFAULT);
		------es 初始化配置相关代码段-------
		
		查看具体的分词结果
			GET _analyze
				{
				  "text": "hello world, java spark",
				  "analyzer": "standard"
				}
							
							
		后台分词配置(配置中配置了field的分词后，查询的时候会按照配置分词分词)
		PUT /user/_mapping
		{
			"properties": {"family":{"type":"text","analyzer": "ik_max_word"}}
			
		}
						
		*******重点********
			queryString  和doc中的field   的分词后几个词语，能找到相等的，就命中。注意这里不是contain关系，而是分词结果存在equals关系
		*******重点********



						
		查看某个index下的_mapping的配置
		http://localhost:9200/user/_mapping/		
							
							
							
							
		在向Elasticsearch中插入数据之前就要给需要的进行分词的属性标记好解析，然后再插入数据即可解决问题！	先分词，再插入数据	
		Elasticsearch的 mapping 一旦新建 只能新增字段 不能修改		
		文档写入的时候会根据字段设置的分词器类型进行分词，如果不指定就是默认的standard分词器。
		写时分词器需要在mapping中指定，而且一旦指定就不能再修改，若要修改必须重建索引。
		读写采用一致的分词器
							
							
							
		
		排序
			searchSourceBuilder.sort(entry.getKey(), entry.getValue());    依次添加排序规则即可   value 是自带的 SortOrder 枚举类 直接追加sort即可

	kibana	
		初始需要创建一个index显示
		切换中文，在config/kibana.yml添加i18n.locale: "zh-CN"
		
		GET /_cat/nodes  查看所有节点
		
		在可视化中创建图表保存，然后在仪表盘集中展示图表
		
	logstash 
		logstash中，包括了三个阶段: 输入input --> 处理filter（不是必须的） --> 输出output
		
		就是收集日志，过滤日志，输出日志
		实际使用中直接手机error.log 的日志
		
		相当于是将手机的日志关联到了elasticsearch上，然后通过kibana查询输出
		
		一般logstash中的filter使用的是 	grok的语法规则是:%{语法：语义}，这个过滤器暂不看
		所有文本数据都是在Logstash的message字段中中的，我们要在过滤器里操作的数据就是message
		
		样例配置
			input {
				file {
					path => "C:/Users/xiaoyuer/git/xye-soa-pom/logs/xye.log"
				}
			}

			filter {
			#定义数据的格式，正则解析日志（根据实际需要对日志日志过滤、收集）
			grok {
				match => { "message" => "%{IPV4:clientIP}|%{GREEDYDATA:request}|%{NUMBER:duration}"}
			}
			#根据需要对数据的类型转换
			mutate { convert => { "duration" => "integer" }}
			}

			output {
				elasticsearch {
					hosts => ["localhost:9200"]
					index => "testlogstash"
				}
				stdout { codec => rubydebug }   #用来测试时,验证有没有启动成功
			}
		
		
		生产(收集的是error.log文件，input使用的是beats组件)
		input 1.从文件中来    2.系统日志方式   3.filebeats方式 

		filebeats
			使用beats(更新Filebeat配置文件后，启动filebeats)
			启动beats  ./filebeat -e -c filebeat.yml
			
				beats配置文件
					output.logstash:
						hosts: ["192.168.194.6:5044"]					#读取log文件，发送到服务端的logstash中
						
				然后logstash监听beats端口
					input {
					  beats {
						port => 5044  # 设置专用端口用于接收各个来源的日志
					  }
					}
		filter 暂不研究
		由于logstash只能收集本机日志，故在其他机器上搭建filbeat，将日志发送给logstash
				input {
				  beats {
					port => 5044
				  }
				}
				output {
				  elasticsearch {
					hosts => ["http://localhost:9200"]
					index => "log-%{+YYYY.MM.dd}"
				  }
				}
	
132.json相关
		JSONArray.toJSONString 和JSONObject.toJSONString 一样 都是调用了父类JSON的toString方法

133.搜索相关，solr相关，Elasticsearch相关，索引相关
		lucence的相关
			倒排索引：	搜索引擎中使用的索引是倒排索引，将文档中的词作为关键字，建立词与文档的映射关系。搜索引擎中使用的索引是倒排索引，将文档中的词作为关键字，建立词与文档的映射关系。
			分词：		英文比较简单，单词划分(空格，符号，段落等)，中文需要合适的中分文词工具，建立倒排索引
			停止词：	类似英文的a the and ,中文的一些语气词，了，这等没有意义，需要忽略
			排序：		不细看，可以针对具体的field,指定排序规则
			文档：		类似数据库，一行记录包含的字段对应文档的域，这条记录就是一个文档	
			查询query:	可能是条件组合查询，termQuery ,也可能是前缀查询等
			高亮：		就是对关键字前后拼接html的颜色代码，高亮的是输入的搜索值(注意这里是搜索值的分词之后的关键字)
		
		搜索引擎
			细节原理暂不看
			正向索引，			文档1”的ID > 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；…………。
			倒排索引，			“关键词1”：“文档1”的ID，“文档2”的ID，…………。	
		
			搜索引擎会将正向索引重新构建为倒排索引，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词
			lucene:全文搜索工具包，依赖于java，不适用于集群环境
			ES：全文搜索服务器，基于lucene，采用Rest HTTP调用方式，对集群支持较好	
	
134.系统优化，系统监控相关
	ping是最常用的心跳检测方法，通过ping命令，使用ICMP协议，检测网络链路是否通畅，远程主机是否可达
		发送字节数据，接收字节收据，和耗时

		可以预留curl地址，检测远程服务是都可用

		业务给出总的访问量（各系统比重不一样），即总的pv,uv后，再推导出每个独立的系统，接口上的流量，评估机器数量，网络带宽和技术实现方式

		压测最关键的两个值，一个是qps，一个是rt。
			
		服务降级，就是，对于调用超时的非核心服务，超时次数超过阈值，先跳过调用，过时间点再重试调用
		增加系统调用白名单，系统压力过大，丢弃非核心的服务，保证核心服务正常
		服务开关，高峰时屏蔽一些非核心链路的调用
			

		负载过高：		用备用机器扩容
		依赖宕机		数据无法写入，先写在本地，依赖恢复后，脚本数据还原
		ddos攻击		针对攻击类型，流量清洗，通过开关开启验证码来验证，启动流控机制。借助公网控制大量的用户计算机，同一时刻攻击一个主机。针对服务器ip的
		物理设备损坏	更换新机器
		网卡流量占满	修改负载均衡策略，流量牵引到其他机房。 	

		服务器防火墙中，只开启使用的端口，比如网站web服务的80端口、数据库的3306端口、SSH服务的22端口等。关闭不必要的服务或端口

		tomcat的发布指定内存是在catalina.sh中指定jvm的内存大小，jar执行  就直接在启动命令中追加即可

		
		实际ddos防护
			高防ip 180.188.20.5
			高防ip拦截了快钱的请求,快钱、建周 回调失败问题确认（高防平台拦截，原因TLS1.0版本太老）
			dnspod 可以配置购买的域名转到哪个ip
			dnspod 配置www.xiaoyuer.com转到 美橙高防ip，美橙后台配置来源域名转到服务器ip
		
		域名解析ip(180.97.15.66)->接收66,防火墙打出到250->转到192.168.8.250(keepalived配置的虚拟ip)内部服务器,指定跳到31->31nginx-机器，转发到相应的服务器。
		
		https://help.aliyun.com/knowledge_detail/43742.html			个别请求绕过防火墙和高防ip

		高防ip拦截异常处理，更换为https://makersxye20141118.xiaoyuer.com/xye-netpay/callback/quickServerCallBack.htm，该路径没有经过高防，dnspod直接转发到服务器。
		
		
135.开源工具，开源jar
		Hutool 工具类可以留意下，常用的工具类封装
		
136.bean生命周期，springbean生命周期
		BeanDefinition 是加载如xml信息后的对象
	实例化对应构造方法，属性赋值对应setter方法的注入；初始化和销毁这两个阶段用户可以自定义
	主要的是4个关键阶段和多个扩展点(实现Aware接口)
		1.实例化 Instantiation		
		2.属性赋值 Populate		
			扩展：检查Spring Awareness 这里是一个扩展点  基本都实现了aware接口
		3.初始化 Initialization			beanpostprcessor，初始化前后自定义初始化逻辑。所有Aware接口的注入就是在这前置完成的。
		4.销毁 Destruction
	
	bean的生命周期，核心是对bean操作，而用的是工厂模式，所以就是，工厂前，工厂，工厂后，生产前，生产，生产后(各个processer的执行顺序)
	
137.编码,乱码,字符编码,转码
		NotePad++
			其实ANSI并不是某一种特定的字符编码，而是在不同的系统中，ANSI表示不同的编码。
			ANSI不是固定的字符编码，不同的系统中，ANSI表示不同的编码。美国系统-ASCII编码，简体中文系统-GBK编码，韩文系统对应EUC-KR编码
		
			*****汉字在电脑显示 实际上计算机只认识编码后的信息，需要根据编码后的信息去字典去找的汉字*****
			这里注意以gbk编码和转为的区别，以是选择当前的编码直接显示(相当于直接去字典找对应编码的字)，转为是转换为另一个字典中的编码

		<META content="text/html; charset=ASCII" http-equiv=Content-Type>
		编码识别正常使用，不识别就默认使用gbk。gb2312没收录的也使用gbk，很可能是因为会去寻找系统默认的编码
		电脑的记事本在打开时候会根据编码自动选择对应编码打开，联通是特殊(默认ANSI保存，编码形同utf8，打开是ut8就会乱码)
		就是说只要编码和解码都是用系统默认就不会有问题
		
		idea中文件是gbk的  汉字显示正常后再转为utf8格式，再复制到eclipse中
		
		
	
138.三方对接，接收参数，请求头信息查看，请求头，参数接收
		@RequestMapping("/eZxServerCallBack")
		public void eZxServerCallBack(HttpServletRequest request, HttpServletResponse response) throws Exception {
			
			LOG.info("--e管家-server-回调开始--");
	 
		//1.查询请求头的信息，这在三方对接的时候，参数接收方式很有用
			  Enumeration headerNames = request.getHeaderNames();
			  while (headerNames.hasMoreElements()) {
			   String paramName = (String) headerNames.nextElement();
			   String value = request.getHeader(paramName);
			   LOG.info("header参数：" + paramName + "=" + value);
			  }
			   
			   @RequestBody 
		//2.这个可以接收到xml格式的信息，从流中获取相关的信息	比如:application/xml
		// 这个也可以接受application/json,是一个json串，可以自己转，也可以@RequestBody接收，该注解最好使用对象接收
			 BufferedReader in = new BufferedReader(new InputStreamReader(request.getInputStream()));
			 StringBuffer strBuff = new StringBuffer();
			 String line;
			 while ((line = in.readLine()) != null) {
				 strBuff.append(line);
			 }
			 Utils.log("HTTPS,服务器响应结果是: \n" + strBuff.toString());
			  
			  
		 //3.key-value格式接收参数 类似form表单
			Map<String,String[]> parameterMap = request.getParameterMap();
			try {
				parameterMap.forEach((key,value)-> LOG.info("key="+key+"-----------value="+value[0]));
			} catch (Exception e) {
				LOG.error("yuichang",e);
			}
			 response.addHeader("HTTP/1.1 200", "OK");
			 response.addHeader("Content-type", "text/html");
			 response.getWriter().write("success");
			 response.getWriter().flush();
			 
			 LOG.info("--e管家-server-回调结束--");
		}
	
		
		res.setHeader("Access-Control-Allow-Origin", req.getHeader("Origin"));  这个是跨域接收

		
		//发送json格式的请求
		//在请求头中指定contentType
		httpPost.setHeader(new BasicHeader("Content-Type","application/json"));

		//设置请求体参数, 请求体直接使用raw json
		String bodyString = JSONObject.toJSONString(bodyParamMap);
		StringEntity postingString = new StringEntity(bodyString);
		httpPost.setEntity(postingString);
			
	
	<?xml version=\"1.0\" encoding=\"UTF-8\"?>    xml头  标识是一个xml结构
	如果是xml格式(application/xml)返回给浏览器<ROOT><RSP_CODE>00000</RSP_CODE></ROOT>，是能识别xml结构的
	若text/html格式返回，<ROOT><RSP_CODE>00000</RSP_CODE></ROOT>，浏览器只能显示00000，因为无法正确解析标签
		
	
	
	
	
	
139.流操作，写文件，流文件，文件流
		拿到字节，直接输出字节，将字节输入流通过输出流转化
		直接存文本格式，输入流转输出流
			private void zxParseReturnMsg(String msg, HttpServletRequest request, String date) {
				String path = request.getSession().getServletContext().getRealPath("");
				String filePath = path + "/zxPay" + date + ".txt";
				try {
					writeBill(msg, filePath, "UTF-8");
				} catch (Exception e) {}
				Scanner s = null;
				try {
					s = new Scanner(new BufferedReader(new FileReader(filePath)));
					while (s.hasNextLine()) {
						lineString=s.nextLine();
						} catch (FileNotFoundException e) {} finally {  
					 if (s != null) {   
						 s.close();   
					 }   
				}
			}
			
			private void writeBill(String billContext, String filePath, String encoding)throws Exception {
				InputStream bis =null;
				try{
					if(encoding==null||"".equals(encoding)){bis = new ByteArrayInputStream(billContext.getBytes("GBK"));}else{bis = new ByteArrayInputStream(billContext.getBytes(encoding));}
					OutputStream os = new FileOutputStream(filePath);
					IOUtils.copy(bis, os);
					bis.close();
					os.close();
					LOGGER.info("保存文件:"+filePath);
				}catch(IOException e){}
			
			}
			
		fileAsString = new String(EntityUtils.toByteArray(response.getEntity()),"utf-8"); 这是http请求返回的

		s = new Scanner(new FileInputStream(new File(filePath)),"GBK");// 解析指定编码，读取文件
		s = new Scanner(new BufferedReader(new FileReader(filePath)));
		
		
		拿到字符串内容后，写文件，直接输出流写字节内容
		 byte[] buffer = new BASE64Decoder().decodeBuffer(base64Code);  
		 FileOutputStream out = new FileOutputStream(targetPath);  
		 out.write(buffer);  
		 out.close(); 
		 log("文件转换完成！  " + targetPath);
		 
		 
		 解压zip就是ZipInputStream 和 FileOutputStream操作
		 
		 
		 
140.	金额转换，分转元，元转分，分元转换，元分转换，BigDecimal
		
		BigDecimal.ROUND_HALF_UP 这个就是 以前的四舍五入
		OrderCodeUtil 老版的就是   完全的String 在那判断.和0的index，然后拼接后返回(没有运算的概念)。不用重复造轮子。正规点

		32.6589这种直接取点后两位，3265 其余的直接忽略  新老都是这样处理的
		
		BigDecimal bigDecimalLong = new BigDecimal(3269874l);
		
		BigDecimal bigDecimal23 = new BigDecimal(3.3d); //这个精确会有问题
		BigDecimal bigDecimal23 = new BigDecimal(String.valueOf(3.3d));
		BigDecimal bigDecimal25 = BigDecimal.valueOf(3.3d);//推荐使用
		
		long longValue = bigDecimal.multiply(new BigDecimal(100)).longValue();	//元转分
		double doubleValue = bigDecimalLong.divide(new BigDecimal(100)).doubleValue();//分转元
		
141. k8s
	k8s调试注意点
	数据库base_config中的
		url_web_host_newadmin  	http://admin1.xiaoyuer.net -> https://admin2.xiaoyuer.net  登录更新
		
		url_web_host			https://www1.xiaoyuer.net  -> https://www2.xiaoyuer.net
		url_web_host_for_admin	http://www1.xiaoyuer.net -> http://www2.xiaoyuer.net
		url_web_hosts			https://www1.xiaoyuer.net ->https://www2.xiaoyuer.net
		
	启动后第一次发需求访问会超时；容器内部是可以相互访问的
	内部环境manager可以访问外面的soa
	
	
	查看当前运行的所有机器	 	kubectl get pod -nsit
	替换文件名进入容器		kubectl exec -it tomcat-xye-manager-cf9bf4887-24dpr -nsit /bin/bash
	查看容器的ip 			kubectl get pod -nsit -owide

	192.168.6.135 root 123456

	k8s的容器中locale环境配置不生效，是没有删除之前的镜像，使用了旧的镜像。
	 upstream xye-ids_upstream {
			server 192.168.6.136:30812 weight=1 max_fails=2 fail_timeout=60s;
			server 192.168.6.137:30812 weight=1 max_fails=2 fail_timeout=60s;
		}

142.查看出网IP
	curl http://ifconfig.me
	curl http://icanhazip.com	
	curl http://cip.cc

	出网ip和本机ip不一样，一个是 221.226.49.186本地  221.226.49.188是pre 
				
	命令行查询(详细):
	UNIX/Linux:#curl cip.cc
	windows环境直接百度ip就有了	

	telnet IP 端口 或者 telnet 域名 端口    telnet就是查看某个端口是否可访问
	ping + ip： 查看某一个ip地址是否能够连通，如： ping 114.80.67.193	用来检查网络是否通畅或者网络连接速度的命令 
　　telnet ip port ： 查看某一个机器上的某一个端口是否可以访问，如：telnet 114.80.67.193 8080
　　退出命令： exit---退出dos窗口，q!,wq---Linux下退出vi编辑器
　　　　　　　ctrl+]，之后在按q ---退出telnet界面
　　　　　　　quit---退出mysql.......
	
	系统出网ip是防火墙的出网ip
	
143.并发相关，线程相关
	死锁
		代码1
			a.lock();
			b.lock();
		代码2
			b.lock();
			a.lock();
		这样两个简单的线程出现的死锁场景就出现了。原因在于用不同的顺序请求了相同的锁(交叉获取),若能保证请求锁的顺序一致，就不会发生死锁(统一先a后b)。
		数据库能再发生死锁的时候，选择一个事务牺牲，释放资源让另一个事务继续下去
		
		1.控制访问锁的顺序
		2.trylock，通过timeout获取锁，时间过长就失败
		3.线程转储thread dump分析
	
		避免死锁的几个常见方法。 
			1.避免一个线程同时获取多个锁。 
			2.避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 
			3.尝试使用定时锁，使用lock.tryLock（timeout）来替代使用内部锁机制。 
			4.对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。
	
	
	上下文切换,cpu时间片
		cpu时间片轮转机制,
			时间片分配算法循环执行任务
			线程任务的状态保存及再加载, 这段过程就叫做上下文切换
			CPU分配给线程cpu时间片(即该进程允许运行的时间，时间很短，所以cpu会不停切换线程执行)，并将当前任务状态保存下来(以便下次切回时可以再次加载该任务状态)，再加载下一任务的状态并执行。
			当前任务执行一个时间片后切换到下一个任务(切换前保存上个任务状态，以便下次切回加载状态)。任务从保存到加载的过程就是一次上下文切换。
			时间片的就是系统分配给线程的时间限额。运行和阻塞两个状态之间的转换需要上下文切换。
			线程有创建和上下文切换的开销。
		
		减少线程切换的几个方法
			1.避免竞争无锁开发，比如id按照hash取模分段，不用线程处理不同段数据
			2.cas算法，java的atomic包使用cas更新，不需要加锁
			3.使用线程池，创建适量的少线程
			
			
		引起线程上下文切换的原因
			1.当前执行任务的时间片用完之后，系统CPU 正常调度下一个任务；
			2.当前执行任务碰到 IO 阻塞，调度器将此任务挂起，继续下一任务；
			3.多个任务抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续下一任务；
			4.用户代码挂起当前任务，让出 CPU 时间；
			5.硬件中断；
	
		
		
		
	线程生命周期
		新建
		就绪	等待被分配cpu时间片，具备运行条件，但还没有被分配到cpu
		运行
		阻塞	让出cpu，暂停执行.	sleep   wait(notify)  suspend(resume)
				等待阻塞 	wait进入等待队列
				同步阻塞  	lock进入锁池 lock pool
				其他阻塞	sleep/join/IO请求
				
		死亡	正常run()结束，异常终止，kill等
		
		yield(),暂停执行，让出cpu，给同priority线程执行，并未阻塞，没有同级别的priority，该方法无效。
		JVM 暂时放弃 CPU 操作，放弃本次时间片的执行权
	
	守护线程
		简单理解是后台线程，是一种支持型线程，主要被用作程序中后台调度以及支持性工作，一般都是一些后台功能。像jvm垃圾回收和内存管理都是守护线程。
		当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出
		注意的是进程结束的判断是前台线程是否结束，即使进程结束了，守护线程可能依然未退出。
	
	
	同步辅助类
		Countdownlatch  倒数计数器，线程阀，无法被重置，类似执行多少次，允许一个或多个线程等待其他线程完成操作
			new countdownlatch(3),作为多个线程共享的变量，主要就是countDown()，倒数一次和await()方法，变为0才能继续，否则阻塞。场景比如上传三个文件，都上传好了才继续执行
			使用场景：比如有一个任务A,它要等到其它3任务完成才能执行,此时就可以用CountDownLatch来实现。 
			*****CountDownLatch也可以实现join的功能。近似的实现了join()功能***** 
			 
			 
		Semaphore （信号量）  限流器，可以阻塞线程并且可以控制同时访问线程的个数
			Semaphore维护了一个许可集合，在创建Semaphore的时候，设置上许可数，每条线程在只有在获得一个许可的时候才可以继续往下执行逻辑（申请一个许可，则Semaphore的许可池中减少一个许可），没有获得许可的线程会进入阻塞状态。
			final Semaphore semaphore = new Semaphore(5);
			semaphore.acquire();			//申请一个许可  许可池-1   若许可池为0则申请许可失败，阻塞线程
			semaphore.release();			//释放一个许可  许可池+1
			也可使用非公平信号量，暂不细看
			Semaphore 的锁释放操作也由手动进行，因此与 ReentrantLock 一样，为避免线程因抛出异常而无法正常释放锁的情况发生，释放锁的操作也必须在 finally 代码块中完成。

				
			
			
			
		CountDownLatch比较适合保证线程执行完后再执行其他处理，因此模拟并发时，使用两者结合起来是最好的。
		Semaphore可以用来做流量分流，特别是对公共资源有限的场景，比如数据库连接。
		
	
	
	
	
	线程池
		线程复用；控制最大并发数；管理线程
		Executors线程的工厂类，用来创建线程池
		线程池可以复用线程，统一分配，管理，监控，防止资源浪费
		其中有两个核心的队列，1是线程等待池，即线程队列blockingqueue 2是任务处理池，正在工作的thread列表，hashset<Worker>
		
		线程池在单例模式下使用，不要重复new多个实例出来。
		线程池的数量不要设置过大，限流方面 可以抛弃请求，稍后重试，服务器忙拒绝请求
		
		ExecutorService
			shutdown() 				只是关闭了提交通道，用submit()是无效的；而内部该怎么跑还是怎么跑，跑完再停。优雅关闭
			shutdownNow() 			能立即停止线程池，正在跑的和正在等待的任务都停下了。
			awaitTermination()		当等待超过设定时间时，会监测ExecutorService是否已经关闭，若关闭则返回true，否则返回false。一般和shutdown组合使用
		
		线程池在需要创建一个线程的时候，会调用Threadfactory创建。
		
		
		任务job提交进线程池，进入到工作队列中，等待工作者线程处理
		线程池的本质就是使用了一个线程安全的工作队列连接工作者线程和客户端线程，客户端线程将任务放入工作队列后便返回，而工作者线程则不断地从工作队列上取出 工作并执行。
		当工作队列为空时，所有的工作者线程均等待在工作队列上，当有客户端提交了 一个任务之后会通知任意一个工作者线程，随着大量的任务被提交，更多的工作者线程会被唤醒。
	
	
	
		核心线程池，工作队列，饱和策略
		线程池工作流程
			1.核心线程池corePoolSize是否满，未满就创建核心线程执行任务，满了就跳2
			2.队列workQueue是否满，没满就任务添加到队列，满了就跳3
			3.线程池是否满，没满就创建非核心线程执行任务，满了就交给饱和策略
	
		corePoolSize（线程池的基本大小）：当提交一个任务到线程池时，线程池会创建一个线 程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任 务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法， 线程池会提前创建并启动所有基本线程
		runnableTaskQueue(任务队列)：用于保存等待执行的任务的阻塞队列。
		maximumPoolSize(线程池最大数量)：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。
		RejectedExecutionHandler(饱和策略)：当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法 处理新任务时抛出异常。
		keepAliveTime(线程活动保持时间)：线程池的工作线程空闲后，保持存活的时间。所以， 如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率。
										 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。
										 当一个线程完成任务时，它会从队列中取下一个任务来执行
	
	
	
	

		
	join()	
		让父线程等待子线程结束之后才能继续运行。
		子线程结束后，子线程的this.notifyAll()会被调用，join()返回，父线程只要获取到锁和CPU，就可以继续运行下去了。
		当我们调用某个线程的这个方法时，这个方法会挂起调用线程，直到被调用线程结束执行，调用线程才会继续执行。
		main{
			threadA.join();
		}
		这里就是A执行完了，main才执行完
		当调用原生的join方法时，也是调用了wait方法，也就是会阻塞调用的线程（切记不是调用join方法的那个线程对象，这只是一个普通对象而已）
	
		join用于让当前执行线程等待join线程执行结束。其实现原理是不停检查join线程是否存活，
		如果join线程存活则让当前线程永远等待。其中，wait(0)表示永远等待下去。
		直到join线程中止后，线程的this.notifyAll()方法会被调用，调用notifyAll()方法是在JVM里实现的，所以在JDK里看不到，大家可以查看JVM源码。
	

		
	futrue机制和futuretask
		future是一个接口，futuretask是其唯一的实现类(实现了runnable和future)。
		futuretask特点在于，ExecutorService.submit()runnable和future都需要future接收操作，但是futuretask直接get()即可，相当于，结果封装进了入参中，不需要future接收。
		如果submit的是futuretask对象，那么获取结果直接futuretask对象.get()方法即可。而不是返回一个future对象再get()。
		
		Future<V>接口是用来获取异步计算结果的(操作具体的Runnable或者Callable对象任务执行的结果)，是ExecutorService.submit()返回的对象
		Executor就是Runnable和Callable的调度容器，Future就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果、设置结果操作
		
		future提供了三种功能：
			1.isDone()			判断任务是否完成
			2.cancel()			中断任务
			3.get()				获取任务执行结果
			
		future.get()方法的阻塞优化
			场景：		普通的Future.get()是按照加入线程池的顺序来获取的，如果前一个没完成会一直阻塞后面future的.get()，一般是用list收集future后面遍历
						使用Future和Callable可以获取线程执行结果，但获取方式确实阻塞的，根据添加到线程池中的线程顺序，依次获取，获取不到就阻塞。
			
			解决方法：	为了解决这种情况，可以采用轮询的做法。CompletionService 来实现异步快速收集线程执行结果。
						CompletionService是按照完成顺序获取，解决了Future.get()按照加入线程池顺序(submit顺序)的阻塞问题。
						CompletionService的好处在于主线程总是能够拿到最先完成的任务的返回值，即take()按照线程完成顺序获取，如果就没有完成的就阻塞，这样优先完成的输出，减小了get的阻塞问题
		
			
		
	wait，notify，notifyall
		wait，notify，notifyall 是Object的方法,不是Thread类的固有方法
		所有的对象都会有一个wait set，用来存放调用该对象wait方法之后进行block状态的线程。
		线程被notify之后，不一定立即得到执行，这是因为可能会抢不到LOCK锁。
		线程从wait set中唤醒的顺序不一定是FIFO。
		线程被唤醒后，必须重新获取锁
		notify，notifyall是对实例调用，interrupt是对线程调用
			

			
		lock和wait实际场景
			public class WaitNotify {
				static boolean flag = true;
				static Object lock = new Object();
				public static void main(String[] args) throws Exception {
					Thread waitThread = new Thread(new Wait(), "WaitThread");
					Thread notifyThread = new Thread(new Notify(), "NotifyThread");
					waitThread.start();
					notifyThread.start();
					
					static class Wait implements Runnable {
						public void run() {
							// 加锁，拥有lock的Monitor
							synchronized (lock) { // 当条件不满足时，继续wait，同时释放了lock的锁
								while (flag) {
									try {
										System.out.println(Thread.currentThread() + " flag is true. wait@");
										lock.wait();
									} catch (InterruptedException e) {
										// 条件满足时，完成工作}
									}
								}
							}
						}
					}
					static class Notify implements Runnable {
						public void run() {
							// 加锁，拥有lock的Monitor
							synchronized (lock) {
								// 获取lock的锁，然后进行通知，通知时不会释放lock的锁， 
								// 直到当前线程释放了lock后，WaitThread才能从wait方法中返回
								System.out.println(Thread.currentThread() + " hold lock. notify@");
								lock.notifyAll();
								flag = false;
								SleepUtils.second(5);
							}
						}
					}
				}
				
		notify 和wait的流程
		1.WaitThread首先获取了对象的锁，然后调用对象的wait()方法，从而放弃了锁 并进入了对象的等待队列WaitQueue中，进入等待状态。
		2.由于WaitThread释放了对象的锁，NotifyThread随后获取了对象的锁，并调用对象的notify()方法，将WaitThread从WaitQueue移到 SynchronizedQueue中，此时WaitThread的状态变为阻塞状态。
		3.NotifyThread释放了锁之后，WaitThread再次获取到锁并从wait()方法返回继续执行。		
		
		
		抽象出来的模型
			等待方遵循如下原则。 
			这里的对象是同一个锁对象，可由其他线程唤醒
				1.获取对象的锁。2.如果条件不满足，那么调用对象的wait()方法，被通知后仍要检查条件。 3.条件满足则执行对应的逻辑 
				 对应的伪代码如下。 
				 synchronized(对象) {
					 while(条件不满足) {
						对象.wait(); 
					 }
					 对应的处理逻辑 
				 }
				 
				 通知方遵循如下原则。 
				1.获得对象的锁。 2.改变条件。 3.通知所有等待在对象上的线程。 
				 对应的伪代码如下。 
				 synchronized(对象) {
					 改变条件 
					 对象.notifyAll(); 
				 }		
					
			即加锁、条件循环和处理逻辑3个步骤
		
		注意事项
			等待/通知机制依托于同步机制，其目的就是确保等待线程从 wait()方法返回时能够感知到通知线程对变量做出的修改。
			1.使用wait()、notify()和notifyAll()时需要先对调用对象加锁。
			2.调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的 等待队列
			3.notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或 notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。
			4.notify值之后，将等待队列中的等待线程从等待队列中移到同步队列中，被移动的线程状态由WAITING变为 BLOCKED
			5.从wait()方法返回的前提是获得了调用对象的锁。
			6.wait可以设置超时时间
			
			
			
			
			
			
			
			
			
			
			
	线程中断
		线程中断，interrupt()方法，暂时用的不多
			synchronized没有timeout，也不能中断
			interrupt只是会改变线程的中断状态(表示线程有没有被中断而已)，只是改变中断状态，不会中断一个正在运行的线程。
		中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。
		
		中断一个线程，其本意是给这个线程一个通知信号，会影响这个线程内部的一个中断标识位。这个线程本身并不会因此而改变状态(如阻塞，终止等)。
		1. 调用 interrupt()方法并不会中断一个正在运行的线程。也就是说处于 Running 状态的线程并不会因为被中断而被终止，仅仅改变了内部维护的中断标识位而已。
		2. 若调用 sleep()而使线程处于 TIMED-WATING 状态，这时调用 interrupt()方法，会抛出InterruptedException,从而使线程提前结束 TIMED-WATING 状态。
		3. 许多声明抛出 InterruptedException 的方法(如 Thread.sleep(long mills 方法))，抛出异常前，都会清除中断标识位，所以抛出异常后，调用 isInterrupted()方法将会返回 false。
		4. 中断状态是线程固有的一个标识位，可以通过此标识位安全的终止线程。比如,你想终止一个线程 thread 的时候，可以调用 thread.interrupt()方法，在线程的 run 方法内部可以根据 thread.isInterrupted()的值来优雅的终止线程。
				
				
		当调用线程的 interrupt()方法时，会抛出 InterruptException 异常。
		通常很多人认为只要调用 interrupt 方法线程就会结束，实际上是错的， 一定要先捕获 InterruptedException 异常之后通过 break 来跳出循环，才能正常结束 run 方法
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
			
	java的内存模型
			涉及多线程环境中的可见性和有序性
				jvm有主内存和工作内存
				主内存就是常说的java堆内存，存放类实例和静态数据等，是多线程共享的
				工作内存是线程从主内存中拷贝来的变量以及访问方法所取得的局部变量，简单就是拷贝副本，是线程私有的
				线程都是先从主内存中拷贝到工作内存中操作，然后回写。线程之间不直接通信，只能通过共享变量进行
				
			Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行
			JMM(内存模型)定义了线程和主内存之间的抽象关系：
				当前线程只有把本地内存写过的值刷新到主内存后，其他线程才能看到最新值
				线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的本地内存（Local Memory），
				本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。
					
				这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证
					1.线程A把本地内存A中更新过的共享变量刷新到主内存中去。 
					2.线程B到主内存中去读取线程A之前已更新过的共享变量
			
			当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主 内存中读取共享变量。
			
			happens-before原则,先行发生原则	是JMM核心的概念
				如果A happens-before B，那么A执行结果将对B可见，而且A的执行顺序排在B之前。(两个操作既可以是在一个线程之内，也可以是在不同线程之间)
				两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么jmm允许这种重排序。
		
			as-if-serial语义的意思是：不管怎么重排序，单线程程序的执行结果不能被改变。
			
			as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提 下，尽可能地提高程序执行的并行度。

			这里有个顺序一致性内存模型，是个理性的模型，线程依次连接主内存，线程安全
				
				
				
				
				
	锁，lock和synchronized
		锁是用来控制多个线程访问共享资源的方式，
		*****用锁来访问协调对象，每次访问都需要同一个锁，这里强调并发针对同一把锁的控制*****
		Reentrantlock相对于内部锁的使用条件：1.可定时，可轮询，可中断的锁获取 	2.公平锁，3.非块状结构锁   		一般建议使用内部锁，除非用这三个特点
			可轮询 这里是指trylock没获取到的时候，可以配合while重复直到获取为止，
			synchronized 是非公平锁，可以重入
		***相比原生的锁，有点是超时，可中断，尝试非阻塞获取锁trylock，***
		在Java中，synchronized就不是可中断锁(一个线程获取锁之后，其他锁只能等待那个线程释放之后才能有获取锁的机会)，而Lock是可中断锁	
		ReentrantLock 是 API 级别的，synchronized 是 JVM 级别的
		锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。
		
		
		synchronized 和 lock的区别
		底层实现不一样， synchronized 是同步阻塞，使用的是悲观并发策略，lock 是同步非阻塞，采用的是乐观并发策略
		7. Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized 是内置的语言实现。
		8. synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而 Lock 在发生异常时，如果没有主动通过 unLock()去释放锁，则很可能造成死锁现象，因此使用 Lock 时需要在 finally 块中释放锁。
		9. Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等待的线程会一直等待下去，不能够响应中断。
		
		
		
		
		
		
		
		synchronized
			synchronized(obj)   锁对象，不建议使用参数作为锁，锁要使用同一个对象？
				静态方法 ，对象是类
				实例方法，对象是this	等同于synchronized(this),默认锁当前对象
				代码块，锁固定代码块，需要指定锁对象，这里的锁对象是用来竞争用的，如果多个线程争的不是同一个锁，那么就不存在竞争。

			
			
			
			
			
			
			
			
			synchronized是互斥锁
			线程在执行synchronized代码块之前获得锁，正常控制路径退出，异常抛出，都会在放弃代码块控制时释自动放锁
			当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁
	
			Synchronized 是通过对象内部的一个叫做监视器锁（monitor）来实现的。但是监视器锁本质又是依赖于底层的操作系统的 Mutex Lock 来实现的。
			而这种依赖于操作系统 Mutex Lock 所实现的锁我们称之为“重量级锁”。
			每个对象都有个 monitor 对象，加锁就是在竞争 monitor 对象，代码块加锁是在前后分别加上 monitorenter 和 monitorexit 指令来实现的，方法加锁是通过一个标记位来判断的


			
			
		lock,Reentrantlock
			lock锁  常用Reetrantlock操作，一定要在finally中释放锁，不能忘
			无条件、可轮询、可定时、可中断，lock的操作都是显式的，
			常见的是用一个byte的对象作为锁，对象。  byte[] lock = new  byte[1];
			
			ReentrantLock的读写锁
			ReentrantReadWriteLock 是 ReadWriteLock 的实现类，readLock() 和 writeLock() 用来获取读锁和写锁。
			注意这里占用读写锁，使用的是同一个ReentrantReadWriteLock锁对象中的读写锁。
			使用读写锁提升读操作的并发性，也保证每次写操作对所有的读写操作的可见性
			ReentrantReadWriteLock.readLock(); 			//获取读锁
			ReentrantReadWriteLock.writeLock();			//获取写锁
			
			场景：
				使用单一ReentrantLock保证了线程安全，但是浪费了一定资源(多个读操作并行，无线程安全问题)，但是写操作不是线程安全的(多个线程同时写，或写的同时进行读操作)
				然后引出读写锁就解决了这样的问题，可以保证多读高效，和写入线程安全
				写锁(可以读数据又可以修改数据);读锁(只能读，可被多个线程同时拥有)

			规则
				Thread A占用了读锁，其他线程申请读锁，成功。
				Thread A占用了读锁，其他线程申请写锁，则申请写锁的线程会一直等待释放读锁，因为读写不能同时操作。
				Thread A占用了写锁，其他线程申请写锁或者读锁，会阻塞，需等待Thread A释放写锁，同样也因为读写不能同时，并且两个线程不应该同时写。
				总结：	要么是一个或多个线程同时有读锁，要么是一个线程有写锁，但是读写不会同时出现。
						也可以总结为：读读共享、其他都互斥（写写互斥、读写互斥、写读互斥）。	
		
			
			ReentrantLock提供的获取锁的方式
				1.lock()
					不可中断，如果获取不到则一直休眠等待	
					lock方法不能被中断。如果一个线程在等待获得一个锁时被中断，中断线程在获得锁之前会一直处于阻塞状态。如果出现死锁，那么lock方法就无法被终止。
				2.tryLock() 	
					用来尝试获取锁，如果获取成功，则返回true
				3.tryLock(long timeout, TimeUnit unit)		会监测中断事件，定时获取，配合while可以轮询获取。
					当获取锁时，锁资源在超时时间之内变为可用，并且在等待时没有被中断，那么当前线程成功获取锁，返回true，同时当前线程持有锁的count设置为1.
					当获取锁时，在超时等待时间之内，被中断了，那么抛出InterruptedException，不再继续等待.
					使用trylock实现可轮询和可定时，这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。
					
				4.lockInterruptibly()						直到获取成功或者被中断为止
					当获取锁时，锁资源不可用，那么该线程开始阻塞休眠等待，但是等待过程中如果有中断事件，那么会停止等待，立即返回.
					线程A执行获得锁占用5秒，同时线程B执行lockInterruptibly()，这时候
					A和B  	都start()
					A中lock()  						//占用5秒
					B中lockInterruptibly()			//等待中
					主线程中B随后调用interrupt()	//这时候lockInterruptibly()发现有中断操作就直接终止等待锁了。	
					
					就是说等待的过程中可以被中断，如果中断发生，就会停止等待，立刻返回    	用的不多暂时不用
					如果当前线程未被中断则获得锁,如果当前线程被中断则出现异常

			锁降级是指把持住（当前拥有的）写锁，再获取到 读锁，随后释放（先前拥有的）写锁的过程
			
			
			lock中的condition
				Condition对象是由Lock对象（调用Lock对象的newCondition()方法）创建出来的，暂时没看到用处，后面用到再说。
				condition是实现lock的等待效果，并且是可选择的，一个lock可以多个condition
				
				synchronized关键字中wait()/notify/notifyAll()，类似ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法
				一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活，
				在使用notify/notifyAll()方法进行通知时，被通知的线程是有JVM选择的，使用ReentrantLock类结合Condition实例可以实现“选择性通知”，这个功能非常重要，而且是Condition接口默认提供的。

				而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。
				如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程
				
				private Lock lock = new ReentrantLock();
				private Condition condition=lock.newCondition();
				lock.lock();//lock 加锁			
				condition.await();	//通过创建 Condition 对象来使线程 wait，必须先执行 lock.lock 方法获得锁，获得同步监视器
				condition.signal();//condition 对象的 signal 方法可以唤醒 wait 线程
				
				Condition 类和 Object 类锁方法区别区别
					1. Condition 类的 awiat 方法和 Object 类的 wait 方法等效
					2. Condition 类的 signal 方法和 Object 类的 notify 方法等效
					3. Condition 类的 signalAll 方法和 Object 类的 notifyAll 方法等效
					4. ReentrantLock 类可以唤醒指定条件的线程，而 object 的唤醒是随机的
						





			
					
					
			减小锁的竞争
				1.减少锁持有时间(减小锁范围)，2.减小请求锁的频率 3.协调机制取代独占锁
			
			sleep和wait的区别
				sleep()调用，没有释放锁,未让出系统资源。使得线程仍然可以同步控制。自动唤醒(interrupt方法强行打断,唤醒)。
				wait(),是进入线程等待池中等待，让出系统资源。需要线程调用 notify / notifyAll 方法唤醒(并不立刻释放锁，执行完锁代码块才释放)，才会进入就绪队列中等待系统分配资源,重新参与获得锁的竞争
				对于 sleep()方法，我们首先要知道该方法是属于 Thread 类中的。而 wait()方法，则是属于Object 类中的。
				而当调用 wait()方法的时候，线程会放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象调用 notify()方法后本线程才进入对象锁定池准备获取对象锁进入运行状态。
				sleep(long)会导致线程进入 TIMED-WATING 状态，而 wait()方法会导致当前线程进入 WATING 状态
			
			
		可重入锁(递归锁)，
			可重入就是说某个线程已经获得某个锁(并且该锁尚未释放)，可以再次获取锁而不会出现死锁。
			重入是基于每线程而言，而不是每调用
			实现原理是为每个锁关联一个计数器和占有的线程，初始为0。jvm会记录锁的占有者和计数器
			synchronized和ReentrantLock都是可重入锁，ReentrantLock的时候一定要手动释放锁，并且加锁次数和释放次数要一致。
			就是一个方法里，外层方法占用了锁，但是里面还有方法要获得锁，如果不是重入锁，程序无法继续运行，陷入死锁，是重入锁就继续执行。
			指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。在 JAVA 环境下 ReentrantLock 和 synchronized都是可重入锁。
	
	
	
		闭锁
			是一种同步工具类，可以延迟线程的进度直到其到达终止状态。闭锁可以用来确保某些活动直到其他活动都完成后才继续执行。CountDownLatch是一种灵活的闭锁实现，它可以使一个或者多个线程等待一组事件发生
	
		自旋锁(spin lock)
			自旋锁的目的是为了占着CPU的资源不释放，等到获取到锁立即进行处理。
			是一种非阻塞锁，也就是说，如果某线程需要获取锁，但该锁已经被其他线程占用时，该线程不会被挂起，而是在不断的消耗CPU的时间，不停的试图获取锁(一般使用闭锁或者条件等待等待状态转换)
			jvm既能自旋等待(不断尝试获取，直到成功)，也可以挂起，
			一般搭配while使用(while获取会浪费资源，使用通知推送机制，会更加的高效点)
			
			自旋CAS实现的基本 思路就是循环进行CAS操作直到成功为止。一般限制是超时或者重试次数
			
			原理，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，避免线程切换的消耗。

			线程自旋是需要消耗 cup 的，如果一直获取不到锁，那线程也不能一直占用 cup 自旋做无用功，所以需要设定一个自旋等待的最大时间。
			如果持有锁的线程执行的时间超过自旋等待的最大时间扔没有释放锁，就会导致其它争用锁的线程在最大等待时间内还是获取不到锁，这时争用线程会停止自旋进入阻塞状态。

		公平锁
			如果在绝对时间上，先对锁进行获取的请求一定先被满足，那么这个锁是公平的，
			锁获取是顺序的。等待时间最长的线 程最优先获取锁
			公平锁能够减少“饥饿”发生的概率，等待越久的请求越是能够得到优先满足
			
			非公平锁实际执行的效率要远远超出公平锁，除非程序有特殊需要，否则最常用非公平锁的分配机制。、
			
			加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待
			1. 非公平锁性能比公平锁高 5~10 倍，因为公平锁需要在多核的情况下维护一个队列
			2. Java 中的 synchronized 是非公平锁，ReentrantLock 默认的 lock()方法采用的是非公平锁。
			
	多线程安全的思路
			1.不要跨线程共享变量		线程副本
			2.使状态变量为不可变		final
			3.访问状态变量使用同步		同步锁
	
	Threadlocal	
		使用场景(在实际使用中，容器把一个事务上下文和一个可执行的线程关联起来。)
			利用静态Threadlocal持有事务上下文，当框架需要使用当前线程所在的上下文中的事务中相关信息时，可以从threadlocal中获取事务上下文中信息，而不用从前到后显式传递，类似系统中的op-gateway那边。
			降低了方法执行链上的过多参数传递，隐藏方法参数。
		线程变量会引入晦涩的类间耦合，像全局变量和创建一种将方法的参数隐藏起来的方法，不要过多使用
		threadLocal一般都是声明在静态变量中，如果不断的创造threadLocal而没有调用remove方法，会导致内存泄漏
		ThreadLocal 的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。
	
	
	常见的原子变量类  内部也是用了CAS比较交换原理处理的
		i++操作不是原子操作，经历了3个操作，获取变量当前值，为该值加一，写回更新值
		atomicInteger,atomicLong，atomicBoolean等
		核心的方法是compareAndSet,用来比较交换
		原子类的几种，1基本类型 2数组 3引用类型等,
		其基本的特性就是在多线程环境下，当有多个线程同时执行这些类的实例包含的方法时，具有排他性，即当某个线程进入方法，执行其中的指令时，不会被其他线程打断，而别的线程就像自旋锁一样，一直等到该方法执行完成，才由 JVM 从等待队列中选择一个另一个线程进入，
	
	
	其他杂知识
		fork/join 并行执行框架，将大任务分成小任务，然后将各小任务结果汇总成大任务结果。暂时用的不多，暂不考虑
		
		nginx可以配置单ip的访问限制。通过域名解析，建立很多子域名，不同业务模块由域名分发到不同的nginx服务器上
		
		tomcat 可创建的最大线程数默认是200
		用户请求request，创建一个计算线程(线程数量有限)，更多的请求要么拒绝 要么进入等待队列会消耗资源，服务器压力增大。每个线程有可能阻塞，进一步卡住资源消耗，造成系统崩溃

		队列相关用的比较少，延迟队列delayqueue用的也不多，因为有了job的存在
		DelayQueue<Worker> queue = new DelayQueue<Worker>();  其中Worker实现了delay接口，实现了compareTo(决定排序)和getDelay(决定是否到期出队)方法
		配合while(true){queue.take()}，取出到时的对象。添加对象是queue.offer()
		
		线程thread中的属性priority优先级，是切换时优先考虑的因素，一般不要去修改线程的优先级，将程序行为和平台隔离开。
		线程的调度是抢占式的
		
		final不可变
			对象不可变和到对象的引用不可变不是一回事
			程序存储在不可变对象中的状态，仍然可以通过替换一个带有新状态的不可变对象的实例得到更新。
		
			去掉set方法，使用private，使用final
			Integer和Long等基本类型的包装类也是final类型的
			final类不嫩被继承
			final方法，实例方法为final，子类无法覆盖该方法；若申明的是final是类方法，则子类无法隐藏
			final字段，只赋值一次
			
		Thread类本身实现了Runnable接口，子类继承Thread后覆盖run()方法即可
		
		线程中suspend()、resume()和stop()都是过期的方法，不要使用

		资源限制
			硬件资源，带宽，硬盘，cpu资源影响着程序的上限。
			硬件，机器，id对机器数取模,集群分发，软件使用资源池复用
	
		ReentrantLock的实现依赖于Java同步器框架AbstractQueuedSynchronizer（本文简称之为 AQS）。
		AbstractQueuedSynchronized（AQS）
			抽象的队列式的同步器，定义了一套多线程访问共享资源的同步器框架
			它提供通用机制来原子性管理同步状态、阻塞和唤醒线程，以及维护被阻塞线程的队列
			许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch
			
		AQS 定义两种资源共享方式
			Exclusive 独占资源-ReentrantLock
				Exclusive（独占，只有一个线程能执行，如 ReentrantLock）
			Share 共享资源-Semaphore/CountDownLatch
				Share（共享，多个线程可同时执行，如 Semaphore/CountDownLatch）。	

				
		阻塞队列
			是一个支持两个附加操作的队列。这两个附加的操作支持阻塞 的插入和移除方法，相对输入输出也是一样
			1）支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。
			2）支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。 
			阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。
	
		限流算法 	漏桶算法(固定出速)、令牌桶算法(用的多点，固定入速，允许流量一定程度的突发。)
			令牌桶算法由于实现简单，且允许某些流量的突发，对用户友好，所以被业界采用地较多。当然我们需要具体情况具体分析，只有最合适的算法，没有最优的算法。
			
			
		当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到 CPU 缓存中。
		如果计算机有多个 CPU，每个线程可能在不同的 CPU 上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。
		而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。
		
		只有在一些特殊的场景下，才能适用 volatile。总的来说，必须同时满足下面两个条件才能保证在并发环境的线程安全：
		（1）对变量的写操作不依赖于当前值（比如 i++），或者说是单纯的变量赋值（boolean flag = true）。 
		（2）该变量没有包含在具有其他变量的不变式中，也就是说，不同的 volatile 变量之间，不能互相依赖。只有在状态真正独立于程序内其他内容时才能使用 volatile。	
	
		java 使用的线程调使用抢占式调度，Java 中线程会按优先级分配 CPU 时间片运行，且优先级越高越优先执行，但优先级高并不代表能独自占用执行时间片，可能是优先级高得到越多的执行时间片，反之，优先级低的分到的执行时间少但不会分配不到执行时间
	
	
144.io相关
	java.io操作类主要有4种(前两种属于数据格式，后两种属于传输方式)
			1.基于字节操作的I/O接口：inputSteam 和outputStream
		    2.基于字符操作的I/O接口：writer 和 Reader
			3.基于磁盘操作的I/O接口：file
			4.基于网络操作的I/O接口：socket
			
			
		磁盘和网络传输，最小的存储单元都是字节，不是字符。
		数据持久化或者网络传输都是以字节进行的。
			InputStreamReader是字节到字符的转化桥梁
			OutputStreamWriter完成字符到字节的编码过程
			
			reader类是java的I/O中读字符的父类，inputStream类是读字节的父类
			writer类对应是写字符的父类，outputStream类是写字节的父类
			
		序列化的注意点
			1.父类实现serializable，所有子类都可以被序列化
			2.子类实现Serializable接口，父类没有，?中的属性不能序列化(不报错，数据会丢失)，但是子类中属性能正确序列化
			3.序列化的属性是对象，这个对象也必须实现序列化接口
			4.序列化过程中对应的序列化id不要变
			
		网络优化的几个点
			1.减少网络交互的次数。能用缓存用缓存，能批量处理就批量处理
			2.减少网络传输数据量的大小。文件的话压缩后再传输
			3.尽量减少编码。尽量提前字符转字节。
			
		编码相关
			计算机中存储信息的最小单元是一个字节，即8bit们表示的字符范围是0~255个
			ascII,gbk,utf8等可以被看做字典，规定了转化的规则。
			通常不要使用操作系统默认的编码，否则跨环境会出现乱码。
		
			string类的字符字节转换
				String str="海绵宝宝";
				byte[] bytes = str.getBytes("UTF-8");
				String strr = new String(bytes, "UTF-8");
			访问数据库中的编码，在url中指定即可
			
		IO 是面向流的，NIO 是面向缓冲区的
			Selector 类是 NIO 的核心类，Selector 能够检测多个注册的通道上是否有事件发生，如果有事件发生，便获取事件然后针对每个事件进行相应的响应处理。
			在 Java NIO 中，是通过 selector.select()去查询每个通道是否有到达事件，如果没有事件，则一直阻塞在那里。
			Java NIO 实际上就是多路复用 IO。在多路复用 IO模型中，会有一个线程不断去轮询多个 socket 的状态，只有当 socket 真正有读写事件时，才真正调用实际的 IO 读写操作
			
			多路复用 IO 模式，通过一个线程就可以管理多个 socket，只有当socket 真正有读写事件发生才会占用资源来进行实际的读写操作。
			多路复用 IO 为何比非阻塞 IO 模型的效率高是因为在非阻塞 IO 中，不断地询问 socket 状态时通过用户线程去进行的，而在多路复用 IO 中，轮询每个 socket 状态是内核在进行的，这个效率要比用户线程要高的多。		
			
			
145.filter相关，servlet相关，interceptor相关
	一次请求只会成功匹配到一个servlet，但是filter只要匹配成功，这些filer都会在请求链filterchain上被调用。
		当filterchain上所有的filter对象执行完成后，会执行最终的servlet。
		servlet中配置的url-pattern,若请求没有匹配到所有的servlet，那么这个请求就直接返回，也不会有后续的调用任何filter对象。
		url-pattern解析规则，对于servlet和filter是一样的
			1.精确匹配	如/foo.htm，只会匹配foo.htm这个url
			2.路径匹配	如/foo/*会匹配以foo为前缀的url
			3.后缀匹配	如*.htm会匹配所有以.htm的url    ()
			
		备注： /foo/、/*.htm 和 */foo 都是不对的
		
		优先精确匹配，其次是最常路径匹配，最后是后缀匹配。但一次请求只会成功匹配到一个servlet
		getsession时候，不存在会直接创建新的，所哟如果getattrubute取不到前设置的session，很可能已经过期，重新创建一个新的session了
		
		
		
146.mybatis相关
	mybatis中的主要操作对象 sqlsession
				redis是操作是原子性的，可存储 字符串，列表，集合多种数据类型
				
				jdk的动态代理需要绑定代理对象和真是对象的关系，可以封装在代理的操作类中，就是说再invocationhandler中直接加上代理创建方法
				cglib创建代理，只需要一个非抽象的类就能实现动态代理，主要代理操作类是实现MethodInterceptor接口，通过加强这Enhancer实现代理
				*Enhancer是cglib中使用频率很高的一个类，它是一个字节码增强器，可以用来为无接口的类创建代理。它的功能与java自带的Proxy类挺相似的。它会根据某个给定的类创建子类，并且所有非final的方法都带有回调钩子。

				public class UserServiceCglib implements MethodInterceptor {
					private Object target;
					public Object getInstance(Object target) {
						this.target = target;
						Enhancer enhancer = new Enhancer();
						enhancer.setSuperclass(this.target.getClass());
						enhancer.setCallback(this);// 设置回调方法
						return enhancer.create();// 创建代理对象
					}
					/**
					 * 实现MethodInterceptor接口中重写的方法，回调方法
					 */
					@Override
					public Object intercept(Object object, Method method, Object[] args, MethodProxy proxy) throws Throwable {
						System.out.println("事务开始。。。");
						Object result = proxy.invokeSuper(object, args);
						System.out.println("事务结束。。。");
						return result;
					}
				}
				
				两种方法相似，通过geProxy方法生成代理对象，执行代理的逻辑类（需要实现一个接口，一个是invocationhandler,一个是MethodInterceptor），决口定义的方法就是代理对象的逻辑方法。可以控制真实对象的方法
				
				拦截器设计思路，用封装设计好的拦截器来实现动态代理的功能。拦截器底层还是动态代理。简化开发
				设计拦截器，让开发者直接使用，隐藏底层动态代理的实现。对于开发者来说就简单了
				interceptor实现原理，就是getProxy方法的参数中(target,interceptorClass)传连个参数,然后invoke中根据情况实现逻辑。这里被代理的类还是target，只不过增加的interceptorClass的一层判断，根据判断结果执行target的相关方法
				这里说明代理的类还是比较灵活的，只要确认代理的类是啥，其他因素也可以添加进来
				
				多个拦截器的处理链即各proxy依次作为下个拦截器代理的target，链式调用。
				
				观察者模式，在Java语言的java.util库里面，提供了一个Observable类以及一个Observer接口，构成Java语言对观察者模式的支持。
				暂不细看，目前还没啥用

				工厂模式，硬是要将对象的创建封装进一个工厂类中
				
				mybatis的几个核心组件
				
					sqlsessionfactorybuilder	构造器，创建sqlsessionfactory
					sqlsessionfactory			工厂接口，用来创建sqlsession，						
							相当于一个数据库连接池，一旦创建长期保存使用
							占据数据库的连接资源，一般是单例
							在mybatis-spring中，使用SqlSessionFactoryBean 支持 SqlSessionFactory 的配置
							
					sqlsession					会话，SqlSession中定义的全是对数据库增删改查的各种方法，mybatis的核心接口对象
							类似jdbc中的connection对象，代表一个连接资源的启用。1.获取mapper接口，2.发送sql给数据库 3.控制数据库事务
							存在一个业务请求中，操作事务，请求完成，关闭连接，归还给sqlsessionfactory。
							
					sql mapper					映射器，由java接口和xml构成，需要给出对应sql和映射规则，发送sql执行，并返回结果
						主要作用就是将查询结果映射到一个pojo中，或者将pojo插入到数据库中。
						mybatis会为mapper接口生成一个动态代理，去处理相关的实现逻辑
				
				这里有点用拦截器代替动态代理的意思，只需要执行拦截器的方法，不需要考虑底层代理实现，都封装好了。
				mybatis默认情况下提供自动映射，只要sql返回的列名能和pojo对应起来即可。
				user_name like concat('%',${userName},'%')
				
				使用接口mapper中注解sql，复杂的sql不好弄，不灵活，难维护，不推荐
				
				
				
				
				mapper的动态代理，核心就接口和sql中的配置，一个方法对应一个mapperMethod
				基于InvocationHandler配置类，生成了实现接口的实现类
				
				public class MapperProxy implements InvocationHandler, Serializable {
					private static final long serialVersionUID = -6424540398559729838L;
					private final SqlSession sqlSession;
					private final Class<T> mapperInterface;
					private final Map<Method, MapperMethod> methodCache;

					public MapperProxy(SqlSession sqlSession, Class<T> mapperInterface, Map<Method, MapperMethod> methodCache) {
						this.sqlSession = sqlSession;
						this.mapperInterface = mapperInterface;
						this.methodCache = methodCache;
					}
								
				  //执行方法				
				  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
						try {
							if (Object.class.equals(method.getDeclaringClass())) {
								return method.invoke(this, args);
							}

							if (this.isDefaultMethod(method)) {
								return this.invokeDefaultMethod(proxy, method, args);
							}
						} catch (Throwable var5) {
							throw ExceptionUtil.unwrapThrowable(var5);
						}

						MapperMethod mapperMethod = this.cachedMapperMethod(method);
						return mapperMethod.execute(this.sqlSession, args);
					}
					
					
					//创建方法和sql配置的执行映射
					private MapperMethod cachedMapperMethod(Method method) {
						MapperMethod mapperMethod = (MapperMethod)this.methodCache.get(method);
						if (mapperMethod == null) {
							mapperMethod = new MapperMethod(this.mapperInterface, method, this.sqlSession.getConfiguration());
							this.methodCache.put(method, mapperMethod);
						}
						return mapperMethod;
					}
					
				  public T newInstance(SqlSession sqlSession) {
					final MapperProxy<T> mapperProxy = new MapperProxy<T>(sqlSession, mapperInterface, methodCache);
					return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[]{mapperInterface}, mapperProxy);
				  }
				}
				
				自定义mapper代理，实现单一接口的代理，执行自定义逻辑
				Testmapper testceshi = (Testmapper) TestMapperProxy.newInstance("测试接口代理",Testmapper.class);
				如果不使用泛型T,那么外层就需要强转。执行了泛型,外层就不用转
				
				public class TestMapperProxy<T> implements InvocationHandler {
					private String  sqlSession;
					private final Class mapperInterface;
					public TestMapperProxy(String sqlSession, Class mapperInterface) {
						this.sqlSession = sqlSession;
						this.mapperInterface = mapperInterface;
					}
					public static <T> T newInstance(String sqlSession,Class<T> mapperInterface) {
						 TestMapperProxy mapperProxy = new TestMapperProxy(sqlSession, mapperInterface);
						return  (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[]{mapperInterface}, mapperProxy);
					}
					@Override
					public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
						if (Object.class.equals(method.getDeclaringClass())) {
				//            System.out.println("Object方法"+method.getName());
							method.invoke(proxy,method,args);
						}else{
							System.out.println("拿到操作对象:"+mapperInterface.getName());
							//进行自定义操作
							System.out.println("操作了对象参数:"+sqlSession);
						}
						return null;
					}
				}		
				
				****
				<T> T 表示不限制类型，不限制类型时候用这个			不受static影响，因为不需要编译期完成指定
				T 这个表示编译时候就要限制类型，限制特定类型，编译就指定，使用这个，受static影响，因为编译期需要完成，和static同时期，但是static修饰后又使用T,冲突
				
				<T> T 	不需要 类上泛型标注，可以使用static修饰
				T 		需要类上使用泛型标注，不可以使用static修饰
				****
				
				如果对数据库用户名和密码加密了(加密后存放在属性文件中)，可以再java程序方式创建数据库连接sqlsessionfactory过程中，解密后，创建数据库的连接。
				之前也有在xml中加密解密的。
				
				
				typeAliases 别名， 用来简化全限定名称，一般也不用。
				typeHandler 类型转换器，用来转换javaType 和 jdbcType
				
			
				映射器定义命名空间(namespace)的方法，对应一个接口的全路径，就是mapper接口。
				映射器能有效消除jdbc的底层代码
				
				两种自动映射
				自动映射，如果编写sql的列名和对象pojo的属性名一致，就会形成自动映射。默认开启
				驼峰映射，严格要求user_name 对应userName属性。不太灵活。不推荐 默认为false
				实际开发中，一般使用的是resultmap，建立结果集映射。
				
				多值传参也可以传map，但是可读性差，难维护，不推荐
				多值传参也可以使用javabean，当只有一个参数对象，可以指定类型后，直接写属性。多个参数，不行
				一般用的是@param单值传输，多于5个就不推荐用了
				sql中，#{userName}也可以不用给出parameterType类型，mybatis会自动识别
				
				
				<resultMap id = "cargo" type="cargo">
					<id property="cargoId" column="cargo_id"/>
					<result property="cargoTypeId" column="cargo_type_id"/>
				</resultMap>
				
				子元素id代表resultMap的主键，而result代表其属性。
				在自定义的resultMap中第一列通常是主键id。
				id和result都是映射单列值到一个属性或字段的简单数据类型。
				唯一不同的是，id是作为唯一标识的，当和其他对象实例对比的时候，这个id很有用，尤其是应用到缓存和内嵌的结果映射。级联
				
				
		rowbounds是一次性取出数据放入内存再分割数据，一次获取所有符合条件的数据，然后在内存中对大数据进行操作
		
		在 mybatis 中，使用 RowBounds 进行分页，不需要在 sql 语句中写 limit。但是由于它是在 sql 查询出所有结果的基础上截取数据的，所以在数据量大的sql中并不适用，它更适合在返回数据结果较少的查询中使用
		最核心的是在 mapper 接口层，传参时传入?RowBounds(int offset, int limit)?对象，即可完成分页
		注意：由于 java 允许的最大整数为 2147，483，647，所以 limit 能使用的最大整数也是?2147483647，一次性取出大量数据可能引起内存溢出，所以在大数据查询场合慎重使用
		mapper 接口层代码如下
		List<Book> selectBookByName(Map<String, Object> map, RowBounds rowBounds);
		
		PageHelper： 物理分页， 通过拦截器加 limit 语句进行分页
			物理分页依赖的是某一物理实体，这个物理实体就是数据库，比如MySQL数据库提供了limit关键字，程序员只需要编写带有limit关键字的SQL语句，数据库返回的就是分页结果。
			每次都要访问数据库，对数据库造成的负担大
			每次只读取一部分数据，占用的内存空间较小
			每次需要数据时都访问数据库，能够获取数据库的最新状态，实时性强
			数据库量大、更新频繁的场合
		RowBounds： 逻辑分页，数据量大的时候压力较大。一般不用
			逻辑分页依赖的是程序员编写的代码。数据库返回的不是分页结果，而是全部数据，然后再由程序员通过代码获取分页数据，常用的操作是一次性从数据库中查询出全部数据并存储到List集合中，因为List集合有序，再根据索引获取指定范围的数
			只需要访问一次数据库
			一次性将数据读取到内存，占用较大的内存空间。如果使用java开发，Java本身引用的框架就占用了很多内存，这无疑加重了服务器的负担
			因为一次性读入到内存，数据发生了改变，数据库最新状态无法实时反映到操作中
			数据量较小、数据稳定的场合
			总结：Mybatis的逻辑分页比较简单，简单来说就是取出所有满足条件的数据，然后舍弃掉前面offset条数据，然后再取剩下的数据的limit条


		1.逻辑分页   逻辑内存中，   查询所有数据 List, list.subList 截取你需要数据
			查出所有的数据，使用程序进行分页。它是针对ResultSet结果集执行的内存分页。
			占用内存大、数据更新不能及时反馈、不用频繁查询数据库

		2.物理分页,直接通过SQL进行在数据库中直接分页,得到的数据就是我们想要分页之后的数据,就是物理分页;其实是依赖物理数据库实体。limit直接数据库查出需要的数据
			每次查询数据库，使用limit，需要计算总数、页数、当前页。
			占用内存小、数据更新及时反馈、频繁查询数据库
			
			1.mybatis自带的分页RowBounds;

? ? ? 		2.mybatis插件或者直接书写sql进行分页;
	????? ????(1).通过自己的封装SQL根据beginNum(开始条数)和endNum(需要的条数)来进行分页  一般不用，不需要造轮子
					有点类似老系统的插件，自己封装查询limit 参数,需要手动进行封装总数以及分页信息,数据返回页面;
					自己创建分页插件，老系统，不推荐这样造轮子。
					
	? ? ? ????(2).PageHelper分页插件
					将pageNum和pageSize封装为page对象，保存在ThreadLocal中，实现线程间数据隔离。
					Pagehelper实现了Mybatis的Interceptor接口，调用拦截StatementHandler（Sql语法的构建处理）方法，按照物理库的不同重构SQL实现分页。
					
					插件拦截的对象：
					Executor：拦截执行器的方法（log记录）
					StatementHandler：sql语法构建处理
					ParameterHandler：拦截参数的处理
					ResultSetHandler：拦截结果集的处理
					
					PageHelper首先将前端传递的参数保存到page这个对象中，接着将page的副本存放入ThreadLoacl中，这样可以保证分页的时候，参数互不影响，接着利用了mybatis提供的拦截器，取得ThreadLocal的值，重新拼装分页SQL，完成分页。

		主键回填，useGeneratedKeys="true"
		update delete insert 返回的int 都表示影响的行数
		${},这样就不会当做普通的参数处理，不会转译这个值，不作为参数设置，变为直出。相当于不限定为参数，直出类型。

		结果集映射，集合，级联，关联，关联查询
		 
		MyBatis的级联分为3种。级联过多会降低查询性能   直接join查询出来关联
		鉴别器（discriminator）：根据某些条件决定采用具体实现类。不常用，忽略。

		1. 关联-association				用于一对一
		2. 集合-collection				用于一对多
		
		
		mybatis不支持多对多，可以拆分成两个一对多级联处理。暂时忽略
		
		比如同时有User.java和Card.java两个类
		public class User{
			private Card card_one;
			private List<Card> card_many;
		}
		在映射card_one属性时用association标签, 映射card_many时用collection标签.

		RIGHT JOIN 关键字会从右表 (Orders) 那里返回所有的行，即使在左表 (Persons) 中没有匹配的行。先查出右表值，再关联左表。一般用left join
			
			
		column 是查询出来的结果集中的列名	
		<association property="reqUserInfo" column="req_User_Id" javaType="com.xiaoyuer.core.dmo.UserInfo">    一般这里column没有必要写
			<id property="id" column="req_User_Id"></id>
			<result property="nickName" column="rNick_Name" />
		</association>	
			
			
		mybatis默认有两级缓存，一级在sqlsession上，一级在sqlsessionfactory上。一般不用
		
		
		<where>和<trim>，<set>都可以达到去除多余 and 和or的效果。
		<if test="">对于字符串类型的判断，需要加上toString()方法
		模糊查询，mysql中常用的是concat方法
		
	mybatis的运行过程两大步，1.服务配置文件缓存到Configuration对象，用以创建sqlsessionfactory
							 2.sqlsession的执行过程。
							 
	sqlsessionfactorybuilder构建sqlsessionfactory两步。
		1.通过xmlconfigbuilder解析配置的xml文件(或注解)，存入Configuration类对象
		2.configuration创建sqlsessionfactory。

	mybatis中一条sql和它相关的配置信息由3个部分组成
		MappedStatement			用来获取某条sql的所有配置信息
		Sqlsource				获取BoundSql对象
		BoundSql				建立sql和参数的地方，
			插件功能中常修改这个地方
			
	mapper的动态代理，最后核心处理方法是sqlsession对象去运行对应的sql。mapper通过关联xml中的namespace，找到对应的执行方法。
	sqlSessioη.getMapper(RoleMapper.class);
	configuration.<T>getMapper(type, this)
	最终代理的是sqlsession的操作。通过namespace将sql和代理对象绑定起来
	
	一条查询SQL的执行过程 ：
		1.预编译sql。Executor调用StatementHandler的prepare()方法预编译SQL，同时设置些基本运行的参数。
		2.设置参数。用parameterize()方法启用ParameterHandler设置参数，
		3.执行sql。完成预编译，执行sql 。
		如果是查询MyBatis会使用ResultSetHandler 封装结果返回给调用者。
		
		
	mybatis中要使用插件，需要实现Interceptor接口。一般不推荐使用插件。

	mybatis的四大对象
		1.Executor				执行sql的全过程，包括组装参数、组装结果返回集和执行sql过程
		*2.StatementHandler		是执行SQL的过程，我们可以重写执行SQL的过程它是插件最常用的拦截对象
		3.ParameterHandler		主要拦截执行SQL参数组装，我们可以重写组装参数规则
		4.ResultSetHandler		用于拦截执行结果的组装，我们可以重写组装结果的规则
	
	分页插件的思路，拦截StatementHandler对象的prepare()方法，在预编译sql之前修改sql，得到结果的返回数量被限制。
	@Intercepts({ @Signature(type = StatementHandler.class, method = "prepare", args = { Connection.class }) })
	public class PagePlugin implements Interceptor {
	
	其中，＠Intercepts 说明它是个拦截器。＠Signature 是注册拦截器签名的地方，只有签名满足条件才能拦截type是四大对象中一个，这里是 StatementHandler。
	method 代表要拦截四大对象的某种接口方法，而args表示该方法参数，要根据拦截对象的方法参数进行设置
	mappedStatement.getId().matches(".*ListPage.*")
	
	mybatis的缓存	基本不用，没有springcache灵活，可以上到service级别。
		Mybatis 的一级缓存原理（sqlsession 级别）
			第一次发出一个查询 sql，sql 查询结果写入 sqlsession 的一级缓存中，缓存使用的数据结构是一个 map。
			同一个 sqlsession 再次发出相同的 sql，就从缓存中取出数据。如果两次中间出现 commit 操作（修改、添加、删除），本 sqlsession 中的一级缓存区域全部清空，下次再去缓存中查询不到所以要从数据库查询，从数据库查询到再写入缓存
		
		二级缓存原理（mapper 基本），
			二级缓存的范围是 mapper 级别（mapper 同一个命名空间），mapper 以命名空间为单位创建缓存数据结构，结构是 map。
	
	
	
147.spring相关
	
	ioc主要是基于BeanFactory 和 ApplicationContext 两个接口，其中ApplicationContext是BeanFactory(BeanFactory)子接口之一，而 ApplicationContext是其高级接口之一，并扩展BeanFactory功能，一般都会使用 applicationContext作为Spring IoC容器

		WebApplicationContext也扩展了它ApplicationContext。
		ClassPathXmlApplicationContext 是 ApplicationContext 的一个子类
		
		ioc容器
			通过xml或者注解，资源丁文，载入beanDefinition，初始化bean。
			
			BeanPostProcessor针对全部Bean,DisposableBean针对spring IoC 容器，其他的接口只针对单－Bean 。
		
		以简写成＠Component(”role”)，甚至直接写成＠Component ，对于不写的，Spring IoC 容器就默认类名，但是以首字母小写的形式作为 id ，为其生成对象，配置到容器中
		
		ComponentScan 代表进行扫描 默认是扫描当前包的路 POJO 包名和它保持一致才能扫描，否则是没有的。不要采用多个＠ComponentScan注解进行配置，有重复的包或子包就会产生重复的对象
		
		＠Bean相对于＠Component，可以注册源码bean，而后者没法做。方法返回对象

		＠ImportResource 引入xml
		＠Import 可以注入配置类
		@PropertySource 要和xml对应起来看
		
		********其实整体上看是xml和注解的两种不同方式。********
		
		使用注解的方式需要用到注解＠Value 在属性文件的读取中使用的是“$，而在 SpringEL中则使用“＃”
		@Value搭配Spring EL使用，可扩展高级点的用法。
		@Value("${query_info_url}")	 最常用
		
		@Value(”#{role .id}”)
		
		@Value("#{'${test.payChannel.pcCode}'.split(',')}")			Spring EL
		private List<String> testPayChannel;
		
		interceptor实现原理，就是getProxy方法的参数中(target,interceptorClass)传连个参数，代理操作类中，针对target的方法，补充interceptorClass的操作。
		代理操作类中有target 和对应的拦截器，在invoke中会实现调用
		
		实现类再容器中有个代理类。至于被代理的obj和拦截器的映射关系，应该已经加入容器了，这样在obj调用的时候能关联对应的拦截器。
		
		aop编程有着重要的意义，首先它可拦截些方法 ,然后将各个对象组织成一个整体，
		
		在面向切面编程的思想里面，把功能分为核心业务功能，和周边功能。
		所谓的核心业务，比如登陆，增加数据，删除数据都叫核心业务；所谓的周边功能，比如性能统计，日志，事务管理等等
		周边功能在Spring的面向切面编程AOP思想里，即被定义为切面在面向切面编程AOP的思想里面，核心业务功能和切面功能分别独立进行开发然后把切面功能和核心业务功能 “编织” 在一起，这就叫AOP
		
		
		 mybatis封装的数据库操作
			public void savePurchaseRecord (Long productid, PurchaseRecord record) {
				SqlSession sqlSession = null;
				try{
						sqlSession = SqlSessionFactoryUtils.openSqlSession() ;
						ProductMapper productMapper = sqlSession.getMapper(ProductMapper.class) ;
						Product product= productMapper.getRole(productid);
						//1.mapper操作  2.commit
						sqlSession .commit() ;
					}catch (Exception ex) {
					//异常回滚事务
						ex.printStackTrace();
						sqlSession.rollback();
					} finally{
						//关闭资源
						if (sqlSession != null) {
							sqlSession.close();
						}
					}
			}
						
		使用aop，这样主要都集中在业务处理上，而不是数据库事务和资源管控上。
		当方法标注为＠Transactional时，则方法启用数据库事务功能。事务操作和，数据库的资源关闭都是封装在aop事务调用中。
		
		这些约定的方法加入默认实现后，你要做的只是执行SQL这步而已。没有数据库资源的获取和关闭，没有事务提交和回滚相关代码。这些 AOP 框架依据约定的流
		程默认实现了，这样对于开发者而言就更为关注业务开发，而不是资源控制、事务异常处理，这些 AOP 框架都可以完成。
		
		AOP是通过动态代理模式，带来管控各个对象操作的切面环境，管理包括日志、数据库事务等操作，让我们拥有可以在反射原有对象方法之前正常返回、异常返回事后插入自
		己的逻辑代码的能力，有时候甚至取代原始方法。在一些常用的流程中，比如数据库事务，AOP会提供默认的实现逻辑。
		
		静态代理是在编译 class 文件时生成的代码逻辑

		SpringAOP 基于方法拦截的 AOP ，换句话说 Spring 只能支持方法拦截的 AOP
		注：有一点非常重要，Spring的AOP只能支持到方法级别的切入。换句话说，切入点只能是某个方法。
		Spring中有4种方式去实现 AOP 拦截功能。
			1.使用＠AspectJ 注解驱动切面			（常用）					@Aspect 创建切面
			2.使用 XML 配置 AOP						（复制，少用）				<aop:aspectj-autoproxy />    <aop:aspect>中定义切面类
			3.使用 ProxyFactoryBean 和对应的接口实现 AOP
			4.使用 AspectJ 注入切面。
		
		从而组织切面，把各类通知织入到流程当中 (在切面类中操作各种before、after等操作)
		对于方法1，主要是以某个类的某个方法作为切点，用动态代理的理论来说，就是要拦截哪个方法织入对应AOP通知。
		@Around  环绕通知，它将被最原有方法，允许你通过反射调用@Around环绕通知取代它原有方法。  jp.proceed(),方法灵活，功能多(环绕通知=前置+目标方法执行+后置通知，proceed方法就是用于启动目标方法执行的)
		实际使用 使用@Pointcut,只需要切一个点即可，简化了代码。不需要每个@Before这样的注解都切一次方法
		
		spring4版本执行顺序，aop的执行顺序   切面,切点入口-around(before插入其中)-after-afterReturning，  spring5版本不一样，after和afterReturning顺序有变化
				around before advice
			before advice
				target method 执行
				around after advice
			after advice
			afterReturning

		Class 加上@Aspect
		@Pointcut（”execution(*com.ssm.service.serviceAImpl.printRole(..))”)            通过动态代理生成一个切面(这切面相当于一个拦截器，工作环境)
		public void print(){}
		
		@Before (”print()”) 
		public void before() { doSomething()}
		
		JointPoint是程序运行过程中可识别的点，用作AOP切入点。JointPoint对象则包含了和切入相关的很多信息。比如切入点的对象，方法，参数等。
		
		Proceedingjoinpoint 继承了 JoinPoint。是在JoinPoint的基础上暴露出 proceed 这个方法。proceed很重要，这个是aop代理链执行的方法。
		这也是@Around 和其他的前后置通知的重要区别。
		
		
		
		
		do（分两个执行，逻辑差不多，应该往上提，上多，提到切面中，小改动某些参数后执行），pjp.proceed 执行两次
		
		
		通过切接口的方法，这里可以改参数操作两次核心方法，
		@Around(value = "execution(public com.xiaoyuer.pay.framework.lang.Result com.xiaoyuer.fpp.op.service.intf.IWithdrawService.submit(..))")

		Result citicRes = (Result)pjp.proceed(citicWithdrawParams);
		if(!citicRes.isSuccess()) {
			logger.error("citic提现失败，不再执行egj");
			return citicRes;
		}
		return (Result)pjp.proceed(egjWithdrawParams);
		
		execution表达式
		execution (* com.sample.service.impl..*.*(..))
			1、execution(): 表达式主体。
			2、第一个*号：表示返回类型，*号表示所有的类型。
			3、包名：表示需要拦截的包名，后面的两个句点表示当前包和当前包的所有子包，com.sample.service.impl包、子孙包下所有类的方法。
			4、第二个*号：表示类名，*号表示所有的类。
			5、*(..):最后这个星号表示方法名，*号表示所有的方法，后面括弧里面表示方法的参数，两个句点表示任何参数。

 
			针对切面，可以变参数执行两次切点核心内容
			@Aspect
			@Component
			public class MultiAspect {

				@Around(value = "execution(public * com.interface.do(..))")
				public Result around(ProceedingJoinPoint pjp) throws Throwable {
					MethodSignature signature = (MethodSignature)pjp.getSignature();
					String[] parameterNames = signature.getParameterNames();//参数名
					boolean condition=true;//虚拟条件，方便显示，无实际意义
					int index = ArrayUtils.indexOf(parameterNames, "userPhone" ,0);
					
					if(condition) {
						return (Result)pjp.proceed(); //满足条件直接放行
					}
					if(condition) {
						Object[] params = new Object[] {new String[] {"Citic"}
						};
						return (Result)pjp.proceed(params);
					}else {
						//特定条件下，分两步走，一个改参数走，一个走原来
						Object[] egjWithdrawParams = new Object[] {
								pjp.getArgs()[index],
								new String[] {"Egj"}
						};
						Result citicRes = (Result)pjp.proceed(citicWithdrawParams);
						if(condition) {
							return citicRes;
						}
						return (Result)pjp.proceed(egjWithdrawParams);
					}
				}
			}
		
		
		
		
		
		
		
		
		
		
		
		
		切点处可以将被代理的方法的参数，传进来使用，这个用的不多
		obj.getClass().getInterfaces() 意味着代理对象挂在多个接口之下。
		
		多切面执行（责任链模式）
		Aspect1类加入＠Order(1),Aspect2 类加入＠Order(2),Aspect3加入Order(3)，再次对其进行测试，得到
		before 1 
		before 2 
		before 3 
		method.do
		after 3 
		afterReturning 3 
		after2 
		afterReturning 2 
		after1 
		afterReturning 1
		
		jdbcTemplate是不能操作事务的
		Spring中数据库事务是通过PlatformTransactionManager进行管理的,而能够支持事务的是TransactionTemplate模板，它是 Spring 所提供的事务管理器的模板
		MyBati框架用得最多的事务管理器是 DataSourceTransactionManager
		
		
		Springboot内部提供的事务管理器是根据autoconfigure来进行决定的。(@EnableTransactionManagement这个也是自动配置的，不需要在boot上添加)
		当我们使用spring-boot-starter-jdbc的时候，构造的事务管理器则是DataSourceTransactionManager。
		这个事务管理器都实现了spring中提供的PlatformTransactionManager接口，这个接口是spring的事务核心接口。
		
		
		@EnableTransactionManagement是 spring-tx 的注解，不是 spring-boot 的
		spring-boot 会自动配置事务，相关的配置在 org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration
		在自动配置类里已经写好了 @EnableTransactionManagement。
		*****这个是在boot的 spring-boot-autoconfigure模块中自动配置的*****

		
		transactionTemplate.execute() 加上jdbcTemplate 操作数据库 这种属于编程式的
		
		
		编程式，实际上pay中transactionTemplate(也实现了TransactionDefinition，继承了DefaultTransactionDefinition)就是对下面的进行了封装，
		
		ApplicationContext ctx ＝ new ClassPathXmlApplicationContext("spring-cfg.xml");
		JdbcTemplate jdbcTemplate = ctx.getBean(JdbcTemplate.class);
		TransactionDefinition def = new DefaultTransactionDefinition();
		PlatformTransactionManager transactionManager =ctx.getBean(PlatformTransactionManager.class);
		TransactionStatus status = transactionManager.getTransaction(def);
		try {
		//        jdbcTemplate.update();     			//sql执行
				transactionManager.commit(status);
			} catch (Exception ex){
				transactionManager.rollback(status)
			}
			
		在spring的事务管理高层抽象层中主要包含3个接口：
			TransactionDefinition：用于描述隔离级别、超时时间、是否为只读事务和事务传播规则
			TransactionStatus：代表一个事务的具体运行状态、以及还原点
			PlatformTransactionManager：一个高层次的接口，包含3个方法。commit、rollback和getTramsaction
			
		XML 配置声明式事务管理，不推荐，费事
			TransactionInterceptor是Spring框架内置实现的一个MethodInterceptor,用于声明式事务管理
			
			
			
		本质是通过动态代理实现
			1.在 Spring IoC 容器初始化时，Spring 会读入这个注解配置的事务信息，并且保存到 事务定义类里面（TransactionDefinition 接口的子类），以备将来使用。
			2.当运行时会让Spring 拦截注解标注的某个方法或者类的所有方法，将自己编写的代码编织到aop流程中
			3.首先Spring通过事务管理器(PlatformTransactionManager子类)创建事务
			4.启动开发者提供的业务代码，Spring会通过反射调度开发者的业务代码，根据结果正常或者异常，决定commit或者回滚

		传播属性NESTED，启用了保存点技术，
		但是由于保存点技术并不是每个数据库都能支持的，所以当你 传播行为设置为NESTED时,sring 会先去探测当前数据库是否能够支持保存点技术。
		如果数据库不予支持，它就会和 REQUIRES_NEW 样创建新事务去运行代码，以达到内部方法发生异常时并不回液当前事务的目的

		@Transactional失效
			对于静态static方法 和 public 方法 注解＠Transactional 是失效
			自调用也会失效，因为并没有使用spring容器提供的代理对象
			
		典型的事务问题优化，@Transactional
			1.在一个controller中，两次调用serviceA.do(),注意这里两次调用是两个不同的事务。
			2.过长时间占用事务，尽量不要把io等长时间操作放在serviceA的事务操作中，因为serviceA方法没执行完，数据库资源就不会释放。应该移到上层controller中操作。
				这么一说在controller中只要调用的serviceA中do方法执行完，数据库资源就会释放。
			
			重测
			3. 上层有事务，try{serviceA.do();serviceB.do()}，如果b异常(先update后异常)这时候，最终a会提交事务，并没有一致。个人认为service中A和B是没有默认事务的，不然会报错(已经验证)。
			   外层没事务，A提交，B回滚；
			   外层有事务，A,B没事务，A,B都提交了
		
	SpringMVC	
		SpringMVC基于Servlet 的技术，提供了核心控制器 DispatcherServlet 和相关组件，
		请求到来时， DispatcherServlet 首先通过HandlerMapping找到对应的处理器(Handler), 运行执行链(处理器和拦截器组成),处理器的适配器(HandlerAdapter),提供对应的环境，
		在处理器返回模型和视图给 DispacherServlet后，DispacherServlet 就会把对应的视图信息传递给视图解析器（ViewResolver ）
		
		Spring MVC会在启动期 就通过＠RequestMapping 的注解解析和处理器的对应关系 在运行的时候通过请求找到对应HandlerMapping然后HandlerExecutionChain对象，它是一个执行的责任链对象
		
		当 Spring MVC 启动的时候就会去解析 MyController 注解，然后生成对应URI和请求的映射关系，并注册对应的方法。
		当请求来到的时候，首先根据 URI 找到对应的 HandlerMapping ，然后组织一个执行链，通过请求类型找到 RequestMappingHandlerAdapter ，它的实例是
		DispatcherServlet 初始化的时候进行创建的。然后通过它去执行 HandlerExecutionChain 的内容，最终在 MyController 方法中将 index 图返回 DispatcherServlet 。
		DispacherServlet 就会把对应的视图信息传递给视图解析器（ViewResolver ），上层渲染展示。
		
		xml中  contextConfigLocation的配置会告诉 SpringMVC spring IoC 的配文件在哪里，这样 Spring 就会找到这些配置文件去加载它们。
		
		Java Web 容器为其生命周期中提供 ServletContextListener 接口，这个接口可以在 Web容器初始化和结束期中执行一定的逻辑 
		换句话说，通过实现它可以使得在DispatcherServlet 初始化前就可以完成 Spring IoC 容器的初始化，
		也可以在结束期完成对Spring IoC 容器的销毁，只要实现 ServletContextListener 接口方法就可以了。 SpringMVC交给了类 ContextLoaderListener，大部分都用这个初始化。
		
		Spring MVC 需要初始 Ioc 容器和DispatcherServlet 请求两个上下文,其中 DispatcherServlet 请求上下文是SpringIoC 上下文扩展,这样就能使得 Spring 各个 Bean 能够形成依赖注入。
		
		mvc中的ModelMap实际上继承LinkedHashMap<String Object>
		
		Web 容器刚开始的时候对其初始化，因为在整个 Web的初始化中，不只是DispatcherServlet 需要使用到 Spring IoC 资源 他的组件可能也需要。在最开始就初始化可以让 Web 中的各个组件共享资源。

		当启动Spring MVC 时候 Spring MV 就会去解析＠Controller 中的
		RequestMapping的配置再结合所配置的拦截器，这样它就会组成多个拦截器和一个控制器的形式 存放到HandlerMapping中去。当请求来到服务器，首先是通过请求信息找到对应的HandlerMapping，进而可以找到对应的拦截器和处理器，这样就能够运行对的控制器和拦截器。
		
		ajax中
			contentType: 告诉服务器，我要发什么类型的数据
			dataType：告诉服务器，我要想什么类型的数据
			
		1.实际mvc接收中，直接(String name)也能接收，没有@requestParam(可以自定义别名)也可以。SpringMV 目前也比较智能化，如果传递过来参数名称和 HTTP 的保存，那么无须任何注解也可 获取参数。这样方式允许参数为空。
		2.使用一个POJO来接收这些参数，显然这个POJO属性和HTTP请求参数名一一对应了。在没有任何注解的情况下SpringMVC也能映射POJO。
			即使没有任何注解 们也能够有效传递参数，但是有时候前端的参数命和后台的不一致，比如前端传参数命名为role_name，这个时候就要进行转换(一般不用转换)
		3.处理url参数类型，@PathVariable
		4.传json类型，这里传的data 是一个data对象，其中pageParams属性也是一个对象。传参的data和接收的pojo保持一致即可
				var data = { 
					roleName :’role ’, 
					note :’note ’ , 
					pageParams: { 
						start: 1 , 
						limit: 20 
					}
				}
				//此处需要告知传递参数类型为json,不能缺少
				contentType：” application/json ”，
				//将JSON 转化为字符串传递     	data:{"dataParam":JSON.stringify(data)},   这种就接收的字符串自己转,这种和放url中拼接一样的接收方式，这种类似表单form提交格式
				data: JSON.stringify(data),    //这种是正规点，传递的是一个json串，而不是json对象。传list<T>类型，后端就@RequestBody List<T>接收
				后端直接json接收方式，，1.@RequestBody方式接收会快点，2.也可以传一个字符串，然后后端自己转成对象
		5.表单提交，serialize()，后端直接类接收也行。
			通过表单序列化也可以将表单数据转换为字符串传递给后台，因为一些隐藏表单需要一定的计算，所以我们也需要在用户点击提交按钮后，通过序列化去提交表单。
			将数据以 roleName=xxx&namee=xxx 传递
		重定向
			内部可以将参数传递给重定向地址
			Spring MVC有个约定， 当返回的字符串带有redirect时候，它就会认为需要的是一个重定向。
			public String addRole(Model model) (){					//ModelMap类似
				model.addAttribute(”id”, role.getid()) ;
				return ”redirect:./do.htm”；
			}
	
			public ModelAndView addRole2 (ModelAndView mv ){
				mv.addObject (” note ”, note);
				mv.setViewName(”redirect:./do.htm”);
				return mv;
			}
	
		拦截器顺序
			preHandle1
			preHandle2
			preHandle3

			postHandle3
			postHandle2 
			postHandlel 
			
			afterCompletion3
			afterCompletion2 
			afterCompletionl
	
		Spring会先从第一个拦截器开始进入前置方法(前置方法是按配置顺序运行的)，然后运行处理器的代码，最后运行后置方法。注意，后置方法和完成方法则是按照配置逆序运行的，这和责任链模式的运行顺序是 致的，
	
		注意，当其中的一个preHandle方法返回为 false 后，按配置顺序，后面的 preHandle方法都不会运行了，而控制器和所有的后置方法postHandle也不会再运行。执行过 preHandle 
		方法且该方法返回为true的拦截器的完成方法afterCompletion会按照配置的逆序运行
		比如拦截器2中的preHandle 中返回false，那么
			preHandlel 
			preHandle2 
			afterCompletionl
			
		SpringMVC 文件上传是通过MultipartResolver (Multipart解析器）处理的。
		对于 MultipartResolver 而言它只是个接口，它有两个实现类CommonsMultipartResolver 和 StandardServletMultipartResolver。前者依赖第三方包，一般用后者，可以xml配置，也可以java配置。
		controller中可以选择request间接接收，也可以选择MultipartFile直接接收
		MultipartFile是Spring MVC提供的类，而Part是ServletAPI 提供的类。
	
		表单中要设置为enctype=”multipart/forrn-data”
	
148.消息队列相关
		消息队列	
			异步处理，应用解耦，流量削锋和消息通讯四个场景
			
			异步的事务――回调机制
			生产者在发送消息的时候，注册一个回调函数，这样生产者便不用停下来等待确认了，而是可以一直持续发送消
			息，当消息到达消息队列服务器的时候，服务器便会调用生产者注册的回调函数，告知生产者消息发送成功了还是
			失败了，进而做进一步的处理，从而提高了并发量
			
			消息的幂等处理
			由于网络原因，生产者可能会重复发送消息，因此消费者方必须做消息的幂等处理，常用的解决方案有：
			1. 查询操作：查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select是天然的幂等操作；
			2. 删除操作：删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个) ； 
			3. 唯一索引，防止新增脏数据。比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可）；
			4. token机制，防止页面重复提交。业务要求： 页面的数据只能被点击提交一次；发生原因： 由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交；解决办法： 集群环境采用token加redis(redis单线程的，处理需要排队)；单JVM环境：采用token加redis或token加jvm内存。处理流程：1. 数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间；2. 提交后后台校验token，同时删除token，生成新的token返回。token特点：要申请，一次有效性，可以限流。注意：redis要用删除操作来判断token，删除成功代表token校验通过，如果用select+delete来校验token，存在并发问题，不建议使用；
			5. 悲观锁――获取数据的时候加锁获取。select * from table_xxx where id='xxx' for update; 注意：id字段一定是主键或者唯一索引，不然是锁表，会死人的悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用；
			6. 乐观锁――乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。乐观锁的实现方式多种多样可以通过version或者其他状态条件：1. 通过版本号实现update table_xxx setname=#name#,version=version+1 where version=#version#如下图(来自网上)；2. 通过条件限制 updatetable_xxx set avai_amount=avai_amount-#subAmount# where avai_amount-#subAmount# >= 0要求：quality-#subQuality# >= ，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高；
			7.分布式锁――还是拿插入数据的例子，如果是分布是系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多多个系统，也就是分布式系统中得解决思路。要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供)；
			8.select + insert――并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了。注意：核心高并发流程不要用这种方法；
			
149.网络传输，网络协议
		TCP/IP 由四个层次组成：网络接口层、网络层、传输层、应用层
		三次握手，四次断开
			TCP 建立连接要进行三次握手，而断开连接要进行四次。这是由于 TCP 的半关闭造成的。因为 TCP 连接是全双工的(即数据可在两个方向上同时传递)所以进行关闭时每个方向上都要单独进行关闭。这个单方向的关闭就叫半关闭。当一方完成它的数据发送任务，就发送一个 FIN 来向另一方通告将要终止这个方向的连接。
			
		HTTPS
		HTTPS是以安全为目标的HTTP通道，是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL。其所用的端口号是 443。 
		简单过程是，建立通信后，服务端发送证书给客户端，客户端用公钥加密传输，服务端私钥解密。
		建立连接获取证书
		1） SSL 客户端通过 TCP 和服务器建立连接之后（443 端口），并且在一般的 tcp 连接协商（握手）过程中请求证书。即客户端发出一个消息给服务器，这个消息里面包含了自己可实现的算法列表和其它一些需要的消息，SSL 的服务器端会回应一个数据包，这里面确定了这次通信所需要的算法，然后服务器向客户端返回证书。（证书里面包含了服务器信息：域名。申请证书的公司，公共秘钥）。
		证书验证
		2） Client 在收到服务器返回的证书后，判断签发这个证书的公共签发机构，并使用这个机构的公共秘钥确认签名是否有效，客户端还会确保证书中列出的域名就是它正在连接的域名。数据加密和传输
		3） 如果确认证书有效，那么生成对称秘钥并使用服务器的公共秘钥进行加密。然后发送给服务器，服务器使用它的私钥对它进行解密，这样两台计算机可以开始进行对称加密进行通信。
	