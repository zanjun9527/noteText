#一级标题
##二级标题
* 无需列表
1. 有序列表
*** 
*** 
*** 

学习小结：
1. 学习新东西，最直接标准的就是官方的API文档。
2. 不要绕业务逻辑，抓重点
3. 背景色：R:204, G:232, B:207
	  红199  绿237  蓝204 豆沙绿
4. 一些设计思路
	思维导图，查看体系知识更清晰

   设计的时候有三种架构图特别的实用，用例图，类图和模块图
	
	新技术:官方的文档，官网的demo。后面再横向对比，看源码(从依赖性最小的模块看起)。
	不是全面的掌握，每个东西都有关键点，抓住关键点是核心，往原理上问，抓重点记忆
	不仅仅是细节化的落实了，要整体的设计，主要核心点，以前是从点研究，后面要从面入点。请求数量 性能指标，aqs等
	系统化 由面入线重点内容再到点

5. 开发经验
	1 Don’t Repeat Yourself ：这是软件开发的基础 ，即不要做重复性劳动，代码重复在软件开发中都是不合理的存在，利用各种手段消除这些重复是软件开发的核心工作准则
	2 Keep it simple stupid 在做软件设计的工作中，很多时候都不要想得过于复杂，也不要过度设计和过早优 ，用最简单且行之有效的方案 就避免了复杂方案带来的各种额外成本 利于后续维护和扩展
	3 You Ain’t Gonna Need It ：即只需要将应用程序必需的功能包含进来，而不要试图添加任何其你认为可能需要的功能 因为在很多软件中，往往请求都花费在核心的功能上
	4 Done is better than perfect ：在面对开发任务时，最佳的思路就是先把东西做出来，再去迭代优化 如果 一开始就面面俱到,考虑到各种细节，那么很容易钻牛角尖而延误项目进度
	5 Choose the most suitable things : 这是在做方案选择 技术选型时候的很重要的原则 在面对许多技术方案、开源实现的时候，务必做到不能盲目求新，要选择最合适的而非被吹得天花乱坠的

## 后续要看的内容
   nacos
   cloud的视频教程
   用例图，类图，时序图
   

## 数据结构相关
   Collection
   ├List （有序，可重复，允许null）
   │├LinkedList （ 底层维护了一个Object数组，，特点： 查询速度慢，增删快）
   │├ArrayList （底层使用了链表数据结构，特点：查询速度快，增删慢）
   │└Vector（底层维护了一个Object的数组，线程安全的，效率低）
   │　
   └Set （无序，不可重复，最多有一个null元素）
   └HashSet	(底层使用哈希表，特点： 存取速度快.)
   └TreeSet   如果元素具备自然顺序的特性，那么就按照元素自然顺序的特性进行排序存储。也可以比较器排序

   Map （没有实现collection接口，key不能重复，value可以重复）
   ├Hashtable （实现Map接口，同步，不允许null作为key和value，用自定义的类当作key的话要复写hashCode和eques方法，）
   ├HashMap （实现Map接口，非同步，允许null作为key和value）
   ├LinkedHashMap    按添加顺序
   ├TreeMap          按自然排序


1. 基本介绍
	LinkedList
       是链表结构，增删块，查找慢
	HashSet 
      首先判断两个元素的哈希值，如果哈希值一样，接着会比较equals 方法 如果 equls 结果为 true ，HashSet 就视为同一个元素。如果 equals 为 false 就不是同一个元素
	hashmap 
         在jdk7中 	数组+链表
         在jdk8中 	数组+链表+红黑树 组成	当链表中的元素超过了8个以后，会将链表转换为红黑树
	ConcurrentHashMap 
      是一个 Segment 数组，Segment 通过继承ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证个 Segment 是线程安全的，也就实现了全局的线程安全。
      默认16个Segments
	队列
      是一种特殊的线性表，特殊之处在于它只允许在表的前端（front）进行删除操作，而在表的后端（rear）进行插入操作，和栈一样，队列是一种操作受限制的线性表。进行插入操作的端称为队尾，进行删除操作的端称为队头。
 
2. Map相关
      https://blog.csdn.net/vking_wang/article/details/14166593  map介绍

   1. HashMap 和 HashTable的区别
      * HashMap对象的key、value值均可为null(hashmap中get返回null，表示不存在该键或者键对应的value是null)。而HahTable对象的key、value值均不可为null
      * HashTable是线程安全，其方法有synchronized来同步；HashMap未经同步，是非线程安全的。hashMap效率上比hashTable要高

      ConcurrentHashMap 和 Hashtable 主要区别就是
         围绕着以及如何锁,可以简单理解成把一个大的 HashTable 分解成多个，形成了锁分离。
         如图:而 Hashtable 的实现方式是---锁整个 hash

         HashTable 的put或get操作都需要做synchronized同步处理，效率低。线程安全
		   concurrentHashMap相对使用的不是方法锁，允许多个并发操作（分布在不同的段上），效率高，内部使用了段segment，每个小段就是hashtable

   2. TreeMap和linkedhashmap的区别
         TreeMap默认实现的是对key进行自然排序 ，使用插入排序使用linkedhashmap，二者都可以用来实现加密时候的顺序固定

   3. HashMap的线程不安全主要体现在下面两个方面：
			1.在JDK1.7中，当并发执行扩容操作时会造成环形链和数据丢失的情况。				使用头插法
				在jdk1.7中，在多线程环境下，扩容时会造成环形链或数据丢失。一个线程已经完成，另一个线程中途挂起了，3-7  7-3。容易形成环链(了解就行)
			2.在JDK1.8中，在并发执行put操作时会发生数据覆盖的情况。							使用尾插法，因此不会出现环形链表的情况
				在jdk1.8中，在多线程环境下，会发生数据覆盖的情况。 
   
   4. **hashmap的原理**	
      底层结构:Entry数组 + 链表
      * 利用key的hashCode重新hash计算出当前对象的元素在数组中的下标，主要判断hashcode和key值，前者确定索引位置，后者确定Entry链位置
         存储时，如果出现hash值相同的key，此时有两种情况。
         (1)如果key相同，则覆盖原始值；
         (2)如果key不同（出现hash冲突），HashMap的单个bucket里存储就是一个 Entry 链。 并将当前的key-value放入链表中
      * 获取时，直接找到hash值对应的下标，在进一步判断key是否相同，从而找到对应值。
         系统只能必须按顺序遍历每个 Entry，直到找到想搜索的 Entry 为止。
      * hashmap的原理是基于hash运算计算存储位置，同一hash位置的多个entry根据key.equals（）取值   
		
      总结就是更具key的hash值确定tables中的存储位置，一般是单槽存放entry，hash冲突时，就是一个entry链，key相同覆盖，不同链表。取的时候遍历获取

      hashmap结构 和 头插法
         HashMap存放的对象是Entry对象，Entry有key,value,hash,next属性，
         entry链表结构如下:
            static class Node<K,V> implements Map.Entry<K,V> {
               final int hash;
               final K key;
               V value;
               Node<K,V> next;
            }

         java8中对HashMap进行了优化，如果链表中元素超过8个时，就将链表转化为红黑树，以减少查询的复杂度，将时间复杂度降低为O(logN)。
			Java 8 中使用 Node 模型来代表每个 HashMap 中的数据节点，都是 key，value，hash 和 next 这四个属性。Node 用于链表，红黑树用 TreeNode。

         在链表头部插入数据,系统总是将新的Entry对象添加到bucketIndex处。数组中每个元素存储的是一个链表的头结点，即最后插入的元素

         如果Entry数组的bucketIndex上已经有Entry存在，就把新的Entry放在bucketIndex位置，那新添加的Entry对象next将指向原Entry对象，形成一条Entry链（在链表头部插入数据）,
         如果bucketIndex位置上没有Entry存在，新插入的Entry的next指向null,不会产生Entry链
	
         示例：C-B-A依次存储，B.next = A,Entry[0] = B,如果又进来C,index也等于0,那么C.next = B,Entry[0] = C

      问题：头插法导致的环形问题
		关键：核心就是头插法导致的顺序反转
		场景：原顺序是原3-7
			在扩容时使用头插法可能出现死循环，两个线程同时put操作，发生扩容，且3,7hash碰撞
			
			1.线程a在读取了3的next后挂起，当前3 next指向7
			2.线程b线程执行扩容完后变成了7-3 
			3.线程a恢复执行，引用到了b更新后的7指向3，但本身之前状态是3-7，这样就形成闭环。当调用get()方法时可能陷入死循环。

   5. hashmap的扩容
	      底层是数组+链表的结构，数组的长度是固定的(默认大小是16)，当元素大于长度的时候就需要扩容，叫做resize。
	      HashMap()：构建一个初始容量为 16，负载因子为 0.75 的 HashMap。超过16*0.75=12，就扩容一倍，比较消耗性能，最好指定已知的初始容量，和合适的负载因子

         hashmap扩容，源码解析：（忽略）
            扩容的源码，总结就是采用头插入法，数组中存储的是entry链表的头节点
            ``` 
               void transfer(Entry[] newTable, boolean rehash) {
                  int newCapacity = newTable.length;									4
                  for (Entry<K,V> e : table) {
                     while(null != e) {
                        Entry<K,V> next = e.next;   获取7									
                        if (rehash) {e.hash = null == e.key ? 0 : hash(e.key);}
                        int i = indexFor(e.hash, newCapacity);
                        e.next = newTable[i];  重新设置3.next=null
                        newTable[i] = e;       新表位置设置3
                        e = next;              将而3变成7
                     }
                  }
               }
               第一行：记录odl hash表中e.next
               第二行：rehash计算出数组的位置(hash表中桶的位置)
               第三行：e要插入链表的头部， 所以要先将e.next指向new hash表中的第一个元素
               第四行：将e放入到new hash表的头部
               第五行： 转移e到下一个节点， 继续循环下去
            ```

   6. concurrentHashMap相关	

         ConcurrentMap继承map接口，实现类是ConcurrentHashMap，并发级别的map
         相对于一般的map结构
            线程问题在于 多个线程同时resize时候，但是有的执行完 有的正在resize，这样数量就会出现问题。所以要加锁控制
            
         segment结构和hashmap类似（数组+链表），一个segment里包含一个hashentry数组，每个hashentry是一个链表结构的元素

         ConcurrentHashMap 默认情况下采用将数据分为 16 个段进行存储，并且每个段各自拥有自己的锁，锁仅用于 put 和 remove 等改变集合对象的操作，基于 voliate 及 hashEntry 链表
         的不变性实现读取的不加锁
         ConcurrentHashMap 锁住的不是全部的hash表，而是以多个segment的形式锁住单独的区域， 相当于把之前的数组分成多个segment，


         concurrentHashMap使用了锁分段，避免竞争同一个资源阻塞。
            核心思想就是将资源分段存储，尽量只锁住需要操作的散列表段。
               jdk1.7版本使用
               结构：segment[]+hashEntry[]   数组结构  *****这是jdk7版本使用，在jdk8版本基本不使用了，只是做了相应的兼容****
               
               Segment数组的意义就是将一个大的table分割成多个小的table来进行加锁，也就是上面的提到的锁分离技术，
               而每一个Segment元素存储的是HashEntry数组+链表，这个和HashMap的数据存储结构一样。本身segment继承ReentrantLock，会在put操作时加锁保证线程安全。每个HashEntry是一个链表结构的元素
               ConcurrentHashMap 基于concurrencyLevel 划分出了多个 segment 来对 key-value 进行存储,每次操作都只对当前segment进行锁定，从而避免每次put操作锁住整个 map。

               通过两次Hash定位到元素位置，第一次是定位segment，第二次是定位hashEntry。
               
               jdk1.7版本	使用了segment中继承的ReentrantLock实现锁控制,头插法会出现的环形链表问题
               jdk1.8版本	使用synchronized+cas实现锁控制,当链表长度太长（默认超过8）时，链表就转换为红黑树
                           当数组大小已经超过64并且链表中的元素个数超过默认设定（8个）时，将链表转化为红黑树。
                  
               锁分段技术(忽略)
                  首先将数据分成一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数 据也能被其他线程访问
                  一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时， 必须首先获得与它对应的Segment锁，
                  输入参数initialCapacity是ConcurrentHashMap的初始化容量(默认16)，loadfactor是每个segment的负载因子(默认0.75)
               
               Segment的优缺点(忽略)
                  段Segment继承了重入锁ReentrantLock，有了锁的功能，每个锁控制的是一段，当每个Segment越来越大时，锁的粒度就变得有些大了。
                  分段锁的优势在于保证在操作不同段 map 的时候可以并发执行，操作同段 map 的时候，进行锁的竞争和等待。这相对于直接对整个map同步synchronized是有优势的。
                  缺点在于分成很多段时会比较浪费内存空间(不连续，碎片化); 操作map时竞争同一个分段锁的概率非常小时，分段锁反而会造成更新等操作的长时间等待; 当某个段很大时，分段锁的性能会下降。
               ***

               JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap。
               虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本。
               
               JDK1.8为什么不用ReentrantLock而用synchronized ?(忽略)
                  减少内存开销:如果使用ReentrantLock则需要节点继承AQS来获得同步支持，增加内存开销，而1.8中只有头节点需要进行同步。
                  内部优化:synchronized则是JVM直接支持的，JVM能够在运行时作出相应的优化措施：锁粗化、锁消除、锁自旋等等。
                  JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点）

                  在大量的数据操作下，对于JVM的内存压力，基于API的ReentrantLock会开销更多的内存
                  因为粒度降低了，在相对而言的低粒度加锁方式，synchronized并不比ReentrantLock差，在粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了
               ***
						
3. List相关
   
   1. list的常用操作
      * subList
         subList(int fromIndex,int toIndex)        截取部分List,方法左闭右开
         返回一个镜像而不是新实例,得保证原来的list不能更改。(使用时，若更改了原来的list,sublist的任何操作都会报错)
      
         如果截取后对原List没有其他操作，可以直接用。如果有的话，要注意
         情况1：修改截取后的subList，导致原list也受到影响，包括增删
         情况2：修改原list，导致sublist的所有操作都会报错，ConcurrentModificationException

      * 使用迭代器 list 的remove操作
         集合在遍历时候，不能修改本身否则或发生并发修改异常,类似list要使用迭代器才能修改list内容
         使用list操作会有索引左移和并发修改问题，建议使用迭代器删除
         Iterator<Integer> it=list.iterator();
         while(it.hasNext()){
               if(it.next()==3){it.remove();}
            }
         System.out.println(list);
         iterator.next() 这里的每次next就会迭到下一个元素，最好用变量接收iterator.next()
   
      * 数组转list
         Arrays.asList() 	
         List<String> strings1 = Arrays.asList("1", "2", "3"); 快速生成一个list

      * list转逗号分割 	
         StringUtils.join(orderPayStrs.toArray(), ",");

   2. ArrayList 和linkedlist区别
         * ArrayList是实现了基于动态数组的数据结构，LinkedList是基于链表结构。
         * 查询的话ArrayList速度快(因为LinkedList要移动指针)，增删的话，LinkedList速度快(因为ArrayList要移动数据)。

   3. 集合排序
      Collections.sort 排序的使用    类的内部实现compare接口，或者外部使用比较器
      Collections.sort(keys);//基本类型的直接排就行

      Arrays.sort(aa[],Collections.reverseOrder());数组倒排序        参数2本质是返回一个Comparator

      Collections.sort(students, new Comparator<Student>(){
         public int compare(Student student1, Student student2){
            return student2.getScore() - student1.getScore();
         }
      });

   4. map和list 对应的是栈中存储的地址，final表示地址不能修改，但是地址对应的内存区域的值是可以修改的；

      list或者数组中存入的是引用地址，存入后地址就不变了，改变数组或者list中的对象的引用，不改变数组的值
		后续person1=person2;是将两个对象都指向了person2的地址

4. Set相关
   1. 当原对象.equals（新对象）等于true时，两者的hashcode却是不一样的，将会存储了两个值一样的对象，导致混淆，因此，就也需要重写hashcode()

      使用treeset去重，复杂对象重写compareto方法实现排序
      TreeSet的底层是用TreeMap来组织数据的
      hashset中的必须满足hashcode和equals方法一样，才算重复,类似hashmap

5. 其他杂内容
   1.  双循环的终止问题，continue 和break都只影响本身的循环，不会影响上层循环体
       非null的，size为0的list，遍历直接跳过

   2. 数据和集合的区别
         数组长度固定，集合长度不固定
         数组必须声明元素类型，而集合可以不声明。

   3. 集合的初始化
      * list初始化
         List<String> names = new ArrayList<String>() {{
            add("Tom");
            add("Sally");
         }};

         List<String> colors = Stream.of("blue", "red", "yellow").collect(toList());

      * map的数据初始换
         ```
            //在类中
            public static final Map<String,String> map=new HashMap<String, String>();
            static{
               map.put("1", "a");
               map.put("2", "b");
            }
            
            //初始化map	
            public final static LinkedHashMap<Integer, String> REQUIRE_STATUS = new LinkedHashMap<Integer, String>() {
                  {
                     put(ZRequireConstantsDB.REQ_STA_FOR_PUBLISH, "待托管");
                     put(ZRequireConstantsDB.REQ_STA_PUBLISHED, "已发布");
                  }
               };

            Map map = new HashMap() {
               { 
                  put("Name", "Unmi"); 
                  put("QQ", "1125535"); 
               }
            };  

         ```
      双大括号初始化（double brace initialization）或者匿名内部类初始化法，实际上是一种取巧的方式。

   4. //JSONArray 和list类似,强转为map
         JSONArray objArray = JSON.parseArray(data);
         for (Object order : objArray) {
            Map map = (Map) order;
            String orderId = String.valueOf(map.get("orderId"));
         }	

   
## 接口和抽象类
类是对事物的抽象(抽象了属性和行为)，抽象类是对类的抽象，接口是对抽象类的抽象

1. 区别：
   * 成员变量：接口是静常，抽象类不限
   * 构造方法:接口无，抽象类有
   * 成员方法：接口是抽象的，全实现，抽象类不是，可抽可不抽
   * 接口多继承，抽象类单继承

2. 抽象类不能被实例化，但可以创建子类对象或匿名内部类  和接口类似
   子类继承抽象类必须实现其抽象方法（普通方法可选），除非子类也为抽象类

3. 接口中的"变量"默认都是 "public static final"类型, 即为常量, 不要使用"常量接口模式", 此模式会导致类中的常量混乱
	public static final int i=10;  或 int i=10；（可以省略掉一部分）

4. class a 实现 接口b(继承接口c)，那么a需要实现所有方法(包括c中的)，但是若a继承了一个实现c接口的类，就不用都实现(只认implements)
	接口中方法注释可以上层显示，实现类中也能看到。优先接口中添加说明备注。

5. 子类继承父类要重写父类的抽象方法吗
	* 1. 普通类继承，并非一定要重写父类方法。
	* 2. 抽象类继承，如果子类也是一个抽象类，并不要求一定重写父类方法。如果子类不是抽象类，则要求子类一定要实现父类中的抽象方法。
	* 3. 接口类继承。如果是一个子接口，可以扩展父接口的方法；如果是一个子抽象类，可以部分或全部实现父接口的方法；如果子类不是抽象类，	则要求子类一定要实现父接口中定义的所有方法。

6. 抽象类实现接口可以自选实现的方法，当子类再继承抽象类时，子类重写的方法即为抽象类中未重写接口中的方法。
   普通类继承抽象类必须实现父类中的抽象方法，普通类实现接口需要实现接口中的所有方法
   只要一个类是抽象的或是一个接口，那么其子类中的方法都可以使用匿名内部类来实现


## 内部类、静态static、匿名内部类
   
1. 内部类的两种方式创建方式，分静态内部类和非静态
   静态内部类不依赖外部类实例的
2. 
3. static是共有的，可被修改的，final是不可修改的，应用类上不能被继承，虽类的加载而存在
   final static常量String不能被重新赋值，只能被替换，替换不改变原来的值，而是返回这个新值，不能放在主函数内 共享常量   方法里声明变量是私有的

4. 重载和多态的区别：重载：类的多个构造函数   多态：接口实现

5. 内部类的两种创建方式：
      1、outer.inner p=new outer().new inner（）；
      2、outer.inner p=new outer.inner();    //静态内部类
      3、Outter outter = new Outter();
         Outter.Inner inner = outter.new Inner();  //必须通过Outter对象来创建
      
      局部内部类访问局部变量必须用final修饰，因为局部变量是栈中随方法的，调用玩就消失，而堆中的不会立刻消失


      静态内部类可以直接创建对象new B.C();
      如果内部类不是静态的，那就得这样
      B b = new B();
      B.C c = b.new C();


8. 匿名内部类
      本质是一个继承抽象类的子类或实现该接口的实现类 匿名对象
      匿名内部类就是没有名字的局部内部类，不使用关键字class, extends, implements, 没有构造方法。

      a·匿名内部类不能有构造方法。
      b·匿名内部类不能定义任何静态成员、方法和类。
      *** e·一个匿名内部类一定是在new的后面，用其隐含实现一个接口或实现一个类。***

      匿名子类还有匿名内部类的叫法
            new Thread(){
               @Override
               public void run() {
                  System.out.println("zenmeshuolaodi");
               }
         }.start();
            
         使用匿名子类，重写了Thread的run()方法，与单独写一个继承于Thread的类在功能上是一致的。
            
         new EStest() {
            public void gott() {                      //这里如果是 @Override必须要覆盖父类方法，如果不加，就是子类自定义新增方法
               System.out.println("222");
            }
         }.got();	


      对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；
            如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。
            在匿名内部类中只能使用final变量，
            使用匿名内部类访问外部方法的局部变量必须是final类型，但是访问成员变量不用

      匿名内部类的使用中 	基于面向对象的思想，外部final变量作为参数传入进行对象构造，在调用对象的方法
      匿名内部类直接使用相关接口，定义实现方法，传入final变量，一次性使用。将final变量作为参数传入匿名内部类，构建成功
      ​	
      ```
         final Person person = new Person();
            person.setName("法海");
            person.setAge("101");
            new ISpiTestService() {
               @Override
               public void say() {
                  person.setName("ad");
                  Object json = JSONObject.toJSON(person);时
                  System.out.println(json.toString());
               }
            }.say();
      ```
      ​

## 异常相关

   Throwable是java.lang包中一个专门用来处理异常的类。它有两个子类，分别用来处理两组异常。	
      * Error（运行环境方面的异常） 
      * Exception（显示-编译/隐式-运行异常），

   try catch多个异常时：顺序匹配，匹配到后面的就跳过（原则是异常范围小的放在前面，范围大的放在后面）
   Exception这个异常的根类一定要刚在最后一个catch里面，否则任何异常都会和Exception匹配的，就会报已捕获到...异常的错误。





## 搜索引擎相关，es相关 Elasticsearch相关，solr相关

lucence的相关
			倒排索引：	搜索引擎中使用的索引是倒排索引，将文档中的词作为关键字，建立词与文档的映射关系。搜索引擎中使用的索引是倒排索引，将文档中的词作为关键字，建立词与文档的映射关系。
			分词：		英文比较简单，单词划分(空格，符号，段落等)，中文需要合适的中分文词工具，建立倒排索引
			停止词：	类似英文的a the and ,中文的一些语气词，了，这等没有意义，需要忽略
			排序：		不细看，可以针对具体的field,指定排序规则
			文档：		类似数据库，一行记录包含的字段对应文档的域，这条记录就是一个文档	
			查询query:	可能是条件组合查询，termQuery ,也可能是前缀查询等
			高亮：		就是对关键字前后拼接html的颜色代码，高亮的是输入的搜索值(注意这里是搜索值的分词之后的关键字)
		
		搜索引擎
			细节原理暂不看
			正向索引，			文档1”的ID > 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；…………。
			倒排索引，			“关键词1”：“文档1”的ID，“文档2”的ID，…………。	
		
			搜索引擎会将正向索引重新构建为倒排索引，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词
			lucene:全文搜索工具包，依赖于java，不适用于集群环境
			ES：全文搜索服务器，基于lucene，采用Rest HTTP调用方式，对集群支持较好	


	solr	
	 CommitWithin
            简单的说就是告诉solr在多少毫秒内提交，比如如果我指定<add commitWithin=10000>，将会高速solr在10s内提交我的document。用法
            1.可以在add方法设置参数，比如 server.add(mySolrInputDocument, 10000);
            solrServer.add(doc, Constants.CREATE_INDEX_MS);		

   使用boost打分，值越高，相关度得分就越高


### solr

1. solr bf 整体打分  qf单个字段打分，输出  schema.xml配置各种field 	查询结果映射问题 ，结果集反射可获取值  也可以注解@Field("title")映射

2. 索引的与或过滤

   ```
   String params = "(title:笔记 OR content:笔记) AND catalog_id:2";
   SolrQuery query = new SolrQuery();
   query.setQuery(params);
   ```

3. solradmin中 sort= id desc实现倒序排序
   id:[533 TO *] 查询533之后的所有

   solr搜索的schema.xml中的id（主键，不可重命名），默认是doc的主键
   solr中的doc=3581 就是需求id 也就是solr设置的主键

4. 权重排序，打分排序
   	query.setParam("bf","addSerJJ^0.8 require_goal^0.6 recip(rord(deal_money),1.0,10000.0,10000.0)^0.1");
   	bf="ord(popularity)^0.5 recip(rord(price),1,1000,1000)^0.3，一般也不这样用，简单的排序规则就可以了

   ​	fl=*,score查询权重得分

   edismax	
   		query.setQuery(keyField + ":" + xyeSolrParam.getQueryStr());
   		query.set("qf", keyField + "^2");

5. solr工程可以使用单核和多核的配置，
   默认情况下，solr的日志是瞬时的，tomcat重启后，就消失了。引入相应的日志jar后，在solr-4.5.1.war的WEB-INF下建classes目录，在该目录创建文件log4j.properties，即可生成日志文件。
   一般我们是在使用solrj的系统中增加日志捕获异常并输出日志，不在solr工程增加。

6. solr异常解决

   * SolrException: Error loading class 'solr.VelocityResponseWriter'
     vi /usr/local/tomcat/solr/collection1/conf/solrconfig.xml
     <queryResponseWriter name="velocity" class="solr.VelocityResponseWriter" enable="${solr.velocity.enabled:true}"/>注释或者disabled - enable:false即可
     重启Tomcat

   * org.apache.solr.common.SolrException:org.apache.solr.common.SolrException: Error opening new searcher

     清空索引 	
     #cd /usr/local/tomcat/solr/collection1/data/index
     #rm -rf *

7. 优化查询效率		
   ```
   query.addFilterQuery("status:0 AND biz_type:1 AND class_id:1 AND xxx:123");  
   query.setQuery("xxx:123");  
   ```


### elasticsearch


## 缓存相关

缓存命中：可以直接通过缓存获取到需要的数据。
缓存适合“读多写少”的业务场景

缓存的选择  ehcache   redis  memcached
		memcached:	服务器端是c编写的，客户端多语言实现，相对下面，效率低
		Ehcache:	纯java编写的,相对上面效率高，
		综合比较，两者会选ehcache。redis另算
   
      ehcache是内部缓存，速度快，跟随java程序存在，项目重启缓存消失，但是redis是独立的缓存，可以持久化
   	
1. 如果你在网站上访问同一张图片，该图片可以从浏览器缓存中调出并几乎立即显现出来。
   首次的request请求到服务器，会生成一个session，返回sessionid到客户端，之后每次请求都会带sessionid到服务端获取相应的session

2. cookie：和 session
         读取-	Cookie[] cookies = request.getCookies(); 然后遍历看cookie.getName()有匹配cookiename参数的，有则存在该cookie，
   	   添加-	cookie添加的时候，需要设置Path默认根路径，Domain域名，MaxAge过期时间

   域名的分配 
      一级域名	taotao.com
      二级域名	www.taotao.com

   客户端的会话状态保存：url参数(常用来传递sessionId)，表单隐藏域(可以再传给server端)，cookie(基于域名传递的)，
	服务端的会话状态是存在内存中的，即session，对应的key是sessionid存放内存映射表中，

	cookie只工作在同一个域名的站点中，若一个站点包含了多个域名，cookie不会在之间传递
	不同domain域对应不同cookie，比如http和https,
	重定向和浏览器直接敲，会把相应的sessionid带到后台，这样是共用一个sessionid
	但是http后台请求每次都会产生一个新的sessionid，类似httpclient

	Cookie newCookie=new Cookie("SESSION",null); 
	newCookie.setMaxAge();//若正数，表示从现在开始，即将过期的seconds。默认值是-1，表示关闭浏览器，cookie就会消失。


   一般用来实现 Session 的方法有两种：
      * URL 重写。
            Web Server 在返回 Response 的时候，检查页面中所有的 URL，包括所有的连接，和
            HTML Form 的 Action 属性，在这些 URL 后面加上“;jsessionid=XXX”。
            下一次，用户访问这个页面中的 URL。jsessionid 就会传回到 Web Server。
      * Cookie
            forward:一般用于用户登陆的时候,根据角色转发到相应的模块.
            redirect:一般用于用户注销登陆时返回主页面和跳转到其它的网站等

   ​	通过读取请求报文头中 Cookie 属性的JSESSIONID 的值，在服务端的一个会话 Map 中，根据这个 JSESSIONID 获取对应的HttpSession 的对象。
   ​	页面请求就会页面，服务端请求就回服务端，浏览器请求页面，页面中js再加载回调的地址
   ​	通联的支付页面，页面倒计时，reload我们的路径地址，服务器的redirect应该也是可以的

      session.removeAttribute()适用于清空指定的属性   
      session.invalidate()是清除当前session的所有相关信息

      1.第一次访问服务器，浏览器会带回一个sessionid，并set-cookie操作：Set-Cookie: SESSION=7004b0d6-dfb0-4756-898e-60379fb8b884; Domain=xiaoyuer.com; Path=/; HttpOnly
      2.当再次访问会将sessionid带入服务器，Cookie: SESSION=7004b0d6-dfb0-4756-898e-60379fb8b884; UM_distinctid=170a97fef53902-05e6115cbcea8b-4446062d-1fa400-170a97fef54e3; CNZZDATA1255347938=2105466369-1583388160-%7C1583388160






3. session共享
   	代码级别的session共享
   	1、走拦截类，但此时request.getsession是不同步的，只能从redis中取出相应的值，存在则已经登录，但是这是redis使用，表面的登录，此时两个服务器的session是不同步的

   	tomcat级别的session共享
   	2、tomcat级别的session共享，相当于使用redis实现两个服务器的缓存共享

4. 缓存同步
   		request session redirect
      		request.getSession().setAttribute(uuid, context);
      		request.getSession().setAttribute("DDDD", "333");


         int interval = request.getSession().getMaxInactiveInterval(); 		查看最大过期时间
         request.getSession().invalidate();     清除session

   RedisSessionManager 中的缓存同步的效果，从request中的同步session到redis中
   一次request中的完成后，才会提交session的保存操作，如果session同步中，有一个没有序列化，那么这次的保存操作失效,序列化和反序列化的包名一致问题
   做session同步的时候，tomcat中的缓存是共享在redis中的，key就是sessionid，未序列化，影响到存入redis的数据，但是不影响本台tomcat的缓存存入。
   多个tomcat缓存同步，1.redis,2.cookie 3.tomcat-redis-session-manager

5. ehcache 和 redis的比较（忽略）
      ehcache(单体) 
         提供了一些缓存方案的框架，底层基于jvm内部缓存开发的。
         直接在jvm虚拟机中缓存，速度快，效率高；但是缓存共享麻烦，集群分布式应用不方便。是可以做集群缓存共享的，但是做服务话不适用，这个是跟着java内存走的
         适用于单个应用或者对缓存访问要求很高的应用

         补充下（忽略）：也可通过RMI或者Jgroup多播方式进行广播缓存通知更新，缓存共享复杂，维护不方便；简单的共享可以，但是涉及到缓存恢复，大数据缓存，则不合适。
            
      redis(分布式)
         是通过socket访问到缓存服务，效率比ecache低，比数据库要快很多，处理集群和分布式缓存方便，有成熟的方案。
         适用于大型系统，存在缓存共享、分布式部署、缓存内容很大的

      如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点：
            1 、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。
            2 、Redis支持数据的备份，即master-slave模式的数据备份。
            3 、Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。


6. 缓存共享，session共享
	1、最原始的是：直接通过tomcat自带的复制功能，即访问其中一台tomcat服务器就会在其他配置好的tomcat服务器上各复制一份session，sessionid一样,存在一定的延迟，十分低效
	2、使用tomcat-redis-session-manager,使用redis同步各tomcat之间的缓存（依赖tomcat，需要修改context.xml文件），但是目前只支持到jdk1.7
	3、更新使用spring session，存储在第三方存储容器redis的spring session，是配置在项目和代码中的
   
   基于IP机制的负载均衡，各自维护自己的session(忽略)
   通过cookie传递或者token机制

   tomcat简单session缓存共享(现在已经不用，忽略)
      1.Tomcat-redis-session-manager
      2.配置tomcat配置文件context.xml
      3.3、添加Tomcat-redis-session-manager的jar包到tomcat/lib目录下，需要的jar包如下：
            commons-pool2-2.2.jar
   	   jedis-2.5.2.jar
	      tomcat-redis-session-manage-tomcat7.jar

7. SpringSession相关
      分布式session的实现思路
		重新实现httpsession的操作接口。操作必须要在进入应用之前完成，可以配一个filter拦截。
		
		配置sessionfilter，请求到达mvc之前封装request和response，并创建自己的innerhttpsession，并设置到request和response中，这样request.getHttpSession就是自己创建的session对象。
		用户请求完成后，会将innerhttpsession的所有内容更新到分布式缓存中，这样用户可跨服务器再次访问。

		解决跨域名共享cookie的问题。
			这样要实现sessioin同步，需要一个跳转引用，该应用可以被多域名访问，功能是，从一个域名下取得sessionid，然后同步到另一个域名下。其实就是一个cookie中的sessionid。
			实现两个域名下的session同步，必须要将统一个sessionId作为cookie写到两个域名下。
		盗用cookie的情况
			增加一个私密加密信息signinfo
			除非用户只盗用的sessionid   要是连singinfo一起盗用，那也没辙
		原理
			在请求request上通过DelegatingFilterProxy代理过滤器封装了一层，将原来存储在容器缓存的session变成存储在redis的session，
			所以在web.xml中的此springSessionRepositoryFilter必须得是在所有filter的前面。用了springsession后，web.xml中设置session过期时间是无效。因为已存储在redis的session。
   
      1. springsession和boot的版本
         Redis的支持，替换掉底层Jedis的依赖，取而代之换成了Lettuce(生菜)
         spring2.0 整合spring session  先检查boot自带的相关依赖，然后移除spring-session的旧版本jar的依赖：
         spring5（springboot2.x）只支持spring-session-core了。
         最高版本的spring-session:1.3.5已经不再更新，而且支持到spring-session-data-redis:1.8。
         官方文档上写了，spring-session-data-redis2.x后只能搭配spring-session-core2.x
    
      2. 使用redis替换应用容器（tomcat的session容器）
		 原先的tomcat管理内存，Cookie 中记录的 Key 值是 JSESSIONID，而替换成 RedisHttpSession 之后变成了 SESSION。
		 pring session 中 每次request请求都会刷新Session，重置原先的过期时间
    
      3. 设置cookie相关
      ```
         @Bean
         public HttpSessionIdResolver httpSessionIdResolver() {
               CookieHttpSessionIdResolver cookieHttpSessionIdResolver = new CookieHttpSessionIdResolver();
               DefaultCookieSerializer cookieSerializer = new DefaultCookieSerializer();

               //cookieSerializer 可以配置cookie相关，包括setCookieName("JSESSIONID") 还原sessionid的name.
      //	    cookieSerializer.setCookieName(redisProperty.getCookieName());//cookies名称 默认是session

               //跨站sessionid会有问题，写的时候进行base64编码，读的时候进行base64解码
               //这个要关闭，否则sessionid会在浏览器编码显示，导致跨手机 和pc的sessionid变化
               cookieSerializer.setUseBase64Encoding(false);
               cookieSerializer.setDomainName(domainName);
               cookieSerializer.setCookiePath("/");
               cookieHttpSessionIdResolver.setCookieSerializer(cookieSerializer);
               return cookieHttpSessionIdResolver;
         }
      ```
      4. 配置
         1.配置redis连接工厂(factory:poolconfig)
         2.RedisHttpSessionConfiguration
         3.springSessionRepositoryFilter
               spring:session 是默认的 Redis HttpSession 前缀(redis中，常用 ‘:’ 分割),
               Spring Session 之所以能够替换默认的 tomcat httpSession  因为配置了springSessionRepositoryFilter
      
               -- redis中目录结构
                  3(A).spring:session::sessions:77112-sdd12-....				对应的是 hash 数据结构		默认35分钟
                  2(B).spring:session:expiretions:123123123						set 结构(存储系列1类型键)	默认30分钟			存放着这一分钟(当前时间+过期时间)应当过期的 session 的 key。
                  1(C).spring:session:sessions:expires:77112-sdd12-....			对应一个空值,String	   默认30分钟			手动改redis无效，有自己的预先缓存机制吧。
      
                  A.存储Session的详细信息，包括Session的过期时间间隔、最近的访问时间、attributes等等。这个k的过期时间为Session的最大过期时间 + 5分钟。如果默认的最大过期时间为30分钟，则这个k的过期时间为35分钟
                  B.存储这个Session的id，是一个Set类型的Redis数据结构。这个k中的最后的1439245080000值是一个时间戳，根据这个Session过期时刻滚动至下一分钟而计算得出。
                  C.表示Session在Redis中的过期，这个k-v不存储任何有用数据，只是表示Session过期而设置。这个k在Redis中的过期时间即为Session的过期时间间隔
      
                  每一个 session 都会有三个相关的 key	，
                  第三个 key 最为重要，它是一个 HASH 数据结构，将内存中的 session 信息序列化到了 redis 中,形如sessionAttr:browser=chrome
                  另外两个过期时间，因为redis 清除过期 key 的行为是一个异步行为且是一个低优先级的行为，
                  用文档中的原话来说便是，可能会导致 session 不被清除。
                  于是引入了专门的 expiresKey，来专门负责 session 的清除，包括我们自己在使用 redis 时也需要关注这一点
      
                  C 类型键存在的意义便是解耦 session 的存储和 session 的过期，并且使得 server 获取到过期通知后可以访问到 session 真实的值。
                  对于用户来说，C 类型键过期后，意味着登录失效，而对于服务端而言，真正的过期其实是 A 类型键过期，这中间会有 5 分钟的误差。
               
               request.getSession().invalidate(),会将c类型的立刻删除，然后B类型的还会多保留5分钟在服务器上。过时自动删除
               在调用request.getSession().invalidate()会销毁当前的session，后续再调用request.getSession()会重新创建一个session
               
               *****对于多余的无用sessioin，一般在filter中的chain.dofilter之后清除当前无用的session*****
               request.getSession().invalidate();  会删除当前的session， 对应springsession中是spring:session:sessions:expires:77112-sdd12-....  这种类型的过期，但是主sessionid还在服务端存放一定时间

               boot使用的就是本身的SessionRepositoryFilter，自动配置了spring-session，一般不建议改动默认配置

   3. SpringSession 技术是解决同域名下的多服务器集群 session 共享问题的，不能解决跨域 Session 共享问题
      spring session 往客户端写sessionID的策略要么是cookies要么是header。
      
      PC端，一般是使用cookies，这样可以实现单点登录（本质还是公用sessionid），
      手机app的话，不支持cookies，只能使用header，服务器响应请求的时候，往header里面写sessionId。

      cookie编码，sessionId的base64编码  boot中web工程设置cookie相关。
      spring-session-1.3.4(默认false)  和spring-session-core 2.0.2(默认true)  注意各版本中的编码默认值
      在高版本的spring-session 的jar中使用的DefaultCookieSerializer

      cookieSerializer.setUseBase64Encoding(false); //这个要关闭，否则sessionid会在浏览器编码显示，导致跨手机 和pc的sessionid变化
      cookieSerializer.setDomainName(domainName);   //xiaoyuer.com
      cookieSerializer.setCookiePath("/");
      
      要统一sessionid的编码方式
         注意的是虽然浏览器64编码了，但是传到后台后会解码为正常的sessionid，redis缓存中存的也是正常的缓存值。意思前到后有个自动编解码的过程。
         场景	手机编码  ids未编码
         手机带过去的是64编码后的，ids未解码直接拿来使用，在redis中找不到sessioinid，重新创建(req.getSession()默认是true)了一个session存值，这样存和取不在一个session中。

8. Springcache相关
    引入springcache，需要自定义缓存管理器，场景：高查询  低改动
	 默认使用rediscache，最终存在Redis中，可跨系统

	@EnableCaching 表示SpringIoC容器启动了缓存机制
	一般查询用＠Cacheable；插入和修改，使用＠CachePut；删除操作，使用＠CacheEvict。

	常规配置: 失效时间，序列化方式(redis服务端易读)，目录的层级(默认是::)

	查询原理：
		每次调用需要缓存功能的方法时，Spring会检查检查指定参数的指定的目标方法是否已经被调用过(依据是value+key,相同会覆盖，一般key中追加参数信息)；
		如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法 并缓存结果后返回给用户。下次调用直接从缓存中获取。
		先执行@Cacheable----->再执行service层的方法，基于注解()，实际就是个拦截器(就是使用注解省去了判断的过程。原先是判断没有缓存，就从数据库查询，这里讲判断隐藏到了注解去实现)

	Spring Cache对Cache进行抽象，提供了@Cacheable、@CachePut、@CacheEvict等注解。
		@Cacheable
			根据方法对其返回结果进行缓存，下次请求时，如果缓存存在，则直接读取缓存数据返回；如果缓存不存在，则执行方法，并把返回的结果存入缓存中。一般用在查询方法上。
		@CachePut
			使用该注解标志的方法，每次都会执行，并将结果存入指定的缓存中。其他方法可以直接从响应的缓存中读取缓存数据，而不需要再去查询数据库。一般用在新增方法上。
			需要返回待缓存的对象，存入cache
		@CacheEvict
			使用该注解标志的方法，会清空指定的缓存。一般用在更新或者删除方法上
			无需返回对象

	Springcache 和redis的区别
		1.Spring cache是代码级的缓存，一般是使用一个ConcurrentMap，也就是说实际上还是是使用JVM的内存来缓存对象的，这势必会造成大量的内存消耗。但好处是显然的：使用方便。
		2.Redis 作为一个缓存服务器，是内存级的缓存。它是使用单纯的内存来进行缓存。
		3.集群环境下，每台服务器的spring cache是不同步的，这样会出问题的，spring cache只适合单机环境。
		4.Redis是设置单独的缓存服务器，所有集群服务器统一访问redis，不会出现缓存不同步的情况。


	通常情况下，直接使用SpEL表达式来指定Key比自定义KeyGenerator更简单。
		key = "'USER:'+#id"
		空参数使用默认的key就是calendar::SimpleKey []，多次调用会覆盖同名key
		可以使用单独的字符串作为key="'zhaoyun'"

		注解使用在mapper上，需要使用p绑定：	@Cacheable(value=ConstantsRedis.CACHE_PAY,key="'withdraw:config:'+#p0+'-'+#p1")
		在service上：	@Cacheable(value="cache",key="'letterBank-'+#firstLetter")  
		若不希望返回值为null时进行缓存，则使用unless="#result == null",排除掉返回值为null的结果	
		若不希望参数为空的时候进行缓存，则需要使用condition = "#i==null",这时函数还没执行，排除掉参数为空的情况
		key="‘wangyun’+#userinfo.id"

	配置
		这个做springcache用的，查询结果缓存使用
   ```
         @Bean
         public CacheManager redisCacheManager(RedisConnectionFactory redisConnectionFactory) {
            //1.设置key 和value的序列化方式
            RedisSerializer<String> redisSerializer = new StringRedisSerializer();
            Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);
            
            ObjectMapper om = new ObjectMapper();
            om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);
            om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);
            jackson2JsonRedisSerializer.setObjectMapper(om);

      //		定义缓存前缀,默认是两个冒号
            CacheKeyPrefix keyPrefix = new CacheKeyPrefix() {
                     @Override
                     public String compute(String cacheName) {return cacheName + ":";}
                  };
                  
            //2.链式调用(返回new的config对象)设置config
            RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig()
                  .entryTtl(Duration.ofMinutes(30))
                  .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(redisSerializer))
                  .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(jackson2JsonRedisSerializer))
                  .disableCachingNullValues().computePrefixWith(keyPrefix);
            return RedisCacheManager.builder(redisConnectionFactory).cacheDefaults(config).build();
         }
   ```
   
   java.lang.IllegalArgumentException: Null key returned for cache operation (maybe you are using named params on classes without debug info?) 
   原因是没有传对应的值可以值
 

9. 登录相关
		主流的单点登录还是使用的是耶鲁大学开源的CAS，验票的环节
	
		xxl的单点更加简单，就是一个sso server基于cookie和session中缓存统一管理，没有就返回
		一般登陆ok是有两个cookie，一个是server端的 一个是client端的
		app,非cookie登陆就是从header存储信息

      顶域的伪跨域
         我们Cookie的domain属性是sso.a.com，在给app1.a.com和app2.a.com发送请求是带不上的。
         sso登录以后，可以将Cookie的域设置为顶域，即.a.com，这样所有子域的系统都可以访问到顶域的Cookie。
         我们在设置Cookie时，只能设置顶域和自己的域，不能设置其他的域。这样可以实现顶域下的跨域处理，但是Cookie顶域的特性这种是属于伪跨域

		单点的简单原理： 		https://yq.aliyun.com/articles/636281
		server验证ok（写sso当前域）后需要ST 返回验票，成功后写app1当前域，
		app2跳转后sso已经登录，直接st返回即可，流程同1

		xxl-sso
         1、cookie 和缓存都没有  跳转server
         2、server成功返回，cookie没有  缓存有，设置cookie
         3、登录后再访问，cookie有直接返回，不用设置cookie

		有状态
			服务端需要记录每次会话的客户端信息，从而识别客户端身份，根据用户身份进行请求的处理，
			类似cookie-session这种会记录用户信息的方式是有状态的
		
		无状态
			微服务集群中的每个服务，对外提供的都使用 RESTful 风格的接口。而 RESTful 风格的一个最重要的规范就是：服务的无状态性，即：
				服务端不保存任何客户端请求者信息
				客户端的每次请求必须具备自描述信息，通过这些信息识别客户端身份
			
		相对常规的session登录，(Android、iOS、小程序等，这些 App 天然的就没有 cookie)
			无状态登录的流程：1.首先客户端发送账户名/密码到服务端进行认证 2.认证通过后，服务端将用户信息加密并且编码成一个 token，返回给客户端
							  以后客户端每次发送请求，都需要携带认证的 token，服务端对客户端发送来的 token 进行解密，判断是否有效，并且获取用户登录信息

		*****
			验证st（有一定时效性的）的必要性在于：
			其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，
			是不是业务系统也认为登录了呢？这是很可怕的。所以需要统一server端验证
		*****

      rememberMe 记住登录
         if("true".equals(rememberMe)){
            Cookie cookie = new Cookie("cellphone",cellphone);
            cookie.setPath("/");//根目录
            cookie.setMaxAge(-1);//永久
            response.addCookie(cookie);
         }else{
            Cookie cookie = new Cookie("cellphone",null);
            cookie.setMaxAge(0);
            response.addCookie(cookie);
         }
         request.getSession().setAttribute("loginUser",JSON.toJSONString(user));

### Redis相关

1. Redis是单线程的，是线程安全的。
   redis快的原因
      1.纯内存操作
      2.单线程操作，避免了频繁的上下文切换
      3.采用了非阻塞I/O多路复用机制

2. redis和mysql的缓存同步
   	1.只做Mysql的增、删、改，同等需要删除Redis里的数据。
   	2.读取Redis数据，只要不存在，就读取Mysql，并且装入Redis中。

   ​	读数据：先读取缓存，若不存在则从DB中读取，并将结果写入到缓存中；下次数据读取时便可以直接从缓存中获取数据。
   ​	改数据：直接失效缓存数据，再修改DB内容(避免突发情况：避免DB修改成功，但由于网络或者其他问题导致缓存数据没有清理，造成了脏数据)

3. redis过期
         后台提供了定时任务去“删除”过期的 key，来补偿 redis 到期未删除的 key。
			方案再描述下：取得当前时间的时间戳作为 key，去redis中定位到 spring:session:expirations:{当前时间戳}这个 set 里面存放的便是所有过期的 key 了。
			
			spring-session中有个定时任务，每个整分钟都会查询相应的spring:session:expirations:整分钟的时间戳中的过期SessionId，然后再访问一次这个SessionId，
			即spring:session:sessions:expires:SessionId，以便能够让Redis及时的产生key过期事件——即Session过期事件。
		
			redis的key过期具有一定延迟性：
            1.redis 在键实际过期之后不一定会被删除，可能会继续存留，但具体存留的时间我没有做过研究，可能是 1~2 分钟，可能会更久。
            2.具有过期时间的 key 有两种方式来保证过期，一是这个键在过期的时候被访问了，二是后台运行一个定时任务自己删除过期的 key。划重点： 这启发我们在 key 到期后只需要访问一下 key 就可以确保 redis 删除该过期键
            3.如果没有指令持续关注 key，并且 redis 中存在许多与 TTL 关联的 key，则 key 真正被删除的时间将会有显著的延迟！显著的延迟！显著的延迟！
			
			   this.redis.hasKey(key)可以触发将 redis过期键删除


      **redis 中的incr方法，如果键不存在，会将默认重新计数，并且，过期时间为-1，需要判断ttl的时间，看情况重置过期时间**

4. boot中的redis
			在引入Redis中一般排除redis的异步客户端lettuce(一般在spring-boot-starter-data-redis 2.x版本默认，用的比较少)，使用jedis。
			spring提供了一个redisconnectionfactory接口，生成redisconnection(对应jedis驱动，其实现类jedisconnection)接口对象,操作Redis各命令

			redis推荐使用模板操作，RedisTemplate，实际使用中最多的还是stringredistemplate(推荐)

         boot2.0整合redisTemplate
            需要引入spring-data-redis，使用默认版本
            引入jedis 2.9.0版本才能统一对应上。
			
			序列化方式	
            字符串定义了key(包括 hash 数据结构)，而值则使用了序列化，这样就能够保存 Java对象了。但是不推荐保存一个java类对象，容易反序列化各系统不兼容	
            1.stringredisserializer(推荐)
                  在Java和Redis中保存的内容是一样的，对String数据进行序列化。序列化后，保存到Redis中的数据，不会有像上面的“\xAC\xED\x00\x05t\x00\x09”多余字符。就是"frequency".	
            2.jdkserializationredisserializer(redistemplate默认序列化器) 
               key-value：
                  key:\xAC\xED\x00\x05t\x00\x08test_key
                  value:\xAC\xED\x00\x05t\x00\x0Dtest_value111	
                  所有的key和value还有hashkey和hashvalue的原始字符前，都加了一串字符。用原来的key就取不到我们保存的数据了。
            3.jacksonJsonRedisSerializer 已经过时，不推荐使用
               在Redis中保存的内容，比Java中多了一对双引号。

         现在Redis一般使用的是 Spring的RedisTemplate
         旧项目中是代码形式是封装的jedis启动类，
            jedisPool = new JedisPool(JedisPoolConfig, ip, port)， 
            Jedis jedis = jedisPool.getResource(); //用jedis操作对象         


         其他（忽略）
            redis事务(忽略)
               通常的命令组合是watch...multi...exec, 
               watch是监控redis的一些键，
               multi命令是开始事务，
               exe命令意义在于执行事务(队列命令执行前会判断被watch监控的redis键数据是否变化过(赋值相同也算变过)，没变才会执行事务)
               redistemplate.execute((RedisOperations operationns) ->{
                  operationns.watch("key1");//监控key1
                  operationns.multi();//开启事务，在exec命令执行前，全部命令都只是进入队列,期间的set后随即get value也是null，
                  ... 	//一些列操作 
                  return operationns.exec();//执行exec命令，将先判别keyl是否在监控后被修改过，如果是则不执行事务，否则就执行事务
               })
         
            使用redis流水线技术可以批量执行redis的命令(忽略)
            redis中也可以做发布订阅模式，只能算是简化版的mq(忽略)
			
        

5. 常用操作命令
      type key	         查询redis的key类型
      TTL KEY_NAME      返回key的剩余过期时间(s)。


6. redis的数据结构，数据类型
      Key 可以是任意类型，但最终都会存储为 byte［］
      redis
         Redis 支持的各种存储结构，数据类型，常用string和hash
            1. String
               简单的k-v存储结构，支持数据的自增, value可以是String也可以是数字

            2. Hash 哈希表数据结构
               (key,(field,value))，这里value存放的是结构化的对象，类似map

            3. list 列表
               使用List的数据结构，可以做简单的消息队列的功能。或利用lrange命令，做基于redis的分页功能

            4. Set  集合，去重

            5. SortedSet 有序集合
                  多了一个权重参数score,集合中的元素能够按score进行排列。可以做排行榜应用，取TOP N操作。

			hash结构  (key,(field,value))  field是hash结构的内部键
				hset  相当于是给hash存储结构添加key-value数据，类似map
				hgetall是取出hash结构的所有数据
				redis.hset("url","google","www.google.cn");
				redis.hset("url","baidu","www.baidu.com");
				
            就相当于是	
                     Map<String,String> map = new hashMap()；
							map.put("google","www.google.cn");
							map.put("baidu","www.baidu.com");
							redis.hmset("url",map)；

             hget pay:bankcard_acct 1366          pay:bankcard_acct       key-1366 value-5

			redis的list
            list形式以进出栈的形式实现队列， Pop Push 等堆栈操作
            是一个链表机构，类似栈或者队列，对元素的push和pop，可分别选择在首尾部添加和删除，lrange可以获取list在指定区间的元素
            列表中 LPUSH 和 RPUSH 可以想左右两边增加元素，对应还有弹出功能LPOP 和 RPOP
            lpush test_list 1 2 4		添加list
            lrange test_list  0 3   	查看、获取list  

         使用场景
            最简单的 String ，可以作为 Memcached 替代品，用作缓存系统
            使用 SetNx 可以实现简单的分布式锁
            使用 list Pop Push 功能可以作为阻塞队列／非阻塞队列
            使用 SUBSCRIBE PUBLISH 可以实现发布／订阅模型对数据进行实时分析，如可以累加统计等
            使用 Set 做去重的计数统计
            使用 SortedSet 可以做排行榜等排序场景

7. redis持久化 
       redis的两种持久化方案：
			方案：相对来说使用aof的sec足够了，先重点，细节用到再说，默认是rdb模式

				* rdb模式   snapshotting 快照
               默认的存储方式，默认写入dump.rdb的二进制文件中，可以配置redis在n秒内如果超过m个key被修改过就自动做快照

               在指定的时间间隔内将内存中的数据集快照写入磁盘
					数据集快照dump到dump.rdb中，可以修改dump的频率(比如:300秒内，如果超过10个key被修改，则发起快照保存)。
               实时的可能会数据丢失，速度快

               原理(忽略)
                  Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。
                  进行快照的条件：当在指定的时间内被更改的键的个数大于指定的数值时就会进行快照。主要参数：时间和改动的键的个数
                  Redis默认会将快照文件存储在当前目录的dump.rdb文件中，可以通过配置dir和 dbfilename两个参数分别指定快照文件的存储路径和文件名
                  通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。

                  fork一个子进程来进行持久化，不影响主进程的io操作
                  快照备份，是备份当前瞬间 Redis 在内存中的数据记录。备份慢一点，恢复快
					
					生成rdb文件的命令（忽略）
                  save	阻塞服务器进程，知道rdb创建完成为止，阻塞期间，不接受任何命令
                  bgsave	用子线程创建rdb文件，服务器进程继续处理命令请求
                  
                  多个save命令(用save选项设置的保存条件)，是或关系，满足条件就会执行bgsave命令
					
				* aof模式   append-only file
               redis会将每一次的函数、写指令操作都追加到aof文件中，当redis重启时会重新执行文件中的保存的写命令在内存中
               其作用就是当 Redis 执行写命令后，在一定的条件下将执行过的写命令依次保存在Redis中， 将来即可依次执行那些保存的命令恢复Redis数据
					追加文件，备份快，恢复慢，备份文件可能大
               速度慢、体积大，可设置不同的fsync 策略，安全性高

               原理(忽略)
                  开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬 盘中的AOF文件。
                  AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认 的文件名是appendonly.aof，可以通过appendfilename参数修改：
                  可见AOF文件是纯文本文件，其内容正是Redis客户端向Redis发送的原始通信协议的内容
                  每当达到一定条件时Redis就会自动重写AOF文件，重写是为了删除冗余命令，比如多条覆盖，只保留最后一条即可
               
                  执行命令由于缓存机制，不是直接写入磁盘(现在硬盘缓存，随后写入硬盘,AOF文件中)
                  在默认情况下系统每30秒会执行一次同步操作，以便将硬盘缓存中的内容真正地 写入硬盘，在这30秒的过程中如果系统异常退出则会导致硬盘缓存中的数据丢失。
                  一般来讲 启用AOF持久化的应用都无法容忍这样的损失，这就需要Redis在写入AOF文件后主动要求系 统将缓存内容同步到硬盘中。
                  
                  在Redis中我们可以通过appendfsync参数设置同步的时机：
                     always				每次修改，即每秒执行一次同步操作
                     everysec 			每秒同步(常用),每次执行写入 都会执行同步，这是最安全也是最慢的方式。（redis默认配置）
                     no 					从不同步,不主动进行同步操作，而是完全交由操作系统来做（即每30秒一次），这是最快但最不安全的方式。
					

            其他(忽略)
					数据恢复时按照丛前到后的顺序再将指令执行一遍
					其中包含重写机制，一条 incr*100，最后可以合成一条incr100
					如果想要保证数据的安全性，建议同时开启 AOF RDB ，此时由于 RDB 有可能丢失文件 Redis 重启 优先使用 AOF 进行数据恢复
					需要注意 ，如果通过 kill -9 或者 Ctrl+C 关闭 Redis ，那么 RDB AOF 都不会被触发，这样会造成数据丢失，建议使用 redis-cli shutdown 或者 kill 优雅关闭 Redis


					在创建新的rdb文件时，会检查键，已经过期的键不会被保存到新建的rdb文件中。在aof重写过程中也是
					当一个过期键被删除后，服务器会追加一条del命令到现有的aof文件末尾，显示删除过期键。
					主服务器删除过期键，会向从发送del命名，统一由主发起删除

8. StringRedisTemplate 和 RedisTemplate 
			两者的数据是不共通的；
			StringRedisTemplate(推荐使用,只要属性文件中配置spring.redis.pool等属性,无需多余配置,直接导入即用)
				默认采用的是String的序列化策略，保存的key和value都是采用此策略序列化保存的。
				使用的是 StringRedisSerializer，序列化String，所以使用的时候key就不会出现一串字符串

            常用操作
               stringRedisTemplate.opsForHash().put(key, hashKey, value);
               (String)stringRedisTemplate.opsForHash().get(key, field);

			RedisTemplate
				默认采用的是JDK的序列化策略，保存的key和value都是采用此策略序列化保存的。 JdkSerializationRedisSerializer 序列化对象，存入key前面会带上一串东西
				RedisTemplate可以用来存储对象，但是要实现Serializable接口，以二进制数组方式存储，内容没有可读性
	
			如果k-v是Object类型，则需要自定义 RedisTemplate。
			Jackson2JsonRedisSerializer(redis客户端jason显示) 和JdkSerializationRedisSerializer() 
			JdkSerializationRedisSerializer:    \xAC\xED\x00\x05t\x00\x0Ezheshizhendeba  取出来还是zheshizhendeba

9. 键值回收策略
			redis采用的是定期删除+惰性删除策略
			***** Redis的key 超时不会被其自动回收，它只会标识哪些键值对超时了。*****

         定期删除：redis默认是每隔100ms就随机抽取一些(注意不是全部key)设置了过期时间的key，检查其是否过期，如果过期就删除。
						   定期删除可能会导致很多过期key到了时间并没有被删除掉。
			惰性删除：一个超时的键，被再次用get命令访问时将触发 Redis 其从内存中清空
			
         如果 key, 超时了， Redis 会回收 key 的存储空间吗 ？ 不会
            只采用定期删除，可能会导致很多key到时间没有删除，所以搭配惰性删除(获取某key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期？若过期就会删除。)
					
			定时删除（忽略）：创建一个定时器，当key设置有过期时间，且过期时间到达时，由定时器任务立即执行对键的删除操作 

10. 内存淘汰机制	 
         用作键值回收的补充操作
			如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。 
         在redis.conf中有一行配置	 
			# maxmemory-policy volatile-lru
				1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。内存达到最大的，它就只能读而不能写 
				2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（*****推荐使用*****） 
				3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。应该也没人用吧，你不删最少使用Key,去随机删。 
				4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。不推荐 
				5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。依然不推荐 
				6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐 ps：如果没有设置 expire 的key, 不满足先决条件(prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。

11. Redis 主从复制、哨兵和集群三者区别
         redis集群分布式节点一般一主一备，当任意master挂了没有备份或半数以上的master挂了则集群不可用
			一主(master)二从(slave)三sentinel的架构模式,  三个哨兵监控三个服务器

         主从模式：数据备份
            读写分离，备份，一个Master可以有多个Slaves。解决数据备份问题
            备份数据、负载均衡，一个Master可以有多个Slaves。
            主写从读，主机是一台，而从机可以是多台
            可以实现读写分离，数据备份。但是并不是「高可用」的
         
         哨兵sentinel：高可用
            监控，自动转移，哨兵发现主服务器挂了后，就会从slave中重新选举一个主服务器。解决故障切换问题
            高可用(主挂了哨兵可切换)，发现master挂了后，就会从slave中重新选举一个master。
            可以看做是主从模式的「高可用」版本，其引入了 Sentinel 对整个 Redis 服务集群进行监控。但是由于只有一个主节点，因此仍然有写入瓶颈。
            
            哨兵的配置在sentinel.conf配置文件中配置，一般2个及以上的哨兵投票才能切换主机(默认超时3分钟后才会进行投票切换主机)
      
            哨兵，以独立的进程监控3台服务器Redis是否正常运行,实际使用一般多个哨兵共同监控(除了监控各个Redis服务外，各个哨兵之间还会相互监控)
            哨兵有两个作用
               1.通过发送命令，让 Redis服务器返回监测其运行状态，包括主服务器和从服务器。
               2.当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知到其他的从服务器，修改配置文件，让它们切换主机
            
         集群cluster：数据分片,分布式存储
            为解决单机Redis容量有限问题,将数据分片，按一定规则分配到多台机器redis实例中，用多分散压力，提高并发量，这样内存/QPS不受限于单机
            每个集群中至少需要三个主数据库才能正常运行，Redis集群至少需要3个节点，每个节点又需要一个从备份节点，所以Redis集群至少需要6台服务器。
            不仅提供了高可用的手段，同时数据是分片保存在各个节点中的，可以支持高并发的写入与读取。
      
            集群不必另外使用 Redis Sentinel，内置了哨兵功能
            因为redis的集群是把内容存储到各个节点上，而哨兵的作用就是监控redis主、从数据库是否正常运行，主出现故障自动将从数据库转换为主数据库
            在Redis Sentinel模式中，每个节点需要保存全量数据，冗余比较多，而在Redis Cluster模式中，每个分片只需要保存一部分的数据，

12. Redis Cluster分区方式 和 Redis扩容(忽略)
      Redis Cluster分区方式(忽略)
         虚拟槽分区，Redis Cluster的节点之间会共享消息，每个节点都会知道是哪个节点负责哪个范围内的数据槽
         客户端访问任意节点时，对数据key按照CRC16规则进行hash运算，然后对运算结果对16383进行取作，如果余数在当前访问的节点管理的槽范围内，则直接返回对应的数据
         如果不在当前节点负责管理的槽范围内，则会告诉客户端去哪个节点获取数据，由客户端去正确的节点获取数据
         把16384个槽平均分配给节点进行管理，每个节点只能对自己负责的槽进行读写操作
         由于每个节点之间都彼此通信，每个节点都知道另外节点负责管理的槽范围			
         保证高可用，每个主节点都有一个从节点，当主节点故障，Cluster会按照规则实现主备的高可用性
      
      Redis扩容(忽略)
         Redis官方推荐使用redis-trib.rb工具快速搭建Redis Cluster
         redis的动态扩容操作都是通过redis-trib.rb脚本文件来完成的
         当添加新节点成功以后，新的节点不会有任何数据，因为他没有分配任何的数据Slot(哈希slots),这一步需要手动操作。
         
         扩容需要新节点没有slot，需要重新分配
         常用的数据分片的方法有：范围分片，哈希分片，一致性哈希算法，哈希槽等
         分配槽位，可以自定义指定节点的分槽范围，一般要均分。slot 0-5460
         多少个节点，自己针对16384个哈希槽，自己分配
         Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽(Slot)，集群的每个节点负责一部分hash槽
         Redis Cluster数据分区规则采用虚拟槽方式(16384个槽)，每个节点负责一部分槽和相关数据，实现数据和请求的负载均衡
         通过对Key进行CRC16(key)%16384运算得到对应的槽是哪一个，从而将读写操作转发到该槽所对应的服务节点
         Redis集群使用数据分片(sharding)而非一致性哈希(consistency hashing)来实现：一个Redis集群包含16384个哈希槽(hash slot)，数据库中的每个键都属于这16384个哈希槽的其中一个，集群使用公式CRC16(key)%16384来计算键key属于哪个槽，其中CRC16(key)语句用于计算键key的CRC16校验和。
         集群自动故障转移过程分为故障发现和节点恢复。节点下线分为主观下线和客观下线，当超过半数节点认为故障节点为主观下线时，标记这个节点为客观下线状态。从节点负责对客观下线的主节点触发故障恢复流程，保证集群的可用性
         
         那么每个节点都和其他N-1个节点保持连接和心跳，节点之间采用Gossip协议进行通信。通信主要确认节点是否存活、节点的数据版本、投票选择新的master等内容。
         
         扩容步骤
            一、添加两个服务节点到集群
               redis-cli --cluster add-node 192.168.8.196:5007 192.168.8.196:5001
               redis-cli --cluster add-node 192.168.8.196:5008 192.168.8.196:5001
            二、为master节点添加分片，重新分配槽
               redis-cli --cluster reshard 192.168.8.196:5007
               指定slot数量和槽来源,选择all，就是全节点指定数量迁移
            三、设置从节点
            
         Redis 集群中内置了 2^14=16384 个哈希槽，当需要在 Redis 集群中放置一个key-value时，redis 先对key使用crc16算法算出一个结果，
         然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。	
         当你往Redis Cluster中加入一个Key时，会根据crc16(key) mod 16384计算这个key应该分布到哪个hash slot中，一个hash slot中会有很多key和value。

13. 其他相关概念
		缓存穿透
			概念访问一个不存在的key，缓存不起作用，请求会穿透到DB，流量大时DB会挂掉。
			解决方案：
            采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的key，不存在的key直接被过滤；
            访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。

		缓存雪崩
			大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。
			由于原有缓存失效，新缓存未到，期间所有原本应该访问缓存的请求都去查询数据库了。

			解决方案
				1. 一般并发量不是特别多的时候，采用分布式锁，加锁排队访问	
				2. 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。
				3. 为 key 设置不同的缓存失效时间,不会集中在同一时刻失效	
				4.采用限流算法，限制流量

		缓存预热(常用)
			缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！
		
		缓存更新
			缓存更新除了缓存服务器自带的缓存失效策略之外（Redis默认的有6种策略可供选择），也可以自定义缓存淘汰。
         常见的策略有两种：
			(1)定时去清理过期的缓存；
			(2)当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。
		
		缓存降级
			当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。
			系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。
         降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）	


14. redis同步
      *****绝大多数的迁移步骤，先迁移整体，同时开启临时记录区域，同步进行*****
      主从同步的过程重点
         1.同步开始，主会备份数据，主将备份文件发送给从，并同时将新增命令放进缓存区，
         2.从会丢弃所有现有的数据，开始载入发送的快照文件
         3.待从执行完备份文件，主会将缓冲区的也发过去执行，后面就主收到一条写就发给从一条同步写(从在解析完备份文件，接收命令，等待命令写入)

      redis和数据库一致。
         redis和数据库的简单同步，不变的不变，微变的间隔变，实时变的直接数据库读后同步redis
         对于时效性不高的，可以每隔一段时间取更新，但是时效性要求高的，就直接从数据库中查询，并同步写入redis

      主从复制
         主服务器在写入数据后，即刻将写入数据的命令发送给从服务器，从而使得主从数据同步。
         基于快照文件和缓存的命令实现
            当一个从数据库启动后，会向主数据库发送SYNC命令，主数据库接收到SYNC命令后会在后台保存快照（即RDB持久化的过程），并将保存期间接收到的命令缓存起来。
            当快照完成后，Redis会将快照文件和所有缓存的命令发送给从数据库。从数据库收到后，会载入快照文件并执行收到的缓存的命令。
            当主从数据库断开重连后会重新执行上述操作，不支持断点续传
            无论是否启用了RDB方式的持久化，Redis在启动时都会尝试读取dir和dbfilename两个参数指定的RDB文件来恢复数据库。

      使用while(判断)+redis实现并发的等待处理  redisUtil.exists(key), redis的setEx，


15. **redis秒杀架构思想**
      限流
         由于活动库存量一般都是很少，对应的只有少部分用户才能秒杀成功。所以我们需要限制大部分用户流量，只准少量用户流量进入后端服务器。
      削峰
         秒杀开始的那一瞬间，会有大量用户冲击进来，所以在开始时候会有一个瞬间流量峰值。如何把瞬间的流量峰值变得更平缓，是能否成功设计好秒杀系统的关键因素。实现流量削峰填谷，一般的采用缓存和 MQ 中间件来解决。
      异步
         秒杀其实可以当做高并发系统来处理，在这个时候，可以考虑从业务上做兼容，将同步的业务，设计成异步处理的任务，提高网站的整体可用性。
      缓存
         秒杀系统的瓶颈主要体现在下订单、扣减库存流程中。在这些流程中主要用到 OLTP 的数据库，类似 MySQL、SQLServer、Oracle。由于数据库底层采用 B+ 树的储存结构，对应我们随机写入与读取的效率，相对较低。如果我们把部分业务逻辑迁移到内存的缓存或者 Redis 中，会极大的提高并发效率。
      
      异常用户
         限制异常用户	 针对同一个用户（ Userid 维度），做页面级别缓存，单元时间内的请求，统一走缓存，返回同一个页面
                        大量请求同时间段查询同一个商品时，可以做页面级别缓存，不管下回是谁来访问，只要是这个页面就直接返回。

         有效请求和无效请求
            1.针对一个用户的无效恶意请求，可以增加图片验证码
            2.短信验证，进一步的限制请求，比如限制用户在单位时间的操作次数，降低请求量， 
            3.一人多账号的，最终集中到实名那步，限制到人信息即可
            4.多人多账号的，僵尸账号排除法，平时没交易，只在特殊节日交易
            5.当然还能使IP封禁，尤其是通过同一IP或者网段频繁请求的，但是可能误伤有效请求，需注意
   
      正常多用户
         对于后端系统的控制可以通过消息队列、异步处理、提高并发等方式解决。对于超过系统水位线的请求，直接采取「Fail-Fast」原则，拒绝掉
         
      流程，用户抢购->是否是异常用户->缓存中的库存是否足够->扔给mq去慢慢操作
      
      总结流程
         核心思想：层层过滤，尽量将请求拦截在上游，降低下游的压力，逐渐递减瞬时访问压力(主要是数据库的压力)
         MQ排队服务，后面下订单与扣减库存的压力都是自己能控制的，根据数据库的压力，可以定制化创建订单消费者的数量，避免出现消费者数据量过多，导致数据库压力过大或者直接宕机。
         库存服务专门为秒杀的商品提供库存管理，实现提前锁定库存，避免超卖的现象。同时，通过超时处理任务发现已抢到商品，但未付款的订单，并在规定付款时间后，处理这些订单，将恢复订单商品对应的库存量。
      
         充分利用缓存与消息队列，提高请求处理速度以及削峰填谷的作用
         针对扣库存的地方，利用redis实现分布式锁(一般是集群，用分布式锁),锁住针对库存的减操作。可以使用redisson已经有对 Reentrantlock的封装操作。

16. 不要存储byte类型的数据，费劲
      RedisClient.set("000000".getBytes(), SerializeUtil.serialize(sd),3600);
      Object object = RedisClient.get("sessionData".getBytes());


## Maven相关

1.  
2. maven相关的打包命令	
     打包，install，跳过测试 
   ​	mvn clean install -Dmaven.test.skip=true        打包整个pom工程 

     mvn -Dmaven.test.skip=true  	跳过测试阶段且不编译测试用例类(推荐) 

     mvn -DskipTests					   跳过测试阶段但编译测试用例类,编译测试用例类生成相应的class文件至target/test-classes下

     maven树，查询依赖(忽略)
         dependency:tree -Dverbose -Dincludes=com.xiaoyuer:xye-core-dao
         Dincludes=org.springframework:spring-tx 过滤串使用groupId:artifactId:version的方式进行过滤	

3. maven的外部jar依赖

   使用外部依赖，war中放入lib，打包可以就行，可以就不上传到maven库。（一般偷懒用）
   maven的依赖，是依赖的源代码+pom中的maven依赖（依赖传递），所以对应的war下也要有对应的依赖jar才行
   引入外部的jar 这个是要在对应的目录下建的，因为项目运行的时候是统一管理lib下的jar的
   导入第三方jar包时报错-----java.lang.NoClassDefFoundError，需要在对应的war文件下也放一份这样配置

   	<dependency>
   		<groupId>com.chinapay.secss</groupId>
   		<artifactId>secss</artifactId>
   		<version>1.0.0</version>
   		<scope>system</scope>	
   		<systemPath>${basedir}/src/main/webapp/WEB-INF/lib/chinapaysecure1_5.jar</systemPath>
   	</dependency>
   	#<systemPath>${basedir}/WebContent/WEB-INF/lib/hamcrest-core-1.3.jar</systemPath>
   	#jar不是从库中拿，而是从本地获取(使用本地导入jar),这里${basedir} pom.xml中的同级目录

4. 快照和发布版，snapshot和release

      release版本优先只依赖本地版本，不会再远程拉，没有本地才拉远程
      snapshot实时，但不稳定，release版本则比较稳定。Maven会根据项目的版本判断将构件分发到哪个仓库。

    	pom中引用统一jar的snapshot大小写必须一致，默认一般是大写
      jenkins打包的maven  release版本需要删除maven库中的，重新拉取。默认nexus不允许release重复部署，



5. 上传远程仓库 

   项目中，maven的仓库管理软件——Sonatype Nexus私服

   ```
   maven的两种版本管理稳定版和快照版，
   pom.xml中带release和snapshot的会自动打包拿到nexus相应的目录，其他不识别的打到release目录下，
   打包的时候需要大写SNAPSHOT才能到正确位置，小写会打到release库下，但是引用大小写都可以。
   <version>0.0.6-SNAPSHOT</version>
   <version>3.2.0.RELEASE</version> 
   ```

   本机maven的settings.xml中配置
   	分发构件到远程仓库需要认证，需在本机maven的settings.xml中配置认证信息：（nexus的账号和密码），

   	

      ```
         <servers>
         <server>
            <id>my-nexus-releases</id>
            <username>admin</username>
            <password>admin123</password>
         </server>
         <server>
            <id>my-nexus-snapshot</id>
            <username>admin</username>
            <password>admin123</password>
         </server>

         <!-- 配置tomcat-/manager/text 访问权限 -->
         <server>
            <id>tomcat</id>
            <username>admin</username>
            <password>admin</password>
         </server>
      </servers>
      ```

      jar上传的配置(多环境测jar上传)
            <profiles>
                  <profile>
                     <id>sit</id>
                     <distributionManagement>
                        <snapshotRepository>
                           <id>my-nexus-snapshot</id>
                           <url>http://192.168.6.251:8087/nexus/content/repositories/sit/</url>
                        </snapshotRepository>
                     </distributionManagement>
                  </profile>
            </profiles>		

            这是以前的，这样看貌似 本地jar环境已经没有了，传到固定的远程仓库
            <distributionManagement>
               <repository>
                  <id>my-nexus-releases</id>
                  <url>http://192.168.6.251:8087/nexus/content/repositories/releases/</url>
               </repository>
               <snapshotRepository>
                  <id>my-nexus-snapshot</id>
                  <url>http://192.168.6.251:8087/nexus/content/repositories/snapshots/</url>
               </snapshotRepository>
            </distributionManagement>

         setting.xml中的<server>，是pom中<distributionManagement>上传的认证配置，server元素的id值必须与pom.xml中需要认证的repository或snapshotRepository元素id完全一致。
         这个id将认证信息与仓库配置联系起来。
         配置多个远程仓库时，如果在一个远程找不到，依次从下一个仓库里找，在activeProfiles设置启用的仓库。默认会有一个ID是central的官方远程仓库。
         将认证信息放到settings下而非POM中，是因为POM往往是它人可见的，而settings.xml是本地的。

6. 项目打包

   maven install  默认是将src/main/java 和 src/main/reources打包
   jar包的引用，一般打成的格式是和meta-inf同级的目录是直接可以访问的

   dev 和其他环境的打包问题，默认的是使用本地的代码，只有maven打包的时候才会选择相应的环境

   **maven多环境 <resources>中，pom中不指定具体的路径，默认 web deployment assembly 中配置的resource 全目录打包，**
   **配置了<resource>就会严格固定pom中的配置目录**

   ```
   #在pom.xml中指定打包的名称
   <build>
       <!-- pojectName就是打包后的名称：pojectName.war -->
       <finalName>pojectName</finalName>
   </build>		
   ```

   ```
   在elclise中直接启动时编译是以本地代码为准的，dev其他路径的环境参数只针对maven 打包后的有效
   通过vars下的properties过滤到.xml下的读取直接用$[]读取，然后通过属性文件的加载配置PropertyPlaceholderConfigurer读取的使用${}读取，<env></env> 空,打出来的就是空的/路径
   <profiles>
   	<profile>
   		<id>prd</id>
   		<properties><env>prd</env></properties>
   		<build>
               <filters>
                   <filter>../vars/vars.prd.properties</filter>
               </filters>
   		</build>
   	</profile>
   <profiles>
   <resource>
       <directory>${basedir}/src/main/resources</directory>
       <filtering>true</filtering> 	----开启过滤功能
   </resource>
   <resource>
       <directory>src/main/resources/${env}</directory>  -----打包的目录，默认全部
       <includes>
           <include>application.properties</include>	  -----打包固定的目录或文件
           <include>tlqb/*</include>
       </includes>
   </resource>
   <resource>
       <directory>../vars/${pay_env}/tlqb</directory>		
       <!-- 目标路径 将directory目录下的全目录内容打包到classes下-->
       <targetPath>WEB-INF/classes/tlqb</targetPath>
   </resource>
   ```

7. 相关标签,基础概念
      * maven三个标识缩写，GAV
      * <optional>true</optional>   maven不会依赖传递

      * 依赖中scope即约定依赖范围： 
         compile：默认值，一直可用，最后会被打包 
         provided：编译期间可用，不会被传递依赖，不会被打包。如tomcat相关jar，容器中有，编译期临时用，运行时由web容器提供。
         test：执行单元测试时可用，不会被打包，不会被传递依赖 
         runtime：运行和测试时需要，但编译时不需要 
         system：不推荐使用 

         <scope>provided</scope>    作用就是让jar包依赖只在编译的时候起作用，运行的时候不起作用。
         比如使用javax.servlet-api 的时候，本地使用编译的，线上使用tomcat中的

         新后台 <scope>provided</scope>的值，在eclipse往IDEA里迁移的时候，容器的scope需要住改成compile

      *  dependencyManagement，子父版本依赖
            <dependencyManagement>	
            父pom工程中集中定义jar或者maven插件的版本信息，这里只是对版本进行管理，不会实际引入jar  
            优先以子工程中的dependency声明version为准，子没有配置，默认继承父pom中的dependencyManagement定义的version

            dependencyManagement导致的传递依赖失效
               在使用spring-boot-starter-parent(boot的两种方式，还有一个是spring-boot-dependencies) 中传递依赖无效
               Maven 父pom中dependencyManagement版本优先级高于传递依赖版本(导致引一个jar中的传递依赖，即使指定了版本也无效)
               解决方式两个：
                  1. 在子模块中<dependencyManagement>定义覆盖父类的申明
                  2.先exclude掉子模块间接依赖，再直接添加依赖并声明版本号

               如果子pom从jar依赖传递了版本过来，但子pom还是优先使用父dependencyManagement中版本
               简单的说就是父dependencyManagement中版本优先级要高于依赖传递过来的jar版本

               emanager中  spring-boot-dependencies    和dubbo-dependencies-bom 2.77 冲突，这种引入dependencyManagement管理的，会优先使用这里集中限定的版本。将pom工程中的依赖限定导入进来了。优先使用。
               
               父pom 中的 dependencyManagement 是允许被子模块的 dependencyManagement 覆盖的。dependencies 优先级高于 dependencyManagement。
               spring-boot-dependencies 同级别 不是子父模块关系，不受版本继承影响


8. 一般建议api工程，不继承父pom，避免pom打包的问题

9. 杂记

   * maven依赖jar，但是没有在eclispe中导入项目就看不到源码，除非jar中有sources文件

     勾选maven下载项的源码选项，点击class文件查看文件所在位置，添加相关的resource文件

   * maven依赖中出现依赖不一致的情况，可能出现了两次以上的maven依赖，maven出现！情况，但是jar引用存在，多是因为jar中缺少相关代码，打包与工程对应的正确的jar即可。

   * pluginManagement的配置和plugins的配置是一样的，只是用于继承，使得可以在子pom中使用父pom

   

10. maven项目编译版本
		*****
			maven-compiler-plugin  在父pom中    一般父pom中编译这个，然后boot单独编译boot的spring-boot-maven-plugin
		*****
      * 方式1
			<maven.compiler.source>1.8</maven.compiler.source>
			<maven.compiler.target>1.8</maven.compiler.target>
			<maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>
		* 方式2
				在build的插件中指定版本
				<plugin>
					<groupId>org.apache.maven.plugins</groupId>
					<artifactId>maven-compiler-plugin</artifactId>
					<configuration>
						<compilerVersion>1.7</compilerVersion>
						<source>8</source>
						<target>8</target>
					</configuration>
				</plugin>
		* 方式3(推荐)
			SpringBoot 工程  继承了spring-boot-starter-parent
				<properties>
					<java.version>1.8</java.version>
				</properties>
			实际上是覆盖了parent中的属性

11. 发布机的maven配置setting.xml
   ```
      <?xml version="1.0" encoding="UTF-8"?>
      <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">
         <servers>
            <server>
               <id>my-nexus-releases</id>
               <username>admin</username>
               <password>admin123</password>
            </server>
            <server>
               <id>my-nexus-snapshot</id>
               <username>admin</username>
               <password>admin123</password>
            </server>
            <server>
               <id>tomcat</id>
               <username>admin</username>
               <password>admin</password>
            </server>
         </servers>
         <mirrors>
            <mirror>   
               <id>nexus</id>    
               <url>http://192.168.6.251:8087/nexus/content/groups/public/</url> 
               <mirrorOf>thirdparty,central</mirrorOf>   
            </mirror>
         </mirrors>

         <profiles>
            <profile>
               <id>thirdparty</id>
               <repositories> 
                  <repository>
                     <id>thirdparty</id>                                   
                     <url>http://192.168.6.251:8087/nexus/content/groups/public/</url>
                     <releases>
                        <enabled>true</enabled>
                     </releases>
                     <snapshots>
                        <enabled>true</enabled>
                        <updatePolicy>always</updatePolicy>
                     </snapshots>
                  </repository>
               </repositories>
            </profile>
            <profile>
               <id>pre</id>
               <repositories>
                  <repository>
                     <id>xiaoyuer</id>                                   
                     <url>http://192.168.6.251:8087/nexus/content/repositories/pre/</url>
                     <releases>
                           <enabled>true</enabled>
                     </releases>
                     <snapshots>
                           <enabled>true</enabled>
                     </snapshots>
                  </repository>
               </repositories>   
               <pluginRepositories>
                  <pluginRepository>
                  <id>xiaoyuer</id>
                  <url>http://192.168.6.251:8087/nexus/content/repositories/pre/</url>
                  <releases>
                     <enabled>true</enabled>
                  </releases>
                  <snapshots>
                        <enabled>true</enabled>  
                        <updatePolicy>always</updatePolicy>  
                        <checksumPolicy>warn</checksumPolicy>  
                  </snapshots>
                  </pluginRepository>
               </pluginRepositories>
            </profile>
            <profile>
               <id>prd</id>
               <repositories>
                  <repository>
                     <id>xiaoyuer</id>                                   
                     <url>http://192.168.6.251:8087/nexus/content/repositories/prd/</url>
                     <releases>
                           <enabled>true</enabled>
                     </releases>
                     <snapshots>
                           <enabled>true</enabled>
                     </snapshots>
                  </repository>
               </repositories>   
               <pluginRepositories>
                  <pluginRepository>
                  <id>xiaoyuer</id>
                  <url>http://192.168.6.251:8087/nexus/content/repositories/prd/</url>
                  <releases>
                     <enabled>true</enabled>
                  </releases>
                  <snapshots>
                        <enabled>true</enabled>  
                        <updatePolicy>always</updatePolicy>  
                        <checksumPolicy>warn</checksumPolicy>  
                  </snapshots>
                  </pluginRepository>
               </pluginRepositories>
            </profile>

            //是自己测试用的仓库地址，也可以通过mirror方式
            <profile>
               <id>myprofile2</id>
               <repositories>
                  <repository>
                     <id>nexus-aliyun</id>
                     <name>Nexus aliyun</name>
                     <url>https://maven.aliyun.com/repository/public</url>
                     <releases>
                        <enabled>true</enabled>
                     </releases>
                     <snapshots>
                        <enabled>false</enabled>
                     </snapshots>
                  </repository>
               </repositories>
            </profile>
         </profiles>
      
         <activeProfiles>
            <activeProfile>thirdparty</activeProfile>
         </activeProfiles>
   </settings>

   ```
12. 
13. 
14. 依赖状态冲突
		compile									      编译成功的。
		omitted for duplicate 					   jar包重复依赖(版本一样)，当前jar包引入的被忽略。
		3.4.8 omitted for conflict with 3.4.6	jar包版本冲突(此处引3.4.6版本)。
		2.4 managed from 2.3  				 	   使用2.4版本，拒绝使用<dependencyManagement>中声明的2.3版本

15. maven依赖特性
      maven采用的是最近依赖原则，同样近的第一个优先。是相对于依赖树中。
            1.路径最近者优先
            2.路径相等(引入的层次相同)，先声明者优先(目前看是jar的引入层次，但是在本pom.xml中顺序靠后的优先)
         Maven默认用的是JDK1.5去编译，使用高版本的maven需要在pom中配置
       ```
         <build>
            <plugins>
            <plugin>
               <groupId>org.apache.maven.plugins</groupId>
               <artifactId>maven-compiler-plugin</artifactId>
               <version>3.5.1</version>
               <configuration>
               <source>1.8</source>
               <target>1.8</target>
               </configuration>
            </plugin>
            </plugins>
         </build>
       ```

      <dependency>  
         <groupId>org.myorg.myapp</groupId>  
         <artifactId>app-util</artifactId>  
         <version>${project.version}</version>  
      </dependency>  

      其中${project.version} 是一个属性引用，指向了POM的project/version的值，也就是这个POM对应的version。
      由于app-dao的version继承于app-parent，因此它的值就是1.0-SNAPSHOT

       ```  
         <parent>
         <relativePath>作用
         <!-- 父项目的pom.xml文件的相对路径。相对路径允许你选择一个不同的路径。默认值是../pom.xml。
            Maven首先在构建当前项目的地方寻找父项目的pom，其次在文件系统的这个位置（relativePath位置），然后在本地仓库，最后在远程仓库寻找父项目的pom。 -->
         <relativePath />
       ```


16. maven-setting.xml配置	
		maven profile 的active 是并集   activeProfiles和之前的default的关系：激活两个     目前系统发布用的就是这个
		maven  <relocation>:，构件的重定位信息(更换了group ID和artifact ID),这里会出现一种现象就是，依赖中的，但是pom.xml中搜索不到

		在同一个 <mirrorOf>central</mirrorOf> 下，多个mirror只有第一个生效。针对的是mirrorOf拦截的。(这里是针对匹配上的代理)
		配置多个mirror时，mirrorOf不能配置" * "，" * " 的意思就是（根据mirrorOf和repository的id）匹配所有的仓库

		updatePolicy    setting.xml中的配置
			该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。
			always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。

      repository 一般有中央仓库，公共仓库，私有仓库以及本地仓库

       ```
         <mirrors> 
            <mirror>
               <id>nexus-aliyun</id>
               <mirrorOf>central</mirrorOf>
               <name>Nexus aliyun</name>
               <url>http://maven.aliyun.com/nexus/content/groups/public</url>
            </mirror>
         
            <!--
               这个不能全部匹配，上面优先的条件是mirrorof要对上
               <mirror>  
            <id>nexus</id>    
               <url>http://192.168.6.251:8087/nexus/content/groups/public/</url>   
               <mirrorOf>*</mirrorOf>   
            </mirror>
            -->
         </mirrors>
       ```
17. 多环境相关
         <directory>src/main/resources/${pay_env}</directory>
         目录中的会覆盖resource下的同名文件

         ***************之前理解是不全的,最好是不要同名文件冲突，加载文件最好不要存在重复覆盖的情况********************
            对于同一application.properties，顺序在前的优先使用，所有一般动态目录要放在前面
            <resources>
               <resource>
                  <directory>src/main/resources/${pay_env}</directory>
               </resource> 
               
               <resource>
                  <directory>src/main/resources</directory>
                  <includes>
                     <include>**.*properties</include>
                  </includes>
               </resource> 
            </resources>
         ********************之前理解是不全的，最好是不要冲突***********************
            1.动态加载指定的文件，2.然后通用文件resources单独指定include下

      pom中的profile中定义好env环境变量，sit/pre/prd
		
		直接指定类resources目录下的某个配置文件
			然后固定一个application.properties，其中指定spring.profiles.active=${environment}   #对应下面的{profile}
			然后多个环境application-{profile}.properties根据主属性文件而定
			
			这里是全配置加载到类路径，通过boot识别属性中配置指定环境。
			 <resource>
                <!--打包该目录下的配置文件 -->
                <directory>src/main/resources</directory>
                <includes>
                    <include>**/*.yml</include>
                    <include>**/*.xml</include>
                </includes>
                <filtering>true</filtering>		#启用过滤 即该资源中的变量将会被过滤器中的值替换
            </resource>
			
			**********
            <filtering>true</filtering>	 这个最开始是将属性加载到filter中的，后续配置开始过滤使用
            只要是在pom.xml中加载的属性文件或者定义的属性值，都要开启，否则后面的resources下的文件获取不到值
            总结，如果后续类路径下配置需要动态读取pom.xml中加载的属性文件或者定义的属性值，需要加<filtering>true</filtering>
            
            1.整体思路，主配置文件中设置通用配置，并固定设置默认dev
            2.分支配置环境中配置不同的内容
            3.命令行激活不同环境配置
			**********

      通过resource的过滤功能，将对应位置的配置加载到类路径下。重新定义了资源路径
		实际项目使用的使用是根据resource目录来过滤，对应文件夹下的属性文件到资源路径下。
      这里是通过加载指定目录到资源路径下，指定环境配置，不会将多余环境配置加载进去
         <resource>
               <directory>src/main/resources/${env}</directory>
         </resource>


      //这个是可以将zx下的目录打到cer下的，并且target和war目录一致,不常用
         ```
            <resource>
               <!-- 元配置文件的目录，相对于pom.xml文件的路径 -针对WEB-INF下面文件内容按照环境区分需要单独配置到该目录 -->
               <directory>../vars/${pay_env}/zx</directory>
               <!-- 目标路径 -->
               <targetPath>WEB-INF/classes/cer</targetPath>
            </resource>

            targetPath:指定build资源到哪个目录，默认是base directory
            directory:指定属性文件的目录，build的过程需要找到它，并且将其放到targetPath下，默认的directory是${basedir}/src/main/resources
         ```
        
      resource 和 webresouce（忽略）
         sit环境真正执行的也就是打成war的不是resource下的，因为有webresouce在
         以webresouce为优先覆盖，如果resource下有就会覆盖。resource有，webresource没有,resource中的这样也会打包到web-inf下，这样就解释通了，最好不要用webresouce，直接用resource即可
         默认使用resource，如果webresource有重的就优先覆盖。


    多环境的几种方案
      * 方式1，以前老的方式（不推荐）
         先加载对应环境属性文件，然后替换到后面文件中
			这种，后面资源xml，pro，yml等文件中需要读取pom.xml中加载的属性文件中的值，需要加<filtering>true</filtering>
			这种走filter的不推荐用，不简洁
			<profile>
				<id>sit</id>
				<properties>
					<env>sit</env>
				</properties>
				<build>
					<filters>
						<filter>../vars/vars.sit.properties
						</filter>
					</filters>
				</build>
			</profile>
			
		* 方式2，boot标准的方式
            先加载环境名称值，然后系统指定对应属性文件application-{profile}.properties
				这种主application.properties和后面的xml文件中需要动态读pom.xml中设置的属性，需要加<filtering>true</filtering>
			<profile>
				<id>sit</id>
				<properties>
					<environment>sit</environment>
				</properties>
			</profile>


         //后续根据application.yml中读取配置，选定环境
          <resources>
            <resource>
                <directory>src/main/resources</directory>
            </resource>
            <resource>
                <!--打包该目录下的配置文件 -->
                <directory>src/main/resources</directory>
                <includes>
                    <include>**/*.yml</include>
                    <include>**/*.xml</include>
                </includes>
                <!-- 启用过滤 即该资源中的变量将会被过滤器中的值替换 -->
                <filtering>true</filtering>
            </resource>
        </resources>

         //后续根据application.yml中配置环境参数
         spring:
            profiles:
               active: ${environment}
			
		* 方式3.现在的方式（目前在用）
            在pom中直接拿到环境名称值，然后resources下过滤对应环境的配置文件
				直接在pom中指定好了对应环境-固定内容的配置文件
				这种对应环境下的配置内容是固定的没有动态读取的，无需加<filtering>true</filtering>
				重复文件可以覆盖。指定优先
			 <profile>
				<id>sit</id>
				<properties>
					<env>sit</env>
				</properties>
			</profile>
			
			 <resource>
                <directory>src/main/resources/${env}</directory>
            </resource>
			
		* 方式4. 命令执行（不推荐）
         主配置中默认写死是dev，在执行命令中指定是哪个环境。这个相对手动，不便于维护
			java -jar xxx.jar --spring.profiles.active=pre

      

18. nexus 搭建
	   nexus仓库类型  
         Group：这是一个仓库聚合的概念，访问顺序取决于配置顺序
         Hosted:私有仓库，专门用来存储我们自己生成的jar文件  
            Snapshots：本地项目的快照仓库  
            Releases： 本地项目发布的正式版本  

		Proxy:公网上发布的jar 例如：spring
		  Central：中央仓库

		1.下载版本，cd目录下 nexus.exe/run  执行 访问
		2.创建相应的maven-proxy库，相应的group库。maven的setting.xml中配置对应的私服即可。
		当项目去私服下载的时候，私服没有会自动远程更新到私服下，然后拉取(实测正确)
			
		配置远程库时nexus的aliyun_proxy，总结就是nexus会远程更新
		1.本地mirror配置了central的代理库  aliyun，私服是远程aliyun代理，那么被本地的拦截了,nexus不会更新jar
		2.本地没有配置mirror，nexus下会更新jar
		3.本地配置mirror关于远程私服aliyun，所有请求都会到私服上，nexus会更新
		
		jar存放路径
		它的默认路径在\nexus-3.2.1-01-win64\sonatype-work\nexus3\blobs\default\content下面
		
		https://blog.csdn.net/m0_38001814/article/details/89494078   nexus 介绍
		
		发布机远程没有的会优先使用本地的jar
		
		子项目在maven库中依赖缺少父pom，上传一个父的pom.xml到nexus中即可。选择pom上传
		nexus中的可以把别的库中的jar文件夹直接复制过去，然后需要发布下或者退出nexus后台重新进入，然后就刷新了

		nexus proxy仓库可用的远程地址，https://repo1.maven.org/maven2/
		
		****
			Nexus私服的Release仓库不允许上传SNAPSHOT版本，会报错，而SNAPSHOT仓库压根就不提供Web界面上传功能。
			手工上传特定的快照jar,SNAPSHOT,到nexus库中
			光copyjar到nexus没用，还需要相关的xml中配置了相关的version等信息
			其中repositoryId是maven的setting.xml中配置的server节点的serverId(server中的配置的是nexus admin的登录账号和密码,获得上传的用户权限)
			mvn deploy:deploy-file -DgroupId=com.xiaoyuer.op -DartifactId=xye-open-dmo -Dversion=1.1.5-SNAPSHOT -Dpackaging=jar -Dfile=D:\k8sjar\xye-open-dmo-1.1.5-SNAPSHOT.jar -Durl=http://192.168.6.251:8087/nexus/content/repositories/K8sJar/ -DrepositoryId=my-nexus-snapshot
		****
		
		调k8s遇到的jar问题
			dubbo 2.8.4原因是没哟同时上传jar，这种关联的jar尽量不要手工上传，update官方的标准jar
			调试k8s的容器，其中dubbo 2.8.4x 手工上传后总是有问题，原先的mavenjar alibaba文件夹可以。替换251也不行，这种很可能是不能单独一个jar 还有其他相关的jar需要关联的。
			所以后来改添加snapshot进发布库，这是可行的，然后251nexus后台目录中删除jar和复制jar到k8sjar,不能立刻nexus页面显示，需要发布一次后才能更新
			
			netpay中的hessian接口也是，使用的是xye-open-dmo的手工上传的copy 的jar，导致报错，应该使用最新的jar，因为自己copy过来的肯定没有install后的实时全面，手工上传容易出错。
			因为可能open-dmo是从打包好的lib下直接拖过来的，但是install本身会传递一个依赖关系。到了lib下，jar已经update完了，不一定存在这种依赖关系。所以copy，不一定存在这种依赖关系。

		maven deploy，上传，部署
			-e 	打印完成的错误日志
			-U 	强制检查快照版本更新，没有就默认是按天检查
         -P    指定profile的参数
				-- 老版本
				<distributionManagement>
					<repository>
						<id>my-nexus-releases</id>	//maven得setting.xml中配置的server验证信息
						<url>http://192.168.6.251:8087/nexus/content/repositories/releases/</url>
					</repository>
					<snapshotRepository>
						<id>my-nexus-snapshot</id>
						<url>http://192.168.6.251:8087/nexus/content/repositories/snapshots/</url>
					</snapshotRepository>
				</distributionManagement>
				-- 分环境版本
				 <profiles>
					 <profile>
						<id>pre</id>
						<distributionManagement>
							<snapshotRepository>
								<id>my-nexus-snapshot</id>
								<url>http://192.168.6.251:8087/nexus/content/repositories/pre/</url>
							</snapshotRepository>
						</distributionManagement>
					</profile>
				</profiles>
	
			maven deploy 错误
				1、部署的仓库类型错误
					nexus的repository分三种类型：Hosted、 Proxy和Virtual，另外还有一个repository group(仓库组)用于对多个仓库进行组合，部署的时候只能部署到Hosted类型的仓库中。
				2、部署的仓库部署策略为禁止部署
					releases仓库的部署策略默认为禁止部署，如果要部署到这个仓库中需要修改部署策略为Allow Redeploy
				3、仓库发布版本与部署的项目发布版本不相符
					项目的发布版本如果为<version>1.0-SNAPSHOT</version>，则不能部署到发布版本为Release的仓库中

19. spring-data-redis在xye-project-util中是1.8.3版本，支付中已经排除了引用，但还是方法找不到   （忽略）
			这里的原因是1.8.3版本是void delete() 编译好了,最终使用的是2.05版本是Boolean delete(),两个不是同一个方法，所以找不到方法
			这样就是 编译是找自身的jar  运行才会找最终的依赖jar。尽量版本统一
			方法定义了  会就近寻找jar依赖  

20. 
   
21.  maven fork的使用（忽略）
      这里的 fork 设置为 true，实际上是会在 maven 编译的时候新创建一个虚拟机执行。这个新创建 JVM 就是这里的 fork。
      它速度会稍慢一些，但是隔离性非常好。消耗更多的资源，更加耗性能
      maven 本身是运行在 jvm 之上的，编译项目的 jdk 可以和运行 maven 的 jdk 不同。

      困扰好几天的dubbo的install问题，终于在StackOverflow 上找到了答案，<fork>false</fork> 很关键。很关键
      ```
         <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <configuration>
                  <fork>false</fork>
                  <source>${java_source_version}</source>
                  <target>${java_target_version}</target>
                  <encoding>${file_encoding}</encoding>
            </configuration>
         </plugin>
      ```			
            
      默认情况下 ，fork 是 false，Maven 使用 运行自己的 jdk （maven 是需要依赖 jdk 存在的） 来进行 compiler ，	
      如果不想使用这个默认的 jvm，就可以通过 fork 来实现				
               
      这个错是因为maven所用的jdk版本号过低，项目中的某些类需要更高的JDK版本。
      通过配置pom.xml，添加如下配置（注意，fork一定要为true）可以设置maven使用的jdk	
   
22. maven仓库
	实际使用的时候，一般配置一个私有库，然后用阿里云镜像代理中央库
	远程仓库：中央仓库+私服+其他公共库，maven使用的默认是中央仓库，私服和阿里云镜像可以同时使用
	依赖的优先级：本地仓库 > 私服（profile）> 远程仓库（repository）。全局和pom中的优先，局部配置优先于全局配置

	mirror镜像
		mirror：拥有远程仓库的所有 jar，包括远程仓库没有的 jar，定义了两个Repository之间的镜像关系（大部分jar包都可以在阿里镜像中找到，少部分jar包没有，需单独配置镜像）
		配置多个mirror，默认只有第一生效（只有当第一个无法连接的时候才会接着往下匹配，但是注意第一个中a.jar没有是不会接着往下面的mirror中找的），可以都放着不影响，择优第一个就行。
		mirror相当于一个拦截器，将远程库的地址重定向到mirror里配置的地址。每个仓库只能使用一个镜像。
		mirrorOf配置了*，相当于代理所有的远程仓库，就只能去该镜像中下载，其他配置的多个库就失效了（因为拦截机制）。可以用来私服下载特定的jar

	全局多仓库设置，是通过修改maven的setting文件实现的。可以将常用的公共库，私服库配置进去。不建议在项目pom配置
	  在setting文件中添加多个profile（也可以在一个profile中包含很多个仓库），并激活（即使是只有一个可用的profile，也需要激活）。
	  <activeProfiles>
         <activeProfile>myRepository1</activeProfile>
         <activeProfile>myRepository2</activeProfile>
	  </activeProfiles>

	发布机的sit仓库引用，发布机只配置了251的public(默认激活该配置)，实际-p 又激活了setting.xml对应环境的jar仓库，这样就就会去这两个私服库去拉取jar。(实测正确)
	<resource>同目录下还是可以覆盖的，但是webresource不是同一目录所以不能覆盖。webresource只针对tomcat部署，boot还是用的target/classes下的

23. github assembly   maven assembly(忽略)
    项目中主要用处是，维持两个jar目录，3rd 和 xye ，前者是基本不变的三方包，后者是小鱼儿的实时jar，这样减少发布时候的更新jar的时间，不算通用的，非标暂不看


## Jenkins

jenkins
	Multiple SCMs Plugin使用 jenkins同时打包两个git项目
	-- 对应jenkins发布的版本号	
	> /usr/bin/git checkout -f 658390e95d5da0389444221d8960b0d0d252c25c
	Commit message: "钱包需求变更"

jenkins插件配置使用的是jenkins的maven_setting.xml文件，
	pipeline使用了sh脚本。sh 'mvn clean install -P sit -Dmaven.test.skip=true -Dmaven.javadoc.skip=true'   这里使用的系统默认的setting.xml,有点不一样。

	
	https://www.jenkins.io/zh/doc/book/pipeline/jenkinsfile/   jenkins文档
	
	
	问题，jenkins中maven编译找不到pom.xml
		but there is no POM in this directory (/home/release/.jenkins/jobs/sit-xye-pay-quartz-pipeline/workspace)
		maven命令	sh 'mvn -f xye-pay-quartz/pom.xml clean install -P sit -Dmaven.test.skip=true -Dmaven.javadoc.skip=true' 
		原因：在git代码pull下来的第一层就需要有pom.xml(默认找第一层目录下的pom.xml)，否则需要使用 -f 指定pom.xml的位置。

Jenkins Pipeline，ansible(忽略)
	备份一个ansible发布的yml脚本
		*docker发布版
		- name: pay-quartz  main.yml
		  shell: echo "pay-quartz deploy start..."
		- name: stop docker container 6.154
		  shell: docker stop docker_pay-quartz_tomcat
		  register: result
		  ignore_errors: True
		- debug:
		  var:  result
		- name: wait for the container stop
		  pause: seconds=5
		- name: remove docker container
		  shell: docker rm -f docker_pay-quartz_tomcat 2>&1
		- name: delete old code
		  file: path=/usr/local/webapps/xye-pay-quartz state=absent
		- name: mkdir ids
		  shell: mkdir -p /data/apps/xye-pay-quartz > /dev/null 2>&1
		- name : copy file
		  copy: src=/home/release/.jenkins/jobs/sit-xye-pay-quartz-pipeline/workspace/xye-pay-quartz/target/xye-pay-quartz.war dest=/data/apps/xye-pay-quartz force=yes backup=yes
		- name: unzip project war
		  shell: unzip /data/apps/xye-pay-quartz/xye-pay-quartz.war -d /usr/local/webapps/xye-pay-quartz
		- name: run docker
		  shell: docker run -itd -h "docker_pay-quartz_tomcat" --add-host docker_pay-quartz_tomcat:192.168.6.154 --net=none  -v /usr/local/webapps/xye-pay-quartz:/usr/local/tomcat/webapps/xye-pay-quartz -v /usr/local/logs/xye-pay-quartz:/usr/local/tomcat/logs  -v /etc/localtime:/etc/localtime  --name docker_pay-quartz_tomcat  192.168.6.138:5000/sit_pay-quartz_tomcat:v1;sudo /usr/local/bin/pipework br0 docker_pay-quartz_tomcat 192.168.6.154/24@192.168.6.1
		- name: check tomcat start
		  wait_for: host=192.168.6.154  port=8080 delay=10 state=present
		- name: container 6.154 ok
		  shell: echo 'pay-quartz deploy over'
		
		*普通发布版
		- name: shutdown tomcat
		  shell: /usr/bin/nohup /usr/local/tomcat/bin/shutdown.sh >/dev/null 2>&1 &
		- name: check tomcat is closed
		  wait_for: port=8080 delay=5 state=absent
		- name: delete old .war
		  file: path=/usr/local/tomcat/webapps/xye-pay-quartz.war state=absent
		- name: scp new .war
		  copy: src=/home/release/.jenkins/jobs/sit-xye-pay-quartz-pipeline/workspace/xye-pay-quartz/target/xye-pay-quartz.war dest=/data/apps/ force=yes backup=yes
		- name: delete old code
		  file: path=/usr/local/tomcat/webapps/xye-pay-quartz state=absent
		- name: unzip project war
		  shell: unzip /data/apps/xye-pay-quartz.war -d /usr/local/tomcat/webapps/xye-pay-quartz
		- name: startup tomcat
		  shell: /usr/bin/nohup /usr/local/tomcat/bin/startup.sh >/dev/null 2>&1 &
		- name: check tomcat is started
		  wait_for: port=8080 delay=10 state=present
		- name: wait for the service to start successfully
		  pause: seconds=10
		  
		  
		*java  jar包运行启动
			#!/bin/bash
			#这里可替换为你自己的执行程序，其他代码无需更改
			APP_NAME=e-manager-pay.jar
			SERVICE_DIR=/usr/local/java_project/bin
			JAR_HOME_DIR=/usr/local/java_project/webapps/

			cd $SERVICE_DIR 

			#使用说明，用来提示输入参数
			usage() {
				echo "Usage: sh java_project.sh [start|stop|restart|status]"
				exit 1
			}
			 
			#检查程序是否在运行
			is_exist() { 
				pid=`ps -ef | grep $APP_NAME | grep -v grep | awk '{print $2}'`
				#如果不存在返回1，存在返回0
				if [ -z "${pid}" ]; then
				  return 1
				else
				  return 0
				fi
			}
			 
			#启动方法
			start() {
			   is_exist
			   if [ $? -eq "0" ]; then
				 echo "${APP_NAME} is already running. pid=${pid} ."
			   else
				 echo -n "Starting ${APP_NAME}: "
				 echo 
				 nohup java -Xms512m -Xmx512m -XX:PermSize=256M -XX:MaxPermSize=256M -jar $JAR_HOME_DIR$APP_NAME > /dev/null 2>&1 &
			   fi
			}
			 
			#停止方法
			stop() {
			   is_exist
			   if [ $? -eq "0" ]; then
				 echo -n "Killing ${APP_NAME}: "
				 echo 
			#     kill -9 $pid
				 kill $pid
				 sleep 10
				 PID=`ps  -ef | grep $APP_NAME | grep -v grep | awk '{print $2}'`
			#     if ["$pid" == ""];then
			#     if [ -z "${pid}" ];then
				 if [ ! ${PID} ]; then
				   echo "=== Kill successfully "
				 else
				   echo "===== Kill fail and try to kill -9"
				   echo "====  the pid is  $PID"
				   kill -9 $PID
				 fi
			   else
				 echo "${APP_NAME} is not running"
			   fi
			}
			 
			#输出运行状态
			status() {
			   is_exist
			   if [ $? -eq "0" ]; then
				 echo "${APP_NAME} is running. Pid is ${pid}"
			   else
				 echo "${APP_NAME} is not running."
			   fi
			}
			 
			#重启
			restart() {
			   stop
			   start
			}
			 
			#根据输入参数，选择执行对应方法，不输入则执行使用说明
			case "$1" in
			   "start")
				 start
				 ;;
			   "stop")
				 stop
				 ;;
			   "status")
				 status
				 ;;
			   "restart")
				 restart
				 ;;
			   *)
				 usage
				 ;;
			esac
		
	Pipeline
		Pipeline是Jenkins2.X的最核心的特性，运行于Jenkins上的工作流框架，将原本独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排与可视化。
			这里的就是将多个分散的配置任务，通过脚本的方式集中到一起，更加可视化。集中管理
		Stage：阶段，一个Pipeline可以划分成若干个Stage，每个Stage代表一组操作,Stage是一个逻辑分组的概念，可以跨多个Node
		Node：节点，一个Node就是一个Jenkins节点，是执行Step的具体运行环境
		Step：步骤，Step是最基本的操作单元，小到创建一个目录，大到构建一个Docker镜像，由各类Jenklins Plugin提供
		Jenlins Pipeline的基础语法
			Pipeline脚本是由Groovy语言实现（无需专门学习）

			支持两种语法
			Declarative 声明式（在Pipeline plugin 2.5中引入）
			Scripted Pipeline 脚本式(推荐)
		
			声明式：
				Jenkinsfile (Declarative Pipeline)
				pipeline {
					agent any 
					stages {
						stage('Build') { 
							steps {
								// 
							}
						}
						stage('Deploy') { 
							steps {
								// 
							}
						}
					}
				}
			
			脚本式
				Jenkinsfile (Scripted Pipeline)
				node {  
					stage('Build') { 
						// 
					}
					stage('Deploy') { 
						// 
					}
				}
				
			特点：stages 必须，包括顺序执行的一个或多个stage命令，在pipeline内仅能使用一次，通常位于agent/options后面
				  steps 必须，steps位于stage指令块内部，包括一个或多个step。仅有一个step的情况下可以忽略关键字step及其{}	
		
		在jenkins中选择构建流水线的项目，然后参数化构建过程中选择git parameter中配置参数，然后脚本中替换分支为${branch}，这里第一次取得是配置，后面就是取的实际git分支了，这样可以灵活切换发布分支，不用固定在脚本中，每次更改配置。
		流水线语法用流水线语法模板生成即可
		
		*ansible发布脚本 pipeline script
			timeout(time: 600, unit: 'SECONDS') {
				node {
					stage('清理workspace') {
						cleanWs()
					}
					stage('获取代码') {
						checkout([$class: 'GitSCM', branches: [[name: '${Branch}']], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: 'e8e810d3-32f0-45b1-8a3e-0b1d975f1d46', url: 'git@gitlab.xiaoyuer.net:xye_pay/xye-pay-quartz.git']]])
					}

					stage('打包') {
						sh 'mvn clean install -P sit -Dmaven.test.skip=true -Dmaven.javadoc.skip=true' 
					}
					 stage('发布') {
						sh "/usr/bin/ansible-playbook /etc/ansible/roles/publish/sit/tomcat/docker_xye-pay-quartz/tomcat_xye-pay-quartz.yml -f 5"
					}
					   
				}
			}
		
	ansible-playbook
		命令运行格式：ansible-playbook <filename.yml> ... [options]
					  ansible-playbook playbook.yml -f 10
		
		playbooks 是一种简单的配置管理系统与多机器部署系统的基础.与现有的其他系统有不同之处,且非常适合于复杂应用的部署，用来进行多机器的部署
		
		https://blog.csdn.net/Doudou_Mylove/article/details/90520836    相关role的介绍
		https://www.cnblogs.com/robertx/p/10840769.html
		https://www.cnblogs.com/yanjieli/p/10969299.html
		https://www.cnblogs.com/hanshanxiaoheshang/p/10206380.html
		https://www.jenkins.io/zh/doc/									jenkins文档
		roles(角色)思想就是把在一个playbook里不同的内容分门别类分别放到不同的目录下进行组织，便于更清晰的管理
		roles能够根据层次型结构自动装载变量文件、tasks以及handlers等
		
		创建role的步骤
			1.创建以roles命名的目录
			2.在roles目录中分别创建以各角色名称命名的目录，如httpd等
			3.在每个角色命名的目录中分别创建files、handlers、meta、tasks、templates和vars目录；用不到的目录可以创建为空目录，也可以不创建。其中tasks是必要的目录
			4.在playbook文件中，调用个角色

			步骤，1.执行脚本，sh "/usr/bin/ansible-playbook /etc/ansible/roles/publish/sit/java/docker_jar_emanagerpay/emanagerpay.yml -f 5"	
				  2.具体脚本
					- name: java service
					  remote_user: sit_yunwei
					  hosts: 192.168.6.32
					  roles:
						- publish/sit/java/jar_emanagerpay       #这里就是指定roles下的一个功能，指定功能的平级目录，会加载(files、handlers、meta、tasks、templates和vars这几个目录的内容)，task中的脚本是执行核心
						

		目录/etc/ansible中，配置了ansible中需要使用到的配置，和hosts文件，还有roles目录
		
		其中的- 更像是一个列表的结构
		
		在一个 play 之中,所有 hosts 会获取相同的任务指令,这是 play 的一个目的所在,也就是将一组选出的 hosts 映射到 task.
		Roles 基于一个已知的文件结构，去自动的加载某些 vars_files，tasks 以及 handlers。基于 roles 对内容进行分组，使得我们可以容易地与其他用户分享 roles 。
		Handlers 最佳的应用场景是用来重启服务,或者触发系统重启操作.除此以外很少用到了. 相对来说用task多点
		定义在task上的用户优先级最高，一个play中可以定义多个task任务。
		
		task中的格式
			推荐使用更常见的格式:”module: options” ,本文档使用的就是这种格式.
			下面是一种基本的 task 的定义,service moudle 使用 key=value 格式的参数,这也是大多数 module 使用的参数格式:

			tasks:
			  - name: make sure apache is running
				service: name=httpd state=running
			
		比较特别的两个 modudle 是 command 和 shell ,它们不使用 key=value 格式的参数,而是这样:
		tasks:
		  - name: disable selinux
			command: /sbin/setenforce 0
		使用 command module 和 shell module 时,我们需要关心返回码信息,如果有一条命令,它的成功执行的返回码不是0, 你或许希望这样做:

		tasks:
		  - name: run this command and ignore the result
			shell: /usr/bin/somecommand || /bin/true
	
		
		- name: check and scp new .jar
		  copy: src=/home/release/.jenkins/jobs/pre-emanagerpay-jar-pipeline/workspace/target/e-manager-pay.jar dest=/usr/local/java_project/webapps/ force=yes backup=yes
		  
		  
		实际的yml脚本中需要使用模块指令
			  wait_for模块，等待一个事情发生，然后继续。
					对象是端口的时候start状态会确保端口是打开的，stoped状态会确认端口是关闭的;
					对象是文件的时候，present或者started会确认文件是存在的，而absent会确认文件是不存在的。
					
			  copy模块，主要用于将管理主机上的数据信息传送给多台主机
			  
			  pause模块，在playbook执行的过程中暂停一定时间或者提示用户进行某些操作
					常用参数：minutes：暂停多少分钟，seconds：暂停多少秒，prompt：打印一串信息提示用户操作
					
			  fetch模块,目标主机上拉取文件到本机，则使用fetch模块。
			  
			  file模块,文件或文件夹的操作
					path参数：必选项，定义文件/目录的路径
					state参数：这个参数很灵活，不同的模块都会有state参数，模块不一样它的值也不一样，后面回结合实例说明state参数不同的值作用和它的作用。
						touch：创建文件
						absent：删除文件或目录
						directory：创建目录
						link：创建软链接。如果创建硬链接state的值就是hard
					src参数：做软链接或硬链接的时候指定链接的源文件
					
			ansible register，当我们需要判断对执行了某个操作或者某个命令后，如何做相应的响应处理（执行其他 ansible 语句），则一般会用到register 。有点类似于shell 中的$?		
					ignore_errors这个关键字很重要，一定要配合设置成True，否则如果命令执行不成功
					register注册变量,ansible的模块在运行之后，其实都会返回一些”返回值”，我们可以把这些返回值写入到某个变量中，这样我们就能够通过引用对应的变量从而获取到这些返回值了，这种将模块的返回值写入到变量中的方法被称为”注册变量”，
					
			- name: check jar is closed
			  wait_for: port=20886 delay=5 state=absent   					#wait_for模块，5秒后检查20886端口已经移除后继续
			  
			- name: shutdown jar
			  shell: /usr/local/java_project/bin/java_project.sh stop		#执行普通的shell脚本
			  
			- name: wait on user input
			   pause: prompt="Warning! Detected slight issue. ENTER to continue CTRL-C a to quit" 
			   
			- name: timed wait
			  pause: seconds=30
			 
			- name: Retrieve SSH key from reference host
			  fetch:
				src: "/home/{{ user }}/.ssh/id_rsa.pub"
				dest: "files/keys/{{ user }}.pub"

			- name: Create a register to represent the status if the /dev/sda6 exsited
			  shell: df -h | grep sda6
			  register: dev_sda6_result
			  ignore_errors: True
			  tags: docker
			 
			- name: Copy docker-thinpool.sh to all hosts
			  copy: src=docker-thinpool.sh dest=/usr/bin/docker-thinpool mode=0755
			  when: dev_sda6_result | succeeded
			  tags: docker
			 
			- name: test shell
				shell: "echo test > /var/testshellfile"
				register: testvar
		    - name: shell module return values
			  debug:
			  var: testvar
			  
			  使用”register”关键字将当前shell任务的返回值写入了名为”testvar”的变量中，第二个任务使用debug模块输出了第一个任务中的注册变量的值，





## 消息队列相关

### 概要

1. 消息队列的主要特点：异步处理，流量削峰。(减少请求响应时间和解耦)

   使用场景：将比较耗时而且不需要即时（同步）返回结果的操作作为消息放入消息队列，一般为非重要操作

   解耦：只要保证消息格式不变，消息的发送方和接收方并不需要彼此联系，也不需要受对方的影响


   消息存储：消息的Header信息(投递次数等基本信息)，消息的Body(主要内容)，消息的投递对象。
				
   消息的可靠性
      一定要业务处理成功之后，再返回确认消息。否则消息就丢失了。
      分布式系统的三个重要点，服务框架，消息中间件，和数据访问层。


2. 消息模型只有两种：

   * 点对点方式（point-to-point） 一旦被消费，消息就不再在消息队列中,消息一次性消费

     主要组件：Message Queue 存贮消息，Sneder 发送消息，receive接收消息.
     		   步骤：1.Sender Client发送Message到Queue
     				   2.receiver Client从Queue中接收消息
     				   3.receiver Client 返回ack到Quere,确认消息已接收
     producer与consumer没有时间上的依赖，producer可以在任何时刻发送信息到Queue，而不需要知道consumer是不是在运行

   * 发布/订阅方式（publish/subscriber Messaging）
     用于多接收客户端的方式.
     可能存在多个consumer，并且consumer与producer存在时间上的依赖(consumer只能接收他创建以后producer发送的信息)。
     作为subscriber ,在接收消息时有两种方法，1.destination的receive方法（同步），2.实现message的listener 接口的onMessage 方法(异步)。

3. 阻塞队列
      是一个支持两个附加操作的队列。这两个附加的操作支持阻塞 的插入和移除方法，相对输入输出也是一样
      1）支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。
      2）支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。 

4. 消息的幂等处理
		由于网络原因，生产者可能会重复发送消息，因此消费者方必须做消息的幂等处理，常用的解决方案有：
		1. 查询操作：
			查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select是天然的幂等操作；
		2. 删除操作：
			删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个) ； 
		*3. 唯一索引，防止新增脏数据。
			比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可）；
		*4. token机制，防止页面重复提交。
			业务要求： 页面的数据只能被点击提交一次；发生原因： 由于重复点击或者网络重发，或者nginx重发等情况会导致数据被重复提交；解决办法： 集群环境采用token加redis(redis单线程的，处理需要排队)；单JVM环境：采用token加redis或token加jvm内存。处理流程：1. 数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间；2. 提交后后台校验token，同时删除token，生成新的token返回。token特点：要申请，一次有效性，可以限流。注意：redis要用删除操作来判断token，删除成功代表token校验通过，如果用select+delete来校验token，存在并发问题，不建议使用；
		5. 悲观锁——
			获取数据的时候加锁获取。select * from table_xxx where id='xxx' for update; 注意：id字段一定是主键或者唯一索引，不然是锁表，会死人的悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用；
		6. 乐观锁——
			乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。乐观锁的实现方式多种多样可以通过version或者其他状态条件：1. 通过版本号实现update table_xxx setname=#name#,version=version+1 where version=#version#如下图(来自网上)；2. 通过条件限制 updatetable_xxx set avai_amount=avai_amount-#subAmount# where avai_amount-#subAmount# >= 0要求：quality-#subQuality# >= ，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高；
		*7.分布式锁——
			还是拿插入数据的例子，如果是分布是系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多多个系统，也就是分布式系统中得解决思路。要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供)；
		8.select + insert——
			并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了。注意：核心高并发流程不要用这种方法；

5. 消息队列	
		异步的事务——回调机制
			生产者在发送消息的时候，注册一个回调函数，这样生产者便不用停下来等待确认了，而是可以一直持续发送消
			息，当消息到达消息队列服务器的时候，服务器便会调用生产者注册的回调函数，告知生产者消息发送成功还是失败，进而做进一步的处理，从而提高了并发量


### Rocketmq相关

   RQ的基本组成包括nameserver、broker、producer、consumer四种节点，前两种构成服务端，后两种在客户端上。

   消息模型：消息模型包括producer,consumer,broker三部分。producer生产消息，consumer消费消息，broker存储消息(消息存储与转发)，broker可以是集群部署，其中topic位于broker中
   rocketmq的消息模型简单来说，producer投递消息到topic中的各个队列，各消费者组订阅topic,消费者组中的消费者并行消费队列中的消息

   rocketmq采用的是发布-订阅的模式，不须要每一个消费者维护本身的消息队列，生产者将消息发送到topic,消费者订阅此topic 读取消息

   Producer:　生产者生产消息到broker,broker接受消息写入topic，需要ack确认。消费同理

   Topic:表示一类消息的集合，每一个主题包含若干条消息，每条消息只能属于一个主题，是RocketMQ进行消息订阅的基本单位

   生产者组：同一类Producer的集合，类似集群
   消费者组：同一类Consumer的集合，消费者组的消费者实例必须订阅彻底相同的Topic，
            组消费暂时不看

   消费者组的消费者实例必须订阅完全相同的Topic。RocketMQ 支持两种消息模式：集群消费（Clustering）和广播消费（Broadcasting）
   消费模式	创建consumer的时候，指定同一groupname，即在同一组中
   广播模式：全部接收，发送给订阅topic中的组的每个消费者consumer
   集群模式：消息均摊，天然的负载均衡，订阅topic中的其中一个consumer组中的成员均分消息，自带多层次（组和组成员）负载均衡，一个topic/queue可以被多个组消费，组之间也是均分的，

1. 运行流程
      生产者发送msg发送给namesrv（路由管理,功能类似zookeeper），namesrv路由后通过broker存储转发消息到consumer集群，再消费给consumer(配合listener订阅topic)
      开启vpn会使rocketmq的broker的ip地址发生变化

      客户端是先从NameServer寻址的，得到可用Broker的IP和端口信息，然后自己去连接broker。

2. rocketmq producer 发送消息失败，总是连接不上服务器地址(忽略)
		1. 目前这种写法Rocket默认开启了VIP通道，VIP通道端口为10911-2=10909。若Rocket服务器未启动端口10909，则报connect to <> failed。
		2. 解决方式：增加一行代码producer.setVipChannelEnabled(false);

7. boot中使用mq
      引入依赖rocketmq-spring-boot-starter，并添加相关配置(暂未使用)
			rocketmq.producer.group = producer_bank2
			rocketmq.name-server = 127.0.0.1:9876
			
			@Component
			@RocketMQMessageListener(topic="topic_notifymsg",consumerGroup="consumer_group_notifymsg_bank1") 
			public class NotifyMsgListener implements RocketMQListener<AccountPay> {
				@Autowired
				AccountInfoService accountInfoService;
				@Override
				public void onMessage(AccountPay accountPay) {
					log.info("接收到消息:{}", JSON.toJSONString(accountPay)); 
					AccountChangeEvent accountChangeEvent = new AccountChangeEvent();
					accountChangeEvent.setAmount(accountPay.getPayAmount());
					accountInfoService.updateAccountBalance(accountChangeEvent); 
					log.info("处理消息完成:{}", JSON.toJSONString(accountChangeEvent));
				} 
			}

8. 系统中使用mq
		小鱼儿中是@PostConstruct中初始化mq的监听配置
		defaultMQPushConsumer.registerMessageListener(new MessageListenerConcurrently() {});
		// Consumer对象在使用之前必须要调用start初始化，初始化一次即可<br>
		消费者初始化，defaultMQPushConsumer.start();
		生产者也是同样的初始化， defaultMQProducer.start();
		
		本地使用
		@Component("egjTradeConsumer")
		public class EgjTradeConsumer {
		@PostConstruct
			public void startConsumer() throws Exception {
				DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(consumerGroup);
				consumer.setVipChannelEnabled(false);
				consumer.setNamesrvAddr(nameServerAddress);
				consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);
				consumer.setMessageModel(MessageModel.CLUSTERING);
				consumer.subscribe(MQConstants.TOPIC_EGJ_ORDER, "*");
				consumer.registerMessageListener(new MessageListenerConcurrently() {
				//目前是只请求一次预付交易接口，用并发消费，后期若出现多次有序请求的场景，请重新评估消费模式是否应当选择顺序消费
					
				@Override
				public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {
					StringBuffer sb = new StringBuffer();//拼装出异常短信的发送内容，人工检查
					for(MessageExt msg:msgs) {JSONObject data = JSONObject.parseObject(new String(msg.getBody()));}
				}
			}
		}

9. 原理相关

   * 1.其中的nameserver相当于zookeeper，服务的发现，
   * 2.nameserver和broker之间是，b发30s心跳发送和2min检测断开
   * 3.客户端指定nameserver的地址，与其长连接，连接后定时查看topic路由信息，默认时间30s
   * 4.broker的持久化，。采用的是ext4文件系统，同步刷盘：写入后告知生产者；异步：收到消息就告知生产者，再异步从内存写入到磁盘
   * 5.消费者中同一个Group中的实例，在集群模式下，以均摊的方式消费；在广播模式下，每个实例都全部消费。
   * 6.广播消费，一条消息被多多个同组中的consumer消费，
   * 7.messagequeue是一个长度无限的数组，offset是下标
   * 8.消费去重，保证幂等性，重复情况少，一般建议业务去重

10. 杂记录

    * 下载源码后进入主目录，执行mvn -Dmaven.test.skip=true clean package install assembly:assembly   -p release-all  -U
         在target目录下拿到压缩包，使用相关的即可

    * mq 的 .RemotingConnectException     需要设置producer.setVipChannelEnabled(false); 

    * 监听者监听不到消息，换用tomcat不要重复启动
      <Host appBase="webRoot" autoDeploy="true" name="localhost" unpackWARs="true"></Host>

      由于tomcat的热部署原因引起的，启动了多个同样消费端，消息均分配，则会丢失消息，将autoDeploy="true"，改为false

    * pushconsumer是通过监听器获取消息，属于长连接，实时被动接受消息
      pullconsumer是主动去获取信息，属于短连接，主动拉取消息

      mq核心的存储概念    commitlog：消息体存放  
                        consumequeue：记录消息的位置
		
      每个broker\consumer与nameserver集群中的每个节点建立长连接，定期从nameserver取topic路由信息，并向broker建立长连接，发送心跳

11. 存储层设计：数据文件+索引文件
    三种发送消息的机制：  1、同步发送 	返回消息发送成功
                        2、异步发送 	有消息丢失的可能		
                        3、oneway		kafak日志收集 肯定丢失
    broker设计，横向扩容两个，支持大数据量：
      topic 主题
      queue 队列
      topic中存在多个消息队列，有下标

12. rockeqmq的producer三种方式(忽略)：
		1、normalProducer(常用，其他两个先不看)
		2、orderProducer 搭配consumer的实现order监听，一般一轮顺序消费指定一个队列，同一消息下的指定队列
		3、transactionProducer（两阶段提交）
			第一阶段：将消息传递给mq，但是消费端不可见，但是数据已经发送到broker
			第二阶段是本地回调处理，成功返回commit_message，则再broker上对消费者可见，失败为rollback_message，消费端不可见

	producer的主要配置：重试次数，maxsize，timeout

13. 消费端 和 生产者的实例
	每种类型的监听者都应该有自己的实现逻辑，增加统一的message处理接口，每个server端的consumer，都有自己的对应的接口实现，直接调用即可
	在消费端：可以自定义listener实现源码中的messagelistenerconcurrently接口，在consumer端，监听消息的时候，可以定义一个接口，
	专门用于处理监听到的消息，这样可以解耦业务处理，这个思想不错

    顺序消费：针对同一队列中的顺序
	 生产端是messagequeueselector，同时指定队列，消费端是注册监听，实现的接口是messagelilstenerorderly
    
    * 消费者实例
      ```
         @Component("egjTradeConsumer")
         public class EgjTradeConsumer {
            private Logger logger = LoggerFactory.getLogger(EgjTradeConsumer.class);
            @Value("${rocketmq.namesrvAddr}")
            private String nameServerAddress;
            @Value("${rocketmq.consumer.order.egj}")
            private String consumerGroup;
            private DefaultMQPushConsumer consumer;
            
            @PostConstruct
            public void startConsumer() throws Exception {
               try {
                  consumer = new DefaultMQPushConsumer(consumerGroup);
                  consumer.setVipChannelEnabled(false);
                  consumer.setNamesrvAddr(nameServerAddress);
                  consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);
                  consumer.setMessageModel(MessageModel.CLUSTERING);
                  consumer.subscribe(MQConstants.TOPIC_EGJ_ORDER, "*");

                  consumer.registerMessageListener(new MessageListenerConcurrently() {
                     //目前是只请求一次预付交易接口，用并发消费，后期若出现多次有序请求的场景，请重新评估消费模式是否应当选择顺序消费
                     @Override
                     public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {
                        for(MessageExt msg:msgs) {
                           String topic = msg.getTopic();
                           String tag = msg.getTags();
                           String msgBody = new String(msg.getBody());

                           JSONObject data = JSONObject.parseObject(msgBody);
                           logger.info("egjTradeConsumer:入口：{}",data.toJSONString());
                        }
                        return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
                     }
                  });
                  consumer.start();
                  logger.info("EgjTradeConsumer启动！");
               } catch (Exception e) {
                  logger.error("启动egjTradeConsumer失败",e);
                  throw e;
               }
            }
            
            @PreDestroy
            public void shutdownConsumer() {
               consumer.shutdown();
            }
         }
      ```
    * 生产者实例
      ```
         @Component("producer")
         public class Producer implements ProducerParent{
            private final Logger logger = LoggerFactory.getLogger(Producer.class);
            private DefaultMQProducer defaultMQProducer;
         
            @Value("${rocketmq.producer.order.citic}")
            private String producerGroup;
            @Value("${rocketmq.namesrvAddr}")
            private String namesrvAddr;
            
            //Spring bean init-method
            @PostConstruct
            public void init() throws MQClientException {
               // 参数信息
               logger.info("DefaultMQProducer initialize!");
               logger.info(producerGroup);
               logger.info(namesrvAddr);
               // 初始化
               defaultMQProducer = new DefaultMQProducer(producerGroup);
               defaultMQProducer.setNamesrvAddr(namesrvAddr);
               defaultMQProducer.setInstanceName(String.valueOf(System.currentTimeMillis()));
               defaultMQProducer.setVipChannelEnabled(false);
               defaultMQProducer.start();

               logger.info("DefaultMQProudcer start success!");
            }

            //Spring bean destroy-method
            @PreDestroy
            public void destroy() {defaultMQProducer.shutdown();}
            public DefaultMQProducer getDefaultMQProducer() {return defaultMQProducer;}

            // ---------------setter -----------------
            public void setProducerGroup(String producerGroup) {this.producerGroup = producerGroup;}
            public void setNamesrvAddr(String namesrvAddr) {this.namesrvAddr = namesrvAddr;}
         }

      //发送消息
      //最主要的是topic，tag和key目前看只是备注作用
      public static void sendMessage(ProducerParent producer, String topic,String tag,String key, String content) {
         try {
               Message msg = new Message(topic, tag ,key, content.getBytes());
               SendResult sendResult = null;

               sendResult = producer.getDefaultMQProducer().send(msg);

               // 当消息发送失败时如何处理
               if (sendResult != null
                     &&sendResult.getSendStatus() != SendStatus.SEND_OK) {
                  logger.warn("消息发送成功 key:{},content:{}",key,content);
               }else{
                  logger.warn("消息发送成功 key:{},content:{}",key,content);
               }
            } catch (Exception e) {
               logger.warn("消息发送异常  key:{},content:{}",key,content,e);
            }
	   }

      ```


14. mq解决数据一致性,mq解决分布式事务
      调用远程服务，扣优惠券，扣余额，如果调用成功——>更改订单状态可见，失败->发送mq消息，进行取消订单
         1、具体的思路，本地的步骤1和2，全部写完，
         2、开始分步调用远程的服务，优惠券，成功了进行下一个，失败直接异常抛出
         3、扣余额，返回失败，异常抛出
         4、扣库存，返回失败，异常抛出
         5、try catch全部步骤，将相关的回退参数传过去，异常发送mq，每个server端监听，这里使用了topic和tag的监听
         6、全部成功，更改订单状态
         7、后续会有定时器轮询，长时间订单状态没改变的，有可能是mq发送失败，补发消息
         8、consumer端，最好的是使用统一的消息处理接口，重点重点重点：监听业务有没有操作记录，有记录回退，没有记录不用处理，即每个server都需要做相应的处理，处理逻辑看情况和操作日志来定
         9、消费端可以启动多个消费端，这样可以将处理的实现类，注入到通用的consumer配置中，实现，可配置处理
         
      mq处理事务
         处理多个服务之间的一致性：后者失败，前者补偿操作，事务不要太多
            1、创建不可见的订单，执行多个小事务，并且这些是同时监听一个回滚topic的，只要有一失败就发送消息到mq，其他的监听到并进行回滚操作
            2、需要执行另一个查询订单状态的任务，异常的全部取消

      使用mq解决分布式事务：
         service-foo 和 service-bar   
         1.foo进表后，增加event表，事件进入mq(写入名为"foo-success-queue"队列)
         2.从mq中获取event，调用bar,然后bar进表。若bar异常，就回滚，将原event写入Failure queue中，
         3.foo将作为消费者从失败队列中找到event，根据eventId找到对应的foo，然后操作回滚

      ***mq解决分布式事务的思路*****
         这里两个事件应该是有依赖关系的，所以增加了event表控制顺序，
         若果是没有依赖关系的，最好是  分支事务id-总事务id，每个分支事务自己独立执行，失败就回滚，并发送mq，更新总事务失败，其他服务监听到总事务失败自行回滚
      ***mq解决分布式事务的思路*****


15. 生产者的可靠性投递和消费者的幂等性去重

      可靠性
         持久化+job补偿
         在发之前将mq消息持久化，发送成功后再删除，开启job定时扫描长时间存在的消息并重新发送
         为保证消息发送可靠性，重要的消息发送前，一定要有消息的备份，创建消息临时表
         防止宕机的情况,事务结束后想发补偿mq消息或者后期job轮询发送，

                
		消息幂等性处理：
         1.数据库联合唯一键去重， 2.业务状态查询去重，3.redis这种分布式锁，redis-key的去重（唯一流水号）
			去重原则：1、幂等性 2业务去重
			去重策略：去重表机制（key去重，主键比如msgid），业务拼接去重策略（比如指纹码、唯一流水号等，联合索引）
			高并发下去重：redis去重（key天然支持原子性且要求不可重复），但是由于不在一个事务内，所以需要适当的补偿策略
							1、利用redis的事务、主键（我们必须把全量的操作数据，都存放到redis，然后定时和db同步）
							2、利用redis和关系型数据库  一起做去重机制
			
      消息重试机制：
			生产者：消息重投重试，保证数据可靠
			自定义属性，msg.putUserProperty("a","1")单独设置属性，传递后面是map形式,msg.getUserProperty("a")
	
			消费者：消息异常处理，失败返回重试标记consumer_later，1s,5s,10s执行重试的策略
					同一个组中两个consumer，其中一个终端，没有返回ack和status，则重试另一个consumer，msg.getconsumertimes，获取重试次数，也可以获取上次的originid（第一次是null）


      不允许补偿（大额转账）
         关于mq的消费去重，可以增加mq消息去重表，groupname，tag ，key，msgid作为联合唯一键，用于并发时主键冲突，
         对于想要重试的消息，直接返回false即可，mq有重试的机制，一般超过3次人工处理

         在下单支付后，第三方有异步的回调操作，可以把回调结果发送给mq，解耦支付的业务耦合。
         消费者要去重和幂等处理，不仅仅以消息id区别，还可以在消息体中增加唯一业务编号
         对于失败的消息需要失败重试处理（失败次数），保证消息完全消费

      为加快发送速度，可以使用线程池发送消息，excutorservice.submit(new runable(){})

      幂等性 去重处理即可，一般建议业务去重   
      https://www.cnblogs.com/wxd0108/p/6038543.html  


16. 消息过滤(忽略)
		1、简单是订阅使用tags过滤即可
		2、在broker端的机器，开启mqfiltersrv，自定义filter的java文件实现msssagefilter,实现match方法，mixall.file2String(),subscribe(a，b，c)即可，filter不允许有中文

17. 集群模式
		2m模式，其中一台宕机，消费可能延迟。起不来就消费补了上次的信息，重点不是双m获得同样的信息，是分配的（系统目前使用，架构简单，性能高）
		多m/s，异步复制 ，主从数据同步的方式：异步复制  m刷盘（未刷盘或尚未同步，m挂了，信息丢失），返回ack给客户端，同时异步复制到s上
		多m/s，同步双写，写完m，刷盘，同步s成功后返回ack，数据不丢失，性能低，安全高

      Broker的集群部署，一句话总结其特征就是：不支持主从自动切换、slave只能读不能写，所以故障后必须人工干预恢复负载。

      多 master 模式
         多个 master 节点组成集群，单个 master 节点宕机或者重启对应用没有影响。
         优点：所有模式中性能最高
         缺点：单个 master 节点宕机期间，未被消费的消息在节点恢复之前不可用，消息的实时性就受到影响。
         注意：使用同步刷盘可以保证消息不丢失，同时 Topic 相对应的 queue 应该分布在集群中各个节点，而不是只在某各节点上，否则，该节点宕机会对订阅该 topic 的应用造成影响。


      Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。
      Broker 部署相对复杂，这里的集群模式主要值得是 Broker

18. mq的部署问题：
      1、4.2.0需要jdk8，需要设置rocketmq_home 启动.cmd格式的就ok
      2、项目中netty-tcnative jar找不到，需要将maven的中无关修饰去掉
      3、启动broker   mqbroker -n localhost:9876 制定nameserver地址即可

19. rocketmq  后台可以新增消息发送   topic选项新增消息发送
	 rocketmq常见异常 	https://blog.csdn.net/weixin_43439073/article/details/95746775




### Rabbitmq

1. rabbitmq(忽略)
	server.port=9898
	spring.application.name=spring-cloud-stream
	spring.rabbitmq.host=192.168.174.128
	spring.rabbitmq.port=5672
	spring.rabbitmq.username=guest
	spring.rabbitmq.password=guest

	#指定输入通道对应的主题名
	spring.cloud.stream.bindings.myInput.destination=minestream
	spring.cloud.stream.bindings.myOutput.destination=minestream

	windows 系统环境标量设置好了后，一定要重新运行cmd测试

	rabbitmq  映射2个端口：15672是Web管理界面的端口；5672是MQ访问的端口。

	安装了rabbitmq测试了下，
		1.安装erl环境包
		2.直接安装rabbitmq服务端
		3.打开插件才能看到后台管理页面
		4.发送和接收要绑定统一个通道名
		
		
		sink.class 和 source.class   通道绑定暂不细看
		
		@EnableBinding用来绑定多个消息通道		启动消息驱动功能
			@EnableBinding(value = { Sink.class, MsgSender.class })   创建发送实例
		@StreamListener	将某方法注册为A消息通道的监听处理器
		
		
	rabbitmq-plugins enable rabbitmq_management   开启插件后才能使用后台管理页面

	ERROR: node with name "rabbit" already running on "xxx"
	tasklist | find /i "erl"指令，发现进程
	taskkill /pid 13776 -t -f		kill进程
	重启目录下rabbitmq启动程序 rabbitmq-server.bat	

2. 


## 开源框架代码学习

### git-zheng相关

1. fluent-validator 是一款百度开源的简洁验证框架，了解一下可以了
   ```
   	Result result = FluentValidator.checkAll().on("go", new NotNullValidator("cjhanu")).doValidate().result(toSimple());
   ```

2. 使用的 toSimple方法是import static com.baidu.unbiz.fluentvalidator.ResultCollectors.toSimple 导入进来的

3. 实现jsp等静态文件的项目共享，可以打成jar包依赖，然后在servlet初始化时解压jar到相应的目录即可：
   JarUtil.decompress(jarPath, resources);

4. 继承AbstractRoutingDataSource类，配合aop原理，切到使用自定义标签的方法上，拿出自定义数据源的标签中的数据源的值，放到相应的treadlocal中，每次执行sql都会选取对应的数据源执行操作，动态数据源切换

   但是这个由于有多个数据源导致我们只能管理默认的数据源的事务！，涉及回滚的时候，尽量读从库，写主库，主备份到从
   实现数据源的切换注意事务的问题，必须要指定事务管理器在aop标签之后

5. 和meta-inf同级的静态文件是可以直接访问的，类似于web-app文件下

###  tcc-transaction 开源框架（忽略）
	TCC解决分布式事物的思路是，一个大事务拆解成多个小事务。
	https://my.oschina.net/sword4j?tab=newest&catalogId=5734750		整体详细的文档

	1  TCC的主要优点有
	因为Try阶段检查并预留了资源，所以confirm阶段一般都可以执行成功。
	资源锁定都是在业务代码中完成，不会block住DB，可以做到对db性能无影响。
	2.TCC的实时性较高，所有的DB写操作都集中在confirm中，写操作的结果实时返回（失败,因定时job，略有延迟）
		
		Try 阶段 创建root事务，添加参与者，全部try完再决定是confirm还是cancel
		
		根事务(root)----对应-----分支事务(branch)	
		
		all branch 事务成功，则修改all branch事务状态为confirm，并执行confirm逻辑
		如果某个分支业务逻辑成功，则删除对应的事务记录；(有一个失败，所有事务状态是cancel，执行cancel逻辑)
		如果业务逻辑执行成功，则删除对应的事务记录。(all branch ok，执行root 的confirm或者cancel逻辑),执行成功后，然后删除事务记录。
		
		每个线程绑定了一个线程变量，存放的是一个事务队列(peek()返回队列的头元素)。
		
		*********
		主try-两次dubbotry(各有两次，一次代理(supports)，一次实际调用)-主confirm-两个dubbo的confirm(各两次，一次代理(supports)，一次实际)
		拦截两次，第一次的代理拦截是在主线程中添加dubbo的两个confirm动作
		子事务的执行是在主conmit中调用的，其中有参与者列表participants，这样第二次进拦截器走的是provide操作，就是commit操作
		
		transaction的xid通过dubbo隐式传参获取，两次 一次是try阶段  还有一次是confirm阶段
		
	主线程会注册一次代理的接口，然后资源拦截器会拦截，添加进参与者（封装的是原接口的方法和参数，添加进当前线程队列，使用的是全局id和分支id），将事务id-上下文注册到缓存中，
	然后进入远程调用，根据传递过来的txid，将confirm等上下文存入缓存

   在接口上也需要实现Compensable，这样才会代理，但是注册参与者的时候是注册的接口(commit生成实例是通过工厂类绑定的代理类)


### ruoyi(忽略)

	session和缓存相关
		查看唯一登录的情况，登录踢出的功能，就是实现用户信息和session绑定的情况，
		其实在SecurityManager中设置的CacheManager组中都会给Realm使用，即：真正使用CacheManager的组件是Realm。

		踢出的功能，是引入了一个 Deque<Serializable> deque，其中存放的是用户的sessionid，存放在相应的cache中，
		当队列中size大于限制数，踢出对应的session， 将对应的session中设置kickout为true，当用户访问时，只要当前session中
		的kickout为true，就logout并重定向。

		shiro中常用的sessionmanager，DefaultWebSessionManager（可以不依赖容器管理会话）
		cache是缓存的操作，session是会话的操作

		Queue strings = new ArrayDeque<Serializable>();  队列顺序有点像栈
		可以用linkedlist代替，但是list的顺序是按照添加的顺序

		核心的路径匹配就是perm.implies(permission)

		使用自定的redis作为缓存需要自己实现
		CacheManager，主要管理缓存，提供一个缓存实例即可
		Cache，操作具体的实例来完成
		userRealm.setCacheManager(cacheManager);  表示开启了内存缓存
		Cache<Object, AuthorizationInfo>，将权限信息缓存进了cache中，下次可以直接使用
					
		SnakeYAML可以加载yaml文件，存进map中使用

		动态数据源，在查询的时候拦截选定指定数据源，后面执行数据操作的时候匹配相应的数据源，主要的是继承AbstractRoutingDataSource
		动态数据源，在一个事务中，只能管理对应的事务管理器的事务，属于其他事务管理器的事务，不起作用，
		https://mp.weixin.qq.com/s/X0ZIRj71HUjhpb5xf89YKQ   多数据源方案
		动态数据源的额数据嵌套(通过代理对象来调用目标对象，而在代理对象中有事务相关的增强处理(begin,do,commit))
　
		nested exception is org.springframework.core.NestedIOException: 
		Failed to deserialize object type; nested exception is java.lang.ClassNotFoundException: com.xiaoyuer.pay.web.SessionData
		以redis为缓存中心，当系统需要从redis中获取缓存信息的时候，其中的类需要反序列化出来，系统中需要有相应的类	


## Spring+Mvc+Mybatis

### Spring
    spring 主要分为5个模块：
		1.IOC	依赖注入
			BeanFactory接口是spring框架的核心，实现了容器许多核心的功能
			Context模块构建于核心模块之上，其中applicationcontext是该模块的核心接口，扩展了BeanFactory，添加了bean生命周期控制，jndi获取，资源加载等。
		2.AOP
			区别于aspect框架，不是同一个事物框架
			借助该功能，实现了声明式事务的功能
		3.数据访问和集成
		4.web及远程操作
			可通过listener或Servlet初始化Spring容器，将其注册到web容器中 
		5.测试框架

		spring的三个核心的组件  bean，core，context
			bean组件(在org.springframework.beans包下)
				bean的定义	BeanDefinition 		完整描述了配置文件中<bean>节点的所有信息，当spring成功解析<bean>节点后就会转化为一个beanDefinition对象，给后续操作
				bean的创建	典型的工厂模式，顶级接口是BeanFactory
				bean的解析	对spring的配置文件的解析
				
			context组件(在org.springframework.context包下)
				作用就是给spring提供一个运行时的环境。作为spring的ioc容器，基本上整合了spring的大部分功能
				ApplicationContext是Context的顶级父类。继承了BeanFactory，resourceloader接口。
				
			core组件

			ioc的一些扩展点
				主要有BeanFactoryPostProcessor 和 BeanPostProcessor    分别在构建BeanFactory 和Bean时调用
				InitializingBean 和DisposableBean					   分别在bean实例创建和销毁时调用
				FactoryBean

#### 	动态代理,AOP

1. 两种动态代理的模式：   
         1、jdk动态代理	   基于接口实现，使用invocationhandler接口

               创建实现类完成目标对象的代理，生成方便，性能稍差
               jdk实现的代理要求被代理类基于统一的接口，基于反射机制实现   缺省方案
               只有public修饰的接口才能实现jdk动态代理，但是public static也不行

   		2、cglib动态代理  基于类的继承，使用MethodInterceptor接口

               创建子类，是对目标类扩展的子类 字节码是实现，可读性差点，无接口限制
               底层采用ASM字节码生成框架，效率比使用Java反射效率要高。
               使用Enhancer类（CGLib中的字节码增强器）对类进行扩展
               
               不能对目标类中的final或者private方法进行代理,因为该方法不能被继承，所以无法动态生成被代理类的子类

               当需要代理的类不是代理接口的时，Spring会切换为使用CGLIB代理，也可强制使用CGLIB,强制cglib代理	<aop:config>的 proxy-target-class属性设为true

         3.javassist(忽略)		
               javassist 字节码生成，提供的动态代理工场，生成代理类，调用 MethodHandler ，使用java反射性能比较低，主要是在反射上面。自己生成代理类 直接调用
				   通过字节码生成代替反射，性能比较好(推荐使用)
               javassist是jboss的一个子项目，其主要的优点，在于简单，而且快速。直接使用java编码的形式，而不需要了解虚拟机指令，就能动态改变类的结构，或者动态生成类。

   * jdk动态代理
      jdk代理基于接口实现，创建代理类实现invocationhandler接口，实质上是对被代理类的增强
         1、统一的接口 
         2、被代理类 
         3、代理核心（需要注入目标类） 然后使用proxy调用，

      jdk的动态代理需要绑定代理对象和真是对象的关系，可以封装在代理的操作类中，就是说再invocationhandler中直接加上代理创建方法
      cglib创建代理，只需要一个非抽象的类就能实现动态代理，主要代理操作类是实现MethodInterceptor接口，通过加强这Enhancer实现代理
      
      *Enhancer是cglib中使用频率很高的一个类，它是一个字节码增强器，可以用来为无接口的类创建代理。会根据给定的类创建子类，并且所有非final的方法都带有回调钩子。

      **>>>核心思想<<<**
            接口和实现类都可以生成代理类
            
            基于jdk的接口的动态代理，其实接口和实现类都可以被代理，因为private Object target;  这里被代理类是obj，接口本身也是属于obj的
            本质上是invoke中拿到method后的处理方式：
               1.实现类处理方式是，用反射，用实例调用对应的方法，所以被代理实例要实现同一接口，这样能对应上
               2.接口的处理方式是，拿到对应的方法后，可以进行些其他的操作，比如，根据接口找到映射的xml中的sql再执行，

            这两种处理方式实际场景就是
               1.创建一个实现类的代理类，做增强处理
               2.创建一个接口的代理类，可以做rpc，也可以是mapper调用
      **>>>核心思想<<<**


   * ciglib(忽略)
         关键点 1.定位连接点   2，增强中编写切面代码 说白了就是定位切入点，将逻辑织入
         
         cglib采用底层的字节码技术，创建一个类的子类，在子类中采用方法拦截的技术，拦截所有父类方法的调用，并顺势植入横切逻辑
         通过切面将切点和advice增强组装起来，aop就是负责实施切面的框架，将切面定义的逻辑织入切面指定的连接点
            
         ```
            ciglib代码实例
            public class UserServiceCglib implements MethodInterceptor {
               private Object target;
               public Object getInstance(Object target) {
                  this.target = target;
                  Enhancer enhancer = new Enhancer();
                  enhancer.setSuperclass(this.target.getClass());
                  enhancer.setCallback(this);// 设置回调方法
                  return enhancer.create();// 创建代理对象
               }
               /**
                  * 实现MethodInterceptor接口中重写的方法，回调方法
                  */
               @Override
               public Object intercept(Object object, Method method, Object[] args, MethodProxy proxy) throws Throwable {
                  System.out.println("事务开始。。。");
                  Object result = proxy.invokeSuper(object, args);
                  System.out.println("事务结束。。。");
                  return result;
               }
            }
         ```
         两种方法相似，通过geProxy方法生成代理对象，执行代理的逻辑类（需要实现一个接口，一个是invocationhandler,一个是MethodInterceptor），决口定义的方法就是代理对象的逻辑方法。可以控制真实对象的方法


2. 动态代理的核心就是代理对象的生成	Proxy.newProxyInstance(classLoader, proxyInterface, handler)

3. 执行顺序，通知执行顺序：前置通知→环绕通知连接点之前→连接点执行→环绕通知连接点之后→返回通知→后通知→(如果发生异常)异常通知→后通知

4. 动态代理的代码实现
   实现invocationHandler接口的代理处理的核心类中，有三个重点的东西：
   		1、构造函数，将代理对象传入
   		2、invoke方法，实现aop的增强的所有逻辑
   		3、getproxy方法，获取代理类

   		所以动态代理的方法实现三个要点：被代理类，代理处理核心handler，生成代理类

      //常用的代理生成
      ```
         public class InvocationHandlerTest implements InvocationHandler {
            private Object target;
            Object bind(Object i) {
               target = i;
               Object proxyObj;
               proxyObj = Proxy.newProxyInstance(target.getClass().getClassLoader(), i.getClass().getInterfaces(), this);
               return proxyObj;
            }
               
         //参数中的接口，提供代理方法和具体的返回类型，handler是代理核心实现，这里可以将Proxy.newProxyInstance也封装进来
            @Override
            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
                  System.out.println("before method excute!");
                  method.invoke(target, args);
                  System.out.println("after method excute!");
                  return null;
            }
            //测试类
            public static void main(String[] args) {
                     Cls c = new Cls();
                     InvocationHandlerTest pxy = new InvocationHandlerTest();
                     Itf itf = (Itf)pxy.bind(c);
                     itf.printSth("Hello");
               }
         }
      ```

      //简单的代理生成，养成使用代理工厂的思想，将一套东西封装起来
      ```
      public class Proxyfactory {
         public Object target;//目标类，一般需要实现接口
         public Proxyfactory(Object target){
            this.target=target;
         }
         public  Object newProxyInstance(){
         //返回代理实例
         return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(),
            new InvocationHandler() {
            @Override
            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
                  System.out.println("kaishiqian");
                  method.invoke(target, args);
                  System.out.println("kaishihou");
                  return null;
                  }
            });
         }
      }
      ```

5. 动态代理的作用：
   		主要用来做方法的增强，代理其实是在原实例前后加了一层处理，对原来的类进行了包装，并使用代理类来执行
      	在方法执行前后做任何你想做的事情（甚至根本不去执行这个方法），
      	因为在InvocationHandler的invoke这个方法中，你可以直接获取正在调用方法对应的Method对象，
      	具体应用，比如，aop，添加日志，做事物控制等。

   		AOP就实现了把这些业务需求与系统需求分开来做。这种解决的方式也称代理机制。	
   		多个不具有继承关系的对象引入一个公共行为，会造成代码重复，这时提现aop的好处		

   		需要被代理的类，需要有个实现了invocationhandler的处理核心handler（针对方法前后添加相关操作），最后再有proxy生成代理的对象，

6. 使用注解aop的相关条件
   	1、依赖相关的spring和aop的jar
​		 2、被拦截的类和aop的aspect类都必须加入spring的容器
   	3、配置先关的拦截规则：

   ```
      //@Aspect相关
      @Aspect
      public class AopAdviser {
         @Pointcut("execution(* test.Person.go(..))")  
         public void say(){
         };
         @Before("say()")
         public void lie(){
         System.out.println("前置执行");
         };
         @After("say()")
         public void gone(){
         System.out.println("后置执行");
         };
      }
      #spring中开启注解标签<aop:aspectj-autoproxy/>  
   		
   ```
      execution(* com.springboot.impl.UserServiceImpl.printUser(..))
				1.execution 表示在执行的时候 ，拦截里面的正则匹配的方法：
				2.＊表示任意返回类型的方法：
				3.(..)表示任意参数进行匹配。
   		
      在@Aspect中，@after最终都会执行
			如果发生了异常,异常通知@afterThrowing会被触发,返回通知 @afterReturning 不会被触发
			获取参数
				JoinPoint类型参数对于非环绕通知而言， Spring AOP 会自动地把它传递到通知中：
				对环绕通知而言，可使用 ProceedingJoinPoint类型的参数。
				
      		在aspect中使用@Around，Object obj=pjp.proceed()这个返回的就是被切方法实际返回的对象，这个可以实际用来添加调用日志的记录。

   
7. 在面向切面编程的思想里面，把功能分为核心业务功能，和周边功能。
	核心业务，比如登陆，增加数据，删除数据都叫核心业务；
	周边功能，比如性能统计，日志，事务管理等等
	周边功能在Spring的面向切面编程AOP思想里，即被定义为切面在面向切面编程AOP的思想里面，
	核心业务功能和切面功能分别独立进行开发然后把切面功能和核心业务功能 “编织” 在一起，这就叫AOP
	从而组织切面，把各类通知织入到流程当中 (在切面类中操作各种before、after等操作)

8. Aop，Aspectj
		SpringAOP 基于方法拦截的 AOP ，换句话说 Spring 只能支持方法拦截的 AOP
		注：有一点非常重要，Spring的AOP只能支持到方法级别的切入。换句话说，切入点只能是某个方法。

		Spring中有4种方式去实现 AOP 拦截功能。
			1.使用 @AspectJ 注解驱动切面			（常用）					   @Aspect 创建切面
			2.使用 XML 配置 AOP						（复制，少用）				<aop:aspectj-autoproxy />    <aop:aspect>中定义切面类
			3.使用 ProxyFactoryBean 和对应的接口实现 AOP
			4.使用 AspectJ 注入切面。
		
		
		对于方法1，主要是以某个类的某个方法作为切点，用动态代理的理论来说，就是要拦截哪个方法织入对应AOP通知。
		@Around  环绕通知，它将被最原有方法，允许你通过反射调用@Around环绕通知取代它原有方法。  
		jp.proceed(),方法灵活，功能多(环绕通知=前置+目标方法执行+后置通知，proceed方法就是用于启动目标方法执行的)
		

		Proceedingjoinpoint
			JointPoint是程序运行过程中可识别的点，用作AOP切入点。JointPoint对象则包含了和切入相关的很多信息。比如切入点的对象，方法，参数等。
			Proceedingjoinpoint 继承了 JoinPoint。是在JoinPoint的基础上暴露出 proceed 这个方法。proceed很重要，这个是aop代理链执行的方法。
			这也是@Around 和其他的前后置通知的重要区别

		spring4版本执行顺序，aop的执行顺序 (待定)
      切面,切点入口-around(before插入其中)-after-afterReturning，  spring5版本不一样，after和afterReturning顺序有变化
				around before advice
			before advice
				target method 执行
				around after advice
			after advice
			afterReturning

		@Around(value = "test.PointCuts.aopDemo()")
		public void around(ProceedingJoinPoint pjp) throws  Throwable{
			System.out.println("[Aspect1] around advise 1");
			pjp.proceed();
			System.out.println("[Aspect1] around advise2");
		}

		aspectj支持编译期织入且不需生成代理类。spring 集成了aspectj,但其不属于spring aop的范围。


	aspectj中  切点直接声明在增强方法出，叫做匿名切点。想复用就使用@Pointcut命名一个。两种方法
   方法1.指定空切点，后续直接使用切点位置
      @Component  
      @Aspect
      public class ValidateAspectHandler {
         Logger logger = LoggerFactory.getLogger(ValidateAspectHandler.class);
      
         @Pointcut("@annotation(com.xiaoyuer.util.validator.annotation.XYEValidator)")				//通过动态代理生成一个切面(这切面相当于一个拦截器，工作环境)
         public void annotationPointCut(){};

         @Around("annotationPointCut()")  
         public Object validateAround(ProceedingJoinPoint joinPoint){}
         }

         @Before ("annotationPointCut()") 
         public void before() { doSomething()}
      }
   
   方法2.指定实切点，后面可能要重复指定切点
      @Aspect
      @Component("withdrawHandleTmpAspect")
      public class WithdrawHandleTmpAspect {
         
         @Around(value = "execution(public com.xiaoyuer.pay.framework.lang.Result com.xiaoyuer.fpp.op.service.intf.IWithdrawService.submit(..))")
         public Result around(ProceedingJoinPoint pjp){}
         
      }

	切接口，实现统一调用修改。
      通过切接口的方法，这里可以改参数操作两次核心方法，
         do（分两个执行，逻辑差不多，应该往上提，上多，提到切面中，小改动某些参数后执行），pjp.proceed 执行两次

         @Around(value = "execution(public com.xiaoyuer.pay.framework.lang.Result com.xiaoyuer.fpp.op.service.intf.IWithdrawService.submit(..))")
            //分两部分提，一部分是老电商管家余额，一部分是e管家余额
            Object[] citicWithdrawParams = new Object[] {pjp.getArgs()[userResultIndex],citicAccountRemain,new String[] {"Citic"}};
            String egjWithdrawRemain = String.valueOf(Arith.formatMoney(MoneyUtil.money2Long(withdrawMoney) - MoneyUtil.money2Long(citicAccountRemain)));
            Object[] egjWithdrawParams = new Object[] {pjp.getArgs()[userResultIndex],egjWithdrawRemain,new String[] {"Egj"}};

         Result citicRes = (Result)pjp.proceed(citicWithdrawParams);
         if(!citicRes.isSuccess()) {
            logger.error("citic提现失败，不再执行egj");
            return citicRes;
         }
         return (Result)pjp.proceed(egjWithdrawParams);


	execution表达式
		execution (* com.sample.service.impl..*.*(..))
			1、execution(): 表达式主体。
			2、第一个*号：表示返回类型，*号表示所有的类型。
			3、包名：表示需要拦截的包名，后面的两个句点表示当前包和当前包的所有子包，com.sample.service.impl包、子孙包下所有类的方法。
			4、第二个*号：表示类名，*号表示所有的类。
			5、*(..):最后这个星号表示方法名，*号表示所有的方法，后面括弧里面表示方法的参数，两个句点表示任何参数。


	多切面执行（责任链模式）
		Aspect1类加入@Order(1),Aspect2 类加入@Order(2),Aspect3加入Order(3)，再次对其进行测试，得到
		before 1 
		before 2 
		before 3 
		method.do
		after 3 
		afterReturning 3 
		after2 
		afterReturning 2 
		after1 
		afterReturning 1

	针对切面，可以变参数执行两次切点核心内容
		@Aspect
		@Component
		public class MultiAspect {
			@Around(value = "execution(public * com.interface.do(..))")
			public Result around(ProceedingJoinPoint pjp) throws Throwable {
				MethodSignature signature = (MethodSignature)pjp.getSignature();
				String[] parameterNames = signature.getParameterNames();//参数名
				boolean condition=true;//虚拟条件，方便显示，无实际意义
				int index = ArrayUtils.indexOf(parameterNames, "userPhone" ,0);
				
				if(condition) {
					return (Result)pjp.proceed(); //满足条件直接放行
				}
				if(condition) {
					Object[] params = new Object[] {new String[] {"Citic"}
					};
					return (Result)pjp.proceed(params);
				}else {
					//特定条件下，分两步走，一个改参数走，一个走原来
					Object[] egjWithdrawParams = new Object[] {
							pjp.getArgs()[index],
							new String[] {"Egj"}
					};
					Result citicRes = (Result)pjp.proceed(citicWithdrawParams);
					if(condition) {
						return citicRes;
					}
					return (Result)pjp.proceed(egjWithdrawParams);
				}
			}
		}

9. 数据库特定操作代理实现(了解即可)
 ```
   public class TransactionHandler implements InvocationHandler {
    private Object targetObject;
    public Object newProxyInstance(Object targetObject){
        this.targetObject = targetObject;
        //使用Proxy类，通过反射得到一个动态的代理对象
        return Proxy.newProxyInstance(targetObject.getClass().getClassLoader(),
                targetObject.getClass().getInterfaces(), this);
    }

     //在invoke方法中做一些其他操作
    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        Connection conn = null;
        Object ret = null;
        try {
            // 从ThreadLocal中取得Connection
            conn = ConnectionManager.getConnection();
            if (method.getName().startsWith("add")
                    || method.getName().startsWith("del")
                    || method.getName().startsWith("modify")) {
                // 手动控制事务提交
                ConnectionManager.beginTransaction(conn);
            }
            // 调用目标对象的业务逻辑方法
            ret = method.invoke(targetObject, args);
            if (!conn.getAutoCommit()) {
                // 提交事务
                ConnectionManager.commitTransaction(conn);
            }
        } catch (ApplicationException e) {
            // 回滚事务
            ConnectionManager.rollbackTransaction(conn);
            throw e;
        } catch (Exception e) {
            if (e instanceof InvocationTargetException) {
                InvocationTargetException ete = (InvocationTargetException) e;
                throw ete.getTargetException();
            }
            // 回滚事务
            ConnectionManager.rollbackTransaction(conn);
            throw new ApplicationException("操作失败！");
        } finally {
            ConnectionManager.closeConnection();
        }
        return ret;
    }
 ```	

10. 


#### Spring的事务管理

* 基础介绍
      只有runtimeexception，才能让spring自动回滚事务@transactional
      不使用事务就是直接提交 dao照样进行数据操作，没有事务控制，直接自动commit
      
      在jdbc3.0后引入了新特性:保存点（savepoint），可以将事务分割多个阶段，方便指定回滚到事务的特定保存点。
      事务只能被提交或者回滚(或回滚到某个保存点后提交)

      spring事务管理spi的抽象层主要包括3个接口，分别是
			platformtransactionmanager:  负责commit或者rollback事务，不同框架提供不同实现类，jpa、datesource等manager
			transactionDefinition: 定义了传播属性，隔离级别等
			transactionStatus: 代表事务的具体运行状态以及还原点，使得异常回滚事务的方式更具可控性，继承了savepointManager接口

      spring事务和数据库连接：
			当spring事务方法运行时，会产生一个事务上下文，
			该上下文在本事务执行线程中针对同一个数据源绑定了一个唯一的数据连接，所有被该事务上下文传播的方法都共享这个数据连接
			这个数据连接从数据源获取到返回给数据源都在spring的掌控之中。

      常见异常，数据库中有记录，但是之前没查到，这种一般是事务还没提交就查询的原因。


1. spring aop 异常捕获原理：被拦截方法需显式抛出异常，并不能经任何处理，这样aop代理才能捕获到方法的异常，才能进行回滚，
   默认情况下aop只捕获runtimeexception的异常，但可以通过配置来捕获特定的异常并回滚 

	jdk的动态代理    public static Object newProxyInstance(ClassLoader loader,Class<?>[] interfaces,InvocationHandler h)
				       通常参数1和被代理的类是同一个loader，这里被代理类被封装进了InvocationHandler实现类中
	
   代理的目的是调用目标方法时可以转而执行InvocationHandler中的invoke方法。就是对调用方法的一种增强实现

2. 事务的处理
   * a.  若是service层处理事务，那么service中方法中不做异常捕获，或者在catch语句中最后增加throw new RuntimeException()语句，以便让aop捕获异常再去回滚，并且在service上层（webservice客户端，view层action）要继续捕获这个异常并处理
   * b.  在service层方法的catch语句中增加：TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();语句，手动回滚，这样上层就无需去处理异常（项目中的旧做法）

3. 配置spring的事务模板的时候，transactionTemplate需要指数据库的事务管理器transactionManager
   ```
   <bean id="netpayTransactionTemplate" class="org.springframework.transaction.support.TransactionTemplate">
      <property name="transactionManager">
         <bean class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
               <property name="dataSource" ref="dataSource" />
         </bean>
      </property>
   </bean>
   ```
4. 		

5. 两种事务管理方式：
      编程式事务，允许用户在代码中精确定义事务的边界，侵入业务代码，使用 TransactionTemplate,直接事务模板处理，可以返回自定义的result。
      声明式事务，基于AOP，有助于用户将操作与事务规则进行解耦。@transactional

      //配置事务管理器
      <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
         <property name="dataSource" ref="dataSource" />
      </bean>   

      原子性：事务是一个原子操作，由一系列动作组成。事务的原子性确保动作要么全部完成，要么完全不起作用。
         Spring事务管理的核心接口是PlatformTransactionManager 				
         TransactionStatus status = this.transactionManager.getTransaction(TransactionDefinition definition); 返回接口的实现类，
         definition中定义的了一些基本的事务属性，根据指定的传播行为返回当前活动的事务或创建一个新的事务


   * 1.编程式事务管理
     		可使用PlatformTransactionManager，spring中使用了TransactionTemplate模板操作，代码级别的rollback

     ```
     <bean id="transactionTemplate" class="org.springframework.transaction.support.TransactionTemplate">
         <property name="transactionManager" ref="transactionManager"></property>
         <!--定义事务隔离级别,-1表示使用数据库默认级别-->
         <property name="isolationLevelName" value="ISOLATION_DEFAULT"></property>
         <property name="propagationBehaviorName" value="PROPAGATION_REQUIRED"></property>
     </bean>
     ```

       ***小鱼儿操作***
       ```
      //创建模板
         this.jdbcTemplate = new JdbcTemplate(this.dataSource);
         this.transactionTemplate = new TransactionTemplate(new DataSourceTransactionManager(this.dataSource));
         //执行事务操作(编程式)
            this.transactionTemplate.execute(new TransactionCallback() {
                  public Object doInTransaction(TransactionStatus status) {
                     jdbcTemplate.update(insert, oneByOne.getBizType(), oneByOne.getBizId(), oneByOne.getMethod());
                     return null;
                  }
               });
               
         //spring的事务模板，使用匿名内部类并进行封装
         Object tmpResult = this.transactionTemplate.execute(new TransactionCallback(){
            public Object doInTransaction(TransactionStatus status){
            return serviceCallback.invoke();
            }
         }); 
         使用TransactionCallback()可以返回一个值。若使用TransactionCallbackWithoutResult则没有返回值。
       ```

      
      编程式手工回滚(忽略)
         TransactionAspectSupport.currentTransactionStatus().setRollbackOnly()
         用在serviceA中try住操作常规B的catch中手动设置回滚，throw运行异常等效

         事务中的保留点，事务处理中设置的临时占位符，可对其发布回退。回退部分事务，回退到某个占位符

      测试事务的demo(忽略)
         ApplicationContext ctx ＝ new ClassPathXmlApplicationContext("spring-cfg.xml");
         JdbcTemplate jdbcTemplate = ctx.getBean(JdbcTemplate.class);
         PlatformTransactionManager transactionManager =ctx.getBean(PlatformTransactionManager.class);
         TransactionDefinition def = new DefaultTransactionDefinition();
         TransactionStatus status = transactionManager.getTransaction(def);
         try {
         //        jdbcTemplate.update();     			//sql执行
               transactionManager.commit(status);
            } catch (Exception ex){
               transactionManager.rollback(status)
            }
 
   * 2.声明式事务管理

     * 一种是基于tx和aop命名空间的xml配置文件（现在不用了，过时）

       ```
         <tx:advice id="advice" transaction-manager="transactionManager">
            <tx:attributes>
            <tx:method name="insert" propagation="REQUIRED" read-only="false"  rollback-for="Exception"/>
            </tx:attributes>
         </tx:advice>
         <aop:config>
            <aop:pointcut id="pointCut" expression="execution (* com.gray.service.*.*(..))"/>
            <aop:advisor advice-ref="advice" pointcut-ref="pointCut"/>
         </aop:config>
       ```

         aop:advisor 与 aop:aspect都可以配置aop
         advisor只持有一个Pointcut和一个advice，而aspect可以多个pointcut和多个advice


     * 一种是基于@Transactional注解	（主流方法）
       需要开启通用注解标签<context:annotation-config />
       <tx:annotation-driven transaction-manager="transactionManager"/>

       ```
       @Transactional(rollbackFor=Exception.class)
       public void insert(String sql, boolean flag) throws Exception {
           dao.insertSql(sql);
           // 如果flag 为 true ，抛出异常
           if (flag){
           	throw new Exception("has exception!!!");
           }
       }
       ```
      
     * 事务的拦截器transactionInterceptor(忽略) 
       ```
         <context:annotation-config />
         <context:component-scan base-package="com.oumyye"/>
         
         <bean id="logInterceptor" class="com.oumyye.aop.LogInterceptor"></bean>
         <aop:config>
            <aop:pointcut expression="execution(public * com.oumyye.service..*.add(..))" id="servicePointcut"/>
            <aop:aspect id="logAspect" ref="logInterceptor">
                  <aop:before method="before"  pointcut-ref="servicePointcut" />
            </aop:aspect>
         </aop:config>
       ```

6. 

7. Spring配置文件中关于事务配置总是由三个组成部分，分别是DataSource、TransactionManager和代理机制这三部分，无论哪种配置方式，一般变化的只是代理机制这部分。

8. **非事务声明方法调用事务声明方法，则事务失效**，事务自调用失败
   spring事务自调用失败
		只有代理对象的相互调用，AOP 有拦截的功能，才能执行事务注解提供的功能。自调用是没有代理对象存在的，所以其注解功能也就失效了。

   使用了@Transactional的方法，被同一个类里面的方法调用， @Transactional无效。比如有一个类Test，它的一个方法A，A再调用Test本类的方法B（不管B是否public还是private），
   但A没有声明注解事务，而B有。则外部调用A之后，B的事务是不会起作用的。（经常在这里出错）

   一个service中声明事务方法，调用了另外一个service中的声明事务方法，则被调用的方法事务也还起作用，事务不太建议放到2个见service中。
   目前只能从外部方法开始另一个事务，否则内部调用的常规方法   先这样理解
   service内部的多个方法默认等效一个事务管理的常规方法，从外部的有效，内部想要有效需要注入自身的接口实现（不推荐），

   其他解决方案，在A方法中使用((Service)AopContext.currentProxy()).B() 来调用B方法，这样能实现切入

   spring提供了自动代理机制，让容器自动生成代理，使用beanPostProcessor来实现（自动在容器实例化bean时为匹配的bean生成代理实例）
	在内部调用方法的时候没有走aop代理，需要注入自身的bean才能实现aop代理的功能。

   sping的事务不能回滚
      使用aop代理对象调用service才有事务，内部调用事务无效，类自身调用无效，必须要走spring的代理对象Aop调用，
      只有代理对象调用才能触发代理方法，目标对象调用无法触发，类似事务本身调用事务无效的场景。

      方法：1.使用注入的service调用， 2.从springcontext中获取bean(是一个代理对象)，调用也行

      代理类中调用的方法a调用了代理方法b，那么b无效，此时b是目标类调用的
      代理类一次调用方法，只走一次代理，内部的方法是目标对象直接执行的，没有走代理，this.test()，目标对象调用，不走代理方法
      
      当@service标注的bean在容器中检查到有@transaction，则会创建一个代理对象，
      开启事务--->创建sqlsession--->使用jdbc连接执行sql--->最后提交事务
      client-aop动态代理-@transaction-service-@transaction提交或回滚，实质上service是被aop代理了，

      直接走方法调用的事务问题
               https://blog.csdn.net/zknxx/article/details/72585822  
         
         ***包含事务的方法异常后回滚会默认被上级接收到
         ***当前方法自身调用事务方法，事务会失效的，当作内部方法处理，必须重新注入interface执行，获取当前aop的代理，然后通过aop的代理调用，才能生效
         
      内部方法调用事务(忽略)
      <aop:aspectj-autoproxy proxy-target-class="true" expose-proxy="true"/>组合((SelfCallService)AopContext.currentProxy()).selfCallB();
      proxy-target-class为true的是用Cglib动态代理，false的时候启用JDK动态代理

		对于静态static方法 和 public 方法 注解@Transactional会失效
			

9. @Transactional
   建议是打在实现类的public（必须）方法上，不建议在类上声明使用，非必要的方法不需要使用事务管理
   和编程式事务相比，声明式事务不足地方是，后者最细粒度只能作用到方法级别，而编程式事务是代码块级别

   问题解决。1. 使用public访求；2. 写在外部类中，可被调用； 3. 使用注入的方式进行该方法的执行。

   放在类级别上等同于该类的每个公有方法都放上了@Transactional，

   配置在public方法，且被外部调用时才有效，（private和protected方法无效）

   事务配置在public方法上，但该public方法被内部调用时事务也是无效的。目标方法由外部调用，目标方法才由 Spring 生成的代理对象来管理
   
   即使打上@Transactional标签的方法运行中抛出异常，是会往上级走的，上级需要处理

   spring事务的原理
		本质是通过动态代理实现
		1.在 Spring IoC 容器初始化时，Spring 会读入这个注解配置的事务信息，并且保存到 事务定义类里面（TransactionDefinition 接口的子类），以备将来使用。
		2.当运行时会让Spring 拦截注解标注的某个方法或者类的所有方法，将自己编写的代码编织到aop流程中
		3.首先Spring通过事务管理器(PlatformTransactionManager子类)创建事务
		4.启动开发者提供的业务代码，Spring会通过反射调度开发者的业务代码，根据结果正常或者异常，决定commit或者回滚


   @transactional 作用范围，所有public 非静态的方法启用 ，最终生成一个TransactionDefinition
		通过该注解属性配置去设置数据库事务，接着调用业务代码，若没有发生异常，spring数据库拦截器就会帮助我们提交事务（异常有事务定义器判断处理），最终释放数据库连接，

		@transactional 可以放在接口或实现类上，但是推荐放在实现类上(这样可以兼容cglib动态代理)，因为放接口上使得类基于接口代理时才生效。
		spring中事务管理是由事务管理器来完成的，最常用的是DataSourceTransactionManager，它是实现了PlatfonnTransactionManager()接口的类
		
		使用@transactional标注类和方法后，spring的事务拦截器会使用事务管理器的方法开启事务功能，将代码织入spring数据库事务的流程中，事务操作和数据库的资源操作都是封装在aop事务调用中。剩余要做的只是执行SQL而已。(获得数据库连接，修改隔离级别，执行sql，最后自动关闭和提交数据库事务)
    
      指定事务管理器(好像是默认的，不需要配置的，忽略)，
      @Bean
      public PlatformTransactionManager txManager(DataSource dataSource) {
         return new DataSourceTransactionManager(dataSource);
      }


10. 事务的几个重要特性 
      acid 四个特性
	   * 隔离级别
         mysql默认的级别是可重复读(只有幻读的可能)。
			隔离级别越高，性能就越是下降。
			
			可重复读:一个事务在执行过程中可以看到其他事务已经提交的新插入的记录，但是不能看到其他事务对已有记录的更新 
                   这是针对本行数据而言是可重复，针对查询数量而言可能有幻读
		
         幻读和可重复读的区别
            幻读，在一次事务中多查询，数量不对，是统计值
            可重复读是针对数据库的单一条记录，这是数据库存储的值 

         可重复读的实现原理(忽略)
            MVCC的控制方式 ，即Mutil-Version Concurrency Control,多版本并发控制
            事务A在读到一条数据之后，此时事务B对该数据进行了修改并提交，那么事务A再读该数据，读到的还是原来的内容
            保证在当前这个事务内读取的结果不会受到其他事务的影响

			在循环的Propagation.REQUIRES NEW 事务操作中，最好使用try catch 将整个包住，这样不影响别的事务进行



      * 事务传播行为（propagation behavior）。
         场景：	当事务方法被另一个事务方法调用时，必须指定事务应该如何传播
         嵌套事务一个非常重要的概念就是内层事务依赖于外层事务。外层事务失败时，会回滚内层事务所做的动作。
         而内层事务操作失败并不会引起外层事务的回，两个方法之间有个savepoint的概念，这个就是嵌套事务的关键点。
            1、REQUIRED 		
                  内外链接一体的事务， 默认支持当前事务，如果内层发生异常，那么内外事务回滚一致
            2、NESTED 	嵌套事务	
                        内部事务是外部事务的子事务，外部异常回滚会触发NESTED子事务回滚,内部事务的异常回滚不影响外部事务。
                        如果上层无事务，功能等价于PROPAGATION_REQUIRED
                        
                        如果A的MethodA()存在事务，则B的methodB()抛出异常，B.methodB()回滚，
                        如果A不捕获异常，则A.methodA()和B.methodB()都会回滚，
                        如果A捕获异常，则B.methodB()回滚,A不回滚；

            3、REQUIRES_NEW		启动新事务，若上层有事务在运行，则先挂起，直到新事务提交或者回滚才恢复执行，连个独立的事务，互不影



            nested: 在当前方法调用子方法时，若子方法发生异常，只回滚子方法执行过的sql功能，子方法回滚而当前事务不回滚
            传播属性NESTED，启用了保存点技术，
               保存点技术并不是每个数据库都能支持的，如果数据库不予支持，它就会和REQUIRES_NEW一样创建新事务去运行代码，以达到内部方法发生异常时并不回滚当前事务的目的
               使用保存点（save point）技术完成让子事务回滚而不致使当前事务回滚的工作。(若不支持保存点技术，就new 事务操作)
               sql执行会设置标志位，后面有异常会回滚到这个标志位的数据状态
		

		典型的事务问题优化，@Transactional
			1.在一个controller中，两次调用serviceA.do(),注意这里两次调用是两个不同的事务。
			2.过长时间占用事务，尽量不要把io等长时间操作放在serviceA的事务操作中，因为serviceA方法没执行完，数据库资源就不会释放。应该移到上层controller中操作。
				这么一说在controller中只要调用的serviceA中do方法执行完，数据库资源就会释放。
			
11. 事务测试

   实际验证，@Transactional并没有捕获异常的功能，遇到运行异常自己回滚后会向上抛出。除非上层捕获才不影响上层

   1. 外层service没事务  里面有事务  里面抛出异常
		外层不受事务控制，直接进库，里层受事务控制，需要更新
   
   2. try{1.update 2.异常 3.update}catch{}  这种事务1走完会放入事务待提交，2中直接被异常抛出，3不会执行，这样对于别的循环次数没有影响，但是本次，还是1会成功。 

   3. 事务中  try 事务操作的问题,不影响，之后接收到runtimeexception才会回滚，单个try住不影响外层的事务，这也是遍历中常用的操作
	
   4. 外部没有事务，然后循环两个事务操作，两个事务互不影响，上层没有就新开一个

   5. serviceA中 try{ serviceB.do();} catch (Exception e) {}   
		如果B是默认事务，那么B中异常后，即使try了，整体commit还是会异常，因为同一事务已经被标记过回滚了。
		
		https://blog.csdn.net/f641385712/article/details/80445912

		org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only
		
		支付的批量验收事务异常原因是，GateWayBatchServiceImpl.systemGateWay(List<Map<String, String>>)入口开启了事务，后面又用了事务模板transactionTemplate做了回滚标记
      一开始测试无效是因为B跑到了open  两个系统当然是新事物
		
      外层事务有事务，这里第一个执行成功了
		try {
				事务A执行正常
				事务B执行异常
            } catch (Exception e) {}
		
		public void ttest() {
			for (int i = 0; i < 2; i++) {try {netpaytranService.ttest(i);} catch (Exception e) {}}
		}
		
12. 

#### IOC相关，容器，Bean

1. 在spring容器中所有实现了FactoryBean<T>的工厂接口的，都是通过getObject方法返回相应的bean，也是实现bean注册进入容器的

   ```
   @Override
   @SuppressWarnings("unchecked")
   public void afterPropertiesSet() {}

   @Override
   public JobDetail getObject() {
   	return this.jobDetail;
   }

   #用Object[0]来代替null,有时我们需要传递参数的类型，而不是传null，所以用Object[0],实际传递的是带泛型的null，new Class<?>[0]
   
   ```

2. 

3. 扫描注解配置

   ```
      xml中的配置
      <context:component-scan>
         可以在指定的package下扫描以及注册javabean ，(不需要<context:annotation-config>)
         属性base-package去扫描指定包下的class和jar文件，扫描到有@Component @Controller@Service等这些注解的类.有了该标签，就无需配置<mvc:annotation-driven/>
      
      <mvc:annotation-driven/> 
         是mvc 必须的标签,会自动注册
         DefaultAnnotationHandlerMapping ：负责扫描带有@Controller注解的类，给此类设置对应的@RequestMapping
         AnnotationMethodHandlerAdapter ：负责扫描Controller类中的方法，给方法设置对应的@RequestMapping

   ```

   ​	
   读取xml配置测试,测试用(忽略)
   ```
      ApplicationContext ac = new ClassPathXmlApplicationContext("applicationContext.xml");
      RegisterDAO registerDAO = (RegisterDAO)ac.getBean("RegisterDAO");
      如果是两个以上:
      ApplicationContext ac = new ClassPathXmlApplicationContext(new String[]{"applicationContext.xml","dao.xml"});
      或者用通配符:
      ApplicationContext ac = new ClassPathXmlApplicationContext("classpath:/*.xml");
   ```


4. 属性文件载入容器管理，加载属性文件，属性文件加载

   * 1.xml中旧版本加载
   ```
      <bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"> 
            <property name="locations"> 
               <list> 
                  <value>classpath: conf/sqlmap/jdbc.properties </value> 
               </list> 
            </property> 
         </bean>
      然后在xml中使用${jdbc.driverClassName}读取
   ```

   * 2.标签引入   <context:property-placeholder location="classpath:jdbc.properties" />   （推荐)

   * 3.代码读取属性文件

     ```
     ResourceBundle resourceBundle = ResourceBundle.getBundle("filters.redis"); 
     加载classpath下的属性文件获取，resourceBundle.getString("redis.port")直接获取属性文件中的值.
     ```

     PropertiesConfiguration 快速读取属性文件，使用apache的PropertiesConfiguration接收
	   * //先看看Properties
			String propertiesFileName="a.properties";
			Properties props = new Properties();
			props.load(new FileInputStream(propertiesFileName));
			String value =props.getProperties("key");

		* //然后是PropertiesConfiguration
			PropertiesConfiguration propsConfig=new PropertiesConfiguration();
			propsConfig.setEncoding("UTF-8") //默认的编码格式是ISO-8859-1，所以才在读取文件之前先设置了编码格式
			propsConfig.load(propertiesFileName);
			String strValue=propsConfig.getString("key");
			String longValue=propsConfig.getLong("longKey");
			String[] strArray=propsConfig.getStringArray(arrayKey);
			//值得一提的是。propsConfig的默认分割符是','，换句话说，如果值使用','分割，使用getString去取的话是会抛出异常的，因为这被认为是个数组，分割符可以使用setListDelimiter设置。
			...
			三、总结
			告别java.util.Properties。

     

5. 

6. spring配置datasource的方式
   	使用jndi需要在tomcat中增加数据源jar和logjar，并在context.xml中新增resourcejndi配置
      DataSourceBuilder 和 JndiObjectFactoryBean 

      jndi是配置在tomcat中的，常规的数据库配置是在项目的mybatis.xml中配置的

      创建数据源的三种方式，
         1.xml配置一个数据元
         2.使用jndi
         3.代码创建数据源，DriverManagerDataSource(没有提供池化连接，适合单元测试用)
         
   * 1. xml指定数据源，tomcat的context.xml中配置
     			xml中注入配置datasource，需要指定数据源，目前用的是数据源jar是druid版本，在指定一些常规数据库连接属性即可
         ```
            <Resource auth="Container" driverClassName="com.mysql.jdbc.Driver" 
            maxActive="100" 
            maxIdle="40" 
            maxWait="12000" 
            name="jdbc/XiaoyuerProject" 
            password="db_xiaoyuer" 
            type="javax.sql.DataSource" 
            url="jdbc:mysql://192.168.6.251:3306/db_xiaoyuer_xq?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;generateSimpleParameterMetadata=true" 
            username="db_xiaoyuer"/> 
         ```

   * 2. 加载使用服务器上的jndi
     		Spring提供引用JNDI资源的JndiObjectFactoryBean类。
         将jndi配置的数据源引入到spring中
         ```
            <bean id="dataSource" class="org.springframework.jndi.JndiObjectFactoryBean">      
               <property name="jndiName" value="java:comp/env/jdbc/bbt"/>      
            </bean>  
            #Spring 2.0提供了一个jee命名空间，通过jee命名空间，简化J2EE资源的引用。
            <jee:jndi-lookup id="dataSource" jndi-name="java:comp/env/jdbc/bbt"/>
         ```

   * 3. boot数据源加载,加载指定数据库文件中的配置信息
         soa数据库加载使用	
         if ("/".equals(File.separator)) {
            path = "/usr/local/java_project/conf/xye-datasource.properties";
         }else {//windows
            path = System.getProperty("user.dir").replace("\\", "/");//获取当前应用所在目录
            path = path.substring(0, path.lastIndexOf("/"));
            path = path + "/conf/xye-datasource.properties";
         }
         
         具体就是运行文件所在的目录
         C:/Users/xiaoyuer/git/xye-soa-pom/conf/xye-datasource.properties		pc
         /usr/local/java_project/webapps  这是生产jar使用的位置					linux

7. 静态代码块
      静态static代码块在首次调用这个类时才触发，而不是一开始就加载。且只加载一次

      类的static加载
      1.static中加载异常，初次是实际加载异常，后续调用报错：java.lang.NoClassDefFoundError: Could not initialize class com.xiaoyuer.netpay.EStest
      2.EStest.class.getName()不触发static加载，调用属性和方法才会触发

      简单讲，NoClassDefFoundError发生在编译时对应的类可用，而运行时在Java的classpath路径中，对应的类不可用导致的错误。
      maven只是其中一种错误类型，也是最常见的

      注意场景（忽略）
      CommandLineRunner 和 @bean 加载顺序，启动mq先消费，CommandLineRunner的常量尚未加载就消费，导致初始化client异常。

8. 依赖注入
   * @Autowired，by type自动装配，注入对应类型的bean

       如果有一个以上的bean，则可以通过@qualifier注解限定bean的名称(byName方式自动装配),其中@Qualifier不能单独使用
      
       如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据
       如果查询的结果不止一个，那么@Autowired会根据名称来查找。
       如果查询的结果为空，那么会抛出异常。解决方法时，使用required=false

       对集合类型的注入，会将符合类型的所有bean都注入到集合中，这个在注入批量插件时有用（忽略）

   * @resource（name=“personDaoBean”）默认byName装配
     	如有指定的name属性，先按该属性进行byName方式查找装配；其次再进行默认的byName方式进行装配；
     	如果以上都不成功，则按byType的方式自动装配。都不成功，则报异常。

   循环依赖问题
		spring对构造函数配置的bean实例化前提是，入参引用需要准备就绪，如果两个bean都采用构造函数注入，会发生循环依赖，改为属性注入即可


9. spring.xml加载容器上下文的过程

    启动加载web.xml,读取<listener>和<contex-param>节点，创建ServletContext，被WEB项目所有部分都将共享这个上下文

    ```
      默认的servlet加载顺顺序
      <servlet>
         <description>spring mvc servlet</description>
         <servlet-name>springMvc</servlet-name>
         <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>
         <init-param>
               <description>spring mvc 配置文件</description>
               <param-name>contextConfigLocation</param-name>
               <param-value>classpath:config/spring-mvc.xml</param-value>
         </init-param>
         <load-on-startup>1</load-on-startup>
      </servlet>
      <servlet-mapping>
         <servlet-name>springMvc</servlet-name>
         <url-pattern>/</url-pattern>
      </servlet-mapping>
      <context-param>
         <param-name>contextConfigLocation</param-name>
         <param-value>classpath:config/spring.xml,classpath:config/spring-dubbo.xml</param-value>
      </context-param>
      <listener> 
         <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class> 
      </listener> 
    ```

    spring的启动过程：

    * 1、项目部署在web容器中，web容器提供其一个全局的上下文环境ServletContext，为后面的spring IoC容器提供宿主环境；

    * 2、在web.xml中会提供有contextLoaderListener，启动触发容器初始化事件，调用contextInitialized方法监听事件

      	在这个方法中，spring会初始化一个启动上下文WebApplicationContext(根上下文)，这是一个接口类，其实际的实现类是XmlWebApplicationContext。
      	这个就是spring的IoC容器，其对应的Bean定义的配置由web.xml中的context-param标签指定。在这个IoC容器初始化完毕后，spring以WebApplicationContext.ROOTWEBAPPLICATIONCONTEXTATTRIBUTE为属性Key，将其存储到ServletContext中，便于获取；

      加载上下文
          在web.xml中，contextloaderlistener是servlet的监听器，用来初始化加载除dispatcherservlet创建的上下文以外，其他的配置文件整合到spring的容器中，
         ```
            <listener>
               <description>spring监听器</description>
               <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>
            </listener>
            默认加载  /WEB-INF/applicationContext.xml这个配置文件。
            一般我们人工指定，
            <context-param>
               <param-name>contextConfigLocation</param-name>
               <param-value>	
                     classpath:config/spring.xml,
                     classpath:config/spring-mybatis.xml,
                     classpath:config/spring-dubbo.xml,
               </param-value>
            </context-param>

            ContextLoaderListener默认去WEB-INF下加载applicationContext.xml配置。默认的applicationContext.xml和x-servlet.xml文件

    * 3、contextLoaderListener初始化完毕后，开始初始化web.xml中配置的Servlet，这个servlet可以配置多个，

      比如DispatcherServlet上下文在初始化的时候会建立自己的IoC上下文，用以持有spring mvc相关的bean。

      在建立DispatcherServlet自己的IoC上下文时，会利用WebApplicationContext.ROOTWEBAPPLICATIONCONTEXTATTRIBUTE先从ServletContext中获取之前的根上下文(即WebApplicationContext)作为自己上下文的parent上下文。

      这个servlet自己持有的上下文默认实现类也是xmlWebApplicationContext。

      初始化完毕后，spring以与servlet的名字相关的属性为属性Key，也将其存到ServletContext中，以便后续使用。

      这样每个servlet就持有自己的上下文，即拥有自己独立的bean空间，同时各个servlet共享相同的bean，即根上下文(第2步中初始化的上下文)定义的那些bean。	

      ```
         <init-parm>配置在<servlet>标签中,用来初始化当前的Servlet,存放在servletConfig对象中;
            通过getServletConfig().getInitParameter("initParam")的方式获取;
         <context-param>直接配置在web.xml的<web-app>标签中,属于web全局上下文参数,因此存放在servletContext对象中(即application对象);
            通过getServletContext().getInitParameter("contextParam")的方式获取;
         <context-param>搭配ContextLoaderListener监听类使用，是spring默认的初始化容器的监听类，默认是 WEB-INF/applicationContext.xml下的路径，可以通过<context-param>自由配置路径	
      ```

    基本的Listener仅仅在启动时执行顺序是：context- param -> listener -> filter -> servlet 

10. web.xml中 处理内存泄漏的问题（忽略）
      JDK中的java.beans.Introspector类的用途是发现Java类是否符合JavaBean规范，专门用来处理Introspector内存泄漏问题的辅助类。如果有的框架或程序用到了Introspector类,那么就会启用一个系统级别的缓存,此缓存会存放一些曾加载并分析过的JavaBean的引用。当Web服务器关闭时,由于此缓存中存放着这些JavaBean的引用,所以垃圾回收器无法回收Web容器中的JavaBean对象,最后导致
      内存变大。IntrospectorCleanupListener会在Web服务器停止时清理Introspector缓存,使那些Javabean能被垃圾回收器正确回收。Spring自身不会出现这种问题，因为Spring在加载并分析完一个类之后会马上刷新
      JavaBeans Introspector缓存,这就保证Spring中不会出现这种内存泄漏的问题。但有些程序和框架在使用了JavaBeans Introspector之后,没有进行清理工作(如 Quartz,Struts),最后导致内存泄漏
    			
      ```
         <listener>
            <listener-class>org.springframework.web.util.IntrospectorCleanupListener</listener-class>
         </listener>	
      ```
      web.xml 启动停止监听类(忽略)
         ServletContextListener 接口，它能够监听 ServletContext 对象的生命周期，即监听 Web 应用的生命周期。
         当Servlet 容器启动或终止Web 应用时，会触发ServletContextEvent 事件，该事件由ServletContextListener 来处理。在 ServletContextListener 接口中定义了处理ServletContextEvent 事件的两个方法。

      * 当Servlet 容器启动Web 应用时调用该方法。在调用完该方法之后，容器再对Filter 初始化， 并且对那些在Web 应用启动时就需要被初始化的Servlet 进行初始化。 contextInitialized(ServletContextEvent sce)   
      * 当Servlet 容器终止Web 应用时调用该方法。在调用该方法之前，容器会先销毁所有的Servlet 和Filter 过滤器。 contextDestroyed(ServletContextEvent sce)  
         在Container 加载Web 应用程序时（例如启动 Container 之后），会呼叫contextInitialized() ，而当容器移除Web 应用程序时，会呼叫contextDestroyed () 方法。

11. 为什么有了@Component,还需要@Bean呢？?

      java 配置方式：取代xml
		   使用@configuration 和@bean  前者用在类上，相当于xml，后者作用方法上，相当于bean,默认将方法名作为bean的id，可以指定@Bean(name = "dataSource")

      现在基本上是主要java配置+少数属性配置用xml，比如dubbo.xml


      如果想将第三方的类变成组件，你又没有没有源代码，也就没办法使用@Component进行自动配置，这种时候使用@Bean就比较合适了。不过同样的也可以通过xml方式来定义。
      Spring的Starter机制，就是通过@Bean注解来定义bean。
      可以搭配@ConditionalOnMissingBean注解?@ConditionalOnMissingClass注解，如果本项目中没有定义该类型的bean则会生效。避免在某个项目中定义或者通过congfig注解来声明大量重复的bean。
      @Bean相对于@Component，可以注册源码bean，而后者没法做。方法返回对象


      @Component (”user")
      @bean(name="testBean"),如果没有配置name属性，那么将方法名作为bean的名称
      内部时使用AnnotationConfigApplicationContext来构建ioc容器(基于注解,这个一般是测试用)

      @configuration 配合@bean 替代xml中配置，本身相当于标注了@component,可以其他类中直接注入
      @ImportResource,可以加载xml配置,实际案例使用的是dubbo.xml

      ```
         //向spring注册了一个bean。方法名作为bean的id
         @bean	
         public Performer duke(){
            return nre Juggler()
         }

         //引用创建bean
         @bean			
         public Performer duke2(){
            return nre Juggler(duke())					//并不是调用，spring拦截找到该bean
         }
      ```

      可写成＠Component(”role”)，甚至直接写成@Component ，不写的，Spring IoC就默认类名，但是以首字母小写的形式作为 id ，为其生成对象，配置到容器中


12. 静态方法中使用springbean
      静态方法中调用spring bean   无法注入static bean
      原因是Spring容器的依赖注入是依赖set方法，而set方法是实例对象的方法，而静态变量属于类，因此注入依赖时无法注入静态成员变量，在调用的时候依赖的Bean才会为null。
         
      SpringIoC容器是一个管理springbean的容器,IoC容器都需要实现接口BeaFactory ，
      ApplicationContext继承beanfactory接口，大部分springioc是实现applicationcontext接口的实现类。

      ApplicationContextAware接口，通过它Spring容器会自动把上下文环境对象调用ApplicationContextAware接口中的setApplicationContext方法，容器加载完后会把上下文set到类中，可以方便使用

      ```
         @Component
         public class SpringContextUtil implements ApplicationContextAware{
            private static ApplicationContext context;   //是为了静态共享，利用容器的getBean方法获得依赖对象。

            //初始化的时候该方法就会被调用,从而获取 SpringIoC的上下文(applicationContext)
            @Override			#这里不加编译器也能识别，但是标准化的都要加
            public void setApplicationContext(ApplicationContext ctx) throws BeansException {context = ctx;}
            
            public static ApplicationContext getApplicationContext(){return context;}
            public static Object getBean(String beanName){return context.getBean(beanName);}
         }
      ```
      
      Redisclient中 初始化方法    加入spring 容器,初始化后  然后加载到静态属性中
      public static JedisPool jedisPool;
      RedisUtil redisUtil = (RedisUtil)SpringContextUtil.getBean("redisUtil");//手工获取
      
13.  bean条件加载，实例加载
		@Conditional(RedisChooseConfig.class)
		RedisChooseConfig实现Condition接口，获取环境变量，动态加载

		Environment environment = context.getEnvironment();
		String property = environment.getProperty("resRoot");

14. 初始化
      初始化就使用初始化的东西，别整什么servlet的启动啥的

      * springbean中的三种初始化和销毁方法
         1. 使用@PostConstruct 注解
               使用@PostConstruct注解初始化，使用@PreDestroy注解销毁Bean (需要搭配@Component并被spring扫描到才行，启动时执行初始化)
               JDK提供的，不是Spring提供
               销毁方法：@PreDestroy注释，注释回调方法上，销毁Bean之前调用；
         2. Bean实现InitializingBean 接口，在 afterPropertiesSet 中做初始化工作
               实现InitializingBean, DisposableBean这两个接口，并复写afterPropertiesSet()和destroy()方法
               销毁方法：实现DisposableBean接口，调用destroy(...)，销毁Bean之前调用
         3. XML 中使用 init-method 指定 Bean 构造完成后调用的方法(忽略)
               使用init-method和destroy-method配置方法，在xml中注入时补全(不常用)
               销毁方法：Bean定义中包含destroy-method（在XML中标签<bean>的属性）或@Bean(destroyMethod="...")指定的方法，销毁Bean之前调用；
   
      * 其他非主流初始化
            @Bean 和  CommandLineRunner
            @Order注解并不能改变Bean加载优先级，对多个CommandLineRunner是有效的，加在普通的方法上或者类上没有作用
            
            用户扩展CommandLineRunner，进行启动项目完毕之后一些业务的预处理。
            InitializingBean，项目启动时，初始化bean的时候都会执行该方法


      @Bean(initMethod = "start", destroyMethod = "destroy") 指定对应bean中的初始化方法相关
      想在所有 Bean 都初始化完毕后做一次初始化工作，可以使用 ApplicationListener ，监昕 ContextRefreshedEvent
               
      @PreDestroy > DisposableBean > destroy-method

      @DependsOn  是有条件的初始化 
         也可使用 BeanFactoryPostProcessor 和 BeanPostProcessor 来做一些更前置的初始化工作
         BeanFactoryPostProcessor> Constructor > BeanPostProcessor.postProcessBeforelnitialization > @PostConstruct > InitializingBean > init-method

15. BeanFactroy和ApplicationContext（忽略）
        1. BeanFactroy采用的是延迟加载形式来注入Bean的，而ApplicationContext是在容器启动时，一次性创建了所有的Bean。 
        2. BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册
        3. ApplicationContext使用ClassPathXmlApplicationContext和FileSystemXMLApplicationContext，前者默认从类路径下加载配置文件，后者默认从文件系统中装载配置文件。
        在获取ApplicationContext实例后，就可以像BeanFactory一样调用getBean(beanName)返回Bean了。

16. 校验：(忽略)
      主要是后端的服务端校验-validation校验框架，springmvc的校验框架（引入hibernate-validator-4.3.0final.jar,jboss-loggging-3.1.0.CR2.jar,validation-api-1.0.0.GA.jar）
      validator需要注入适配器（配置校验器bean和返回bean，这里使用的是返回properties，个人认为是加入容器，快速读取，{}读取即可），通过mvc注解驱动注入，validator="beanname"
      在形参校验的bean前加@validated 其后需要跟着BindingResult bindingResult，有一个pojo校验就需要跟一个bindingResult,形参顺序一前一后，在对bindingResult进行判断即可。
      多个controller共用一个校验pojo时，pojo中可以指定校验分组，group属性group={validate1.class},校验分组是一个空接口，形参中指定@validated（validate1.class）即可

      spring中的异步调用使用（忽略）
         场景,对于一串操作中的某个耗时的、不影响主流程的，可以异步一个线程单独处理，异步处理
         1.@EnableAsync开启spring异步功能，  2.实现AsyncConfigurer接口配置线程池 3.@Async 方法声明使用异步调用	 
         
      在spring中定时（忽略）
         1.使用＠EnableScheduling注解启动定时任务，2.使用＠Scheduled配置具体的某个任务

17. spring扩展自定义标签，spring中自定义组件标签(忽略)
		1.采用xsd描述自定义标签的元素属性
		2.编写bean定义的解析器
		3.注册自定义标签解析器
		4.绑定命名空间解析器
   
18. ApplicationContext和BeanFactory初始化不同： 
      applicatincontext就叫做spring容器,	
	   beanfactory会缓存bean实例到ioc容器中，缓存原理是一个hashmap实现的缓存器，key是beanname，value是单例bean
	
	   BeanFactory：初始化容器时，没有实例化bean，第一次访问才实例化目标bean
	   ApplicationContext：在初始化应用上下文时就实例化所有的单例bean，时间稍微长点。实际开发中普遍使用前者，后者功能比较少

19. Spring支持如下三种方式创建Bean
		1：调用构造器创建Bean
		2：调用静态工厂方法创建Bean
		3：调用实例工厂方法创建Bean


20. spring中的单例和多例
		singleton（单例）：只有一个共享的实例存在，所有对这个bean的请求都会返回这个唯一的实例。Spring bean 默认是单例模式
		prototype（多例）：对这个bean的每次请求都会创建一个新的bean实例，类似于new。

21. xml中配置spring相关
    1. xml中schema 
         xmlns是默认命名空间，
         xmlns:xsi是标准的命名空间，
         xmlns:aop这种就是自定义的命名空间（1.指定命名空间的名称，2.指定命名空间的Schema文档格式文件的位置）
         其中aop就是命名空间的别名（用以区分文档后面的元素），对应的还有权限定名和schema文件地址。
         命名空间使用全限定名，一般使用一个引用的url地址指定命名空间对应的schema文件。
    2. bean配置
      ```
         如果在<bean>中没有指定bean的id，那么自动将全限定类名作为bean的名称，<bean>中使用的<property>,就是set注入，需要在类中有对应的set方法
            bean注入
            可注入多种对象，list、map、set、实体类、property等
            1.set注入
                  java中的属性命名规范：xxx属性对应setXxx()方法，
                  变量的前两个字母要么全大写，要么全小写，否则xml中set注入会报错。比如iDCode和iCcard。
            2.使用构造函数，类中必须要存在有参构造，可以显示指定入参顺序，不推荐
            3.工厂方法注入 ，不推荐


         bean标签的原理
            xml中定义的bean标签，Spring会解析成一个BeanDefinition(存储bean标签的信息，用来生成bean实例)，这个BeanDefinition就是bean标签对应的javabean。
            spring的  applicationcontext启动时，将配置文件中的<bean>生成一个beandefinition对象（<bean>在spring容器中的内部表示），一个个的BeanDefinition形成了bean的注册列表。
            根据注册表，加载相关的bean，并放在bean缓存池中，供外层的程序调用
      ```
    3. 在两个xml中有包含关系的时候，加载主动包含的那个xml即可(忽略)

22. Bean的生命周期

   java对象的创建过程往往包括 类初始化 和 类实例化 ，
		初始化：静态的（变量，方法，代码块）会被执行，只在类加载的时候执行一次
		实例化：创建一个类的实例对象。可多次实例化，堆中开内存
		在Java对象初始化过程中，主要涉及三种执行对象初始化的结构，分别是 1.实例变量初始化、2.实例代码块初始化 以及 3.构造函数初始化。
		Java要求在实例化类之前，必须先实例化其超类，以保证所创建实例的完整性	


   bean级生命周期的几个常用控制接口，
		beannameaware：是通过set注入当前bean在容器中的bean_ID，
		beanfactoryaware：让bean获得配置文件中对应的配置名称
		InitializingBean	
		DisposableBean

	* BeanFactoryPostProcessor和BeanPostProcessor。两个后置处理器的区别
		**********
			BeanFactoryPostProcessor在bean实例化之前执行，
			之后实例化bean（调用构造函数，并调用set方法注入属性值），
			然后在调用两个初始化方法前后，执行了BeanPostProcessor。
			初始化方法的执行顺序是，先执行afterPropertiesSet，再执行init-method。
		**********

		BeanPostProcessor(bean级别的处理)	
			执行时机:	实例化之后,操作的是具体的bean
						spring容器实例化bean之后，在执行bean的初始化方法(InitializingBean,init-method)前后
			
			使用案例:	@Autowired,ApplicationContextAwareProcessor	
			
			BeanPostProcessor的执行顺序是在BeanFactoryPostProcessor之后
			
		BeanFactoryPostProcessor(BeanFactory级别的处理)	,多个按优先级处理
			
			执行时机：	在beanDefinition加载完成，bean实例化之前执行
			操作对象：	BeanDefinition	针对整个Bean的工厂进行处理,不能实例化操作
			使用案例:	PropertyPlaceholderConfigurer
			

   * springbean生命周期
         实例化对应构造方法，属性赋值对应setter方法的注入；初始化和销毁这两个阶段用户可以自定义
         主要的是4个关键阶段和多个扩展点(实现Aware接口)
            1.实例化 Instantiation		
            2.属性赋值 Populate		
               扩展：检查Spring Awareness 这里是一个扩展点  基本都实现了aware接口
            3.初始化 Initialization			beanpostprcessor，初始化前后自定义初始化逻辑。所有Aware接口的注入就是在这前置完成的。
            4.销毁 Destruction
         bean的生命周期，核心是对bean操作，而用的是工厂模式，所以就是，工厂前，工厂，工厂后，生产前，生产，生产后(各个processer的执行顺序)

         ioc相关
            ComponentScan 代表进行扫描,默认是扫描当前包的路 POJO 包名和它保持一致才能扫描，否则是没有的

            ioc主要是基于BeanFactory 和 ApplicationContext两个接口，后者是前者的子接口之一，并扩展BeanFactory功能，一般用 applicationContext作为Spring IoC容器
            WebApplicationContext也扩展了它ApplicationContext，ClassPathXmlApplicationContext 是 ApplicationContext 的一个子类
            
            通过xml或者注解，资源丁文，载入beanDefinition，初始化bean。
            BeanPostProcessor针对全部Bean,DisposableBean针对spring IoC 容器，其他的接口只针对单一Bean 。


23. 容器后处理器：实现BeanFactoryPostProcessor接口，在spring容器启动之后会查找实现了BeanFactoryPostProcessor接口的bean，并实例化调用  postProcessBeanFactory()方法。
   需加入@component。只处理实现相应接口的地方

	bean后处理器：实现BeanPostProcessor接口，在容器创建了bean对象实例之后，调用bean的初始化方法之前后会调用相关方法。需加入@Component。统一处理的地方
   BeanPostProcessor接口的作用是：我们可以通过该接口中方法在bean实例化、配置以及其他初始化方法前后添加一些我们自己的逻辑。动态代理和aop都是通过该接口实现的
	BeanPostProcessor是spring的后处理器。工厂后处理器是容器级的，仅在应用上下文初始化时调用一次，完成配置文件的加工处理工作。
	该接口的实现类为“后处理器”，一般不由bean本身实现，当spring容器创建bean的时候，可以合理对bean进行加工处理。也就是代理的思想
				
            
24. spring的循环依赖
      场景：循环引用，简单的说就是，A依赖B，B依赖C，C又依赖A，这样形成了一个闭环

      spring对象初始化三个步骤：
         （1）createBeanInstance：实例化，其实也就是调用对象的构造方法实例化对象
         （2）populateBean：填充属性，这一步主要是多bean的依赖属性进行填充
         （3）initializeBean：调用spring xml中的init 方法。
         循环依赖主要发生在第一、第二部。也就是构造器循环依赖和field循环依赖。

      原理：（忽略）
         Spring容器会将每一个正在创建的Bean 标识符放在一个“当前创建Bean池”中，Bean标识符在创建过程中将一直保持在这个池中。
         因此如果在创建Bean过程中发现自己已经在“当前创建Bean池”里时,将抛出BeanCurrentlyInCreationException异常表示循环依赖；
         而对于创建完毕的Bean将从“当前创建Bean池”中清除掉。初始化完的Bean会从池中移除
      

      解决方案：
         1.spring的循环依赖问题：在注入@Autowired 下加@Lazy 注解即可(两边都加比较保险)
            Spring的懒加载是在需要用到bean的时候，就是getBean的时候才创建，这样就不会报BeanCurrentlyInCreationException。
         2.将相互依赖的两个Bean中的其中一个Bean采用Setter注入(也就是属性注入)的方式即可。

25. 属性文件加载，属性文件相关，属性配置相关，属性配置加载

      @PropertySource   
         加载指定的属性文件（*.properties）到 Spring 的 Environment 中。
         可以配合 @Value 和 @ConfigurationProperties，将属性文件与一个Java类绑定，将属性文件中的变量值注入到该Java类的成员变量中。

         @PropertySource可以读取指定的配置文件，@value获取，需要搭配@configuration，等同于在xml中配置properties文件
         @PropertySource(value={"a.properties" , "b.properties"},ignoreresoureNotFound=true)


      @ConfigurationProperties
         属性文件和bean之间的转化
         @ConfigurationProperties注解主要用来把properties配置文件转化为bean来使用的，
         @configurationProperties(prefix="spring.redis")读取属性文件的值
            foo.enabled=false
            foo.security.username=user
            foo.security.password=pwd						#其中security是内部静态类，具有相关的属性
            
         而@EnableConfigurationProperties注解的作用是@ConfigurationProperties注解生效。(不加也行，现在默认加了)
         当然在@ConfigurationProperties加入注解的类上加@Component也可以使交于springboot管理。

         @Component
         @PropertySource("classpath:application.properties")		#添加自定义的属性文件进来,将属性文件加入到容器，后续可以直接用@value注入
         @ConfigurationProperties(prefix = "application.dubbo.demo.server")	#写在类上，省略前缀，匹配后面的名字，但是注意要有set方法
         Class some{}

		   默认从全局配置文件中获取值,如果想加载指定属性文件，就得使用@PropertySource进行加载后再使用@ConfigurationProperties。


         @ConfigurationProperties作用在方法上，属性注入，需要配合@bean	
         @Bean
         @ConfigurationProperties(prefix = "account")
         public AccountProperties accountProperties() {
            return new AccountProperties();
         }


      @value
         @Value搭配Spring EL使用，可扩展高级点的用法。
         @Value("${query_info_url}")	 最常用
         @Value(”#{role.id}”)          // SpringEL中则使用“＃”
         
         @Value("#{'${test.payChannel.pcCode}'.split(',')}")			Spring EL
         private List<String> testPayChannel;

         @Value(”$(database.driverName)”),${}代表占位符，会读取上下文的属性值装配到属性中，是一个spring表达式

         从属性文件中加载list或者map
            blog-top-links={"home":"/home"}
            blog-list=1,2,3

            @Value("#{'${blog-list}'.split(',')}")
            private List<String> pList;

            @Value("#{${blog-top-links}}")
            private Map<String, String> topLinks;
            
            @Value("#{'${key}'.split(',')}")

      @value注解加载属性值的时候支持两种表达式配置
         1.placeholder方式，${...}，括号内饰placeholder
         2。SpEL表达式，#{...},括号内为SpEL表达式


      使用工具类加载属性文件
         使用 Configuration configs = new PropertiesConfiguration(filePath);直接临时读取属性文件，初始化

         PropertiesUtil   自动状态属性类 	  这里就可以在static初始化中加载属性文件，后面直接静态方法调用即可
         InputStream is = PropertiesUtil.class.getClassLoader().getResourceAsStream(propName);
         prop.load(is);


		普通文件的读取输出
      ```
            从classpath下加载
               InputStream resourceAsStream = this.getClass().getClassLoader().getResourceAsStream("cer/nihao_dev.txt");
               InputStreamReader isr = new InputStreamReader(resourceAsStream);
               BufferedReader br = new BufferedReader(isr);
               String lineTxt = null;
               while ((lineTxt = br.readLine()) != null) {
                     System.out.println(lineTxt);
                  }
                  br.close();
               }
      ```

      /**获取文件路径*/
      private static String getRootPath() {
         String oriPath = DSGJ0001.class.getClassLoader().getResource("").getFile();
         if ("\\".equals(File.separator)) {
            oriPath = oriPath.substring(1, oriPath.length());
         } else if ("/".equals(File.separator)) {
            // linux
         }
         oriPath=oriPath.replace("%20", " ");
         return oriPath;
      }



      File in = new FileInputStream(file);  这个是直接路径加载。类似d盘某个路径，不常用，忽略

		boot的配置加载顺序优先级（忽略）
			1.命令行参数
		*2.java:comp/env 的JNDI 属性
			3.Java 系统属性（ System.getProperties()); 
			4.操作系统环境变量
			5.Random ValuePrope1tySource random.*属性值
		*6.jar 包外部的 application-{profie}.properties或pplication.yml (带spring.profile)配置文件：
			7.jar 包内部的 application-{profile}.properties或application.ym(带 spring.profile)配置文件
		*8.jar 包外部的 application.properties或application.yml(不带spring.profile)配置文件：
			9.jar 包内部的 application.properties application.yml(不带spring.profile)配置文件
		*10.Configuration 注解类上的＠PropertySource; 
			11.通过 SpringApplication.setDefaultProperties指定的默认属性。



### Springmvc相关
   典型的MVC框架如 SpringMVC、Jersey 、国人开发的 JFinal

   mvc 模型视图控制器，请求流程 
   	request  -->c(控制器controller)--->m模型处理（dao，service）--->返回c--->v（view）视图渲染
   
   springmvc 请求流程 
      request	-->前端控制器(dispatcherservlet接受请求和响应结果相当于转发器)
               -->处理器映射器(handlerMappering 根据url匹配相应的handler)
               -->返回一个执行链(HandlerExecutionChain包含interceptor和handler(平时controller)，不同handler由不同处理器适配器(handlerAdapter：按特定规则执行handler)调用执行）
               -->返回modelandview到适配器再到前端控制器DispacherServlet
               -->由视图解析器返回view（是个接口，支持不同的view类型）进行视图渲染


1. 

2. ​常用注解
    开启注解，映射器和适配器              <mvc:annotation-driven></mvc:annotation-driven>  
    使用mvc注解驱动，需要添加注解扫描      <context:componet-scan base-package="" />
    
    @controller     即表示该类是Handler处理器。		
       本质是适配器都实现handlerAdapter接口，有多种handlerAdapter实现，比如：requestMappingHandlerAdapter
   
    @requestMapping 实现方法和url的映射	
      本质是映射器都实现handlerMappering接口，有多种实现比如requestMappingHandlerMapping

3. 配合视图解析器会用，配置欢迎页，在web.xml中的欢迎页中也可以使用
   ```
   <mvc:view-controller path="/" view-name="/index.jsp"/>
   ```
4. Spring MVC 中的Servlet加载
   dispatcherServlet中的init-param需要指定contextconfiguration和springmvc.xml，
   默认是加载/WEB-INF/servlet名称—servlet.xml


5. mvc和spring的父子容器

   spring.xml和spring-mvc.xml中配置的是不同的上下文，service交给spring管理，conroller交给spring-mvc管理，
   两个xml中的各自配置各自的属性文件，否则对应的访问不到，

   原理（忽略）
      SpringMVC启动时的配置文件，包含组件扫描、url映射以及设置freemarker参数，让spring不扫描带有@Service注解的类。
      因为servlet-context.xml与service-context.xml不是同时加载，
      如果不进行这样的设置，那么，spring就会将所有带@Service注解的类都扫描到容器中，等到加载service-context.xml的时候，
      会因为容器已经存在Service类，使得cglib将不对Service进行代理，直接导致的结果就是在service-context中的事务配置不起作用，发生异常时，无法对数据进行回滚。



   spring中的子父容器
      父子容器，子容器可以访问父容器中的bean，反之不行，在容器内bean id唯一，但子容器可以有一个和父容器id相同的bean

      spring使用mvc时会产生两个context上下文，它们俩是父子关系
      一个是ContextLoaderListener进行初始化产生的ApplicationContext
      一个是由DispatcherServlet产生的webapplicationcontext

      WebApplicationContext
         ApplicationContext是spring的核心，spring把bean放在这个容器中，在需要的时候，用getBean()方法取出，在web应用中，会用到webApplicationContext(继承自ApplicationContext)
         webapplicationcontext可以获得servletcontext引用,整个web上下文将作为属性放置到servletcontext中，这样web应用就可以访问spring应用上下文。
         spring提供了用于启动webApplicationContext的web容器监听器：ContextLoaderListener,通过容器的上下文参数contextConfigLocation获取Spring配置文件的位置

      ServletContext
         是Servlet与Servlet容器之间直接通信的接口，Servlet容器(tomcat等)启动创建一个ServletContext对象（每个web应用唯一），所有Servlet共享一个ServletContext，Servlet对象可以通过它来访问容器中的各种资源
         在web框架中，每个DispatcherServlet有它自己的WebApplicationContext上下文（私有的，继承了根上下文中的所有东西），WebApplicationContext被绑定在ServletContext上，
         


      Spring MVC 需要初始化 Ioc 容器和DispatcherServlet 请求两个上下文,后者是前者上下文的扩展,这样使得 Spring 各个 Bean 能够形成依赖注入。
      通过实现 ServletContextListener 可以使得在DispatcherServlet 初始化前就可以完成 Spring IoC 容器的初始化，也可以在结束期完成对Spring IoC 容器的销毁 


   	在 web.xml 中使用 Listener 监听器来加载 Spring 的配置，Spring 会创建一个全局的 WebApplicationContext 上下文，即根上下文，保存在 ServletContext 中，
		可以使用工具类取出上下文 WebApplicationContextUtils.getWebApplicationContext(ServletContext）
      从 WebApplicationContext 中可以获得ServletContext 的引用，整个Web 应用上下文对象将作为属性放置到 ServletContext 中，以便 Web 应用环境可以访问 Spring 应用上下文。


6. 

7. springmvc中xml配置拦截器的路径，
   ```
   <mvc:interceptors>
       <mvc:interceptor>
       <!-- 匹配的是url路径， 如果不配置或/**,将拦截所有的Controller -->
       <mvc:mapping path="/**"/>
       <bean class="com.cckj.util.auth.AuthInterceptor"></bean>
       </mvc:interceptor>
       <!-- 当设置多个拦截器时，先按顺序调用preHandle方法，然后逆序调用每个拦截器的postHandle和afterCompletion方法 -->
   </mvc:interceptors>
   ```

8. 静态资源路径
   ```
   <resources mapping="/resources/**" location="/resources/" />
   <resources mapping="/images/**" location="/images/" />
   <resources mapping="/js/**" location="/js/" />
   
   location：本地资源路径，注意必须是webapp根目录下的路径。
   两个*，它表示映射resources/下所有的URL，包括子路径（即接多个/）
   WEB-INF是Java的WEB应用的安全目录。所谓安全就是客户端无法访问，只有服务端可以访问的目录
   ```

   静态资源加载
		**********
			一般是建议资源加载(spring.resources.static-locations)和映射(addResourceHandler.addResourceLocations)相统一，一致。就是都写上，不写使用boot默认。
			资源加载，重名的相对而言 static优先。都是统一加载到一起作为资源，不会重复
			********映射的优先级高于资源加载********
			静态资源 js等要配置映射，否则会被拦截，这个不一定 两种选择  1映射 2默认资源路径

			http://192.168.6.222:8086/static/js/runtime.6a3e6d1f.js 	走的资源映射
			如果是 http://192.168.6.222:8086/js/runtime.6a3e6d1f.js，   走的boot静态资源加载  

			对于静态映射的文件，无需资源加载
			http://192.168.6.222:8086/templates/index.html 需配置资源映射后直接访问(因为不能直接访问WEB-INF下的文件)，不用指定加载目录，映射的优先级高。
			
		**********


9. 重定向和跳转相关，重定向相关
   通过重定向到另一个页面我们能够避免表单的重复提交

   三种重定向方式：
      1. mv级别的重定向:，ModelAndView mv = new ModelAndView("redirect:/404.htm");
      2. return "redirect:/user/show?id="+user.getId() ;
          Spring MVC有个约定， 当返回的字符串带有redirect时候，它就会认为需要的是一个重定向。
            public String addRole(Model model) (){					//ModelMap类似
               model.addAttribute(”id”, role.getid()) ;
               return ”redirect:./do.htm”；
            }
      3. mv.setViewName("redirect:/user/show?id="+user.getId())


   response.sendredirect(url);   
     	对服务器的响应进行重定向。当server作出响应后，client客户端的请求的生存周期就终止了。这个时候再用request.getparameter()或request.getattribute()得到的只能是null。 
      response.sendredirect(“sendredirect.htm?name=sparkwu&e-mail=spark.wu@cobra-tech.com”) 这样可以传值  

   redirect 和 forward 区别
      redirect 地址栏变化，无法共享request 
         return "redirect:queryItem.action" 	#重定向路径，同级类省略类根路径，地址栏改变
         response.sendirect("url")
         
      forward 地址栏不变，共享request 
         return "forward:queryItem.action"
         request.getRequestDipatcher("path").forward(request,response)


      response.sendRedirect(response.encodeURL(url))的好处就是他能将用户的session追加到网址的末尾,
      也就是能够保证用户在不同的页面时的session对象是一致的. 防止某些浏览器不支持或禁用了COOKIE导致session跟踪失败

      request.getRequestDispatcher("/list.jsp").forward(request, response); 
      提交的request做处理完了，分发到下一个JSP页面或者下一个Action继续处理。
      会有forward()和redirect()两种情况，forward()是request中的参数继续传递，redirect()则是重新生成request了。


	
   路径追加（忽略）
      http://www.yu.com/xye-open/open-api/pay/test/cache.htm
      response.sendRedirect("withdrawSubmit");
      http://www.yu.com/xye-open/open-api/pay/test/withdrawSubmit
      实际测试，同一个controller，重定向是在最后一个路径后面追加路径
      

10. 	request.getParameter() 是从浏览器传递到服务器中的参数
        	request.getAttribute() 是服务器代码暂时保留在request的值，这些值在代码中通过setAttribute后才会有值
        	request.getSession().setAttribute("accountNo", accountNo); 中的值相比在modelmap中优先返回
        	this.pageContext.setAttribute("ninini", userInfo);//这里是用在jsp的标签tag中
        	和request.getSession().getAttribute(），优先pagecontext，如果pagecontext为null，则取后者

      使用addviewController()实现无业务逻辑跳转
         registry.addViewController("/").setViewName("forward:/index.html");//这里直接是相当于浏览器发送新请求
         @Configuration
         public  class WebConfig implements WebMvcConfigurer{

            @Override
            public void addViewControllers(ViewControllerRegistry registry) {
               registry.addViewController("test").setViewName("emp/test");
            }
         }



11. servlet的路径配置中
   <url-pattern>/</url-pattern> 会匹配到/login这样的路径型url，不会匹配到模式为*.jsp这样的后缀型url
   <url-pattern>/*</url-pattern> 会匹配所有url：路径型的和后缀型的url(包括/login,*.jsp,*.js和*.html等)

   配置dispatcherServlet的url-pattern有三种方式：
      1、*.action 
      2、/ 
      3、/*    这种不对，当转发到一个jsp时，仍然会由dispatcherServlet解析jsp,不找到handler

12. 

13. 参数绑定，参数接收相关

      ajax 返回前台注意数据暴露(可以在network中看到返回的参数),页面跳转没有数据暴露的问题	
      httpclient的请求属于后端请求，接收端无法获取页面的cookie,httpclient请求的sessionid和页面的请求是不共享的
      请求表单页面创建一个token存入缓存中，带到页面的hidden token中  提交的时候校验


    * 参数绑定:处理器适配器调用springmvc的参数绑定组件（converter）获取形参，执行controller
      简单传入：
         1)、简单类型 @requestparam（value，required，defaultvalue） 不使用则request传入的名称和controller的形参名需一致,默认是绑定到同名查询参数上
         2)、pojo类型，不使用标签，页面的name属性和形参pojo的属性name一致，使用的话应该是@modelattribute
             @RequestParam传空串是可以的，类似这种?cellPhone=&userName=要小心

      高级绑定：
         pojo： 传入pojo中的pojo属性：页面中name="userInfo.name"
         数组：页面中共用相同的name值即可，name="userId"，传到后台就是 Integer[] userIds
         list: userInfo中有个List<pojo> pojoList，页面name="pojoList[index].name"
         map:页面中name="userInfo['name']" -----key-value

		传参
			@requestBody注解常用来处理content-type=application/json类型。默认是application/x-www-form-urlcoded类型，这是key-value形式传参
			数组  支持逗号分割的数组参数，用intp[] intArr接收
			json	ajax中的data： JSON.stringfy(param对象)，后台@requestBody接收  $.post({})
					contentType:”application/json”, //此处需要告知传递参数类型JSON,不能缺少
					data:JSON.stringify(params)'	//将JSON转化为字符串传递

         使用@RequestHeader 来获取请求头中的参数，前段ajax中就headers:{id:'1')
	      在Requestmapping中consumes= MediaType.ALL_VALUE,//接收任意类型的请求体， produces=MediaType.TEXT_PLAIN_VALUE)//限定返回的媒体类型为文本

         json数据交互:
            @requestbody 可以将json串转为java对象，形参绑定
            @responsebody可以将java对象转为json串输出(完全绕过视图解析，返回一个json信息)
            public @ResponseBody UserInfo getUserInfo(@RequestBody Item item){
                  return userInfo;
            }
            请求时key/value对象：在ajax中 data:'name=shouji&price=90';



      io流的参数接收
         BufferedReader in = new BufferedReader(new InputStreamReader(request.getInputStream()));
         StringBuffer strBuff = new StringBuffer();
         String line;
         while ((line = in.readLine()) != null) {
            strBuff.append(line);
         }
         Utils.log("HTTPS,服务器响应结果是: \n" + strBuff.toString());

      ```
         参数转map
         private static Map<String, String> getParametersStartingWith(ServletRequest request)
            {
               Enumeration<?> paramNames = request.getParameterNames();
               Map<String, String> params = new TreeMap<String, String>();
               while (paramNames != null && paramNames.hasMoreElements())
               {
                  String paramName = String.valueOf(paramNames.nextElement());
                  String[] values = request.getParameterValues(paramName);

                  if(values!=null && values.length>0){
                     params.put(paramName, values[0]);
                  }
               }
               return params;
            }
      ```

    * mvc几种参数接收方式
      1. 实际mvc接收中，直接(String name)也能接收，没有@requestParam(可以自定义别名)也可以。
         SpringMV 目前也比较智能化，如果传递过来参数名称和 HTTP 的保存，那么无须任何注解也可 获取参数。这样方式允许参数为空。

      2. 使用一个POJO来接收这些参数，显然这个POJO属性和HTTP请求参数名一一对应了。在没有任何注解的情况下SpringMVC也能映射POJO。
            即使没有任何注解 们也能够有效传递参数，但是有时候前端的参数命和后台的不一致，比如前端传参数命名为role_name，这个时候就要进行转换(一般不用转换)

      3. 处理url参数类型，@PathVariable

      4. 传json类型，这里传的data 是一个data对象，其中pageParams属性也是一个对象。传参的data和接收的pojo保持一致即可
            var data = { 
               roleName :’role ’, 
               note :’note ’ , 
               pageParams: { 
                  start: 1 , 
                  limit: 20 
               }
            }
            //此处需要告知传递参数类型为json,不能缺少
            contentType：” application/json ”，
            //将JSON 转化为字符串传递     	data:{"dataParam":JSON.stringify(data)},   这种就接收的字符串自己转,这种和放url中拼接一样的接收方式，这种类似表单form提交格式
            data: JSON.stringify(data),    //这种是正规点，传递的是一个json串，而不是json对象。传list<T>类型，后端就@RequestBody List<T>接收
            后端直接json接收方式，，1.@RequestBody方式接收会快点，2.也可以传一个字符串，然后后端自己转成对象


      5. 表单提交，serialize()，后端直接类接收也行。
            通过表单序列化也可以将表单数据转换为字符串传递给后台，因为一些隐藏表单需要一定的计算，所以我们也需要在用户点击提交按钮后，通过序列化去提交表单。
            将数据以 roleName=xxx&namee=xxx 传递


    * 三方对接，接收参数，请求头信息查看，请求头，参数接收	
         Map<String, String[]> parameterMap = request.getParameterMap();
         也可 Enumeration<?> paramNames = request.getParameterNames()  遍历名称获取

         @RequestMapping("/eZxServerCallBack")
            public void eZxServerCallBack(HttpServletRequest request, HttpServletResponse response) throws Exception {
               LOG.info("--e管家-server-回调开始--");
         
            //1.查询请求头的信息，这在三方对接的时候，参数接收方式很有用
               Enumeration headerNames = request.getHeaderNames();
               while (headerNames.hasMoreElements()) {
                  String paramName = (String) headerNames.nextElement();
                  String value = request.getHeader(paramName);
                  LOG.info("header参数：" + paramName + "=" + value);
               }
                  
                  @RequestBody 
            //2.这个可以接收到xml格式的信息，从流中获取相关的信息	比如:application/xml
            // 这个也可以接受application/json,是一个json串，可以自己转，也可以@RequestBody接收，该注解最好使用对象接收
               BufferedReader in = new BufferedReader(new InputStreamReader(request.getInputStream()));
               StringBuffer strBuff = new StringBuffer();
               String line;
               while ((line = in.readLine()) != null) {
                  strBuff.append(line);
               }
               Utils.log("HTTPS,服务器响应结果是: \n" + strBuff.toString());
               
               
            //3.key-value格式接收参数 类似form表单
               Map<String,String[]> parameterMap = request.getParameterMap();
               try {
                  parameterMap.forEach((key,value)-> LOG.info("key="+key+"-----------value="+value[0]));
               } catch (Exception e) {
                  LOG.error("yuichang",e);
               }
               response.addHeader("HTTP/1.1 200", "OK");
               response.addHeader("Content-type", "text/html");
               response.getWriter().write("success");
               response.getWriter().flush();
               
               LOG.info("--e管家-server-回调结束--");
            }
         
            res.setHeader("Access-Control-Allow-Origin", req.getHeader("Origin"));  这个是跨域接收
            
            //发送json格式的请求
            //在请求头中指定contentType
            httpPost.setHeader(new BasicHeader("Content-Type","application/json"));

            //设置请求体参数, 请求体直接使用raw json
            String bodyString = JSONObject.toJSONString(bodyParamMap);
            StringEntity postingString = new StringEntity(bodyString);
            httpPost.setEntity(postingString);
         
      ```	
         <?xml version=\"1.0\" encoding=\"UTF-8\"?>    xml头  标识是一个xml结构
         如果是xml格式(application/xml)返回给浏览器<ROOT><RSP_CODE>00000</RSP_CODE></ROOT>，是能识别xml结构的
         若text/html格式返回，<ROOT><RSP_CODE>00000</RSP_CODE></ROOT>，浏览器只能显示00000，因为无法正确解析标签
      ```


      参数转jsonobject
         Map<String, String[]> map = request.getParameterMap();

         //{"age":["33"],"name":["张三"]}
         //{"age":"33","name":"张三"}
         //	String aa = JSON.toJSONString(request.getParameterMap());
         //	JSON.parseObject(JSON.toJSONString(request.getParameterMap()).replace(":[", ":").replace("],", ",").replace("]}", "}"));   //不推荐


         JSONObject jsonObject = new JSONObject();
   //		for (Map.Entry<String, String[]> set : map.entrySet()) {
   //			jsonObject.put(set.getKey(), set.getValue()[0]);
   //		}
         
         map.forEach((key, value) -> jsonObject.put(key, value[0]));    //推荐

         
         //	String[] yanggbs = (String[])map.get("name");
         //	if (yanggbs.length > 0) {
         //	   System.out.println(yanggbs[0]); // yanggb
         //	}


14. post 和 get 的区别
      method=”post”:这是传递大量数据时用的，传递之前会先将数据打包，数据传输是不可见的，抓包可在HTTP包的包体中看到
      method=”get”：以URL传递的，地址栏长度有限，对数据量是有限制的，数据传输是可见的

15. controller的三种返回对象：1、modelandview 2、string（需要形参model） 3、void

	 MVC的应用中，如果在控制器方法的参数中使用 ModelAndView、Model或者ModelMap作为参数类型,SpringMVC会自动创建数据模型对象，	
	 modelandview，持有一个modelmap和一个view对象，该map会传递到view对应的viewresolvers中，不同模板(jsp，freemarker等)处理map方式不同


    返回的mv是一个map，将其填充到request域中，前端页面可取值
    modelAndView的addObject()相当于request的setAttribute方法，在jsp中取数据
	 mvc中的ModelMap实际上继承LinkedHashMap<String Object>

16. xml整合ssm
      通过spring管理持久层的mapper接口，通过spring管理业务层的service，通过spring管理表现层的handler
         1、整合dao层：mybatis和spring整合，mapperscanner扫描
         2、整合service：管理service接口，配置或标签，实现事务控制
         3、整合springmvc：是spring的模，无需整合
      
      简约版配置：
      整合mybatis
         1)、配置mybatis的mybatis.xml中只需要配置分页插件
         2)、spring-mybatis.xml主要将dao整合到spring中，主要是将sqlsessionfactory（包含mybatis.xml和datasource）注入容器中,再将mapper扫描注入容器（实现@autowire注入）

      整合spring
      配置spring的事务管理：
         1）、事务管理器，spring-jdbc
         2）、通知<tx:advice></txadvice>
         3)、<aop:config></aop:config>
      
      整合springmvc
         1）、组件扫描<context:componet-scan base-packge=""></context:componet-scan>扫描controller
         2）、mvc注解驱动，<mvc:annotation-driven></mvc:annotation-driven>
         3)、视图解析器

17. 异常处理：
      全局异常处理
         springmvc提供了全局异常处理器进行统一异常处理（唯一），是抛到前端控制器交给统一异常处理器处理（默认异常runtimeException类型）
	      
       * 编程式：
	         定义handler异常处理器,需要实现handlerExceptionResolver接口，并需要在springmvc.xml中注入，旧版的是在web.xml中配置错误页面
            @Component
            public class GlobalExceptionResolver implements HandlerExceptionResolver {
               public ModelAndView resolveException(HttpServletRequest request,HttpServletResponse response, Object handler, Exception ex) {
                  logger.error("统一异常处理  url:{}",request.getRequestURI(),ex);
                  ModelAndView modelAndView = new ModelAndView();
                  modelAndView.setViewName("error/500");
                  return modelAndView;
               }
            } 
         
       * 注解式
            使用@ControllerAdvice注解，全局捕获异常类，只要作用在@RequestMapping上，所有的异常都会被捕获
            处理异常：@controllerAdvice 和 @ExceptionHandler 	前者用来定义控制器通知，后者指定异常发生的处理方法
               //全局异常捕捉处理
               @ControllerAdvice
               public class CustomExceptionHandler {
                  //json返回
                  @ResponseBody
                  @ExceptionHandler(value = Exception.class)
                  public Map errorHandler(Exception ex) {
                     Map map = new HashMap();
                     map.put("code", 400);
                     if(ex instanceof MyException){map.put("msg","这是自定义异常");}
                     return map;
                  }
                  //页面返回
                  @ExceptionHandler(value = MyException.class)
                  public ModelAndView myErrorHandler(MyException ex) {
                     ModelAndView modelAndView = new ModelAndView();
                     //指定错误页面的模板页
                     modelAndView.setViewName("error");
                     modelAndView.addObject("code", ex.getCode());
                     modelAndView.addObject("msg", ex.getMsg());
                     return modelAndView;
                  }
               }

18. @RequestMapping网关实现原理(忽略)
    Springmvc是@RequestMapping网关实现：(handlermapping负责映射用户的url和对应的处理类)
        1.扫描所有注册的Bean并遍历这些Bean，依次判断是否是处理器，并检测其HandlerMethod
        2.遍历Handler中的所有方法，找出其中被@RequestMapping注解标记的方法。获取方法method上的@RequestMapping实例
        3.将类层次的RequestMapping和方法级别的RequestMapping结合，存入一个urlHandlerMap中key-value:(url,handler) 
        5.当请求到达时，去urlMap中匹配url，以及获取对应mapping实例，然后去handlerMethods中获取匹配HandlerMethod实例

19. 配置多个dispatcherservlet（忽略）
    DispatcherServlet之间的上下文是分离的，势必会出现多个dataSource的情况，
    由contextConfigLocation统一制定上下问，即可共享
    原先的hessian是自己的dispatcherservlet 加载自己的 beancontext
        
    在Spring MVC中，每个DispatcherServlet拥有其独立的WebApplicationContext，继承了root/global WebApplicationContext中所定义的所有的bean。
    使用RequestContextUtils中的方法获得DispatcherServlet特定的WebApplicationContext(被包含到ServletContext中)；
    使用WebApplicationContextUtils中的方法获得ApplicationContext；

20. 

21. request的隐式调用
      RequestContextHolder
      可以实现隐式的request调用，不一定要从controller传递
      本质上使用threadlocal变量，直接在需要用的地方使用如下方式取HttpServletRequest即可
      
      HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();

      //开启新线程之前，将RequestAttributes对象设置为子线程共享
      ServletRequestAttributes sra = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
      RequestContextHolder.setRequestAttributes(sra, true);
      若service层的函数是异步的话，是获取不到request的。通常RequestContextHolder.getRequestAttributes()无法在子线程等异步情况下使用，
	
22. 上传图片:
        tomcat的server.xml中添加虚拟目录，方便代码和图片在一台电脑上测试用	
        <Context path="/helloapp" docBase="docBase="D:\web\helloapp" reloadable="true"/>
        springmvc的上传功能：
            1、form的中增加enctype=”multipart/form-data”
            2、 spring.mvc中需要增加multipartResolver解析器
            3、controller中使用MultipartFile 接受处理
            4、图片目录分级可以提高io能力
            5、采用file.Transto 来保存上传的文件

    上传文件
      单文件上传 MultipartFile
            1.既然是文件上传，form 中enctype="multipart/form-data"  method="post"
            2.commons-fileupload和commons-io这两个文件上传的依赖包
            3.配置commonsMultipartResolver
            <form action="/talents/importTalents"  method="post" enctype="multipart/form-data">
            选择文件 <input type="file" name="excel">
               <input type="text" name="eUserId" value="123">
               <input type="text" name="groupId" value="1">
               <button type="submit">提交</button>
            </form>

      SpringMVC 文件上传是通过MultipartResolver (Multipart解析器）处理的。
      对于 MultipartResolver 而言它只是个接口，它有两个实现类CommonsMultipartResolver 和 StandardServletMultipartResolver。前者依赖第三方包，一般用后者，可以xml配置，也可以java配置。
      controller中可以选择request间接接收，也可以选择MultipartFile直接接收
      MultipartFile是Spring MVC提供的类，而Part是ServletAPI 提供的类。
      表单中要设置为enctype=”multipart/forrn-data”

      操作pdf    使用itextpdf jar。上传  MultipartFile 用 HttpServletRequest、MultipartFile、Part参数

23.   媒体类型

   * MediaType,互联网媒体类型,也叫做MIME类型
		在Http协议消息头中，使用Content-Type来表示具体请求中的媒体类型信息。（推荐使用这个，下二）
		常见的媒体格式类型如下,text/html:HTML格式,text/xml:XML格式
		
		以application开头的媒体格式类型：
			application/xml:XML数据格式
			application/json:JSON数据格式
			application/octet-stream:二进制流数据（常见的文件下载)
			application/x-www-form-urlencoded:表单中默认的encType,表单数据被编码为key/value格式发送到服务器

		另外一种常见的媒体格式是上传文件时使用：
		multipart/form-data:需要在表单中进行文件上传时，就需要使用该格式
		
		前段页面中的network中的doc类型  xhr类型(XMLHttpRequest)
		它依赖的是现有的CSS/HTML/Javascript，而其中最核心的依赖是浏览器提供的XMLHttpRequest对象，是这个对象使得浏览器可以发出HTTP请求与接收HTTP响应。
		总结两者的关系：我们使用XMLHttpRequest对象来发送一个Ajax请求


   * HttpRequest中常见的四种ContentType
		客户端发起 HTTP POST/PUT 请求时，可以指定Request Header 的 Content-Type，表示向服务端发送的 Request Body 中的数据的格式
		服务端返回数据时，也可以指定Content-Type，表示返回数据的格式

			HTTP 请求分为三个部分：状态行、请求头、消息主体。
			服务端通常是根据请求头（headers）中的 Content-Type 字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。
			POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。
		 <method> <request-URL> <version>
		 <headers>
		 <entity-body>
		===============
		POST http://www.example.com HTTP/1.1
		Content-Type: application/x-www-form-urlencoded;charset=utf-8
		title=test&sub%5B%5D=1&sub%5B%5D=2&sub%5B%5D=3
		
		1.application/x-www-form-urlencoded	
			浏览器的原生 form 表单,不设置 enctype 属性,默认属性
			浏览器原生支持
		2.multipart/form-data
			一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 form 的 enctyped 等于这个值
			浏览器原生支持
		3.application/json
			低版本 IE 之外的各大浏览器都原生支持 JSON.stringify
			直接提交json串
			POST http://www.example.com HTTP/1.1
			Content-Type: application/json;charset=utf-8
			{"title":"test","sub":[1,2,3]}
			当时我是把 JSON 字符串作为 val，仍然放在键值对里，以 x-www-form-urlencoded 方式提交。(这种就有点old了)
		4.text/xml
			一般不用，没有json灵活
   
24. 跨域相关		
      https://blog.csdn.net/itcats_cn/article/details/82318092
      跨域问题是针对JS和ajax的，html本身没有跨域问题
      出于安全原因，浏览器限制从脚本内发起的跨源HTTP请求。 例如，XMLHttpRequest和Fetch API遵循同源策略。 这意味着使用这些API的Web应用程序只能从加载应用程序的同一个域请求HTTP资源，除非使用CORS头文件。

      JavaScript的"同源策略"，即只有 协议+主机名+端口号 (如存在)相同
      请注意：localhost和127.0.0.1虽然都指向本机，但也属于跨域。
     
      解决方法：对于3和4，重点是使用同一域名
         1、响应头添加Header允许访问											类似文件 CorsFilter，后端filter中设置设置res.setHeader("Access-Control-Allow-Origin", "*");
         2、jsonp 只支持get请求不支持post请求
         3、httpClient内部转发，转发到后端去请求资源（用过）
         4、使用接口网关——nginx、springcloud zuul   (互联网公司常规解决方案)     同域名访问，用nginx转发请求（用过，比如手机前后端）
      
      在前后分离的架构下，跨域问题难免会遇见比如，站点 http://domain-a.com 的某 HTML 页面通过  的 src 请求 http://domain-b.com/image.jpg。网络上的许多页面都会加载来自不同域的CSS样式表，图像和脚本等资源。

      @crossorigin可以针对单个的路径实现，要求Spring4.2及以上的版本

      公司在几年前就采用了前后端分离的开发模式，前端所有请求都使用ajax
      传统结构项目中，shiro从cookie中读取sessionId以此来维持会话，在前后端分离的项目中（也可在移动APP项目使用），我们选择在ajax的请求头中传递sessionId，

      https 和http的 的cookie 是不一样的  所有同一个xyeAuthId 在两个域名是不共享的

      * filter中配置相关
      @Override
	   public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {
		   ServletContext context = request.getServletContext();
         ApplicationContext app = WebApplicationContextUtils.getWebApplicationContext(context);
		   // 解决跨域问题
		   HttpServletRequest req = (HttpServletRequest) request;
         HttpServletResponse res = (HttpServletResponse) response;
      
         // 响应标头指定 指定可以访问资源的URI路径，"*"表示所有的域都可以接受
         res.setHeader("Access-Control-Allow-Origin", req.getHeader("Origin"));
         // 响应标头指定响应访问所述资源到时允许的一种或多种方法
         res.setHeader("Access-Control-Allow-Methods", "POST, GET, PUT, OPTIONS, DELETE");
         // 设置 缓存可以生存的最大秒数
         res.setHeader("Access-Control-Max-Age", "3600");
         // 设置受支持请求标头
         res.setHeader("Access-Control-Allow-Headers", "x-requested-with");
         // 指示的请求的响应是否可以暴露于该页面。当true值返回时它可以被暴露
         res.setHeader("Access-Control-Allow-Credentials","true");
      }

      * CROS协议解决跨域问题
         * 自建filter，加入http的头信息，实现跨域访问,
               但是服务器端 Access-Control-Allow-Credentials = true时，参数Access-Control-Allow-Origin 的值不能为 '*' 。;
               后端处理：一般是放在filter中处理的
               ```
                  response.setHeader("Access-Control-Allow-Origin", "*");  
                  response.setHeader("Access-Control-Allow-Methods", "POST, GET, OPTIONS, DELETE");  
                  response.setHeader("Access-Control-Max-Age", "3600");  
                  response.setHeader("Access-Control-Allow-Headers", "x-requested-with");  	
               ```

         * 但是也可以引用org.apache.catalina.filters.CorsFilter现成的jar，web.xml中做相关的配置即可，解决了上诉问题

         * 还可以通过跳转controller中使用httpclient来实现访问，作为中间层来回调数据，这也是目前系统最稳定的方案。






### Mybatis相关

#### 基本介绍
1. sql中的${} 和#{} sql注入

   * #{}: 是以预编译的形式，将参数设置到SQL语句中;PreparedStatement:防止SQL注入，将传入的数据都当成一个字符串处理;
             表示一个占位符,体现在sql上是 =？ parameters：
             MyBatis创建预处理语句属性并以它为背景设置安全的值

     ${}: 取出的值直接拼装在SQL语句中;会有安全问题，传入的数据直接显ss示生成在sql中 
             表示拼接sql串，将接收到的参数内容不加任何修饰拼接在sql中，直接拼接在sql中，
             只是做字符串替换，不会修改或转义字符串 因此，能使用＃的地方就不要使用$，除非是像 order by 这种不是参数的地方

   * mybaties排序时使用order by 动态参数时需要注意，使用${}而不用#{};  表名也用${}
     如果你要做动态的排序，比如 order by column，要用${},因为如果你使用了#{},会拼上字符串，那么打印出来的将会是
      select * from table order by 'name' ,这样是无法排序。

   * select * from user where id=${id} and username=#{username}

     在经过编译后，得到如下语句
     ```
     select * from user where id=2 and username=?
     ```

     	如果是#{}的形式是编译成?，而如果${}是编译成直接的数据。
     	sql语句查询精确查询id等字段需要#{ }，根据字段排序用${ }

     ```
     sql攻击案例：
     select * from ${tableName} where name = #{name}  
     我们的参数 tableName 为 user; delete user; --，
     select * from user; delete user; -- where name = ?;  
     ```

      注入过程的工作方式是提前终止文本字符串，然后追加一个新的命令。 提前;-- 结束
         防止sql注入的方法：
            1. 普通用户与系统管理员用户的权限要有严格的区分。普通用户不能有Drop Table等表结构的权限
            2. 强迫使用参数化语，不能让sql直接嵌入进去,参数化的语句使用参数而不是将用户输入变量嵌入到 SQL 语句中。视为字符值而不是可执行代码,类似的就是sql中的#和$,
            3. 加强对用户输入内容的检查与验证， 在 SQLServer 数据库中，有比较多的用户输入内容验证工具，检查输入内容的合法性(大小和数据类型,长度、格式和范围)


2. mybatis常用标签
      * 动态sql：
            1.<if test="a and b"></if>
            2.<where>可以自动去掉条件中的第一个and
            3.<sql id="acc"></sql>抽取sql片段,		
            引用sql片段，<include refid="acc"></include> 若引用id不存在本mapper中，需前边追加namesapce
            4.<foreach collection="" item="" open="" close="" separator="">
                  每次拼接对象
            </foreach>

            <where>和<trim>，<set>都可以达到去除多余 and 和or的效果。
            <if test="">对于字符串类型的判断，需要加上toString()方法
            模糊查询，mysql中常用的是concat方法

      * 遍历
         <foreach item="item" index="index" collection="idStr" open="(" separator="," close=")">
            ${item}
         </foreach>

3. UNION ALL，Union

   Union：对两个结果集进行并集操作，不包括重复行，同时进行默认规则的排序；
   Union All：对两个结果集进行并集操作，包括重复行，不进行排序；

   UNION ALL 的Order by，是依据最终的字段排序的，union all 的没有字段 null补齐

4. mybatis的对象和核心组件
      * mybatis的四大对象
            1.Executor				执行sql的全过程，包括组装参数、组装结果返回集和执行sql过程
            *2.StatementHandler		是执行SQL的过程，我们可以重写执行SQL的过程它是插件最常用的拦截对象
            3.ParameterHandler		主要拦截执行SQL参数组装，我们可以重写组装参数规则
            4.ResultSetHandler		用于拦截执行结果的组装，我们可以重写组装结果的规则	
		
      * mybatis的几个核心组件
            sqlsessionfactorybuilder	构造器，创建sqlsessionfactory
            sqlsessionfactory			工厂接口，用来创建sqlsession，						
                  相当于一个数据库连接池，一旦创建长期保存使用，占据数据库的连接资源，一般是单例
                  在mybatis-spring中，使用SqlSessionFactoryBean 支持 SqlSessionFactory 的配置
            sqlsession					会话，SqlSession中定义的全是对数据库增删改查的各种方法，mybatis的核心接口对象、
                  mybatis中的主要操作对象
                  类似jdbc中的connection对象，代表一个连接资源的启用。1.获取mapper接口，2.发送sql给数据库 3.控制数据库事务
                  存在一个业务请求中，操作事务，请求完成，关闭连接，归还给sqlsessionfactory。
            sqlmapper					映射器，由java接口和xml构成，需要给出对应sql和映射规则，发送sql执行，并返回结果
               主要作用就是将查询结果映射到一个pojo中，或者将pojo插入到数据库中。
               mybatis会为mapper接口生成一个动态代理，去处理相关的实现逻辑



5. 整合mybatis的方式

    * 原生的jdbc操作需要具体拼装sql执行
    * mybatis初始，可以使用dao，sqlsession.执行（）实现，也可使用mapper代理，namespace需要和mapper路径一致，sqlsession.getmapper()，未整合spring，无法扫描到mapper接口的位置
    * 整合spring之后，直接使用mapper方法调用(推荐，默认)

    ```
    使用SqlSessionDaoSupport ，在mybatis的中实现dao实现类，使用可获得sqlsession
    	@Repository
    	public class UserDAOImpl extends SqlSessionDaoSupport implements UserDAO
    	@Autowired(required = false)
    	 @Qualifier("sqlSessionFactory")
    	 public void setSqlSessionFactory(SqlSessionFactory sqlSessionFactory) {
    			super.setSqlSessionFactory(sqlSessionFactory);
    		}
    使用sqlsession.getMapper(CLASS)使用的是初始的mybatis操作，加载mybatis配置后使用，
    在整合spring以后，直接使用mapper方法即可
    ```


6. mybatis和hibernate(忽略)
        hibernate:是一个标准的orm框架，不需要写sql，自动生成
            应用场景：适应需求变化不多的中小型项目，比如后台管理项目
        mybatis：专注sql本身，需要自己写sql，比较灵活
            应用场景：需求变化较多的项目，互联网项目

7. mybatis提供查询缓存(不实用，了解即可)
		没有springcache灵活，可以上到service级别
        一级缓存:sqlSession级别的缓存(不共享),一次service事务操作，sqlSession就结束清空缓存
            当调用SqlSession的修改、添加、删除、commit()、close()等方法时，就会清空一级缓存
            第一次查询，没有则数据库中查出并添加进一级缓存，中间如果有commit操作(修改、添加、删除)，则会清空全部，保证数据实时更新
            第二次查，缓存中有直接获取，有commit会从数据库重新查后再写入缓存

        二级缓存:mapper级别的缓存
            默认不开启， usecache=false设置禁用二级缓存
            多个sqlsession可以共享一个mapper的二级缓存区域（按照namespace区分,以命名空间为单位创建缓存数据结构）
            sqlsession关闭后才将数据写到二级缓存，commit后都会清空二级缓存，多个sqlsession之间可以跨，因为二级缓存是针对mapper的
            如果只是执行了增删改而没有提交，只会清空一级缓存的数据，不会清空二级缓存的数据。
            二级缓存适合共享的数据，且这种数据不能经常增删改，一改就清空缓存了，没用

            查出的数据首先放在一级缓存中，只有一级缓存被关闭或者提交以后，一级缓存数据才会转移到二级缓存
        
        整合方法：mybatis自带cach的接口，实现即可
            1、引入相关jar，配置mapper中的cache中的type为其实现类<cache type="">
            2、classpath下配置ehcache.xml


8. 级联查询
      结果集映射，集合，级联，关联，关联查询
		MyBatis的级联分为3种。级联过多会降低查询性能   直接join查询出来关联

      mybatis不支持多对多，可以拆分成两个一对多级联处理。暂时忽略
		1. 关联-association				用于一对一
		2. 集合-collection				用于一对多
		
		比如同时有User.java和Card.java两个类
		public class User{
			private Card card_one;
			private List<Card> card_many;
		}
		在映射card_one属性时用association标签, 映射card_many时用collection标签.

      column 是查询出来的结果集中的列名	
		<association property="reqUserInfo" column="req_User_Id" javaType="com.xiaoyuer.core.dmo.UserInfo">    一般这里column没有必要写
			<id property="id" column="req_User_Id"></id>
			<result property="nickName" column="rNick_Name" />
		</association>	

      collection一对多映射,查询出对象中的list属性
      ```
        <resultMap type="" id="">
            <id column="" property=""/>
            <result column="" property=""/>
            <association property="" javaType="">
                <id column="" property=""/>
                <result column="" property=""/>	
            </association>
        </resultMap>
      ```
		一般用left join
		RIGHT JOIN(忽略) 
			关键字会从右表 (Orders) 那里返回所有的行，即使在左表 (Persons) 中没有匹配的行。先查出右表值，再关联左表。


9. sql记录 

      * 集合判空
         数组：<if test="object!=null and object.length>0">
         参数为集合List：<if test="object!=null and object.size()>0">

      * 插入返回主键
         ```
         <insert id="insertSelective" parameterType="com.xiaoyuer.core.dmo.RequireInfo" useGeneratedKeys="true" keyProperty="id">
         ```

      * 模糊查询： user_name like concat('%',${userName},'%')
         ```
         select * from where 字段名 like '%‘+str1+’%'; 查询变量      concat(concat('%',#{username}),'%')
         SELECT * FROM user_base_info WHERE cell_phone LIKE '%/_%' ESCAPE '/'
         赋值模糊查询   and user_name like '%'#{map.userName}'%'
         sql使用`区别关键字
         ```
      * 等值判断
         但是空串是可以相等的
         ```
         <if test="param.startTime !=null and param.startTime ==''">	
            haha
         </if>
         
         #基础的 也需要 不然 all 对1就是转型错误
         <if test="status != null and status=='1'.toString()" > 
          mybatis中的== 使用注意  目前看是map的对象参数都需要
         <if test="param.pageType !=null and param.pageType =='1'.toString()">
         ```

10. mybatis的dmo中如果有了有参构造函数，还要添加无参构造
	   InstantiationException，实例化异常，缺少无参构造，抛出该异常，或者在orm框架中实现对象的映射，缺少无参构造也会异常
	   mybatis的dmo的构造方法必须要有无参的构造方法

11. 逆向工程   generator
      mybatis的逆向在github的官方文档有相关的xml配置说明。
      mybatis dmo使用自定义构造函数后，需要加一个默认的构造函数，否则会报错
      mybatis默认情况下提供自动映射，只要sql返回的列名能和pojo对应起来即可。

      mybatis generator 源码会默认设置追加属性为ture，
      typeAlias定义别名，mybatis扫描packsge中的po类，别名是类名resultType=“user”

      
      http://mybatis.org/generator/quickstart.html			   文档地址
      https://github.com/mybatis/generator/releases			jar包下载地址

      generator.xml这个配置需要自建，上面的文档中有配置示例
      mysql需要添加启动包，mysql-connector-java-5.1.21.jar，并在generator.xml中指定路径

      执行语句，	java -jar mybatis-generator-core-1.3.2.jar -configfile generator.xml -overwrite

      generator.xml配置样例代码
      ```
         <?xml version="1.0" encoding="UTF-8"?>
         <!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd">
         <generatorConfiguration>
            <!-- 数据库驱动包位置 -->
            <classPathEntry location="D:\generator\mybatis\mysql-connector-java-5.1.21.jar" /> 

            <context id="DB2Tables" targetRuntime="MyBatis3">
               <commentGenerator>
                  <property name="suppressAllComments" value="true" />
               </commentGenerator>
               <!-- 数据库链接URL、用户名、密码 -->
               <jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql://192.168.6.251:3306/db_xiaoyuer_pay" userId="root" password="db_xiaoyuer">
               </jdbcConnection>
               <javaTypeResolver>
                  <property name="forceBigDecimals" value="false" />
               </javaTypeResolver>
               <!-- 生成模型的包名和位置 -->
               <javaModelGenerator targetPackage="com.xiaoyuer.pay.dmo" targetProject="D:\generator\mybatis\src">
                  <property name="enableSubPackages" value="true" />
                  <property name="trimStrings" value="true" />
               </javaModelGenerator>
               <!-- 生成的映射文件包名和位置 -->
               <sqlMapGenerator targetPackage="com.xiaoyuer.pay.mapping" targetProject="D:\generator\mybatis\src">
                  <property name="enableSubPackages" value="true" />
               </sqlMapGenerator>
               <!-- 生成DAO的包名和位置 -->
               <javaClientGenerator type="XMLMAPPER" targetPackage="com.xiaoyuer.pay.dao" targetProject="D:\generator\mybatis\src">
                  <property name="enableSubPackages" value="true" />
               </javaClientGenerator>

               <!-- 要生成那些表(更改tableName和domainObjectName就可以) -->	
               <!-- 前台表 -->
               <table tableName="pp_citic_order_clear" domainObjectName="PpCiticOrderClear" enableCountByExample="false" enableUpdateByExample="false" enableDeleteByExample="false" enableSelectByExample="false" selectByExampleQueryId="false" />
            </context>
         </generatorConfiguration>
      ```


12. MyBatis的入参和映射相关
      MyBatis的传入参数parameterType类型分两种
         1.基本数据类型：int,string,long,Date;
         2.复杂数据类型：类和Map
         如何获取参数中的值:
         1.基本数据类型：#{参数} 获取参数中的值
         2.复杂数据类型：#{属性名}  ，map中则是#{key}	

      两种自动映射
         自动映射，如果编写sql的列名和对象pojo的属性名一致，就会形成自动映射。默认开启
         驼峰映射，严格要求user_name 对应userName属性。不太灵活。不推荐 默认为false
         实际开发中，一般使用的是resultmap，建立结果集映射。
         
      多值传参
         多值传参也可以传map，但是可读性差，难维护，不推荐
         多值传参也可以使用javabean，当只有一个参数对象，可以指定类型后，直接写属性。多个参数，不行
         一般用的是@param单值传输，多于5个就不推荐用了
         sql中，#{userName}也可以不用给出parameterType类型，mybatis会自动识别

         
      <resultMap id = "cargo" type="cargo">
         <id property="cargoId" column="cargo_id"/>
         <result property="cargoTypeId" column="cargo_type_id"/>
      </resultMap>
         
      子元素id代表resultMap的主键，而result代表其属性。
      在自定义的resultMap中第一列通常是主键id。
      id和result都是映射单列值到一个属性或字段的简单数据类型。
      唯一不同的是，id是作为唯一标识的，当和其他对象实例对比的时候，这个id很有用，尤其是应用到缓存和内嵌的结果映射。级联


      LIST<map<k,v>>  Mybatis返回的Map是这样的一种格式：Map<字段名称，字段值> ，对应的是和你查询出来的行数决定的，一个Map对象代表一行数据！
      查询  map 和list<map>  一行就是一条记录，对应的是一个resultType，如果类型是map，多条记录就是list<map>





13. dao，idea中的dao中的xml需要单独配置加载(忽略)
      ```
         <resources>
            <resource>
               <directory>src/main/java</directory>
               <includes>
                  <include>**/*.java</include>
                  <include>**/*.xml</include>
               </includes>
               <filtering>false</filtering>
            </resource>
         </resources>
      ```

      idea中maven编译dao资源

         配置的时候除了先idea的坑，需要再pom中<build>属性中<resource>相关mapper.xml到classpath下，
         springboot再javapackage中自己是不会编译xml的，只会编译java类文件，resource只会编译资源文件

         再spingboot的tomcat部署，会默认java 和resources 打到classpath下，直接可用，但是一旦使用了resource，就必须指定资源文件了，
         默认tomcat配置将失效，但是maven编译则需要再build中添加进来

         总结就两点 1.source root下不会编译xml文件，2.使用<resource>后，覆盖默认，需要指定所有资源文件

14. @MapperScan扫描后无需添加@Mapper、@Repository等注解(忽略)
		这种映射需要自己写，只适合简单的表，复杂的表不适用，不灵活，难维护，不推荐

		@Select("select * from bank_resc where id=#{id}")
		@Results({@Result(column = "bank_code",property = "bankCode"),@Result(column = "bank_name",property = "bankName")})
		BankResc selectByIdd(Integer id);	


		@Select("select user_name from user_base_info where id=#{id}")
		String getuserInfobyid(Integer id);


15. mysql中的格式化
      ```
      mysql中的数字格式化
         FORMAT（）目前是String类型展示，金额大会出现逗号
         round（）数字形式的格式化，没有逗号分割
         mysql中的日期格式化	DATE_FORMAT(created_time, '%Y-%m-%d %H:%i:%s')
      Cast(字段名 as 转换的类型 ) sql中的数转换
         DATE_ADD(t.pay_date,INTERVAL 90 DAY)  #sql中的日期加
         DATE_SUB(d,INTERVAL expr type) 	
         DATE_FORMAT(ubm.deal_date,'%Y-%m-%d %H:%i:%S')    #日期格式化 		yyyy-MM-dd HH:mm:ss  注意大小写
      ```

16. mybatis中的aop实现
   数据库事务也是通过aop实现的，在jdbc中大量的try catch finally，大量的冗余代码，
	比如打开和关闭数据库连接，以及事回滚代码，用aop都封装在了一起，只关心业务即可，将业务sql操作切入操作即可
	
	mybatis封装的数据库操作，对应的也是aop的事务操作
	public void savePurchaseRecord (Long productid, PurchaseRecord record) {
		SqlSession sqlSession = null;
		try{
				sqlSession = SqlSessionFactoryUtils.openSqlSession() ;
				ProductMapper productMapper = sqlSession.getMapper(ProductMapper.class) ;
				Product product= productMapper.getRole(productid);
				//1.mapper操作  2.commit
				sqlSession .commit() ;
			}catch (Exception ex) {
			//异常回滚事务
				ex.printStackTrace();
				sqlSession.rollback();
			} finally{
				//关闭资源
				if (sqlSession != null) {
					sqlSession.close();
				}
			}
	   }

17. 分页相关
	   * 1. 逻辑分页   逻辑内存中，   查询所有数据 List, list.subList 截取你需要数据
			查出所有的数据，使用程序进行分页。它是针对ResultSet结果集执行的内存分页。
			占用内存大、数据更新不能及时反馈、不用频繁查询数据库
         利用游标分页，好处是所有数据库都统一，坏处就是效率低。
			
			mybatis自带的分页RowBounds(忽略)
				RowBounds： 逻辑分页，数据量大的时候压力较大。一般不用
				数据库返回的不是分页结果，而是全部数据，然后再由程序员通过代码获取分页数据。
				rowbounds是一次性取出数据放入内存再分割数据，一次获取所有符合条件的数据，然后在内存中对大数据进行操作
				常用的操作是一次性从数据库中查询出全部数据并存储到List集合中，因为List集合有序，再根据索引获取指定范围的数
				
				一次性将数据读取到内存，占用较大的内存空间(数据过大可能引起内存溢出)。数据发生了改变，数据库逇最新状态无法实时反映到操作中，适合数据量较小、数据稳定的场合
				总结：Mybatis的逻辑分页比较简单，简单来说就是取出所有满足条件的数据，然后舍弃掉前面offset条数据，然后再取剩下的数据的limit条
					
				在 mybatis 中，使用 RowBounds 进行分页，不需要在 sql 语句中写 limit。
				但是由于它是在 sql 查询出所有结果的基础上截取数据的，所以在数据量大的sql中并不适用，它更适合在返回数据结果较少的查询中使用
				最核心的是在 mapper 接口层，传参时传入RowBounds(int offset, int limit)对象，即可完成分页
				mapper 接口层代码如下
				List<Book> selectBookByName(Map<String, Object> map, RowBounds rowBounds);

	   * 2. 物理分页,直接通过SQL进行在数据库中直接分页,得到的数据就是我们想要分页之后的数据,就是物理分页;其实是依赖物理数据库实体。limit直接数据库查出需要的数据
			每次查询数据库，使用limit，需要计算总数、页数、当前页。
			占用内存小、数据更新及时反馈、频繁查询数据库
         数据库本身提供了分页方式，如mysql的limit，好处是效率高
			
			mybatis插件或者直接书写sql进行分页;
				PageHelper： 物理分页， 通过拦截器加 limit 语句进行分页
				物理分页依赖的是某一物理实体，这个物理实体就是数据库，比如MySQL数据库提供了limit关键字，程序员只需要编写带有limit关键字的SQL语句，数据库返回的就是分页结果。
				每次都要访问数据库，对数据库造成的负担大
				每次只读取一部分数据，占用的内存空间较小
				每次需要数据时都访问数据库，能够获取数据库的最新状态，实时性强
				数据库量大、更新频繁的场合
			
				(1).通过自己的封装SQL根据beginNum(开始条数)和endNum(需要的条数)来进行分页  一般不用，不需要造轮子
						有点类似老系统的插件，自己封装查询limit 参数,需要手动进行封装总数以及分页信息,数据返回页面;
						自己创建分页插件，老系统，不推荐这样造轮子。

					分页插件的思路，拦截StatementHandler对象的prepare()方法，在预编译sql之前修改sql，得到结果的返回数量被限制。
					@Intercepts({ @Signature(type = StatementHandler.class, method = "prepare", args = { Connection.class }) })
					public class PagePlugin implements Interceptor {
					
					其中，@Intercepts 说明它是个拦截器。@Signature 是注册拦截器签名的地方，只有签名满足条件才能拦截type是四大对象中一个，这里是 StatementHandler。
					method 代表要拦截四大对象的某种接口方法，而args表示该方法参数，要根据拦截对象的方法参数进行设置
					mappedStatement.getId().matches(".*ListPage.*")

				(2).PageHelper分页插件(推荐)
						将pageNum和pageSize封装为page对象，保存在ThreadLocal中，实现线程间数据隔离。
						Pagehelper实现了Mybatis的Interceptor接口，调用拦截StatementHandler（Sql语法的构建处理）方法，按照物理库的不同重构SQL实现分页。
						
						插件拦截的对象：
						Executor：拦截执行器的方法（log记录）
						StatementHandler：sql语法构建处理
						ParameterHandler：拦截参数的处理
						ResultSetHandler：拦截结果集的处理
						
            查询结果是page转对象问题
               使用pagehelper查询出对象是page对象，然后放入json中，如果取的时候是getString,然后用arrayList转会报错。除非放入限定好的对象中或者直接list<E>强转
               就是pagehelper查询出的page对象，放json中，然后用getString反转为对象就要注意，直接强转

            实现原理(了解即可，忽略) 
               PageHelper首先将前端传递的参数保存到page这个对象中，接着将page的副本存放入ThreadLoacl中，这样可以保证分页的时候，参数互不影响，接着利用了mybatis提供的拦截器，取得ThreadLocal的值，重新拼装分页SQL，完成分页。
               在threadlocal中设置分页参数，之后在查询的时候，获取当前线程中的分页参数，执行查询的时候通过拦截器再sql中添加分页参数，之后实现分页查询，查询结束后在finally语句中清除threadlocal中的查询参数

               使用ThreadLocal来传递和保存Page对象，每次查询，都需要单独设置PageHelper.startPage()方法
               PageHelper.startPage()和查询方法连着用，实际就是拦截器再查询方法的时候，从线程变量中拿到分页信息组装的结果。

               1.在你要使用分页查询的时候，先使用PageHelper.startPage这样的语句在当前线程上下文中设置一个ThreadLocal变量，
               2.再利用mybatis提供的拦截器（插件）实现一个com.github.pagehelper.PageInterceptor接口，
               3.这个分页拦截器拦截到后会从ThreadLocal中拿到分页的信息，如果有分页信息，这进行分页查询，最后再把ThreadLocal中的东西清除掉。
               4.最后实在finally中清除的


      



      * 目前的分页插件PagePlugin，也是网上随便copy的一份下来用的，对mapper有代码侵入，无法封装总页数
         常规配置pagehelper
         ```
               <dependency>
                  <groupId>com.github.pagehelper</groupId>
                  <artifactId>pagehelper-spring-boot-starter</artifactId>
                  <version>1.2.5</version>
               </dependency>

               PageHelper.startPage(1,3);
               List<UserBean> byEmail = userMapper.findByEmail("100");//实际查询返回的是page对象，也是实现list接口的额，
               PageInfo page = new PageInfo(byEmail);
               long total = page.getTotal();

               //分页时，实际返回的结果list类型是Page<E>，如果想取出分页信息，需要强制转换为Page<E>，
               //或者使用PageInfo类对结果进行包装，可以拿到多有的page属性
               //继承arraylist后，数据都是放在elementData数组中的,返回的page<e>就是一个数组，元素在elementData中（list中的数据组），
               PageInfo page = new PageInfo(list);
               Page<UserBean> byEmail = (Page)userMapper.findByEmail("100");
         ```    

         boot 2.0引入pagehelper
            1.pom引入pagehelper-spring-boot-starter
            2.属性文件配置
               #pagehelper
               pagehelper.helper-dialect=mysql
               pagehelper.reasonable=true							这个属性要注意，默认是false(超限返回空数据)，设置为true，页数超限就显示最后一页的内容
               pagehelper.support-methods-arguments=true
               pagehelper.params=count=countSql



      * mybatis中使用foreach带来的分页插件问题(忽略)
         https://segmentfault.com/a/1190000038554749?ivk_sa=1024320u	
         https://blog.csdn.net/isea533/article/details/44002219/
         
         报错
            nested exception is org.apache.ibatis.binding.BindingException: 
            Parameter '__frch_item_0' not found. Available parameters are [deleteMark, param5, nickName, userIds, page, param3, userName, param4, param1, param2]
            return userInfoMapper.getUsersByNameListPage(page, userIds, nickName, userName, deleteMark);
                  
         sql代码 
            这里换成 ${item}就没事，版本问题，解析方式不一样

         <if test="userIds!=null ">
            and ubi.id IN
            <foreach item="item" index="index" collection="userIds" open="(" separator="," close=")">
            #{item}
            </foreach>		
         </if>		

         #{}这种是带引号的，${}这种是不带引号的。主要看你怎么用，一般如果表是动态的话，这种就需要用${}
         注意${item}两边要有引号
         这是List<Integer> userIds 所以不加，如果是string， 可能要'${item}'
         
         原理
            mybatis分页插件解析foreach，走到hasAdditionalParameter时，因为boundSql的additionalParameter是空的。数据在  BoundSql的metaParameters中
            
            else if (propertyName.startsWith(ForEachSqlNode.ITEM_PREFIX)&& boundSql.hasAdditionalParameter(prop.getName())) {
               value = boundSql.getAdditionalParameter(prop.getName());
               if (value != null) {
                  value = configuration.newMetaObject(value).getValue(propertyName.substring(prop.getName().length()));
               }
            }else if (propertyName.startsWith(ForEachSqlNode.ITEM_PREFIX)&& boundSql.getAdditionalParameter(prop.getName()) != null) {
               value = boundSql.getAdditionalParameter(prop.getName());
            }

      * MyBatis 拦截器 （实现分页功能） 
         @Intercepts({ @Signature(type = StatementHandler.class, method = "prepare", args = { Connection.class }) })
         具体的原理是在StatementHandler上进行拦截，并进行代理，实现的思路是StatementHandler-> BOUNDSQL-->加工boundsql-->return ivk.proceed();

         mybatis 分页插件版本变化(忽略)
         MyBatis 3.4.1或者其以上版本(使用MyBatis 3.4.1(不包含)以下没有Integer.class)
         @Intercepts({ @Signature(type = StatementHandler.class, method = "prepare", args = { Connection.class，Interger.class}) })


18. sql的组成部分和执行过程
      mybatis中一条sql和它相关的配置信息由3个部分组成
         MappedStatement			用来获取某条sql的所有配置信息
         Sqlsource				   获取BoundSql对象
         BoundSql				      建立sql和参数的地方，插件功能中常修改这个地方

      一.PreparedStatement 是预编译的,对于批量处理可以大大提高效率. 也叫JDBC存储过程,是Statement的子接口，表示一条预编译过的SQL语句，可添加参数,可防止SQL注入
      二.使用Statement 对象。在对数据库只执行一次性存取的时侯，用Statement 对象进行处理。PreparedStatement对象的开销比 Statement 大，对于一次性操作并不会带来额外的好处。
      三.statement 每次执行 sql 语句，相关数据库都要执行 sql 语句的编译，preparedstatement 是预编译得,preparedstatement 支持批
         
      一条查询SQL的执行过程 ：
         1.预编译sql。Executor调用StatementHandler的prepare()方法预编译SQL，同时设置些基本运行的参数。
         2.设置参数。用parameterize()方法启用ParameterHandler设置参数，
         3.执行sql。完成预编译，执行sql 。
         如果是查询MyBatis会使用ResultSetHandler 封装结果返回给调用者。

19. 原始的ibatis的调用实例(忽略)
      ```     
         Class.forName("com.mysql.cj.jdbc.Driver");
         Connection connection = DriverManager.getConnection("url", "user", "password");
         PreparedStatement ps = connection.prepareStatement(sql); // ？作为参数占位符
         ps.setInt(1, 10);
         ps.execute();
         ResultSet resultSet = ps.getResultSet();
         while(resultSet.next()) {
            String value = resultSet.getString("columName");
         }
      ```	

20. mapper代理，mapper接口代理
         SqlSessionFactoryBean,主要是把*Mapper*.xml文件与*Mapper*.java加载进来，每个mybatis应用程序都以一个sqlsessionfactory对象的实例为核心。
         根据namespace加载对应的接口类到MapperRegistry，
         把方法名与*Mapper*.xml里的Select id对应起来等等。
         MapperRegistry相当于是一个缓存，后面创建代理对象是会用到。


      ***核心思路***
         mapper的动态代理，最后核心处理方法是sqlsession对象去运行对应的sql。mapper通过关联xml中的namespace，找到对应的执行方法。
         sqlSessioη.getMapper(RoleMapper.class);
         configuration.<T>getMapper(type, this)
         最终代理的是sqlsession的操作。通过namespace将sql和代理对象绑定起来

      ***核心思路***

         编写好的dao接口，1.通过sqlSessionTemplate.getMapper获取实例(需要在xml中配置好对应的sqlSessionTemplate bean)
                        2.使用mapperscaner 扫描生成接口实例,对mapper创建代理对象				 
         mybatis-spring中mapperScannerConfigurer,可以将映射接口直接转换为spring容器中的bean，即可注入service使用。
         扫描basepackage所指定的包下所有接口类（包括子包），如果在sql映射文件中定义过，则将他们定义为一个spring bean。


         创建一个代理核心MapperProxy，并使用jdk的动态代理创建代理类，将sqlsession封装在invoke方法中
         public static <T> T newMapperProxy(Class<T> mapperInterface, SqlSession sqlSession) {
            ClassLoader classLoader = mapperInterface.getClassLoader();
            Class<?>[] interfaces = new Class[]{mapperInterface};
            MapperProxy proxy = new MapperProxy(sqlSession);
            return (T) Proxy.newProxyInstance(classLoader, interfaces, proxy);
         }



      ```
         //自定义mapper代理，实现单一接口的代理，执行自定义逻辑
         Testmapper testceshi = (Testmapper) TestMapperProxy.newInstance("测试接口代理sql",Testmapper.class);
         
         public class TestMapperProxy<T> implements InvocationHandler {
            private String  sqlSession;
            private final Class mapperInterface;
            public TestMapperProxy(String sqlSession, Class mapperInterface) {
               this.sqlSession = sqlSession;
               this.mapperInterface = mapperInterface;
            }
            public static <T> T newInstance(String sqlSession,Class<T> mapperInterface) {
                  TestMapperProxy mapperProxy = new TestMapperProxy(sqlSession, mapperInterface);
               return  (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[]{mapperInterface}, mapperProxy);
            }
            @Override
            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
               if (Object.class.equals(method.getDeclaringClass())) {
         //          System.out.println("Object方法"+method.getName());
                  method.invoke(proxy,method,args);
               }else{
                  System.out.println("拿到操作对象:"+mapperInterface.getName());
                  //进行自定义操作
                  System.out.println("操作了对象参数:"+sqlSession);
               }
               return null;
            }
         }		
      ```	

      // mapper代理代理源码（忽略）
      ```
         //mapper的动态代理，核心就接口和sql中的配置，一个方法对应一个mapperMethod
         public class MapperProxy implements InvocationHandler, Serializable {
            private static final long serialVersionUID = -6424540398559729838L;
            private final SqlSession sqlSession;
            private final Class<T> mapperInterface;
            private final Map<Method, MapperMethod> methodCache;
            public MapperProxy(SqlSession sqlSession, Class<T> mapperInterface, Map<Method, MapperMethod> methodCache) {
               this.sqlSession = sqlSession;
               this.mapperInterface = mapperInterface;
               this.methodCache = methodCache;
            }
            //执行方法				
            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
               try {
                  if (Object.class.equals(method.getDeclaringClass())) {
                     return method.invoke(this, args);
                  }
                  if (this.isDefaultMethod(method)) {
                     return this.invokeDefaultMethod(proxy, method, args);
                  }
               } catch (Throwable var5) {
                  throw ExceptionUtil.unwrapThrowable(var5);
               }
               MapperMethod mapperMethod = this.cachedMapperMethod(method);
               return mapperMethod.execute(this.sqlSession, args);
            }
            //创建方法和sql配置的执行映射
            private MapperMethod cachedMapperMethod(Method method) {
               MapperMethod mapperMethod = (MapperMethod)this.methodCache.get(method);
               if (mapperMethod == null) {
                  mapperMethod = new MapperMethod(this.mapperInterface, method, this.sqlSession.getConfiguration());
                  this.methodCache.put(method, mapperMethod);
               }
               return mapperMethod;
            }
            public T newInstance(SqlSession sqlSession) {
               final MapperProxy<T> mapperProxy = new MapperProxy<T>(sqlSession, mapperInterface, methodCache);
               return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[]{mapperInterface}, mapperProxy);
            }
         }
      ```

21. spring和mybatis整合

      * 传统的spring整合
        思路：
            1.需要spring通过单例方式管理sqlsession
            2.spring和mybatis整合代理对象，使用sqlsessionfactory创建sqlsession(spring和mybatis自动完成）
            3.持久层的mapper都需要由spring进行管理

        整合过程：
            1.相关spring的jar mybatis的jar  和spring-mybatis整合jar
            2.配置sqlsessionfactory （1.configlocation：mybatis.xml和2.datasource）,存在spring-mybatis的jar中

         mybatis的运行过程两大步，
            1.服务配置文件缓存到Configuration对象，用以创建sqlsessionfactory
            2.sqlsession的执行过程。
                        
         sqlsessionfactorybuilder构建sqlsessionfactory两步。
            1.通过xmlconfigbuilder解析配置的xml文件(或注解)，存入Configuration类对象
            2.configuration创建sqlsessionfactory。

         mybatis中要使用插件，需要实现Interceptor接口。一般不推荐使用插件。

      * boot属性配置mybatis
         boot原本配置
            mybatis:
               mapper-locations: classpath:mappers/*.xml
               虽然可以配置这项来进行pojo包扫描，但我更倾向于在mapper.xml写全类名
               type-aliases-package: com.rhine.blog.po     配置之后好像可以省略xml中的包名，<resultMap id="userMap" type="UserBean">

            plus插件的配置，但是需要排斥其他的mybatis依赖，在boot中
               <!-- 
               <dependency>
                     <groupId>org.mybatis.spring.boot</groupId>
                     <artifactId>mybatis-spring-boot-starter</artifactId>
                     <version>1.3.2</version>
                  </dependency>
               -->
            mybatis-plus(暂时没用，忽略)
               插件介绍	https://www.cnblogs.com/leeego-123/p/10734330.html

               mybatis-plus:（忽略）
                  mapper-locations: classpath:mappers/*.xml
                  type-aliases-package: com.rhine.blog.po

         配置mybatis-spring-boot-starter需指定version，不属于boot启动依赖，不属于同一个groupId

         这个是放在资源路径下的
         mybatis.mapper-locations=classpath:/mybatis-mapper/*Mapper.xml

         #mybatis
         mybatis.mapper-locations=classpath*:com/xiaoyuer/pay/mapping/*.xml
         mybatis.config-location=classpath:config/mybatis-config.xml


22. jdbcTemplate是不能操作事务的
	 Spring中数据库事务是通过PlatformTransactionManager进行管理的,而能够支持事务的是TransactionTemplate模板，它是 Spring 所提供的事务管理器的模板
	 MyBati框架用得最多的事务管理器是 DataSourceTransactionManager

    重启db之类的操作，会导致大事务回滚，
    重启系统的时候，可能会有大事务，最好避开大事务执行时间


23. 使用mybatis
      执行顺序：
      配置mybatis配置文件，-->sqlSessionFactory-->sqlsession-->sqlsession内部executor执行器操作数据库-->mappedstatement（底层封装对象）

		MyBatis基于SqlSessionFactory(单例)构建的框架。对于SqlSessionFactory它的作用是生成SqlSession接口对象，这个接口对象是MyBatis操作的核心

		typeHandlers，mybatis中，用来写入和读取数据库过程中不同类型的数据(java-javaType，数据库-jdbcType)进行自定义转换。
		MyBatis 自动识别 javaType巳和 jdbcType，从而实现各类型转换。

		MapperFactoryBean 和 MapperScannerConfigurer 是有区别的， 这两个需要代码开发方式(一般不是首选)。前者单个，后者集中
		MapperFactoryBean 针对一个接口配置，而MapperScannerConfigurer 则是扫描装配，
		实际上@mapperScan更加简便使用建议使用


    原生jdbc访问：(忽略)
      需要加入mysql连接驱动 
      一般顺序，
      1、加载数据库驱动 				    Class.forName("com.mysql.jdbc.Driver")
      2、获取数据库连接				       connection
      3、获取拼装预处理sql			       prepareStament
      4、执行prepareStament得到结果集   resultSet

      问题存在：
      1、数据库连接，使用创建connection，不用立即释放，频繁开启关闭，影响性能 
         解决方案：使用数据库连接池

      2、将sql语句以及设置参数，获取resultSet，硬编码java代码中，维护不方便
         解决方案：sql语句和参数配置在xml文件中，resultSet转为java对象

24. dao的开发方法：
        1、原始dao开发方法，需要写dao接口和dao实现类(忽略)
            思路：	1.daoImpl可以继承sqlsessiondaosupport,注入sessionfactory属性值(由构造函数引入)，在方法体内通过sessionfactory，创建sqlsession
                    2.实现类中使用sqlsession操作，sqlsession在方法体内操作数据库

            总结：1、dao接口实现类存在大量的模板方法，
                2、调用sqlsession方法时，将statement的id硬编码，sqlsession.selectOne("test.findUserById",id)		首参是namespace的方法id,次参是传入参数
                3、调用sqlsession方法时传入变量，由于使用的是泛型，编译阶段即使变量传参类型错误，也发现不了问题

        2、mapper代理方法，只需要mapper接口(dao接口)
            1、namespace映射mapper
            2、xml中的id和mapper中方法名一致
            3、入参传参一致
            总结:封装了sqlsession的方法操作
                1、通过sqlsessionfactory得到sqlsession
                2、生成mapper代理对象 	UserMapper userMapper=sqlsession.getMapper(UserMapper.class )
                3、mapper方法执行，可以统一dmo或者map等对象入参(不灵活)，也可以@param多参数入参

        mapper代理开发
            1.使用mapperFactoryBean单一添加mapper代理对象，指定mapperInterface和sqlsessionfactory（因为bean继承了sqlsessiondaosupport），不常用，忽略
            2.项目中使用的是mapperscanner，扫描出mapper接口，自动创建代理对象并且在spring的容器中注入
         ```
            <bean class="....mapperscannerconfigurer">
                <property name="basePackage" value=""> 扫描多个包，使用半角，分割
                <property name="sqlsessionfactoryBeanName" value="sqlsessionfactory">      #不能配置name="sqlsessionfactory" 
            </bean>
            <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean">  
                <property name="dataSource" ref="dataSource" />  
                <!-- 自动扫描mapping.xml文件 -->  
                <property name="mapperLocations" value="classpath*:dao/*.xml"></property>  
            </bean>  
         ```

25. 	mybatis版本冲突（忽略）
      同一个项目中不版本需统一
      dao中引入了tk-mybatis，其中使用了3.4.5的mybatis版本。然后admin中引入的pagehelper和mybatis对应的是mybatis是3.5几的版本


26. 多环境数据源的配置
      根据环境选择数据源   使用jndi-JndiObjectFactoryBean，使用配置的数据源-DataSourceBuilder
         ```
            @Bean(destroyMethod="")
            public DataSource jndiDataSource() throws IllegalArgumentException, NamingException {
               Properties pro = new Properties();
               InputStream input =Application.class.getClassLoader().getResourceAsStream("application.properties");
               try {
                  pro.load(input);
               } catch (Exception e) {
                  logger.error("STARTUP ERROR WHEN LOADING application.properties", e);
               }	
               String debug = String.valueOf(pro.get("dev.debug"));
               if("0".equals(debug)){
                  JndiObjectFactoryBean bean = new JndiObjectFactoryBean();
                  bean.setJndiName("java:comp/env/jdbc/xye-manager");
                  bean.setProxyInterface(DataSource.class);
                  bean.setLookupOnStartup(false);
                  bean.afterPropertiesSet();
                  dataSource=(DataSource)bean.getObject();
               }else{
                  driverName = String.valueOf(pro.get("spring.datasource.driver-class-name"));
                  url = String.valueOf(pro.get("spring.datasource.url"));
                  userName = String.valueOf(pro.get("spring.datasource.username"));
                  password = String.valueOf(pro.get("spring.datasource.password"));
                  DataSourceBuilder factory = DataSourceBuilder  
                  .create(Application.class.getClassLoader()).driverClassName(driverName)  
                  .url(url).username(userName).password(password);  
                  dataSource= factory.build();
            }
         ```

            配置数据源可以使一个new DruidDataSource 返回
            JndiObjectFactoryBean bean = new JndiObjectFactoryBean(); 		jndi
            DataSourceBuilder 							普通数据源

            @ConfigurationProperties 使用方式有两种 
            1、在类上使用该注解 
            2、在工厂方法上使用该注解 （@bean）

            @Bean("sakilaDataSource")
            @ConfigurationProperties(prefix = "spring.datasource.sakila")
            public DataSource libraryDataSource() {
               return DataSourceBuilder.create().build();
            }

27. mybatis-plus  tkmybatis不同的快捷框架

28. 杂项记录
      * xml中<![CDATA[ ]]>标记的作为纯文本，编辑器忽略编译，但是一般不把<if>标签包括在内，包含sql语句部分即可。

      * 在xml中&amp;代表&，常见的配置数据库连接
         zeroDateTimeBehavior=convertToNull  否则java中,0000-00-00 00:00:00转义会失败

      * 每次的连上251都是一个数据库的连接，开两个连接调试

      * 数据库查询空字段使用is null，null的直接就不在比较的范围内

      * update delete insert 返回的int 都表示影响的行数

      * 配置数据库连接的时候只有在xml中&才需要转义成&amp;,在properties中不需要转义
         com.mysql.jdbc.Driver 是 mysql-connector-java 5中的，
         com.mysql.cj.jdbc.Driver 是 mysql-connector-java 6以后的

      * xml中加减的时候，ua.account_fix = (ua.account_fix + #{param.freezeAmt}) 可以是一个负数



#### 动态的数据元切换(忽略)

	aop切面相关
	实现接口	   MethodBeforeAdvice该拦截器会在调用方法前执行
   实现接口   	AfterReturningAdvice该拦截器会在调用方法后执行
   实现接口  	MethodInterceptor该拦截器会在调用方法前后都执行，实现环绕结果。

   JDBC通常有java语言标准接口，由第三方或者数据库厂商来提供驱动程序的实现，即驱动程序就是实现接口的，主要组成的接口有
      操作数据库时最终都是通过与服务器建立socket来通信的。
      Driver			用于注册到Drivermanager中并提供对外统一的connection的接口
      Connection		连接的对象，包装了连接操作的对象
      DataSource		对Connection操作的相关管理API,它的实现部分一般由第三方提供
      Statement		提供通用的语句级别对象处理，子类接口为preparedStatement
      ResultSet		返回值结果集的一个接口，包含对结果集操作的规范API的定义，如next(),getString()等

1. 动态数据源的切换，datasource必须继承AbstractRoutingDataSource类的，再结合aop的标签类，根据方法动态放入配置的数据源

2. **动态切换数据源和service的事务关系**

   * 在事务开启后，数据源就不能再进行随意切换了(一个事务对应一个数据源)。否则会切换失败

   * <tx:annotation-driven transaction-manager="transactionManager" order="2"/>  
     修改事务管理器的数据源为动态数据源，指定事务注解的排序为2，并指定切换数据源的注解为1，这样在事务之前切换数据源，否则在事务之后切换的的话，无效。
     定义数据源的aop标签加载顺序是1，保证数据源在事务之前
     @Aspect  
     @Order(1)  

   * 代码配置，主要就是事务和数据源注解的aop顺序

     ```
     <!--这样才能进行数据库事务的相关操作 -->
     <tx:advice id="transactionAdvice" transaction-manager="transactionManager">
      	<tx:attributes>
        	<tx:method name="add*" propagation="REQUIRED"/>
       	</tx:attributes>
      	</tx:advice>
     <!-- 配置数据库注解aop -->
     <bean id="dataSourceAspect" class="com.we.database.DataSourceAspect"/>
     <aop:config>
     	<aop:pointcut id="transactionPointcut" expression="execution(* com.wewe.licai.service..*Impl.*(..))"/>
     	<aop:advisor pointcut-ref="transactionPointcut" advice-ref="transactionAdvice" order="2"/>
     	<!--数据源选择切面,保证在事务开始之前执行-->
     	<aop:advisor pointcut-ref="transactionPointcut" advice-ref="dataSourceAspect" order="1" />
     </aop:config>
     #这里的advice-ref，指定了具体的事务逻辑
     #一个Spring增强（advisor）=切面(advice)+切入点(PointCut)
     ```

   * 数据源的切换，aop的代码实现，aop实现过程中可以根据方法名选择数据源，也可以自定义标签加载相应的数据源

   * DynamicDataSource继承 AbstractRoutingDataSource.java  重写父类 determineCurrentLookupKey  用来获取当前线程链接的数据源名
     数据源动态切换主要由重写AbstractRoutingDataSource中determineTargetDataSource()方法，ThreadLocal 存储数据源名
     也有静态多数据源，@Transactional(value = "prodTransactionManager") 在每个事物上都标注好事物管理器，但是动态的可以拦截切换


3. jdbc层面上，一个库的事务是同在一个通信连接上的，数据库端也保持了一个对应的连接状态，才会有会话与事务的概念。
	spring将connection放入一个threadlocal变量中，是一个Map<DataSource,connection>
	具体使用的时候，动态代理对connection进行二次包装，使用的是connection的代理类





## Springboot相关

   https://docs.spring.io/spring-boot/docs/2.1.1.RELEASE/reference/htmlsingle/   boot官方文档

   约定优于配置的原理
      自动配置加载一个类的配置时，首先读取项目中的配置，项目中没有才启用默认配置,屏蔽Spring内部的细节
	
   starter组件
      对于项目必须的功能，boot 提供starter的maven依赖(使用默认的自动配置类，将对应的jar包加载到工程中，并将绑定的服务器加载到工程中)
      boot中的starter组件，解决了pom中复杂依赖关系的繁琐配置，这样可以模块化配置，简化依赖管理配置。
      通过start包的依赖，来实现功能模块的整体依赖，不需要单独零散的配置maven依赖了。
	
1. springboot默认contextpath的路径为/,部署在了root下       @RestController =@Controller +  @ResponseBody 

   @configuration声明当前类时一个配置类，相当于spring的xml文件
   @bean注解在方法上，声明当前方法返回一个bean，方法名就是返回的bean名称
   一般@Configuration 和 @bean是搭配使用的

   在spring容器中，只要容器中存在某个bean，就可以在另一个bean的声明方法的参数中写入，入参中
		@Bean
		public A (B b){
			return a;
		}

	这里b实际上是容器中的一个bean

2.  引入xml配置
    在启动类上加上@ImportResource({"classpath:config/spring-mvc.xml"，"classpath:spring.xml"})即可注入xml配置 ，配置spring-mvc建议使用官方的xml头文件

3. springboot整合mybatis

      一般spingboot整合mybatis：1.dataSource 2.sqlSessionfactory  3.mapperscan  
      思路是由mapper映射xml查询数据库，mapperscan-sqlfactory-datasource

      需要引入数据库连接依赖
      主要是配置sqlsessionfactory（注入datasource目前是application中配置，内含*mapper.xml配置，
      @Configuration和@EnableTransactionManagement(proxyTargetClass=true)）和
      mapper扫描（需要在sqlsessionfactory配置完成之后，@Configuration和@AutoConfigureAfter(MybatisConfig.class)），
      具体实现是mybaitsConfig.java和mapperScanner.java

      如果是java代码配置(现在都用boot配置，忽略)
      需要指定@AutoConfigureAfter(MybatisConfig.class)，mapperscan需要sqlSessionfactory配置后才生效

      Boot中,若项目依赖了mybatis-spring-boot-starter后，SpringBoot会自动在IoC容器中自动创建一个 DataSourceTransactionManager事务管理器(还有MyBatis sqlSessionFactory、    sqlSessionTemplate内容，自动创建，直接注入使用事务管理器即可)


      springboot  bean.setTypeAliasesPackage("com.xiaoyuer.core.**.dmo");
      通配符配置多路径，com.xiaoyuer.core.dmo和com.xiaoyuer.core.talents.dmo 
	
4. boot中的web配置

   SpringBootServletInitializer初始化servlet代替了web.xml
   boot中WebMvcConfigurer配置web属性，然后boot中配置类 WebMvcAutoConfiguration 自动配置(即属性中的mvc配置)
   
   springboot中的：
   		WebMvcConfigurationSupport 与WebMvcConfigurerAdapter 都可以配置MVC,
   		WebMvcConfigurationSupport 支持的自定义的配置更多更全，
   		WebMvcConfigurerAdapter有的WebMvcConfigurationSupport 都有


   springboot中开启spring mvc的使用
      1. 继承webmvcconfigureradapter，重写相关的配置方法，
      2. 必须使用@enablewebmvc开启springmvc的支持，
      3. 添加拦截器 WebMvcConfigurationSupport或上面的webmvcconfigureradapter，重写addInterceptors方法添加自定义的拦截器
      4. 项目中需要很多地方使用仅页面转向功能，直接使用addviewcontrollers添加即可

      
      spring boot默认选择禁用后缀模式匹配
      需要开启spring.mvc.pathmatch.use-suffix-pattern=true
      配置/       路径是 /aaa      aaa.htm不进
      配置*.htm   路径是 /aaa      aaa.htm进

   实现mvc的相关配置,	WebMvcConfigurationSupport 与WebMvcConfigurerAdapter 都可以配置MVC,
	WebMvcConfigurationSupport 支持的自定义的配置更多更全，
	WebMvcConfigurerAdapter有的WebMvcConfigurationSupport 都有
	mvc.pathmatch.use-suffix-pattern: true #启用这个之后， server.servlet.path配置后缀才会生效。(mvc配置拦截路径)




5. Spring Boot应用程序在启动后，会遍历CommandLineRunner接口的实例并运行它们的run方法，实现启动类的加载，从容器中寻找。



6. boot创建maven工程
      创建boot父工程起始就决定你是普通maven工程还是boot工程。是boot就直接父，普通就直接maven
      普通maven工程，子模块再继续


      简单概括，子模块用boot好处是目录和启动类直接ok，但是pom中的parent引用要变，这个改动小点。
      子模块用普通maven，好处是pom中的parent引用现成的，但是目录和启动类没有。

      建议创建聚合工程，都是用maven创建子模块。但是boot子模块启动目录和类没有
      子模块用boot，需要更新更改下parent引用
      如果是单独一个boot工程直接boot创建

      两种都可以
      起点普通mavenpom，就需要删除目录，复制引用(可以从子boot模块中复制)。
      起点是boot，pom工程，一步到位，

      这里其实也可以普通maven pom，然后子模块直接建boot，这样两个不是关联的，不存在jar的相互引用，就是boot不继承pom
         子模块不一定要继承parent，module聚合功能1.公用父依赖，2.整体打包

      方式1 
         1.建普通maven工程，删除无效目录，直接在父pom中引入spring-boot-starter-parent。  #需要删除手工目录
            File --> New --> Project --> Maven --> Next --> 填写包路径，项目名称 --> Next --> 修改pom.xml文件
            更改为pom工程
         2.创建其他module(修改pom继承父pom)
         
      方式2
         1.直接建boot父工程，只建一个pom.xml不需要其他的目录    							#目录结构不需要变
         2.创建其他module(修改pom继承父pom)												
         
      maven modules(模块)
         pom.xml中的modules用来管理同个项目中的各个模块
         1.需求场景
            如果我们的项目分成了好几个模块，那么我们构建的时候是不是有几个模块就需要构建几次了（到每个模块的目录下执行mvn命令）？当然，你逐个构建没问题，但是非要这么麻烦的一个一个的构建吗，那么简单的做法就是使用聚合，一次构建全部模块。
         2.具体实现
            a.既然使用聚合，那么就需要一个聚合的载体，先创建一个普通的maven项目account-aggregator,
            因为是个聚合体，仅仅负责聚合其他模块，那么就只需要上述目录，该删除的就删了；注意的是pom文件的书写（红色标明的）：

         子模块中不一定要parent继承，这是eclipse默认补全的，不是必须的。


      springboot中的maven多模块继承
         boot 的两种方式(考虑单继承的原因)，一种是直接继承bootparent，还有一种是版本管理。本质上都是集中定义了boot相关jar的版本号(版本管理)
            <dependencyManagement>
               <dependencies>
                  <dependency>
                     <groupId>org.springframework.boot</groupId>
                     <artifactId>spring-boot-dependencies</artifactId>
                     <version>2.2.11.RELEASE</version>
                     <type>pom</type>						默认是jar，，设置为pom说明导入的是一个父模块
                     <scope>import</scope>					解决单继承的问题
                  </dependency>
               </dependencies>
            </dependencyManagement>
            
            后面使用一个stater-web  中自动附带了boot-starer的jar。
            <type>pom</type>，<scope>import</scope>，只能在dependencyManagement中使用，且type为pom类型。


7. application.properties配置，属性配置相关， 常用的boot属性配置
      
      1. 属性文件中debug=true  就可以查看详细的加载配置
      2. 在boot中使用了server.servlet.session.cookie.name=ADMINSESSION来指定cookieName
		3. spring.profiles.active=${environment} 放在初始的属性文件中，后续添加sit、pre多个环境的配置
      4. 路径匹配，后缀匹配
         #start url match mode
         server.servlet.path=*.htm
         spring.mvc.pathmatch.use-suffix-pattern=true				开启后缀匹配模式，默认为false，不支持/users匹配/users.*，开启后支持后缀匹配，

         RequestMapping("/test")可以跟/test匹配，但是匹配任何不了带后缀的请求。
         Spring MVC是默认支持匹配后缀的，如/test.json,/test.html，哪怕是/test.55都是可以匹配。
         但是boot需要单独开启

         server.servlet-path=/xiaomaomao
         这里配置的 DispatcherServlet 拦截路径就是 /xiaomaomao,但是我们配置 DispatcherServlet 都是使用默认的路径 / (拦截所有请求,不拦截 jsp)

      boot中的多环境配置
         application.properties
         application-{profile}.properties
         spring.profiles.active=dev 开启默认的profile属性


8. boot的自动配置原理：
      自动配置是通过spring-boot-autoconfigure的jar包实现的(配置核心)，其中有很多的配置类，加载默认配置。

      1. @SpringBootApplication启动入口，是boot的入口
      2. boot在springapplication对象实列化时会加载META-INF/spring.factories文件，将该文件中的配置加入到spring的容器（对应是各自功能的AutoConfigurationBean
      3. 根据条件注解@ConditionalOnMissingBean等，配置相关的功能bean(由jar引入)，）各bean根据属性读取自动配置
		
		spring-boot-autoconfigure-2.0.0.RELEASE.jar 包下的自动配置 /META-INF/spring.factories，其中是加载自动配置项

      其中@conditional、@conditionalOnClass、@ConditionalOnMissingBean 和 @EnableConfigurationProperties(开启注解属性配置) 属于条件注解，实现条件加载
      这里@Conditional(RedisChooseConfig.class)，其中RedisChooseConfig implements Condition，实现条件装载bean

      
      关于自动配置：boot在启动时，都要做类似，thymeleaf在不在classpath下，在就配置一个模板解析器，视图解析器，模板引擎，类似的判断
      在springboot中的spring-boot-autoconfigure包是boot的自动配置包，默认读取class下的资源配置
      自动配置的重点，classpath下有，就自动配置，一般通过起步依赖加当class下


9. @SpringBootApplication扫描范围
      SpringBoot  默认扫描@SpringBootApplication所在类(启动类)的目录包以及其子包                
      如果当前启动类没有包，则在启动时会报错,启动类不能直接放在main/java文件夹下,必须要建一个包把它放进去或者使用@ComponentScan指明要扫描的包。
      

10.   原先使用<tx:advice>定义事务策略，使用<aop:config>定义切点，并融合策略。
      boot中推荐使用@transactional 实现申明事务，需要jdbc起步依赖，自动创建事务管理器，
      需要 @EnableTransactionManagement 开启事务(这个好像是默认开启了)，相当于<tx:annotation-driven /> 

11. 多个事务管理器(忽略)
      ```
         //实现接口TransactionManagementConfigurer方法，用来多事务管理器下，指定默认管理器，
         @EnableTransactionManagement // 开启注解事务管理，等同于xml配置文件中的 <tx:annotation-driven />
         @SpringBootApplication
         public class ProfiledemoApplication implements TransactionManagementConfigurer {
            @Resource(name="txManager2")
            private PlatformTransactionManager txManager2;
            
            // 创建事务管理器1
            @Bean(name = "txManager1")
            public PlatformTransactionManager txManager(DataSource dataSource) {
               return new DataSourceTransactionManager(dataSource);
            }

            // 创建事务管理器2
            @Bean(name = "txManager2")
            public PlatformTransactionManager txManager2(EntityManagerFactory factory) {
               return new JpaTransactionManager(factory);
            }

            // 实现接口 TransactionManagementConfigurer 方法，其返回值代表在拥有多个事务管理器的情况下默认使用的事务管理器
            @Override
            public PlatformTransactionManager annotationDrivenTransactionManager() {
               return txManager2;
            }

            public static void main(String[] args) {
               SpringApplication.run(ProfiledemoApplication.class, args);
            }
         }
      ```

      测试bean，有个小细节就是框架会自动为我们注入有参参数，容器默认的bean。(测试用，忽略)
      @Bean
      public Object testBean(PlatformTransactionManager platformTransactionManager){
         System.out.println(">>>>>>>>>>" + platformTransactionManager.getClass().getName());
         return new Object();
      }




12. boot启动方式
		1.java 启动类中，main启动
		2.maven 配置中boot插件，mvn:spring-boot:run启动
			maven启动
			 <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
			
			这样开发时不用每次去找主类启动或者打包jar运行，通过mvn:spring-boot:run快速启动
			但是对于编译器中的小虫子debug，也挺方便找的，这样命令实际用的不多
		3.服务器上，java -jar xxx.jar
			命令行可以追加参数，优先级高
			--server.port=8888的方式会覆盖properties中的属性值。

      4. 通过Servlet容器启动，如Tomcat、Jetty等。


      启动方式(忽略)
         基本常规使用
            SpringApplication.run(SpringBootApplication.class, args)

         自定义SpringApplication
            通过SpringApplication API调整
               SpringApplication springApplication = new SpringApplication(SpringBootApplication.class);
               springApplication.setBannerMode(Banner.Mode.CONSOLE);
               springApplication.setWebApplicationType(WebApplicationType.NONE);
               springApplication.setAdditionalProfiles("prod");
               springApplication.setHeadless(true);

            通过 SpringApplicationBuilder API调整
                  new SpringApplicationBuilder(SpringBootApplication.class)
                  .bannerMode(Banner.Mode.CONSOLE)
                  .web(WebApplicationType.NONE)
                  .profiles("prod")
                  .headless(true)
                  .run(args);



13. 杂项
      * springboot中使用jsp，支持需要添加tomcat-jsp解析包(忽略)
            ```
               <dependency>
                  <groupId>org.apache.tomcat.embed</groupId>
                  <artifactId>tomcat-embed-jasper</artifactId>
               </dependency>
            ```

         boot创建jsp文件的时候把后缀加上
         boot中使用jsp
         1、启动类继承SpringBootServletInitializer
         2、相关的maven依赖，关键<artifactId>tomcat-embed-jasper</artifactId><scope>provided</scope>，使用外部的tomcat实现
            <artifactId>spring-boot-starter-tomcat</artifactId> <scope>provided</scope>
            
         3、需要新建webapp文件夹，然后再添加web-inf/jsp文件夹，打war，部署tomcat访问
         

         jsp不能用Application.java的main方法启动，只能用Tomcat启动，并且要打war包，所以修改打包方式为war：<packaging>war</packaging>
         在src/main/下创建webapp文件夹，在webapp文件夹下创建WEB-INF文件夹

      
         https://www.cnblogs.com/zs-notes/p/9365705.html
         需要这三个依赖，更重要的是必须将创建webapp目录，下面创建WEB-INF,然后放文件进去使用，还是使用老板的webapp结构
         若不借助任何模板引擎的话，JSP页面放在resources目录下（包括缺省的几个子目录）是访问不到的！
         <artifactId>javax.servlet-api</artifactId>    
         <artifactId>jstl</artifactId>
         <artifactId>tomcat-embed-jasper</artifactId>***
         
         
         jsp视图文件应该放在webapps下，使用模板引擎，js、图片等静态资源文件才是放resources
         1.想放到resource下必须使用模板引擎，	
         1.直接放在webapp下，以前的使用方法
         2.可以借助模板引擎，或者META-INF实现，路子比较野，不常用

         server.servlet.jsp.init-parameters.development=true  实时更新

         普通的mvc配置，一般用来配置jsp 
         #spring.mvc.view.prefix=/templates/
         #spring.mvc.view.suffix=.html
         需要在webapp下建WEB-INF等常规的web目录


         
         使用Thymeleaf实现mvc，遵循默认的原则
         根据默认原则，静态资源应放置在src/main/resources/static下
         根据默认原则，页面应放置在src/main/resources/templates下



         boot默认是不推荐jsp的，内嵌的tomcat不支持jsp,集成jsp需要引入相关的maven依赖，需要创建webapp的文件夹，打成war后在外置的tomcat容器运行

         springboot建议是用freemaker，jsp不推荐（页面可以编写java代码，前后端分离不推荐），不需要加载成class文件，技术更新
		   使用jsp，还要加入tomcat-embed-jasper依赖，还要创建webapp目录，不推荐  https://segmentfault.com/a/1190000012263699


      * 新建的项目，boot默认将web引用打包为jar，而非war，因为默认的web模块依赖会包含嵌入式的tomcat，这样应用jar自身就具备了提供web服务的能力。

      * @SpringBootApplication(exclude={a.class})排除自动配置的类，排除的类在spingboot-autoconfigure包中有（忽略）
         一个组合注解，开启自动配置，boot会扫描该注解所在类的同级包和下级包下的bean

      * 日志配置， 加载的优先顺序是logback.xml--->properties--->logback-spring.xml，一般用后者，不行用前者，区别在于是否使用属性值

      *  @EnableDubbo是@EnableDubboConfig以及@DubboComponentScan合并的一个注解。
         EnableDubboConfig：实现Dubbo的配置，功能类似与xml配置文件中的各种配置，在该注解中可以实现类似的功能，通过绑定的方式来实现。
         DubboComponentScan：扫描Dubbo的组件类，将Dubbo的组件类自动注入到Spring容器中。

      * 在同一个configuration中
         @bean中引用另一个bean
         1.bean(),直接调用方法拿到一个bean，(被@Bean注解的方法会被Spring重写，多次调用会返回同一个Bean对象)
         2.在入参中直接引入springbean，类型-名称,前提是在同一个configuration中

      * boot中的yml格式也挺好用的有提示，空格区分，:后面需要加上空格，配置mybatis也可以，mvc相关等

      * freemarker和Thymeleaf语法，两种不同的模板引擎，语法不同，前端显示而已，从语法习惯上来讲，更推荐freemarker
         spring.freemarker.template-loader-path=/WEB-INF/template,     最后必须要加","  不然访问不到   这个需要精确到ftl的文件位置

      * 静态资源
         1.虽然SpringBoot默认的静态资源地址是resources下的static，但是他只限存放文件，即如果你在static下直接放css样式是可以访问的，
         但是为了管理，我们一般都会在static下新建文件夹，如css，js等分类。这样的话就会出错了，因为SpringBoot不会访问到static文件夹下的子文件夹。

         boot 2.0后 拦截器尽量不要拦截静态资源
            boot说下默认映射的文件夹有：
            *****注意是classpath! 注意是classpath! 注意是classpath!***** 都在WEB-INF下，以前的webapp可以直接访问
            spring boot对静态资源的默认扫描路径是：
               classpath:/META-INF/resources
               classpath:/resources
               classpath:/static
               classpath:/public
               上面这几个都是静态资源的映射路径，优先级顺序为：META-INF/resources > resources > static > public
               
               *********
                  我们可以通过修改spring.mvc.static-path-pattern来修改默认的映射规则
               *********
               
            上面五个目录下放静态资源（比如：a.js等），可以直接访问（http://localhost:8080/a.js），类似于以前web项目的webapp下；放到其他目录下无法被访问。
            WebMvcAutoConfiguration类自动为我们注册了如下目录为静态资源目录，也就是说直接可访问到资源的目录。
            自定义静态资源映射目录的话，只需重写addResourceHandlers
               
            boot的 templates文件夹，是放置模板文件的，因此需要视图解析器来解析它。所以必须通过服务器内部进行访问，也就是要走控制器--服务--视图解析器这个流程才行。
               
            这里boot可以使用默认的配置，static下的文件可以直接访问(下级目录也可以，比如js/a.js，可省略static不写)，
            这样就理清了，别的自定义目录就需要addResourceHandlers单独配置了
            比如templates下的目录，就要去单独配置资源映射了
            
            templates 一般需要配置对应的模板，jsp  ftl 和 thymeleaf 等  配置view的映射，属于模板路径

      * <property name="logbackpath" value="../logs/"></property> springboot的jar启动，当前的位置就是jar的位置
	      自定义了log配置以后，简单的配置也需要<appender name="STDOUT">,否则控制台不会打印日志

		



14. 创建boot工程 
      1.http://start.spring.io/  快速创建工程   官网的模板下载
      2.自定义banner的地址：http://patorjk.com/software/taag
      3.使用Spring Tools Suite，其中使用spring stater project创建工程boot工程，是属于eclipse的一个插件
      4.单纯的简历maven工程，添加boot的parent依赖，添加相关的简化配置
      5.使用idea创建boot工程，使用spring initializr

   
15. 使用https配置(忽略)
      1、需要一个证书，可以自己生成或者购买。下面是我们通过keytool自己生成。 生成一个PKCS12格式的叫做keystore.p12的证书
      生成命令： keytool -genkey -alias tomcat -storetype PKCS12 -keyalg RSA -keysize 2048 -keystore keystore.p12 -validity 3650

      2、在application.properties中配置HTTPS

      server.port:8443
      server.ssl.key-store: classpath:keystore.p12
      server.ssl.key-store-password: aqjcpt
      server.ssl.keyStoreType: PKCS12
      server.ssl.keyAlias: tomcat
      这就完成了SpringBoot的HTTPS协议配置，重新启

      springboot https http   这个也没啥 直接搜索配置即可，生产通过nginx实现https到http的转发,使用到TomcatServletWebServerFactory开启http
	   本身属性文件中添加ssl证书相关

16. springboot 注册 servlet、filter、listener
      1、@configuration配合@bean标签，通过ServletRegistrationBean、 FilterRegistrationBean 和 ServletListenerRegistrationBean 获得控制。
      2、在 SpringBootApplication 上使用@ServletComponentScan 注解后，Servlet、Filter、Listener 可以直接通过 @WebServlet(urlPatterns = "/test/*")、@WebFilter、@WebListener 注解自动注册,这些注解都是JDK的，无需其他代码。
      
      方法二： 
      1）写一个WebMvcConfig类继承WebMvcConfigurerAdapter 
      2）使用@EnableWebMvc，@Configuration注解 
      3）写一个filter的初始化方法，用@Bean注解 
      4）写一个FilterRegistrationBean的初始化方法，用@Bean注解，在初始化方法中配置之前的filter。 

      interceptor：实现WebMvcConfigurerAdapter，或者WebMvcConfigurationSupport可以注册interceptor，
               新版本实现WebMvcConfigurer，addInterceptors方法添加自定义的拦截器即可
      
      过滤器一日游
      servlet-get过滤器一日游
      myinterc prehandler
      myinterc posthandler
      myinterc aftercompletion
      
      拦截顺序时filter-servlet-interceptor
      
      静态资源映射：同上

      boot中配置静态资源ResourceHandler后，静态资源路径不会再进入拦截器了。
      
      boot 默认了如下的资源目录，可以直接访问到资源的目录。类似webapp
      1.classpath:/META-INF/resources/ 
      2.classpath:/resources/
      3.classpath:/static/ 
      4.classpath:/public/
      5./：当前项目的根路径
         优先级从上到下
         可以自行修改 spring.resources.static-locations
         addResourceHandler 时出了以上目录外的资源映射，可以跳过mvc拦截
		

17. spring-boot-maven-plugin插件
      spring-boot-maven-plugin 插件可将工程打成一个fat jar，直接java -jar 执行即可，和普通的maven打包相比，除了classes下内容，还包含lib，和boot的一些jar启动的类
      spring-boot-maven-plugin(打的jar包是可直接java -jar  name.jar执行)
         1.只在独立运行的模块(启动项工程，如provider)pom中添加,
         2.作为jar依赖的项目Module不要引入该插件。(不需要有启动类，也不需要执行)
         3.不要定义在聚合工程父pom.xml(或者使用maven-compiler-plugin也行)，否则每个子模块都要启动main class，否则mvn install报错。父pom不需要<build>

      spring-boot-maven-plugin原因：因为这个插件会重新打包(repackage),打包成spring-boot可通过-jar形式运行的jar或war包，并且把原来maven打包重名称成.war.original结尾

18. 异常信息
      server.error.path=/error.htm   这是默认的异常路径
      boot freemarker中默认前缀是/，所以资源路径要设置到精确的位置。可以server.error.path=/error.htm 控制异常路径 ，搭配ErrorController使用
      ErrorController可对全局错误进行处理，但是其获取不到异常的具体信息，例如对自定义异常的处理
      @ControllerAdvice可对全局异常进行捕获，而404这样不会进入控制器处理的异常不起作用，可搭配@ExceptionHandler使用
      RestControllerAdvice	  相比增加了json返回

19. 部署tomcat的步骤(忽略)
      1、项目设置成war
      2、将springboot-starter-tomcat的范围scope设置为provided，时打包时将改包排除，因为放独立的tomcat中时不需要的
      3、启动类继承SpringBootServletInitializer ，这样在tomcat下没有web.xml也能运行，具体的启动类在org下的相关类中
      重写@Override
      protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) {
         return builder.sources(applicationClass);
      }
      设置启动类，用于独立的tomcat的入口
      在拥有 @SpringBootApplication 注解的类中，使用 SpringApplication 的 run 方法可以通过JAR启动项目。
      继承 SpringBootServletInitializer 类并实现 configure 方法，使用 application 的 sources 方法可以通过WAR启动项目
      
      spring-boot-starter-web 包含了 Spring MVC 的相关依赖（包括 Json 支持的 Jackson 和数据校验的 Hibernate Validator）和一个内置的 Tomcat 容器，这使得在开发阶段可以直接通过 main 方法或是 JAR 包独立运行一个 WEB 项目。
      而在部署阶段也可以打成 WAR 包放到生产环境运行。
      
      实际部署，将war拖到webapps下，只保留一个root的空目录，解压到root下，删除war即可


      ====================netpay boot ==========================
      普通的是在webapp下部署的，但是boot的本地是直接在xye-netpay-boot/target/classes/下运行的
      file:/C:/Users/xiaoyuer/git/xye-netpay/xye-netpay-pom/xye-netpay-boot/target/classes/

      然后lib 和web-inf下的文件貌似是取的webapp下的工程目录，比如jar 和静态文件。

      ====================netpay war ==========================
      使用的ParallelWebappClassLoader，context: xye-netpay
      file:/D:/apache-tomcat-8.5-pay/webapps/xye-netpay-war/WEB-INF/classes/

      本地boot运行环境 ,生产是war，在tomcat下运行，使用的是以上的资源文件，boot本地直接运行target/classes
      导致boot在target下加载不到文件  原先是tomcat启动没有影响。记得测试war后真实加载的目录

20. 配置文件中的属性加载
		Properties pro = new Properties();
		InputStream input = EManagerAdminApplication.class.getClassLoader().getResourceAsStream("application.properties");
		pro.load(input);
		
		String debug = String.valueOf(pro.get("xye.env"));
			<resource>
                <!--打包该目录下的配置文件 -->
                <directory>src/main/resources</directory>
                <includes>
                    <include>**/*.yml</include>
                    <include>**/*.xml</include>
                </includes>
                <!-- 启用过滤 即该资源中的变量将会被过滤器中的值替换 -->
                <filtering>true</filtering>
            </resource>
		
	
		boot默认的占位符是@key@,想使用${},使用下面的插件
		将占位符替换为@@(如@username@)。
		当前项目的pom使用resources-plugin，并指定使用默认delimiter。
		
		坑
			<plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-resources-plugin</artifactId>
                <configuration>
                    <useDefaultDelimiters>true</useDefaultDelimiters>
                </configuration>
            </plugin>
		
		filtering对应的是yml中的spring:profiles:active: ${environment} 这里才能激活，  <profile>中的properties加载的属性，${env}在properties文件中可以读取

		发现spring-boot为了保护application.yml和application.properties，修改了默认的占位符${...}为@...@，然后发现resources插件有一个配置项：
		<useDefaultDelimiters>true</useDefaultDelimiters> 可以环境再属性文件中使用@${}


21. spring-boot 热加载(忽略)

      热部署：动态替换你修改的class，效率会更高。
      热启动：修改代码保存时会自动重新启动项目

      热部署
         使用 Spring Loaded
         当系统通过 mvn spring-boot:run启动或者 右键application debug 启动
         （系统监视classes文件，当有classes文件被改动时，系统会重新加载类文件，不用重启启动服务）

         Spring-boot 1.5.6.RELEASE版本在Intellj可以做到自带热部署(左下角会自动1 class reloaded Stop debug session)

      热启动
         使用 spring-boot-devtools（项目会自动重启，一般不用）
         1.增加依赖，
         2.plugin中添加
         <configuration>
            <fork>true</fork> <!-- 如果没有该配置，devtools不会生效 -->？
         </configuration>
         3.File -> Settings -> Compiler，勾选 Build Project automatically
         4.按快捷键Ctrl+Shift+Alt+/，选择1.Registry..---->勾选 compiler.automake.allow.when.app.running 即可






## 拦截器，过滤器，Servlet

1. 注意Filter的匹配规则与servlet一样，但对于filter，不会像servlet那样只匹配一个servlet，因为filter的集合是一个链，所以只会有处理的顺序不同，
   而不会出现只选择一个filter。Filter的处理顺序和filter-mapping在web.xml中定义的顺序相同。
   
2. 执行顺序： Filter -> Servlet -> Interceptor
  
	过滤前-拦截前-action执行-拦截后-过滤后
	先走filter,然后走servlet,然后回到filter，一个filter可以用chain.doFilter()分成前后两部分。 

   两个filter 和一个servlet测试
		输出结果是：
		filter_1_before
		filter_2_before
		来到servlet了
		filter_2_after
		filter_1_after

3. 三者的区别
   * Filter与Servlet的区别在于：Filter不能直接向用户生成一个请求或者响应（只是修改对某一资源的请求或某一的响应。）。
      完整的流程是：Filter对用户请求进行预处理，接着将请求交给Servlet进行处理并生成响应，最后Filter再对服务器响应进行后处理。

   * interceptor 和 filter的区别
      interceptor不是在web.xml,不针对URL，只针对action,当页面提交action时，进行过滤操作，可由action自己指定用哪个interceptor来在接收之前做事
      Filter是一个链式处理，只要你想继续处理就可以传递下去；而Servlet则是一次处理并返回，可以对几乎所有请求起作用
      

   * Filter面对的是所有的请求，而HandlerInterceptor是面对具体的Controller。
     Filter总是先于HandlerInterceptor发挥作用，在Filter中甚至可以中断请求，从而使它无法到达相应的Servlet。
     而且两者的配置也不一样，Filter是在web.xml中进行配置，HandlerInterceptor是在具体的applicationContext.xml中进行配置。

   * 1.针对url：servlet，filter都是针对url进行的操作，
     2.针对对象：listener是针对对象的操作，它是在某个对象发生某些动作的时候执行，所以listener是提前封装好的对特定的对象的操作，只需要声明名称和类的位置即可。
     3.针对action：interceptor是针对action对象进行操作的，它在配置的时候需要和action一起配置才能起作用，当页面提交action时，进行过滤操作。

   * 1）Filter基于回调函数，而Interceptor则基于java本身的反射机制,这是两者最本质的区别。
     2）Filter过滤范围比Interceptor大,Filter除了过滤请求外通过通配符可以保护页面，图片，文件等等，而Interceptor只能过滤请求。

4. 一次请求只会成功匹配到一个servlet，但是filter只要匹配成功，这些filer都会在请求链filterchain上被调用。filterchain上所有的filter对象执行完成后，会执行最终的servlet。

### Servlet

1. 路径匹配，三种不能组合
   	当有一个servlet匹配成功以后，就不会去理会剩下的servlet了。  
	   url-pattern解析规则，对于servlet和filter是一样的
         1.精确匹配
            如/foo.htm，只会匹配foo.htm这个url
         2.路径匹配	
            如/foo/*会匹配以foo为前缀的url，（以“/”字符开头，并以“/*”结尾），filter中用的较多
            先最长路径匹配，再最短路径匹配
         3.后缀匹配(扩展名匹配)	
            以“*.”开头
            如*.htm会匹配所有以.htm的url
         4.缺省匹配
            以上都找不到servlet，就用默认的servlet，配置为"/"
         
            配置“/”后，因servlet容器有内置的“*.jsp”匹配器，而扩展名匹配的优先级高于缺省匹配。
            静态资源映射不需要考虑jsp，容器自带*.jsp的匹配，相对/优先级别高
            /是匹配不带*.jsp的，
            /*是全路径匹配，会匹配到*.jsp路径
      
               
         备注： /foo/、/*.htm 和 */foo 都是不对的，/user/*.action是非法的,
         /aa/*/bb 是精确匹配，合法
         优先级：精确匹配 > 最长路径匹配 > 后缀匹配>缺省匹配。

         比如servletA的url-pattern为/test/*，而servletB的url-pattern为/test/a/*，此时访问http://localhost/test/a时，容器会选择路径最长的servlet来匹配，也就是这里的servletB。 


2. HttpServlet 包含  init()、destroy()、service()方法
    	init()：初始化servlet，并用ServletConfig 对象参数来启动配置。
    	service()：客户请求一个HttpServlet 对象，该对象的service() 方法就要被调用，且入参是ServletRequest和ServletResponse

    ​	servlet中首先执行doService(),判断是请求是get还是post,get就调用doGet(), post就调用doPost()。

    ​	也可以直接过载doService()方法，这样不管是get还是post，都会执行这个方法。  

    当服务器调用sevlet 的Service()、doGet()和doPost()这三个方法时，均需要 "请求"和"响应"对象作为参数。

    "请求"对象提供有关请求的信息，而"响应"对象提供了一个将响应信息返回给浏览器的一个通信途径。  
    使用GET，form中的数据将编码到url中，而使用POST的form中的数据则在http协议的header中传输

3. (忽略)多DispatcherServlet的情况下，是registration.setName("rest")，默认为“dispatcherServlet”，
   因为name相同的ServletRegistrationBean只有一个会生效，也就是说，后注册的会覆盖掉name相同的ServletRegistrationBean。
   配置对应的DispatcherServlet，是需要加载对应的context的。

4. @webservlet 注册到web容器中作为一个servlet(只适用自定义的servlet，DispatcherServlet是框架提供的servlet,需要ServletRegistrationBean定义)


5. 

    			

### Filter 过滤器

   1. filter：用途是过滤字符编码、业务逻辑判断(登录、权限：session中的权限不够可以直接重定向)，用于请求预处理(Request、Response)，也可对HttpServletResponse进行后处理，是典型的处理链。
   
   2. Filter只是链式处理，请求依然放行到目的地址
      filter的处理过程	HttpRequest ----> Filter ----> Servlet ----> Controller/Action/... ----> Filter ----> HttpResponse     

      filter中不想执行的程序直接return;掉即可,response sendRedirect之后要 return;

   3. fitler中注入bean
      现象：filter中注入的spring bean就为null
      原因：（忽略） 
         Filter的优先级大于Servlet，而springMVC又是基于Servlet来进行注入bean的，所以这就导致了Filter无法注入bean
         
         在Spring中，web应用启动的顺序是：listener ->filter -> servlet，先初始化listener，然后再来就filter的初始化，再接着才到我们的 dispathServlet 的初始化，因此，当我们需要在filter里注入一个注解的bean时，就会注入失败，因为filter初始化时，注解的bean还没初始化，不能注入 。
            （1）容器启动时，会先加载filter，然后再加载Spring中的Bean。所以如果是直接在Filter 中进行SpringBean的注入，那么无法成功进行注入，因为要注入的Bean还没有进行初始化，是null。
            （2）DelegatingFilterrProxy是一个Filter。容器启动时会加载这个Filter，对这个类的操作将会委托到targetBeanName对应的Bean进行处理(Spring容器管理)，因为TargetBean是Spring的一个Bean，所以可以进行SpringBean的注入。
      
      解决方法：
         1. 配置使用DelegatingFilterProxy
               DelegatingFilterProxy类存在与spring-web包中,其作用就是一个filter的代理,用这个类的好处是可以通过spring容器来管理filter的生命周期,还有就是,可以通过spring注入的形式,来代理一个filter执行,如shiro

               将filter中加入spring管理，这样可以注入bean操作，filter中注入bean管理
               <filter-name>userInfoFilter</filter-name>    //这个对应的是spring中的bean
               <filter-class>org.springframework.web.filter.DelegatingFilterProxy</filter-class>
               是对一个filter的代理，通过Spring容器来管理servlet filter的生命周期。
               ***不用原始的filter,是因为filter的类里面使用了Spring的注解，所以也必须也使用Spring的DelegatingFilterProxy
               
               用这个类的好处主要是
               1.还有就是如果filter中需要一些Spring容器的实例，可以通过spring直接注入，
               2.另外读取一些配置文件这些便利的操作都可以通过Spring来配置实现。
               拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，这个情况就需要使用filter代理，即DelegatingFilterProxy

         2. 在filter中使用SpringContextUtil读取bean
            ```
               //外部直接静态调用
               @Component     //这个必须加上，这样spring启动的时候才会将context加载到static属性中，实现类级别的共享
               public class SpringContextUtil implements ApplicationContextAware{
                  
                  private static ApplicationContext context;
                  
                  public static ApplicationContext getApplicationContext(){return context;}

                  public void setApplicationContext(ApplicationContext ctx) throws BeansException {context = ctx;}
                  
                  public static Object getBean(String beanName){
                     return context.getBean(beanName);
                  }
                  
                  /**
                  * 根据Bean名称返回String类型的BEAN
                  */
                  public static String getStringBean(String beanName){
                     return context.getBean(beanName, String.class);
                  }
               }
            ```

            在filter中需要注入相应的bean，
            @Override 
            public void init(FilterConfig config) throws ServletException { 
         ​	    ServletContext context = config.getServletContext(); 
               ApplicationContext ac = WebApplicationContextUtils .getWebApplicationContext(context); 
               userService1 = (UserService)ac.getBean("userService"); 
            }

   4. 重复返回页面信息（忽略）
         在filter中，response.getWriter().print()后,chain.do()到controller后，继续，
         也可以继续response.getWriter().print()，但是不能通过view层直接返回字符串信息，也不能返回页面

   5.  简单的用@webfilter，需要springbean的就需要使用代理filter
       filter的执行顺序:FilterRegistrationBean注册时，filter顺序与@Bean注解实例顺序一致

   6. filter中获取context
      public class OpenFilter implements Filter {
         @Override
         public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {
            ServletContext context = request.getServletContext();
            ApplicationContext app = WebApplicationContextUtils.getWebApplicationContext(context);
            RedisUtil redisUtil = (RedisUtil)app.getBean("redisUtil");
         }

         @Override
         public void init(FilterConfig filterConfig) throws ServletException {}
      }

   7. filter类实现filter接口即可，在web.xml中配置，其中共有三个方法 
   		1、init（）是在启动项目的时候进行初始化
   		2、dofilter（）是在拦截打匹配的路径时候进行处理，处理完用chain.doFilter(request, response)进行继续访问请求
   		3、destroy（）当应用服务被停止或重新装载了，则会执行Filter的destroy方法，Filter对象销毁。比如改动项目的代码重新reload时候（Reloading Context）


### Interceptor   拦截器

   拦截器设计思路，用封装设计好的拦截器来实现动态代理的功能。拦截器底层还是动态代理。开发者直接使用，隐藏底层动态代理的实现。简化开发
   
1. spingmvc中的HandlerInterceptor的web请求流程：
      HttpRequest ----> DispactherServlet ----> HandlerInterceptor ---->Controller----> HandlerInterceptor ----> HttpResponse

2. Interceptor执行流程(prehandle、posthandle 和 aftercompletion)
      * prehandler(request,response,object handler)-----进入handler方法之前执行，
            调用时间：Controller方法处理之前
            执行顺序：链式Intercepter情况下，Intercepter按照声明的顺序一个接一个执行
            
            SpringMVC中的Interceptor拦截器是链式的，可以同时存在多个Interceptor，然后SpringMVC会根据声明的前后顺序一个接一个的执行，
            而且所有的Interceptor中的preHandle方法都会在Controller方法调用之前调用。若返回false，则中断执行，注意：不会进入afterCompletion
            令preHandle的返回值为false，当preHandle的返回值为false的时候整个请求就结束了
               
            用于身份认证授权，未登录，return false 拦截不向下执行，return true 表示放行

      * posthandler(request,response,object handler ,mv)--进入handler方法之后，返回mv之前执行
            调用前提：preHandle返回true
            调用时间：Controller方法处理完之后，DispatcherServlet进行视图的渲染之前，也就是说在这个方法中你可以对ModelAndView进行操作
            执行顺序：链式Intercepter情况下，Intercepter按照声明的顺序倒着执行(先声明的Interceptor拦截器该方法反而会后调用)。
            备注：postHandle虽然post打头，但post、get方法都能处理

            应用场景从mv出发，可在此处理模型和数据

      * afterCompletion(request,response,object handler ,mv,exeception)---执行handler完成执行，在view渲染完成、在dipatchersrvlet返回之前执行
            调用前提：preHandle返回true
            调用时间：整个请求完成之后,DispatcherServlet进行视图的渲染之后
            
            常用于清理资源、统一异常和日志处理

3. 多拦截器顺序
      多个拦截器，责任链模式的规则，对于处理器前方法采用先注册先执行，而处理后法和完成方法是先注册后执行的规则

      当多个拦截器拦截统一请求时，
         1、拦截器1、2同时放行
            prehandler按顺序执行，posthandler，afterCompletion按拦截器配置的逆向顺序执行
         2、拦截器1放行，2不放行
            拦截器1prehandler放行，拦截器2的prehandler才会执行，拦截器2prehandler不放行，1、2后续都不执行
         3、拦截器1、2都不放行
            执行到拦截器1prehandler return false 结束

         preHandle1
         preHandle2
         preHandle3

         postHandle3
         postHandle2 
         postHandlel 
         
         afterCompletion3
         afterCompletion2 
         afterCompletionl

      mvc中拦截器的执行顺序
         前置方法preHandle(false则结束所有流程：true则执行下一步)->处理器handler(controller执行内容)->后置方法postHandle->视图解析和渲染视图->完成方法afterCompletion

         定义了ABC拦截器，顺序为A、B、C，preHandle是顺序执行的，postHandle与afterCompletion方法倒序执行的。
         且B拦截器的preHandle方法返回false。则拦截器的执行行为如下：
         A.preHandle
         B.preHandle
         A.afterCompletion

         只要有一个拦截器不放行，postHandle不会执行。
         注意，当其中的一个preHandle方法返回为 false 后，后面的正序的preHandle都不会运行了，而控制器和所有的后置方法postHandle不会再运行。
         但会逆向的执行preHandle方法且返回为true的拦截器的afterCompletion方法

      配置类似全局的拦截器，注入到每个handlermapping中,多个拦截器顺序执行
         ```
            <mvc:interceptors>
               <mvc:interceptors>
                  <mvc:mapping path='/**'/>                    ------/**表示拦截所有url包括子url，只配/*只拦截根url
                  <bean class="">
               </mvc:interceptors>	
            </mvc:interceptors>
         ```

4. interceptor实现原理(忽略)
   getProxy方法的参数中(target,interceptorClass)传连个参数，然后invoke中根据情况实现逻辑，代理操作类中，针对target的方法，补充interceptorClass的操作。
   代理操作类中有target 和对应的拦截器，在invoke中会实现调用
   这里被代理的类还是target，只不过增加的interceptorClass的一层判断，根据判断结果执行target的相关方法
   实现类再容器中有个代理类。至于被代理的obj和拦截器的映射关系，应该已经加入容器了，这样在obj调用的时候能关联对应的拦截器。
   多个拦截器的处理链即各proxy依次作为下个拦截器代理的target，链式调用。

5. 一般在拦截器中定义全局的ctx，建议定义在afterCompletion中，这样在异常处理返回后页面中还能获取到值，拦截器中重定向return false，则重定向无效
   interceptor中处理request.setAttribute("baseUrl", Constants.URL_WEB_HOST_NEWADMIN);这样可以定义访问路径级别的全局的常量

## 设计模式

1. 单例模式	,延迟加载，懒加载
      
	单例模式实现方式：饿汉模式、懒汉模式、静态内部类模式、枚举模式(枚举元素本身就是单例)
	使用场景：一般通用配置类会考虑使用单例，本来就是一次性加载的东西
	重点：核心是私有化构造函数

   单例实现方式：构造私有化 + synchronized	实现单例模式的获取(防止多线程同时获取),并提供static访问实例的方法即可
   *****确保一个类只有一个实例，并且提供一个全局访问点。*****分别对应构造私有和静态访问
   确保只有一个实例会被创建(线程池，日志对象只能有一个实例)



   java中实现单例模式需要：1私有构造，2静态方法 和 3静态变量
   典型代码：
      这里可能会有线程问题，使用双重检查加锁，性能还可以
      public class Singleton{
         private static Singleton uniqueInstance;
         //利用一个静态变量记录该类的唯一实例，这里使用静态变量有两层意思：1.静态方法中变量必须是静态变量。2.单例就是共享一个实例，符合静态共享
         private Singleton(){};
         
         public static Singleton getInstance(){
            if(uniqueInstance==null){
               uniqueInstance=new Singleton();
            }
            return uniqueInstance;
         }

         //private static Singleton uniqueInstance=new new Singleton();  这个就是线程安全的，直接创建，这是饿汉式
      }
   
   几种实现模式
      * 饿汉模式
         //直接先把单例对象创建出来，不管后面用不用
            public class ImageLoader{ 
               private static ImageLoader instance = new ImageLoader; 
               private ImageLoader(){} 
               public static ImageLoader getInstance(){  
                  return instance;  
               } 
            }
            
      * 懒汉模式
            延迟一次性加载，后续直接使用，这里的懒汉模式会有线程安全问题
            实际项目使用的CommercePayRegister使用静态延迟创建一个实例，然后进行属性复制，后面这个静态类可以直接使用获得同一实例对象了。
            public class CommercePayRegister{
               private static CommercePayRegister instance;
               private CommercePayRegister(){}
                  
               public static CommercePayRegister getInstance(){
                     if (instance == null){instance = new CommercePayRegister();}
                     return instance;
                  }
            }
            
            懒汉改进：双重锁检查（避免对除第一次调用外的所有调用都实行同步），因为创建一次后就不用创建了，这里使用lock也一样
            注意一个实例化同步问题，避免创建了两个实例。
            if (instance == null){
                  synchronized(Singleton.class) {
                     if (instance == null) instance = new Singleton();
                  }
            }

      * 静态内部类模式
           //静态的成员内部类，该内部类的实例与外部类的实例没有绑定关系，而且只有被调用到才会装载，从而实现了延迟加载的单例模式（调用时候再加载，而不是一开始就加载）
            public class SingletonDemo {
               private static class SingletonHolder{
                  //静态初始化器，由JVM来保证线程安全,外部类加载时并不需要立即加载内部类，内部类不被加载则不去初始化INSTANCE，故而不占内存，当使用了静态内部类才加载
                  private  static AlipayClient alipayClient = new DefaultAlipayClient(Configs.getOpenApiDomain(), Configs.getAppid(),Configs.getKey());
                  private  static AlipayTradeService tradeService = new AlipayTradeServiceImpl.ClientBuilder().build();
               }
            }	 

            private static  只能在内部静态调用
            静态内部类的形式去创建单例的，故外部无法传递参数进，静态变量是和类一起加载的，static实现共享，延迟实例化，调用的时候实现共享
            静态内部类实现单例   延迟加载  https://blog.csdn.net/mnb65482/article/details/80458571
            
            ***实际使用推荐这个***
            public class UserSingleton {
                  /** 私有化构造器 */
                  private UserSingleton() {}
                  
                  /** 对外提供公共的访问方法 */
                  public static UserSingleton getInstance() {
                     return UserSingletonHolder.INSTANCE;
                  }

                  /** 写一个静态内部类，里面实例化外部类 */
                  private static class UserSingletonHolder {
                     private static final UserSingleton INSTANCE = new UserSingleton();
                  }
               }

         * 延时加载（静态延时加载，这里类被调用就加载了，比较靠前）
           ```
               private static RequestConfig requestConfig = null;
                  static
                  {
                     // 设置请求和传输超时时间
                     requestConfig = RequestConfig.custom().setSocketTimeout(2000).setConnectTimeout(2000).build();
                  }
           ```

   单例模式和spring单例的区别
		不过spring中的bean默认是单例的，初始化配置UserSingleton的bean，其实也可以，就是不是延迟加载。
		注意静态属性是和类绑定的，和实例无关，这样就实现了实例共享静态属性
		spring的bean是每次注入都是同一个bean，但是还是可以new出来的。上面的方法就new不出来，限制的更严格(防止别的开发人员，new对象然后使用错误。总之就是保证绝对的唯一使用)
		使用@Lazy，springbean也可以实现延迟加载，(减少springIOC容器启动的加载时间,不过一般没必要)
		
		这个特点是：1. 仅在需要使用单例时调用getInstance进行单例的创建。(相当于懒汉吧这个)，静态内部类调用的时候加载
					2. JVM虚拟机保证了静态内部类SingletonHolder的类初始化只执行一次(单例特性，不用null判断之类的了)，不需要我们手动保证并发的同步。相当于将实例化的过程交给了jvm
					3. 用静态的内部类实现单例模式的原理：静态内部类可以不依赖外部类的实例而被实例化(延迟特性)。


2. 工厂模式
      用来封装对象的创建，让子类决定该创建的对象时什么。
      定义了一个创建对象的接口，由子类决定要实例化的接口是哪一个。工厂方法让类把实例化推迟到子类。
      将对象创建的方法封装起来 

3. 观察者模式（忽略）
      消息队列基于的设计模式是观察者模式

4. 装饰者模式（Decorator）(忽略)
      案例：ServletRequestWrapper 和 HttpServletRequestWrapper 提供对 request 对象进行包装的
      方法，但是默认情况下每个方法都是调用原来 request 对象的方法，
      也就是说包装类并没有对 request 进行增强在这两个包装类基础上，继承 HttpServletRequestWrapper ，覆盖需要增强的方法即可

      //装饰器模式的写法，必须将被装饰的对象初始化到装饰类中
      HttpServletRequestWrapper的使用  
      HttpServletRequestWrapper 是HttpServletRequest的装饰器。
      一般在filter中装饰request对象，进入servlet中，相当于封装了相应的request

      sitemesh 和 io 是装饰模式
      SiteMesh使用一个Servlet过滤器，它可以拦截返回的Web浏览器的HTML，提取相关内容，并将其合并到被称为装饰器（Decorator）的模板。
      装饰器模式可以动态的把新的职责添加到对象上。这里关键点是“动态”，也就是运行时；而继承在编译的时候已经确定了。

## 分布式框架，分布式相关
### 基本知识点
   CAP定理
      一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效)，数据一致更新，所有数据变动都是同步的。最终一致和强一致性  这里一致性用最终一致即可。
      可用性(Availability) ： 每个操作都必须以可预期的响应结束
      分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成

      CAP三进二
         CAP理论就是说在分布式存储系统中，最多三取二。
         而由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡。
         CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。
         CP - 满足一致性，分区容忍必的系统，通常性能不是特别高。
         AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。(常用)

      zookeeper实现了cap中的cp特性，因为zk集群有leader选举机制，牺牲了短暂的可用性，但是保证了容错性
      zk的地址信息变动会通知其他的客户端，要么全成功，要么全失败，类似事务的原子性，实现一致性。

   BASE理论
		Basically Available（基本可用）         分布式系统出现不可预知故障，允许损失部分可用性
		Soft state（软状态）                    允许中间状态，允许一定的处理延迟
		Eventually consistent（最终一致性）     类似异步mq，流量取峰，最终一致即可
		BASE理论是对CAP中的一致性和可用性进行一个权衡的结果:无法做到强一致,采用适当的方式来使系统达到最终一致性



​	hessian 和dubbo
### 分布式事务相关

* 1. 基础知识
         通常所说的柔性事务分为：两阶段型、补偿型、异步确保型、最大努力通知型几种
         一致性是事务的最终目的，原子性、隔离性、持久性都是为了实现一致性。数据一致性等

         协调者和参与者
            1.需要一个全局协调者，全局事务管理器向所有事务参与者发送准备请求，事务参与者向全局事务管理器回复自己是否准备就绪
            2.全局事务管理器接收到所有事务参与者的回复后判断，如果所有事务参与者都可以提交，则向所有事务提交者发送提交申请，否则进行回滚。事务参与者根据全局事务管理器的指令进行提交或回滚。

             就是ABC 同事开启事务，执行成功后,ABC同时提交事务(引入一个协调,ABC同时开启事务,执行,然后都不提交，等待都执行完后,收到消息同时提交，最后结果都是同时提交或者同时回滚)

		*************重点就是全局的事务管理器，控制多个分支事务。*************
		全局的事务管理器，管理事务的启动，提交和回滚。
		事务管理器在协调资源时，需要增加相应的日志记录。
			分布式系统中，两机器无法状态一致，需要引入协调器(事务管理器)，控制全局事务，管理事务生命周期，协调资源。
		
		长流程、例如注册、审核、发布需求，开票等，对非核心业务通过队列异步实现，核心走事务，非核心通过对账补偿。


* 2. 两段式提交(two-phase commit)。
      主要成员，协调者(Coordinator)和参与者(Participants),  将事务管理器称为协调者，将资源管理器称为参与者
               协调者统一掌控所有节点(称作参与者)的操作结果，根据结果最终决定各参与者提交操作或中止操作。

      第一个阶段：预提交阶段(投票阶段)，一票否决性,各个server端，操作数据库先做各自的预提交
      第二个阶段：提交决定阶段。
                  协调者根据上一个阶段的投票结果决定是Commit还是Abort，这个决定是全局性的，会通知到所有的参与者执行最终的决定，并回传一个ack确认信息。
                  判断预提交都是ok的，在进行第二次的统一事务提交，若有错误，回滚全部，整个完成才释放资源，锁力度大，性能差 

      准备阶段： 只写日志，不提交事务
         写 redo 或 undo 日志，然后锁定资源，
         事务协调者(事务管理器)给每个参与者(资源管理器)发送 Prepare 消息，每个参与者要么直接返回失败，要么在本地执行事务，写本地的 redo 和 undo 日志，但不提交
      
      提交阶段：
         预留资源和执行操作成功，则变更释放资源，否则回退执行undo，释放锁定资源
         如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；
         参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)

      失败情况
         在XA的第一阶段，如果某个事务参与者反馈失败消息，说明该节点的本地事务执行不成功，必须回滚。
         于是在第二阶段，事务协调节点向所有的事务参与者发送Abort请求。接收到Abort请求之后，各个事务参与者节点需要在本地进行事务的回滚操作，回滚操作依照Undo Log来进行。

      无法解决的问题（数据状态不确定）忽略
         协调者再发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

      两阶段提交保持一致性
      采用两阶段提交保证多master数据一致性，
         1.开启事务
         2.通知每个master执行某操作
         3.所有master接到请求后，锁定执行此操作需要的资源，如扣款动作，就冻结相应款项，冻结完毕后返回
         4.收到所有master的反馈后，如均为可执行此操作，则继续之后的步骤，如有一个master不能执行或者一段时间内无反馈，则通知所有master回滚操作
         5.通知所有master完成操作

         先冻结，全部冻结ok了，就执行，全部执行ok就成功了，有一个不成功，就反馈回滚

      在实现2pc或者3pc提交时，为了避免通知所有master时出现问题，通常会借助消息中间件或让任意一个master接管成为通知者

      相对于单库中的事务提交，数据操作时候只有一个动作，提交或回滚
		分布式中，有两个动作，准备和提交，所以称为两阶段提交。

* 3. 三阶段提交：新增超时机制，解决阻塞问题，新增询问阶段 (忽略)
      询问阶段：协调者询问参与者是否可以完成指令，无需执行真正的操作，超时会终止
      
      与两阶段提交不同的是，三阶段提交有两个改动点。
         1. 引入超时机制。同时在协调者和参与者中都引入超时机制,2PC只有协调者才有超时机制。
         2. 3PC把2PC的准备阶段再次一分为二，增加一个准备阶段CanCommit,这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。

         优点： 1.避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源
               2. 多设置了一个缓冲阶段保证了在最后提交阶段之前各参与节点的状态是一致的

      在第一阶段，
         只询问所有参与者是否可以执行事务操作，当协调者收到所有的参与者都返回YES时，才执行第二阶段
      在第二阶段
         执行事务操作(事务预提交)
         协调者会向所有的参与者节点发送PreCommit请求，参与者收到后开始执行事务操作，并将Undo和Redo信息记录到事务日志中。
         参与者执行完事务操作后（此时属于未提交事务的状态），就会向协调者反馈“Ack”表示我已经准备好提交了，并等待协调者的下一步指令。
      第三阶段
         在执行commit或者rollback
         一旦事物参与者迟迟没有接到协调者的commit请求，会自动进行本地commit。有效解决了协调者单点故障的问题。
      
      1.协调者询问事务是否可以执行，这一步不会锁定资源
      2.参与者反馈，协调者接收到所有YES指令
      3.协调者发送事务执行指令，这一步锁住资源
      4.参与者执行事务操作，反馈状态，协调者收到所有参与者的ACK响应，通知所有参与者执行事务的commit
      5.执行commit操作，反馈状态

      相比2pc,主要解决的就是协调者和参与者同时挂掉的问题（忽略）
         如果挂掉的那个参与者已经执行commit，那么协调者可以从所有未挂掉的参与者的状态中分析出来，并执行commit。
         如果挂掉的那个参与者已经执行rollback，那么协调者和其他的参与者执行的肯定也是rollback操作。
         所以，多引入一个阶段之后，3PC解决了2PC中存在的那种由于协调者和参与者同时挂掉有可能导致的数据一致性问题。


* 4. TCC
      TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。
      tcc只对需要的资源进行加锁，且try commit Cancel都是本地短事务(操作功能需业务提供)，能保证强一致性。
		
      TCC包含了三个阶段：Try，Confirm，Cancel，因此而得名「TCC」。一种柔性事务解决方案
         Try: 尝试执行业务
               完成所有业务检查(一致性)
               预留必须业务资源(准隔离性)          
         Confirm:确认执行业务
               真正执行业务
               不作任何业务检查
               只使用Try阶段预留的业务资源 
               Confirm操作要满足幂等性
         Cancel: 取消执行业务
               释放Try阶段预留的业务资源 
               Cancel操作要满足幂等性

      目前tcc,try confirm cancel
         我们给出一个使用 TCC 的实际案例，在秒杀的场景中，用户发起下订单请求，应用层先
         询库存，确认商品库存还有余量，则锁定库存，此时订单状态为待支付，然后指引用户去支付
         由于某种原因用户支付失败或者支付超时，则系统会自动将锁定的库存解锁以供其他用户秒杀。
         类似买票先预定，事务也是提交的，后购票。

      tcc(try-confirm-cancel)两阶段补偿方案
         try阶段；完成所有业务检查（一致性），预留业务资源（准隔离性）
         confirm阶段：确认执行业务操作，只使用上阶段预留的业务资源
         cancel阶段：有一个失败，取消所有业务资源请求
            失败重试机制，
            补偿机制，比如日志，继续或回滚

      tcc和xa/jta(忽略)
         前者时业务层面的分布式事务，最终一致性，不会一致持有资源锁，分布释放锁
         后者是资源层面的分布式事务，强一致性，两阶段提交的过程中一致会持有资源锁
      
      tcc开源框架：
         atomikos，tcc-transcation（推荐），spring-cloud-rest-tcc，支付宝xts

      缺点是对应用的侵入性强，业务逻辑的每个分支都需要实现try、confirm、cancel三个操作。
      其实现难度也比较大，需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为满足一致性要求，confirm和cancel接口还必须实现幂等。


* 5. 最大努力通知方案（多次尝试），补偿操作
      		
      一致性要求较低，适用于一些最终一致性时间敏感度低的业务。
      
      消息中间件mq方案

		最大努力通知方案需要实现如下功能：
			1、消息重复通知机制。   账户系统没收到就一直发送，收到返回ack
			2、消息校对机制。		账户系统先查询充值结果，确认ok了再进行更新

      多系统间的调用，补偿的job还是很有必要的，增加操作日志很关键
      对复杂的分布式事务进行拆解，每个步骤都需记录状态，有问题时可以根据记录的状态来继续执行任务（或者终止），达到最终一致。
      
      java.net.SocketTimeoutException: connect timed out
      系统间的调用，会出现超时的情况，关键的回调，需要做job的补偿回调。对于及时性比较高的，需要在超时间内，得到补偿信息


      最常用的一致性方案是使用带有事务功能的MQ（忽略）
			在做本地事务之前，先向MQ发送一个prepare消息，然后执行本地事务，执行ok，向MQ发送一个commit消息。否则发送一个rollback消息，取消之前的消息。
         MQ只会在收到commit确认才会将消息投递出去，保证一致性。
			网络超时等原因可能收不到commit，那么MQ就不会吧prepare消息投递出去。MQ会根据策略尝试回调checkcommit
			检查该消息是用还是不用，确认后，MQ处理。


* 6. 几种类型适用场景
      1.2PC/3PC：依赖于数据库，具有强一致性和强事务性，适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。
      2.TCC：适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。
      3.本地消息表/MQ 事务：都适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账/校验系统兜底。
      4.Saga事务：由于Saga事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。Saga缺少预提交动作，导致补偿动作难实现，适用于补偿动作容易处理的场景

* 7. TCC与2PC协议比较：
        位于业务服务层而非资源层
        没有单独的准备(Prepare)阶段， Try操作兼备资源操作与准备能力 
        Try操作可以灵活选择业务资源的锁定粒度(以业务定粒度) 
        2PC的强一致性依赖于数据库，而TCC的强一致性依赖于应用层的Commit与cancel
        2PC是有数据库来保证回滚，而TCC是应用层实现回滚：为每一个try操作提供一个对应的cancel操作


* 8. 其他
      seata中间件，之前叫 Fescar，阿里开源的分布式事务框架

      分布式id要考虑两个点 1.唯一性  2.连续性

## 多线程相关

   多线程安全的思路
			1.不要跨线程共享变量		   线程副本
			2.使状态变量为不可变		   final
			3.访问状态变量使用同步		同步锁

   线程修改变量的过程：(了解即可)
		每一个线程运行时都有一个线程栈，线程栈保存了线程运行时候变量值信息。
		当线程访问某一个对象时候值的时候，首先通过对象的引用找到对应在堆内存的变量的值，然后把堆内存变量的具体值 load 到线程本地内存中，建立一个变量副本，
      之后线程就不再和对象在堆内存变量值有任何关系，而是直接修改副本变量的值，在修改完之后的某一个时刻（线程退出之前），自动把线程变量副本的值回写到对象在堆中变量。
      这样在堆中的对象的值就产生变化了
		
		Java内存模型规定了所有的变量都存储在主内存中。
		每条线程有自己的工作内存，其中使用的变量是主内存中的拷贝的变量副本，线程对变量的所有操作（读取，赋值）都必须在工作内存中进行。
		不同线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。


### 线程基本概念

1. Runnable和Thread
      Runnable接口相比Thread类的优势：
         1. 可以避免java中的单继承的限制
         2. 线程池只能放入实现Runable或callable类线程，不能直接放入继承Thread的类

2. 在java中，每次程序运行至少启动2个线程。一个是main线程，一个是垃圾收集线程。
   每当使用java命令执行一个类的时候，实际上都会启动一个jvm，每一个jvm实际在就是在操作系统中启动了一个进程。
   一个请求对应一个服务器线程

3. sleep()和yield()的区别
      Thread.sleep()
            实际测试的时候synchronized加在方法上的，方法体内sleep，另一个线程是等待的，进不来
      sleep()
      ​	   对当前正在运行的线程睡眠，进入阻塞状态,在指定的时间内肯定不会被执行

      yield()
         * 让出CPU占有权，时间不可设定，同级别的让步，让步到就绪状态，使当前线程重新回到可执行状态。所以yield()方法称为“退让”，它把运行机会让给了同等优先级的其他线程

         * 处理过程：先检测当前是否有相同优先级的线程处于同可运行状态，如有，则把CPU的占有权交给此线程，否则，继续运行原来的线程。

         * yield(),暂停执行，让出cpu，给同priority线程执行，并未阻塞，没有同级别的priority，该方法无效。JVM 暂时放弃 CPU 操作，放弃本次时间片的执行权


      1、sleep方法暂停当前线程后，会进入阻塞状态，只有当睡眠时间到了，才会转入就绪状态。而yield方法调用后 ，是直接进入就绪状态，所以有可能刚进入就绪状态，又被调度到运行状态。
      2、sleep方法声明抛出了InterruptedException，所以调用sleep方法的时候要捕获该异常，或者显示声明抛出该异常。而yield方法则没有声明抛出任务异常。	


4. sleep和wait的区别
      sleep()方法是 Thread 类中方法，而 wait()方法是 Object 类中的方法。

      sleep()调用，程序暂停指定时间，让出cpu给其他线程,但未释放锁(线程仍然可以同步控制),未让出系统资源。当指定时间到了又会自动恢复运行状态，自动唤醒(interrupt方法强行打断,唤醒)。

      wait(),线程会放弃对象锁,进入线程等待池中等待，让出系统资源。需要线程调用 notify/notifyAll方法唤醒(并不立刻释放锁，执行完锁代码块才释放)，才会进入就绪队列中等待系统分配资源,重新参与获得锁的竞争
      进入等待此对象的等待锁定池，只有针对此对象调用 notify()方法后本线程才进入对象锁定池准备获取对象锁进入运行状态。


      wait()和notify()必须放在synchronized块中使用
      sleep(long)会导致线程进入 TIMED-WATING 状态，而 wait()方法会导致当前线程进入 WATING 状态
      sleep相对于wait：
         sleep可以在任何地方使用。而wait，notify，notifyAll只能在同步控制方法或者同步控制块中使用。Obj.wait(),Obj.notify必须在synchronized(Obj){...}语句块内。
         sleep必须捕获异常，而wait，notify，notifyAll的不需要捕获异常。

5. wait(),notify(),notifyall()

   * wait(),notify(),notifyall(),  在使用时必须标识它们所操作的线程持有的锁，因为等待和唤醒必须是同一锁下的线程；而锁可以是任意对象，这3个方法都是Object类中的方法。
   * 线程执行wait()后，就放弃了运行资格，处于冻结状态；
     		线程运行时，内存中会建立一个线程池，冻结状态的线程都存在于线程池中，
       		notify()执行时唤醒的也是线程池中的线程，线程池中有多个线程时唤醒第一个被冻结的线程。
       		notifyall(), 唤醒线程池中所有线程。

   notify()后会沿wait()方法之后的路径继续执行，注意是wait方法之后
   唤醒一个等待的线程，唤醒哪一个和线程优先级有关

6. 常用方法
	* join()	
         join作用是让当前执行线程等待join线程执行结束。
   ​	    join在当前的线程中加入该线程，父线程等待子线程结束后才能继续运行。
         子线程结束后，子线程的this.notifyAll()会被调用，join()返回，父线程只要获取到锁和CPU，就可以继续运行下去了。
         当我们调用某个线程的这个方法时，这个方法会挂起调用线程，直到被调用线程结束执行，调用线程才会继续执行。
         
         main{
            threadA.join();
         }
         这里就是A执行完了，main才执行完
         
         实现原理(忽略)
            其实现原理是不停检查join线程是否存活，
            如果join线程存活则让当前线程永远等待。其中，wait(0)表示永远等待下去。
            直到join线程中止后，线程的this.notifyAll()方法会被调用，调用notifyAll()方法是在JVM里实现的，所以在JDK里看不到，大家可以查看JVM源码。

            当调用原生的join方法时，也是调用了wait方法，也就是会阻塞调用的线程（切记不是调用join方法的那个线程对象，这只是一个普通对象而已）
	
	* wait，notify，notifyall
			wait，notify，notifyall 是Object的方法,不是Thread类的固有方法
			所有的对象都会有一个wait set，用来存放调用该对象wait方法之后进行block状态的线程。
         线程从wait set中唤醒的顺序不一定是FIFO(先入先出模式)。但是线程被唤醒后，必须重新获取锁

			线程被notify之后，不一定立即得到执行，这是因为可能会抢不到LOCK锁。
			
         wait() 和 notify() 实现的基础是基于对象存在
         synchronized (Object object){
            object.wait();   //object.notify();
         }

         synchronized fun(){
            this.wait();    //this.notify();
         }
		
			notify，notifyall是对实例调用，interrupt是对线程调用

	* interrupt()   忽略
			synchronized没有timeout，也不能中断
			interrupt只是会改变线程的中断状态(表示线程有没有被中断而已)，只是改变中断状态，不会中断一个正在运行的线程。
			中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。
			
			中断一个线程，其本意是给这个线程一个通知信号，会影响这个线程内部的一个中断标识位。这个线程本身并不会因此而改变状态(如阻塞，终止等)。
			1. 调用 interrupt()方法并不会中断一个正在运行的线程。也就是说处于 Running 状态的线程并不会因为被中断而被终止，仅仅改变了内部维护的中断标识位而已。
			2. 若调用 sleep()而使线程处于 TIMED-WATING 状态，这时调用 interrupt()方法，会抛出InterruptedException,从而使线程提前结束 TIMED-WATING 状态。
			3. 许多声明抛出 InterruptedException 的方法(如 Thread.sleep(long mills 方法))，抛出异常前，都会清除中断标识位，所以抛出异常后，调用 isInterrupted()方法将会返回 false。
			4. 中断状态是线程固有的一个标识位，可以通过此标识位安全的终止线程。比如,你想终止一个线程 thread 的时候，可以调用 thread.interrupt()方法，在线程的 run 方法内部可以根据 thread.isInterrupted()的值来优雅的终止线程。
				
			当调用线程的 interrupt()方法时，会抛出 InterruptException 异常。
			通常很多人认为只要调用 interrupt 方法线程就会结束，实际上是错的， 一定要先捕获 InterruptedException 异常之后通过 break 来跳出循环，才能正常结束 run 方法

	* 线程中suspend()、resume()和stop()都是过期的方法，不要使用

   * Thread类中的setPriority方法可以设置优先级

7. 守护线程
      守护主线程，主消失 守护消失，不用手动触发

      简单理解是后台线程，是一种支持型线程，主要被用作程序中后台调度以及支持性工作。像jvm垃圾回收和内存管理都是守护线程。
      当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出
      
      注意的是进程结束的判断是前台线程是否结束，即使进程结束了，守护线程可能依然未退出。

      标记为daemon的线程是守护线程，一般查找问题定为的是主线程
   	******查看WAITING的线程，重点查看包含自己代码行的方法调用栈。******

8. lock和wait实际场景
      模型抽象
         有加锁、条件循环和处理逻辑3个步骤
         等待方遵循如下原则。 
            这里的对象是同一个锁对象，可由其他线程唤醒
            1.获取对象的锁。2.如果条件不满足，那么调用对象的wait()方法，被通知后仍要检查条件。 3.条件满足则执行对应的逻辑 
            对应的伪代码如下。 
            synchronized(对象) {
               while(条件不满足) {
                  对象.wait(); 
               }
               对应的处理逻辑 
            }
            
         通知方遵循如下原则。 
            1.获得对象的锁。 2.改变条件。 3.通知所有等待在对象上的线程。 
            对应的伪代码如下。 
            synchronized(对象) {
               改变条件 
               对象.notifyAll(); 
            }	
  
      public class WaitNotify {
         static boolean flag = true;
         static Object lock = new Object();
         public static void main(String[] args) throws Exception {
            Thread waitThread = new Thread(new Wait(), "WaitThread");
            Thread notifyThread = new Thread(new Notify(), "NotifyThread");
            waitThread.start();
            notifyThread.start();
            
            static class Wait implements Runnable {
               public void run() {
                  // 加锁，拥有lock的Monitor
                  synchronized (lock) { // 当条件不满足时，继续wait，同时释放了lock的锁
                     while (flag) {
                        try {
                           System.out.println(Thread.currentThread() + " flag is true. wait@");
                           lock.wait();
                        } catch (InterruptedException e) { // 条件满足时，完成工作}
                     }
                  }
               }
            }
         }
            
      static class Notify implements Runnable {
            public void run() {
               // 加锁，拥有lock的Monitor
               synchronized (lock) {
                  // 获取lock的锁，然后进行通知，通知时不会释放lock的锁， 
                  // 直到当前线程释放了lock后，WaitThread才能从wait方法中返回
                  System.out.println(Thread.currentThread() + " hold lock. notify@");
                  lock.notifyAll();
                  flag = false;
                  SleepUtils.second(5);
               }
            }
         }
      }
      
      notify 和wait的流程
      1.WaitThread首先获取了对象的锁，然后调用对象的wait()方法，从而放弃了锁 并进入了对象的等待队列WaitQueue中，进入等待状态。
      2.由于WaitThread释放了对象的锁，NotifyThread随后获取了对象的锁，并调用对象的notify()方法，将WaitThread从WaitQueue移到SynchronizedQueue中，此时WaitThread变为阻塞状态。
      3.NotifyThread释放了锁之后，WaitThread再次获取到锁并从wait()方法返回继续执行。		
			
   	
   注意事项
      等待/通知机制依托于同步机制，其目的就是确保等待线程从 wait()方法返回时能够感知到通知线程对变量做出的修改。
      1.使用wait()、notify()和notifyAll()时需要先对调用对象加锁。
      2.调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的 等待队列
      3.notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或 notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。
      4.notify值之后，将等待队列中的等待线程从等待队列中移到同步队列中，被移动的线程状态由WAITING变为 BLOCKED
      5.从wait()方法返回的前提是获得了调用对象的锁。
      6.wait可以设置超时时间


9. Callable的future模式：并发模式的一种，可以有两种形式，即无阻塞和阻塞，分别是isDone和get。其中Future对象用来存放该线程的返回值以及状态

10. 上下文切换,cpu时间片，线程切换

      线程上下文切换(抢占式)，当io阻塞或者有高优先级线程要执行时，就会切换，切换时要存储目前线程的执行状态，并且恢复要执行线程的状态。
      java 使用的线程调使用抢占式调度，线程会按优先级分配CPU时间片运行，且优先级越高越优先执行，但优先级高并不代表能独自占用执行时间片，可能是优先级高得到越多的执行时间片，反之，优先级低的分到的执行时间少但不会分配不到执行时间

 	   每个cpu（或者多核cpu中的每核cpu）在同一时间只能执行一个线程，默认优先级是5

      当调度程序临时挂起当前运行的线程，另一个线程开始运行。保存和恢复线程执行的上下文，cpu的时间会花费在对线程的调度而不是运行上。

      cpu时间片轮转机制,
         时间片的就是系统分配给线程的时间限额。运行和阻塞两个状态之间的转换需要上下文切换。
         时间片分配算法循环执行任务
         线程有创建和上下文切换的开销。
         线程任务的状态保存及再加载, 这段过程就叫做上下文切换
            1.CPU分配给线程cpu时间片(即该进程允许运行的时间，时间很短，所以cpu会不停切换线程执行)，
            2.并将当前任务状态保存下来(以便下次切回时可以再次加载该任务状态)，再加载下一任务的状态并执行。
            3.当前任务执行一个时间片后切换到下一个任务(切换前保存上个任务状态，以便下次切回加载状态)。
         
      减少线程切换的几个方法
         1.避免竞争无锁开发，比如id按照hash取模分段，不用线程处理不同段数据
         2.cas算法，java的atomic包使用cas更新，不需要加锁
         3.使用线程池，创建适量的少线程
         
      引起线程上下文切换的原因
         1.当前执行任务的时间片用完之后，系统CPU 正常调度下一个任务；
         2.当前执行任务碰到 IO 阻塞，调度器将此任务挂起，继续下一任务；
         3.多个任务抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续下一任务；
         4.用户代码挂起当前任务，让出 CPU 时间；
         5.硬件中断；

11. java的内存模型

      java的内存模型：java的主内存+线程私有内存的模型，这种内存模型是线程安全问题产生的根本
         java程序中，所有线程都共享主内存，但是每一个线程都有自己的工作内存。

      JMM(内存模型)定义了线程和主内存之间的抽象关系：
           
            工作内存(栈内存)和主存(堆内存)之间会采用read/write的方式进行通信，

            jvm中主内存和工作内存
               主内存，是java堆内存，存放类实例和静态数据等，存放的是线程之间的共享变量
               工作内存，是线程从主内存中拷贝副本(包含主内存中的变量以及访问方法所取得的局部变量),是线程私有的，每个线程都有一个私有的本地内存（Local Memory），
                        

            
            线程都是先从主内存中拷贝到工作内存中操作，然后回写。当前线程只有把本地内存写过的值刷新到主内存后，其他线程才能看到最新值
            本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。

            栈空间内部包含局部变量，操作数栈，当前方法的常量池指针当前方法的返回地址等信息，是线程私有空间，
		      当调用某方法开始时将给私有栈分配空间，在内部再调用方法还会继续使用相应的栈空间，方法返回时回收相应的栈空间(不论是否抛出异常)。这块空间叫做工作内存。

            JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证
            这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。
               1.线程A把本地内存A中更新过的共享变量刷新到主内存中去。 
               2.线程B到主内存中去读取线程A之前已更新过的共享变量

         当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。


      * as-if-serial 和 happens-before原则(忽略) 
         happens-before原则,先行发生原则	是JMM核心的概念
            如果A happens-before B，那么A执行结果将对B可见，而且A的执行顺序排在B之前。(两个操作既可以是在一个线程之内，也可以是在不同线程之间)
            两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。若重排序后的执行结果与按照happens-before关系来执行的结果一致，jmm允许这种重排序。
         
         as-if-serial语义的意思是：不管怎么重排序，单线程程序的执行结果不能被改变。
         as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提 下，尽可能地提高程序执行的并行度。
         这里有个顺序一致性内存模型，是个理性的模型，线程依次连接主内存，线程安全


      * 重排序(忽略)
         简单说就是在不改变语义的情况的程序性能优化
         指令重排也是有限制的，即不会出现下面的顺序，进行重排时候，必须考虑到指令之间的数据依赖性，是一种性能优化
         指令重排序(不改变程序语义的前提下)，必须考虑到指令之间的数据依赖性.实现执行效率优化

         只要程序的最终结果等同于他在严格的顺序化环境中执行的结果那么上诉的所有行为就是被允许的。
         处理器在进行重排序时会考虑指令之间的数据依赖性，指令重排序不影响单线程的处理，那么理解就是按顺序执行就行（忽略）
         
         指令重排序：
         编译期重排序的典型就是通过调整指令顺序，做到在不改变程序语义的前提下，尽可能减少寄存器的读取、存储次数，充分复用寄存器的存储值。
         
         单线程中只要重排序不影响最后结果，就不能保证按照程序执行顺序执行，即使这样会对其他线程产生影响
         单线程中，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序
         单线程中，不会对存在数据依赖关系的操作做重排序。对不存在数据依赖的，可能会重排序
         不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变
         单线程程序是按程序的顺序来执行的，其实不是这样的，但是也可以近似这样认为，不影响结果。
               
         这里重排序对多线程的影响 感觉和时间片的切换执行有点类似的结果






12. 同步辅助类

	CountDownLatch比较适合保证线程执行完后再执行其他处理，因此模拟并发时，使用两者结合起来是最好的。
	Semaphore可以用来做流量分流，特别是对公共资源有限的场景，比如数据库连接。

	* Countdownlatch
         倒数计数器，线程阀，无法被重置，类似执行多少次，允许一个或多个线程等待其他线程完成操作
         是一个同步工具类,是一个原子操作的计数器，是一次性的，当CountDownLatch使用完毕后，它不能再次被使用。
         调用await()方法的线程会被挂起，它会等待直到count值为0才继续执行

         new countdownlatch(3),作为多个线程共享的变量，主要就是countDown()，倒数一次和await()方法，变为0才能继续，否则阻塞。
         
         ***CountDownLatch也可以实现join的功能。近似的实现了join()功能*** 

         使用场景：比如有一个任务A,它要等到其它3任务完成才能执行,此时就可以用CountDownLatch来实现。 
            CountDownLatch latch = new CountDownLatch(3);  
            Worker w1 = new Worker(latch,"张三");  
            Worker w2 = new Worker(latch,"李四");  		this.downLatch.countDown();
            Boss boss = new Boss(latch);  				this.downLatch.await();
            executor.execute(w2);  
            executor.execute(w1);  
            executor.execute(boss);  		
            executor.shutdown();  
         共用一个CountDownLatch，调用await，一直阻塞等待，直到这个CountDownLatch对象的计数值减到0为止

	* Semaphore（信号量）
         限流器，可以阻塞线程并且可以控制同时访问线程的个数
         Semaphore维护了一个许可集合，在创建Semaphore的时候，设置上许可数，每条线程在只有在获得一个许可的时候才可以继续往下执行逻辑（申请一个许可，则Semaphore的许可池中减少一个许可），没有获得许可的线程会进入阻塞状态。
         final Semaphore semaphore = new Semaphore(5);
         semaphore.acquire();			//申请一个许可  许可池-1   若许可池为0则申请许可失败，阻塞线程
         semaphore.release();			//释放一个许可  许可池+1
         
         Semaphore 的锁释放操作也由手动进行，因此与 ReentrantLock 一样，为避免线程因抛出异常而无法正常释放锁的情况发生，释放锁的操作也必须在 finally 代码块中完成。

         限流算法 	漏桶算法(固定出速)、令牌桶算法(用的多点，固定入速，允许流量一定程度的突发。)
         令牌桶算法由于实现简单，且允许某些流量的突发，对用户友好，所以被业界采用地较多。当然我们需要具体情况具体分析，只有最合适的算法，没有最优的算法。

13. volatile变量，volatile相关
		对于volatile修饰的变量，jvm虚拟机只是保证从主内存加载到线程工作内存的值是最新的(相当于get和reload操作)
      加锁可以保证可见性和原子性，volatile变量只能保证可见性
      当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去主内存中读取新值。


      volatile ：此关键 的语义保证了新值能够立即同步到主内存，并且每次使用前都即从主内存刷新 volatile 保证了多线程操作时变量的可见性
			只是保证了同一个变量在多线程中的可见性，更多是用于修饰作为开关状态的变量。
			同一个变量线程间的可见性与多个线程中操作互斥是两件事，操作互斥是提供了操作整体的原子性，不要混淆。

      可见性。
			共享变量写入到内存的行为称为“冲刷处理器缓存”。也就是把共享变量从处理器缓存，冲刷到内存中。
			此时，线程 B 刚好运行在 CPU B 上，指令为了获取共享变量，需要从内存中的共享变量进行同步。
			这个缓存同步的过程被称为，“刷新处理器缓存”。也就是从内存中刷新缓存到处理器的寄存器中。
			经过这两个步骤以后，运行在 CPU B 上的线程就能够同步到，CPU A 上线程处理的共享变量来。也保证了共享变量的可见性。

		
      原理（忽略）
         第一：使用volatile关键字会强制将修改的值立即写入主存；
         第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量的缓存行无效；
            每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态。
            操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，用时会从内存重新读取
         第三：由于线程1的工作内存中缓存变量的缓存行无效，所以线程1再次读取变量的值时会去主存读取。
            当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里
         
            对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。
            对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性。

         当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到 CPU 缓存中。
         若有多个CPU，每个线程可能在不同的CPU 上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。
         而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。
		
		总的来说，必须同时满足下面两个条件才能保证在并发环境的线程安全：
		（1）对变量的写操作不依赖于当前值（比如 i++），或者说是单纯的变量赋值（boolean flag = true）。 
		（2）该变量没有包含在具有其他变量的不变式中，也就是说，不同的 volatile 变量之间，不能互相依赖。只有在状态真正独立于程序内其他内容时才能使用 volatile。	
	
14. 多线程的实现
      1.继承 Thread 类、
      2.实现 Runnable 接口
      3.Future 实现有返回结果,
         实现callable接口，搭配线程池使用，可以实现带返回结果future

         搭配线程池，ExecutorService,callable,Future实现有返回结果的多线程


      Thread类本身实现了Runnable接口，子类继承Thread后覆盖run()方法即可

      多线程的两种方法：继承tread类或者实现runnable接口
      * 继承Thread类
      ```
      class MyThread extends Thread {
               private String name ;
               public MyThread(String name) {this.name = name;}
               public void run() {//覆写Thread类中的run方法
                  System.out.println("MyThread-->"+ name);
               }
            }
      public class TestThread {
               public static void main(String args[]) {
               MyThread t1 = new MyThread("线程1");
               MyThread t2 = new MyThread("线程2");
               t1.start();//调用线程启动方法
               t2.start();//调用线程启动方法
            }
         }
      ```

      * 实现runnable接口（解决了单继承的局限性）
      ```
            class Demo implements Runnable{
               private String name;
               public Demo(String name) {this.name = name;}
               public void run() {
                     System.out.println("Demo-->"+ name);
               }
            }
            public class ThreadDemo {
               public static void main(String[] args) {
                  Demo d = new Demo("Demo");
                  Thread t = new Thread(d);
                  Thread t2 = new Thread(d);
                  t.start();
                  t2.start();
               }
            }
      ```


15. 线程状态和生命周期

      线程的生命周期：新建（start），就绪，运行，阻塞（synchronized，sleep，wait，可以notify唤醒），死亡

      线程状态如下所示：
         1)  死锁，Deadlock（重点关注）
         2)  执行中，Runnable  
         3)  等待资源，Waiting on condition（重点关注，等待什么资源）
         4)  等待获取监视器，Waiting on monitor entry（重点关注）
         5)  暂停，Suspended
         6)  对象等待中，Object.wait() 或 TIMED_WAITING
         7)  阻塞，Blocked（重点关注） 
         8)  停止，Parked

    线程生命周期
         新建
         就绪	等待被分配cpu时间片，具备运行条件，但还没有被分配到cpu
         运行
         阻塞	让出cpu，暂停执行.	sleep   wait(notify)  suspend(resume)
               等待阻塞 	wait进入等待队列
               同步阻塞  	lock进入锁池 lock pool
               其他阻塞	sleep/join/IO请求
         死亡	正常run()结束，异常终止，kill等


    结束线程的安全方案：
         正常执行完run方法，然后结束掉；
         控制循环条件和判断条件的标识符来结束掉线程。

   

16. lock基本操作
		lock.lock();
		try{
			//业务代码
		}finally{
			lock.unlock();
		}

      安全性问题，加synchronized或者lock.lock(最后要释放锁)

17. (忽略)java中3种方法可终止正在运行的线程：
      1.使用退出标志，使线程正常退出，也就是当 run 方法完成后线程终止。
      2.使用 stop 方法强行终止，但是不推荐这个方法，因为 stop 和 suspend 及 resume 一样都是过期作废的方法。
      3.使用 interrupt 方法中断线

18. 线程调度的方式
		协同式，执行时间由线程本身控制，任务做完才通知系统切换线程，即线程执行之间不可控，容易阻塞进程
		抢占式，由系统分配执行时间，执行时间系统可控，不会阻塞整个进程。是基于优先级的调度


### 线程锁相关
   
#### 锁基本概念
	****用锁来访问协调对象，每次访问都需要同一个锁，这里强调并发针对同一把锁的控制****
	锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。
   像i/o等耗时的计算或操作，执行这些操作期间不要占有锁


   几种锁类型
      自旋锁，偏向锁，轻量级锁，可重入锁，公平锁，非公平锁，乐观锁，悲观锁

      线程锁的相关概念：
		公平锁以请求锁的顺序来获取锁，非公平锁则是无法保证按照请求的顺序执行
		线程中的集中锁的分类：
		1、重入锁
		2、可中断锁
		3、公平锁
		4、读写锁

   * 1. 可重入锁(递归锁)

         指的是同一线程外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。
         可重入就是说某个线程已经获得某个锁(并且该锁尚未释放)，可以再次获取锁而不会出现死锁。
         在 JAVA 环境下 ReentrantLock 和 synchronized都是可重入锁。
         重入是基于每线程而言，而不是每调用

         synchronized 和 ReentrantLock 都是可重入锁，进入对象的第二个锁方法，不需要重新获取锁
            class MyClass {
               public synchronized void method1() {method2();}
               public synchronized void method2() {}
            }

         实现原理(忽略)
            是为每个锁关联一个计数器和占有的线程，初始为0。jvm会记录锁的占有者和计数器
            synchronized和ReentrantLock都是可重入锁，ReentrantLock的时候一定要手动释放锁，并且加锁次数和释放次数要一致。
            就是一个方法里，外层方法占用了锁，但是里面还有方法要获得锁，如果不是重入锁，程序无法继续运行，陷入死锁，是重入锁就继续执行。

   * 2. 自旋锁(spin lock)
         自旋锁是指当一个线程尝试获取某个锁时，如果该锁已被其他线程占用，就一直循环检测锁是否被释放，而不是进入线程挂起或睡眠状态。
         自旋锁的目的是为了占着CPU的资源不释放，等到获取到锁立即进行处理。
         是一种非阻塞锁，也就是说，如果某线程需要获取锁，但该锁已经被其他线程占用时，该线程不会被挂起，而是在不断的消耗CPU的时间，不停的试图获取锁(一般使用闭锁或者条件等待等待状态转换)
         jvm既能自旋等待(不断尝试获取，直到成功)，也可以挂起，
         一般搭配while使用(while获取会浪费资源，使用通知推送机制，会更加的高效点)
         
         自旋CAS实现的基本 思路就是循环进行CAS操作直到成功为止。一般限制是超时或者重试次数

         基于cas做的自旋(常用的就是while()实现不断的进行cas尝试，次数太多转悲观处理，不要空耗cpu)

         
		   自旋锁: 让线程执行一个忙循环(自旋转)，这样可以让线程等待，不用放弃处理器的执行时间，
				     好处是避免了线程切换的开销，但是要占用处理器的时间，因此有个自旋转限定次数



         实现原理（忽略）
            如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要进入阻塞挂起状态，只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，避免线程切换的消耗。
            线程自旋是需要消耗 cup 的，如果一直获取不到锁，那线程也不能一直占用 cup 自旋做无用功，所以需要设定一个自旋等待的最大时间。
            如果持有锁的线程执行的时间超过自旋等待的最大时间扔没有释放锁，就会导致其它争用锁的线程在最大等待时间内还是获取不到锁，这时争用线程会停止自旋进入阻塞状态。
      
   * 3. 公平锁
         公平锁：即尽量以请求锁的顺序来获取锁。
         比如同时有多个线程在等待一个锁，当这个锁被释放时，等待时间最久的线程（最先请求的线程）会优先获得该锁，这种就是公平锁。

         非公平锁实际执行的效率要远远超出公平锁，除非程序有特殊需要，否则最常用非公平锁的分配机制。
         非公平锁性能比公平锁高 5~10 倍，因为公平锁需要在多核的情况下维护一个队列(加锁时不考虑排队等待问题，直接尝试获取锁，获取不到自动到队尾等待)

         Java 中的 synchronized 就是非公平锁，它无法保证等待的线程获取锁的顺序。
         ReentrantLock 和 ReentrantReadWriteLock，默认的lock()方法采用的是非公平锁，但是可以设置为公平锁

   * 4.  可中断锁
            synchronized就不是可中断锁(一个线程获取锁之后，其他锁只能等待那个线程释放之后才能有获取锁的机会)，而Lock是可中断锁

   * 5.  读写锁
            ReentrantReadWriteLock并未实现 Lock 接口，它实现的是ReadWriteLock 接口
            可以通过 readLock()获取读锁，通过 writeLock()获取写锁。
   

#### lock锁和synchronized同步锁，Lock 和 synchronized
   1. Lock是一个类，其锁定是通过代码实现，可实现同步访问，且一定要手工释放，且须在finally 中释放。Lock 锁的范围有局限性，块范围，
      synchronized是java的关键字，在JVM 层面上实现的，synchronized 会自动释放锁;而 synchronized 可以锁住块、对象、类
   
   2. Lock lock = new ReentrantLock(); //注意这个地方
        在 insert 方法中的 lock 变量是局部变量，每个线程执行该方法时
        都会保存一个副本，那么理所当然每个线程执行到 lock.lock()处获取的是不同的锁，所以就不会发生冲突。

      reentrantlock,排他锁，concurrenthashmap.segment内部也是reentrantlock的子类

   3. synchronized
         缺点：
         代码块被 synchronized 修饰，当一个线程获取了对应的锁，并执行该代码块时，其他线程便只能一直等待，等待获取锁的线程释放锁，而这里获取锁的线程释放锁只会有两种情况：
         * 1)获取锁的线程执行完了该代码块，然后线程释放对锁的占有；
         * 2)线程执行发生异常，此时 JVM 会让线程自动释放锁。

         那么如果这个获取锁的线程由于要等待 IO 或者其他原因（比如调用 sleep 方法）被阻塞了，但是又没有释放锁，其他线程便只能干巴巴地等待，影响程序执行效率
         使用synchronized关键字的话，多个读操作就要相互等待，浪费资源，
         因此就需要有一种机制可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间或者能够响应中断），通过 Lock 就可以办到,ock可以控制等待的时间

         synchronized(obj)   锁对象，
            静态方法 ，对象是类
            实例方法，对象是this	等同于synchronized(this),默认锁当前实例对象, sleep过程中其他的所方法也不能执行
            代码块，锁固定代码块，需要指定锁对象，这里的锁对象是用来竞争用的，如果多个线程争的不是同一个锁，那么就不存在竞争。

            1. 普通方法前加synchronized
               synchronized public void test() {}      
               等价于在方法体前后包装了一个synchronized(this),
               互斥的情况
                  1.在该类的所有非静态方法中发生synchronized(this)
                  2.在该类的所有非静态方法前面加上了synchronized关键字
                  3.在其他类中得到该对象引用，并对该对象进行synchronized操作


               对同一个实例锁资源的理解
                  一、当两个并发线程访问同一个对象object中的这个synchronized(this)同步代码块时，一个时间内只能有一个线程得到执行。另一个线程必须等待当前线程执行完这个代码块以后才能执行该代码块。
                  二、然而，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。
                  三、尤其关键的是，当一个线程访问object的一个synchronized(this)同步代码块时，其他线程对object中所有其它synchronized(this)同步代码块的访问将被阻塞。
                  四、第三个例子同样适用其它同步代码块。也就是说，当一个线程访问object的一个synchronized(this)同步代码块时，它就获得了这个object的对象锁。结果，其它线程对该object对象所有同步代码部分的访问都被暂时阻塞。
                  五、以上规则对其它对象锁同样适用.
            
            2.静态方法前面加synchronized
               将会
               synchronized public static void test() {}    
                  等价锁住了当前类的class对象，锁住整个类 synchronized(A.class)
               互斥情况
                  代码中任意一个地方发生synchronized(A.class)
                  在该类的所有静态方法前面增加了synchronized关键字
               锁住了类，并不代表锁住了类所在的对象，类本身也是一种对象，它与类实例是不同的对象，加锁时不互相依赖。
               锁住了子类或者子类对象，与锁住父类或者父类对象不相关，是独立的

               方法中无法使用this，所以它锁的不是this，而是类的Class对象，所以，static synchronized方法也相当于全局锁，相当于锁住了代码段，锁住了整个类对象

            //一般使用synchronized代码块同步关键代码即可
            synchronized (object){
               //代码内容，
            }
            这里锁住的不是代码块，而是object这个对象，其他代码中发生synchronized (object)时会发生互斥，

            * synchronized锁住的是代码还是对象。
               ​	synchronized锁住的是括号里的对象，而不是代码。
               ​	对于非static的synchronized方法，锁的就是对象本身也就是this。实现同步必须是同一个对象（是锁方法所在的对象），
               ​	如果不是同一个对象，那么多个线程同时运行synchronized方法或代码段，就不起效果

         Synchronized底层原理（忽略）
            是通过对象内部的一个叫做监视器锁（monitor）来实现的。但是监视器锁本质又是依赖于底层的操作系统的 Mutex Lock 来实现的。
            而这种依赖于操作系统 Mutex Lock 所实现的锁我们称之为“重量级锁”。
            每个对象都有个 monitor 对象，加锁就是在竞争 monitor 对象，代码块加锁是在前后分别加上 monitorenter 和 monitorexit 指令来实现的，方法加锁是通过一个标记位来判断的

   4. Lock 和 synchronized 的区别
		1. 底层实现不一样， synchronized 是同步阻塞，使用的是悲观并发策略，lock 是同步非阻塞，采用的是乐观并发策略
		2. Lock 是一个接口(API级别)，是java类，而 synchronized 是Java中的关键字(JVM 级别)，是内置的语言实现。
		3. synchronized 在成功完成功能或者抛出异常时，jvm会自动释放线程占有的锁，因此不会导致死锁现象发生；
		   Lock 在发生异常时，如果没有主动手工通过unLock()去释放锁，则很可能造成死锁现象，因此使用 Lock 时需要在 finally 块中释放锁。
		4. Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用 synchronized 时，等待的线程会一直等待下去，不能够响应中断。
    	5. synchronized,锁对象和方法(就是该方法所在对象本身)，静态的synchronized方法从class对象上获取锁,优先锁代码块
      6. synchronized无法判断是否获取锁的状态，Lock可以判断是否获取到锁；使用tryLock()返回结果
      7. synchronized的锁可重入、不可中断、非公平，而Lock锁可重入、可判断、可公平（两者皆可）

      Reentrantlock相对于内部锁synchronized优点是：
         1.可定时，可轮询，可中断的锁获取 			
            可轮询 这里是指trylock(非阻塞获取)没获取到的时候，可以配合while重复直到获取为止，
            通过 Lock 可以知道有没有成功获取锁，而synchronized 却无法办到
         2.公平锁，
         3.非块状结构锁 
         4.可以绑定多个条件等特性(忽略)
            reentrantLock可同时绑定多个condition对象
         5.等待可中断

      lock,Reentrantlock
			lock锁  常用Reetrantlock操作，一定要在finally中释放锁，不能忘
			无条件、可轮询、可定时、可中断，lock的操作都是显式的，
			常见的是用一个byte的对象作为锁，对象。  byte[] lock = new  byte[1];

      lock 和 synchronized，两种锁锁方法
         public static void lock(int i){
            lock.lock();
            num1 ++;
            lock.unlock();
         }

         public static synchronized void sync(int i){
            num2 ++;
         }

		AbstractQueuedSynchronized（AQS）(忽略)
         ReentrantLock的实现依赖于Java同步器框架AbstractQueuedSynchronizer(本文简称之为 AQS)。
			抽象的队列式的同步器，定义了一套多线程访问共享资源的同步器框架
			它提供通用机制来原子性管理同步状态、阻塞和唤醒线程，以及维护被阻塞线程的队列
			许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch
			
         AQS 定义两种资源共享方式
            Exclusive 独占资源-ReentrantLock
               Exclusive（独占，只有一个线程能执行，如 ReentrantLock）
            Share 共享资源-Semaphore/CountDownLatch
               Share（共享，多个线程可同时执行，如 Semaphore/CountDownLatch）。

   5. 解决并发思路：
		1.锁，小粒度，reetrantlock
		2.无锁
			CAS ：即 Compare And Swap ，这是一种类似于乐观锁的机制 每次更新值的时候都使用旧值与变量的当前值做比较，如果相同则进行更新，否则重试直到成功
			Threadlocal ：本地存储变量，这样每 个线程都有一份数据的副本，也就不会存在并发问题了
			不可变对象： 不可变对象自然不会有并发问题
		
   6. ReentrantLock提供的获取锁的方式
			1.lock()
				不可中断，如果获取不到则一直休眠等待	
				lock方法不能被中断。如果一个线程在等待获得一个锁时被中断，中断线程在获得锁之前会一直处于阻塞状态。如果出现死锁，那么lock方法就无法被终止。
			2.tryLock() 	
				用来尝试获取锁，如果获取成功，则返回true
			3.tryLock(long timeout, TimeUnit unit)		会监测中断事件，定时获取，配合while可以轮询获取。
				当获取锁时，锁资源在超时时间之内变为可用，并且在等待时没有被中断，那么当前线程成功获取锁，返回true，同时当前线程持有锁的count设置为1.
				当获取锁时，在超时等待时间之内，被中断了，那么抛出InterruptedException，不再继续等待.
				使用trylock实现可轮询和可定时，这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。
			4.lockInterruptibly()						直到获取成功或者被中断为止
				当获取锁时，锁资源不可用，那么该线程开始阻塞休眠等待，但是等待过程中如果有中断事件，那么会停止等待，立即返回.
				线程A执行获得锁占用5秒，同时线程B执行lockInterruptibly()，这时候
				A和B  	都start()
				A中lock()  						//占用5秒
				B中lockInterruptibly()			//等待中
				主线程中B随后调用interrupt()	//这时候lockInterruptibly()发现有中断操作就直接终止等待锁了。	
				
				就是说等待的过程中可以被中断，如果中断发生，就会停止等待，立刻返回    	用的不多暂时不用
				如果当前线程未被中断则获得锁,如果当前线程被中断则出现异常
            

   7. lock中的condition(忽略)
			Condition对象是由Lock对象（调用Lock对象的newCondition()方法）创建出来的，暂时没看到用处，后面用到再说。
			condition是实现lock的等待效果，并且是可选择的，一个lock可以多个condition
			
			synchronized关键字中wait()/notify/notifyAll()，类似ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法
			一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活，
			在使用notify/notifyAll()方法进行通知时，被通知的线程是有JVM选择的，使用ReentrantLock类结合Condition实例可以实现“选择性通知”，这个功能非常重要，而且是Condition接口默认提供的。

			而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。
			如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程
			
			private Lock lock = new ReentrantLock();
			private Condition condition=lock.newCondition();
			lock.lock();//lock 加锁			
			condition.await();	//通过创建 Condition 对象来使线程 wait，必须先执行 lock.lock 方法获得锁，获得同步监视器
			condition.signal();//condition 对象的 signal 方法可以唤醒 wait 线程
			
			Condition 类和 Object 类锁方法区别区别
				1. Condition 类的 awiat 方法和 Object 类的 wait 方法等效
				2. Condition 类的 signal 方法和 Object 类的 notify 方法等效
				3. Condition 类的 signalAll 方法和 Object 类的 notifyAll 方法等效
				4. ReentrantLock 类可以唤醒指定条件的线程，而 object 的唤醒是随机的
						
			减小锁的竞争
				1.减少锁持有时间(减小锁范围)，2.减小请求锁的频率 3.协调机制取代独占锁


   8. ReentrantLock的读写锁（忽略）
			ReentrantReadWriteLock 是 ReadWriteLock 的实现类，readLock() 和 writeLock() 用来获取读锁和写锁。
			注意这里占用读写锁，使用的是同一个ReentrantReadWriteLock锁对象中的读写锁。
			使用读写锁提升读操作的并发性，也保证每次写操作对所有的读写操作的可见性
			ReentrantReadWriteLock.readLock(); 			//获取读锁
			ReentrantReadWriteLock.writeLock();			//获取写锁
			
			场景：
				使用单一ReentrantLock保证了线程安全，但是浪费了一定资源(多个读操作并行，无线程安全问题)，但是写操作不是线程安全的(多个线程同时写，或写的同时进行读操作)
				然后引出读写锁就解决了这样的问题，可以保证多读高效，和写入线程安全
				写锁(可以读数据又可以修改数据);读锁(只能读，可被多个线程同时拥有)

			规则
				Thread A占用了读锁，其他线程申请读锁，成功。
				Thread A占用了读锁，其他线程申请写锁，则申请写锁的线程会一直等待释放读锁，因为读写不能同时操作。
				Thread A占用了写锁，其他线程申请写锁或者读锁，会阻塞，需等待Thread A释放写锁，同样也因为读写不能同时，并且两个线程不应该同时写。
				总结：	要么是一个或多个线程同时有读锁，要么是一个线程有写锁，但是读写不会同时出现。
						也可以总结为：读读共享、其他都互斥（写写互斥、读写互斥、写读互斥）。	
			
			锁降级是指把持住（当前拥有的）写锁，再获取到 读锁，随后释放（先前拥有的）写锁的过程


### 线程池_doing，线程池相关
   线程池可以复用线程（减少创建和销毁次数），统一分配，管理，监控，防止资源浪费，调控线程数量，防止过多消耗内存。
	其中有两个核心的队列，1是线程等待池，即线程队列blockingqueue 2是任务处理池，正在工作的thread列表，hashset<Worker>

1. Executor(实现线程池的功能)的3个组成部分
      1.工作任务：Runnable/Callable 接口
      2.任务的执行。包括执行机制的核心接口Executor(ExecutorService接口是其子接口)。Executor框架有两个关键类，ThreadPoolExecutor和ScheduledThreadPoolExecutor。
      3.异步计算的结果。包括接口Future和实现Future接口的FutureTask类。

2. Executor, ExecutorService和Executors相关
      三者的区别
         1. ExecutorService 接口继承了Executor 接口，是Executor 的子接口。

         2. Executor定义的execute()方法，用来接收一个Runnable接口的对象，无返回结果；
            ExecutorService 中定义的submit()方法可以接收Runnable和Callable接口对象，通过Future对象返回运算结果。真正的线程池接口，Executors创建线程池后的对象
               当将Callable的对象传递给ExecutorService的submit方法，则该call方法自动在一个线程上执行，并且会返回执行结果Future对象。
               当将Runnable的对象传递给ExecutorService的submit方法，则该run方法自动在一个线程上执行，并且会返回执行结果Future对象，但是在该Future对象上调用get方法，将返回null。

         3. Executor和ExecutorService除了允许客户端提交一个任务，ExecutorService 还提供用来控制线程池的方法(比如：调用 shutDown() 方法终止线程池)。
         
         4. Executors是工具类(创建线程池)，他提供对ThreadPoolExecutor的封装产生ExecutorService的具体实现类。

         ThreadPoolExecutor 是jdk中的线程池类
         ThreadPoolTaskExecutor这个类则是spring包下的，是sring为我们提供的线程池类

      案例
         ExecutorService executorService = Executors.newCachedThreadPool(); 
         Future<String> future = executorService.submit(new TaskWithResult(i));

      * Executors是线程池的工厂类
            ExecutorService executorService = Executors.newFixedThreadPool(11);
            new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>()); 这样默认的是最大的队列值

            Executors 类提供工厂方法创建不同类型的线程池(本质上使用了 ThreadPoolExecutor 的构造方法创建线程池)。
               1.newSingleThreadExecutor()?创建一个只有一个线程的线程池，
               2.newFixedThreadPool(int numOfThreads)来创建固定线程数的线程池，
               3.newCachedThreadPool()   缓存线程池，线程数量不限制，可能oom
               4.Executors.newScheduledThreadPool(int corePoolSize) 创建一个线程池(任务调度的线程池实现)，以在给定的延迟后运行命令，或者定期执行命令(它比Timer更灵活)。不常用
            
               不足
                  1）newFixedThreadPool 和 newSingleThreadExecutor: 	 主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至 OOM。
                  2）newCachedThreadPool 和 newScheduledThreadPool:   主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。

            ThreadPoolExecutor
               是最核心的线程池实现，用来执行被提交的任务，
               实现了ExecutorService接口，所以Executors创建的线程池对象也是ExecutorService的实现类
               可以使用execute和submit两个方法向线程池提交任务,前者不带返回值(参数是runable)，后者带返回值(参数是futuretask，实现callable接口)

      * ExecutorService
            shutdown() 				   只是关闭了提交通道，不可以再submit新的task，已经submit的将继续执行。优雅关闭
            shutdownNow() 			   试图停止当前正执行的task(正在跑的和正在等待的任务都停下)，并返回尚未执行的task的list
            awaitTermination()		当等待超过设定时间时，会监测ExecutorService是否已经关闭，若关闭则返回true，否则返回false。一般和shutdown组合使用

3. 原始的new Thread(new RunnableTask())).start()，但在Executor中，可以使用Executor而不用显示地创建线程：executor.execute(new RunnableTask());
	   基本所有的 ExecutorService 对任务的异常都做了捕获，当你的任务代码抛出异常时，你是拿不到错误的 所以需要你在自己的任务中（ Runnable 或者 Callable ）捕获异常并处理

4. 核心线程池，工作队列，饱和策略
      线程池工作流程
         1.核心线程池corePoolSize是否满，未满就创建核心线程执行任务，满了就跳2
         2.队列workQueue是否满，没满就任务添加到队列，满了就跳3
         3.线程池maximumPoolSize是否满，没满就创建非核心线程执行任务，满了就交给饱和策略
   
      corePoolSize（线程池的基本大小）：
         池中保存的线程数，包括空闲线程
         当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程
      runnableTaskQueue(任务队列)：
         用于保存等待执行的任务的阻塞队列。
      maximumPoolSize(线程池最大数量)：
         线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。
      RejectedExecutionHandler(饱和策略)：
         当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。
      keepAliveTime(线程活动保持时间)：
         线程池的工作线程空闲后，保持存活的时间。所以， 如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率。
         当一个线程空闲超过一定时间keepAliveTime时，线程池会判断，若当前运行线程数大于corePoolSize，那么该线程就被停掉。所以线程池所有任务完成后，最终会收缩到 corePoolSize 的大小。
         当一个线程完成任务时，它会从队列中取下一个任务来执行
	
5. future相关 和 futuretask相关，

      Future<V>接口是用来获取异步计算结果的(操作具体的Runnable或者Callable对象任务执行的结果)，是ExecutorService.submit()返回的对象，futuretask是其唯一的实现类(实现了runnable和future)

		futuretask优点
         ExecutorService.submit()的是runnable和future都需要返回一个future对象再get()，但是futuretask直接futuretask对象.get()即可，相当于，结果封装进了入参中，不需要future接收。

      Executor就是Runnable和Callable的调度容器，Future就是对于具体的Runnable或者Callable任务的执行结果进行操作


      使用场景
         使用future模式，就是先开线程获取结果，然后继续处理其他事，等到需要结果的时候，再get()出结果处理即可。
         那优先将长请求使用future前置，后续要用了再get出来。或者使用回调的方式处理阻塞的请求
         实现异步future+callable 实现返回值线程请求，对执行结果进行监听，获取异步任务的返回结果

		future提供了三种功能：
			1.isDone()			判断任务是否完成
			2.cancel()			中断任务
			3.get()				获取任务执行结果(常用)

		future.get()方法的阻塞优化，CompletionService方案
			场景：	 普通的Future.get()是按照加入线程池的顺序来获取的，如果前一个没完成会一直阻塞后面future的.get()，一般是用list收集future后面遍历
						使用Future和Callable可以获取线程执行结果，但获取方式确实阻塞的，根据添加到线程池中的线程顺序，依次获取，获取不到就阻塞。
			
			解决方法：	为了解决这种情况，可以采用轮询的做法。CompletionService 来实现异步快速收集线程执行结果。
						CompletionService是按照完成顺序获取，解决了Future.get()按照加入线程池顺序(submit顺序)的阻塞问题。
						CompletionService的好处在于主线程总是能够拿到最先完成的任务的返回值，即take()按照线程完成顺序获取，如果就没有完成的就阻塞，这样优先完成的输出，减小了get的阻塞问题

		
		Future是返回各个线程的处理结果，并且多个线程，get的时候如果还没处理完就会阻塞（这是同步的），返回的每个future都是绑定固定的线程号的。
		线程只要执行完就能拿到结果。线程是进行中的，调用future.get()时会阻塞主线程
         ```
            final List<Future<String>> resultList = new ArrayList<Future<String>>(); 
               for(Seckill seckill:list){
                  resultList.add(executor.submit(new createhtml(seckill)));
               }
            for (Future<String> fs : resultList) { 
               try {
                     System.out.println(fs.get());//打印各个线任务执行的结果，调用future.get() 阻塞主线程，获取异步任务的返回结果
                  } catch(Exception e) {
                     e.printStackTrace();
                  }
            } 
               return Result.ok();
            }
            
            class createhtml implements Callable<String>  {
               Seckill seckill;
               public createhtml(Seckill seckill) {this.seckill = seckill;}
               @Override
               public String call() throws Exception {
                  system.out.println(seckill.getSeckillId());
                  return "success";
               }
            }
         ```

         ```
            public static void main(String[] args) {
               ExecutorService executorService = Executors.newFixedThreadPool(1);
               FutureTask<String> task = new FutureTask<String>(new Callable() {
                  @Override
                  public String call() throws Exception {return "nihao";}
               });
               executorService.submit(task);
               try {
                  task.get();
               } catch (Exception e) {
                  e.printStackTrace();
               }
               System.out.println(sign);
            }	
         ```

6. 线程池的工作原理(忽略)：
      任务job提交进线程池，进入到工作队列中，等待工作者线程处理
      线程池的本质就是使用了一个线程安全的工作队列连接工作者线程和客户端线程，客户端线程将任务放入工作队列后便返回，而工作者线程则不断地从工作队列上取出 工作并执行。
      当工作队列为空时，所有的工作者线程均等待在工作队列上，当有客户端提交了一个任务之后会通知任意一个工作者线程，随着大量的任务被提交，更多的工作者线程会被唤醒。

7. ForkJoinPool是一个并发执行框架（忽略）
      异步计算的结果：Future接口
      实现Future接口的FutureTask类，代表异步计算的结果，持有计算结果

8. Guava Future
      ListenableFuture是guava中的多线程能得到结果的方法,可得到多线程调用的结果  这个知道概念就行了
      Future的扩展增强，有点是监听任务完成，不用手动获取
      解决的是监听future返回结果阻塞问题，引入Guava Future

      使用场景
         ListenableFuture可以监听的Future，是对java原生Future的扩展增强。
         Future表示一个异步计算任务，当任务完成时可以得到计算结果。
         如果我们希望一旦计算完成就拿到结果展示给用户或者做另外的计算，就必须使用另一个线程不断的查询计算状态。这样做，代码复杂，而且效率低下。
         使用ListenableFuture Guava帮我们检测Future是否完成了，如果完成就自动调用回调函数，这样可以减少并发程序的复杂度。
   
         可见虽然主线程中的多个任务是异步执行，但是无法确定任务什么时候执行完成，只能通过不断去监听以获取结果，所以这里是阻塞的。这样，可能某一个任务执行时间很长会拖累整个主任务的执行。
         一个任务的执行结果:Future接口中的isDone()方法来判断任务是否执行完，如果执行完成则可获取结果，如果没有完成则需要等待。 
   
         这里使用while(true)实现,对某一个futuretask完成后才可以get，否则阻塞没有意义
         while (true) {
            if (booleanTask.isDone() && !booleanTask.isCancelled()) {
               Boolean result = booleanTask.get();
               System.err.println("任务1-10s： " + result);
               break;
            }
         }
         ​
         多线程中的Future.get()没有结束会一直阻塞
         那么多个while (true) 的get，如果第一个阻塞很久，那么后面的也卡主了，相当于阻塞了主线程的流程
         因为我们一开始用 Thread1.get() 获取第一个线程的结果时，是阻塞的，而且我们假定任务1执行了10s钟，导致了线程2（3s就执行完任务）和线程3（2s就执行完任务）都执行完了任务，也不打印出来。那在实际业务中，这种方法肯定是不可取的。所以引入 Guava Future
         
         手工Future.get会一直阻塞在那，这时可以使用
         ListeningExecutorService executorService = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(5));装饰下原线程池，
         使用ListenableFuture监听future返回结果，Futures.addCallback(listenableFuture, new FutureCallback<Integer>() {}即可，这里这个方法是异步的，主线程会顺序先执行，主线程不会阻塞，监听再依次输出

      使用限流  使用Guava 的 RateLimiter,并发限流器
         RateLimiter是单机的，也就是说它无法跨JVM使用,
         使用的是令牌桶算法是最常用的限流算法，它最大的特点就是容许一定程度的突发流量。
         以固定频率向桶中放入令牌，例如一秒钟10枚令牌，实际业务在每次响应请求之前都从桶中获取令牌，只有取到令牌的请求才会被成功响应，获取的方式有两种：阻塞等待令牌或者取不到立即返回失败，
         RateLimiter 允许某次请求拿走超出剩余令牌数的令牌(马上许可)，但是下一次请求将为此付出代价，一直等到令牌亏空补上，并且桶中有足够本次请求使用的令牌为止.(可以预支，后续延迟)

   
   ​	 Future表示一个异步计算任务，当任务完成时可以得到计算结果。  如果我们希望一旦计算完成就拿到结果展示给用户或者做另外的计算，就必须使用另一个线程不断的查询计算状态。
   ​	 使用ListenableFuture.addCallback()检测Future,如果完成就自动调用回调函数，这样可以减少并发程序的复杂度
   ​	 如果我们关心线程池执行的结果，则需要使用submit来提交task，使用Future处理，
   ​	 如果我们不关心这个任务的结果，execute方法（实际是继承Executor接口）来直接去执行任务，

   ​	 executor框架利用futuretask来完成异步任务
      ​	futuretask的计算是通过callable实现的，它等价于一个可携带结果的runnable，并有三个状态：等待，运行和完成
      ​	futuretask.get是一个阻塞的方法，知道结果返回或者接收到一个异常
      ​	futuretask把计算的结果从运行计算的线程传送到需要这个结果的线程，futuretask的规约保证了这种传递建立在结果的安全发布的基础上
      ​	给定runnable或callable实例化一个futuretask

      项目开票实例代码
         ```
            @Override
            public String accept(Integer mainId, Integer id) {
               UserBillMain userBillMain = this.selectById(mainId);
               List<UserBillDetail> list = userBillDetailService.selectAllByMainId(mainId);
               ListeningExecutorService executorService = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(5));

               RateLimiter limiter = RateLimiter.create(4); // 每秒不超过4个任务被提交
               BaseTaxBureau btb = baseTaxBureauMapper.selectById(userBillMain.getTaxBureauId());
               try {
                  for (UserBillDetail ubd : list) {
                     limiter.acquire(); // 请求RateLimiter, 超过permits会被阻塞
                     final ListenableFuture<String> listenableFuture = executorService
                           .submit(new TaskCall(ubd, btb, userBillMain, id));

                     Futures.addCallback(listenableFuture, new FutureCallback<String>() {
                        @Override
                        public void onSuccess(String result) {
                           logger.info("getlimiter callback " + result + "---" + System.currentTimeMillis());
                           if(!StringUtils.equals(result, "success")){
                              throw new RuntimeException();
                           }
                        }
                        @Override
                        public void onFailure(Throwable t) {
                           logger.error("批量处理合同异常",t);
                        }
                     });

                  }
               } catch (Exception e) {
                  logger.error("处理合同或税费异常",e);
                  return "failed";
               }finally{
                  executorService.shutdown();
               }
               userBillMain.setStatus(ConstantsForDB.BILL_APP_DEAL);// 状态改为处理中
               updateByIdSelective(userBillMain, UserBillMainMapper.class);// 更新开票单
               return "success";
            }

            class TaskCall implements Callable<String> {
               UserBillDetail ubd;
               BaseTaxBureau btb;
               UserBillMain userBillMain;
               Integer id;

               public TaskCall(UserBillDetail ubd, BaseTaxBureau btb, UserBillMain userBillMain, Integer id) {
                  this.ubd = ubd;
                  this.btb = btb;
                  this.userBillMain = userBillMain;
                  this.id = id;
               }
               @Override
               public String call() throws Exception {
                  logger.error("测试日志",e);
                  return "success";
               }
            }
         ```

### 线程变量Threadlocal，ThreadLocal相关
   
   任何一个线程可以并发访问threadlocal该变量，线程安全。其中每个线程都有对应的threadLocalMap。
   实质就是通过统一操作一个线程变量ThreadLocal，来给不同的线程赋值私有属性

   本质线程共享使用 private static final ThreadLocal<T>变量 实现线程私有属性的操作，每次操作线程变量即可。本质就是操作Thread.ThreadLocalMap<ThreadLocal, Object>这个对象
   定义static是多线程共享，定义成 final 就是为了不能改变。ThreadLocal 就像是一个公用的盒子，大家都可以放取东西。但盒子不能换掉，不然放进去的东西就丢失了。
   

   //这里是将value存在了每个线程的ThreadLocalMap中，key是共用的ThreadLocal静态实例(每个线程都绑定了该实例，然后将value设置到线程变量中)，value是当前线程的值
   //将一个共用的 ThreadLocal 静态实例(static final)作为 key，将不同对象的引用保存到不同线程的ThreadLocalMap 中，后续通过get()方法取回
   //private static final ThreadLocal<ApplicationContext> opContextHolder = new ThreadLocal<ApplicationContext>();  

   threadLocal一般都是声明在静态变量中，如果不断的创造threadLocal而没有调用remove方法，会导致内存泄漏
   ThreadLocal 的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用。

   * 实现原理：         
      inheritableThreadLocals是子父线程继承时用的（忽略）

      ThreadLocalMap对象(忽略)
         用来操作ThreadLocal 和 value。等效操作 Thread.ThreadLocalMap<ThreadLocal, Object>,这里的ThreadLocal一般就是定义好的private static final ThreadLocal<T>变量。

         每个thread中都有一个自己的属性threadLocals，Thread 类中定义为 ThreadLocal.ThreadLocalMap threadLocals = null，用作线程私有保存对象

         ThreadLocalMap是ThreadLocal的一个静态内部类，借用ThreadLocalMap的内部类Entry(ThreadLocal<?> k, Object v)结构操作(类似map(this，value))，
         等效于Thread.ThreadLocalMap<ThreadLocal, Object>
            
            static class Entry extends WeakReference<ThreadLocal<?>> {
                  /** The value associated with this ThreadLocal. */
                  Object value;

                  Entry(ThreadLocal<?> k, Object v) {
                     super(k);
                     value = v;
                  }
            }

         该对象set操作存储了<threadlocal.threadlocalhashcode,本地线程变量>的k-v键值对，保证线程变量的唯一性，使用的是同一个线程变量threadlocal，给不同的线程赋值私有属性

         这里的共享变量实际是抽象出来的操作对象，否则每次都要操作线程Thread.currentThread()的对象，
         最简化的就是直接在线程中设置一个属性T,这里抽象出一个变量来处理。

         //ThreadLocal 的get和set方法（忽略）
            public void set(T value) {
               Thread t = Thread.currentThread();
               ThreadLocalMap map = getMap(t);
               if (map != null)
                     map.set(this, value);
               else
                     createMap(t, value);
            }
            
            //get总是返回由当前执行线程通过set设置的最新值
           public T get() {
               Thread t = Thread.currentThread();
               ThreadLocalMap map = getMap(t);
               if (map != null) {
                     ThreadLocalMap.Entry e = map.getEntry(this);
                     if (e != null) {
                        T result = (T)e.value;
                        return result;
                     }
               }
               return setInitialValue();
            }

            void createMap(Thread t, T firstValue) {
               t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue);
            }

            //当前线程的ThreadLocalMap属性
            ThreadLocalMap getMap(Thread t) {
               return t.threadLocals;
            }

1. 线程安全的条件
      使用同步机制要求：
         1.变量读写时间，2.锁定对象的时间 3.释放对象的时间
            若多个线程之间需要共享资源，以达到线程间的通信时，就使用同步机制；

      同步机制：		 
         时间换空间，访问串行化，对象共享化。同一份变量，排队

      threadlocal：	
         空间换时间，访问并行化，对象独享化。不同变量副本，不用排队
         若仅仅需要隔离多线程之间的关系资源，则可以使用ThreadLocal
         这个变量一般是跟着private static用的，在一个共同的外部类使用threadlocal保存资源。
         threadlocal是保存本地线程变量副本的容器，是当前线程的专有属性，与其他线程相隔离，是一次线程的操作，存一个对象 key-value，可以存一个map，这样可以多存几个值，用完记得移除

2. 使用场景
      * 隐式传参
         (在实际使用中，容器把一个事务上下文和一个可执行的线程关联起来。)
         利用静态Threadlocal持有事务上下文，当框架需要使用当前线程所在上下文中的事务中相关信息时，可以从threadlocal中获取事务上下文中信息，而不用从前到后显式传递，类似系统中op-gateway。
         降低了方法执行链上的过多参数传递，隐藏方法参数。减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度。
         
         缺点:线程变量会引入晦涩的类间耦合，像全局变量和创建一种将方法的参数隐藏起来的方法，不要过多使用
         
      * 数据库连接池的使用
            数据库连接池，新线程，将新connection放进threadlocal里的，以保证每个线程从连接池中get获得的都是线程自己的connection。
            注意pool.getConnection()，都是先从threadlocal里面拿的，若threadlocal里面有，则用，保证线程里的多个dao操作，用的是同一个connection，以保证事务。

      * 常用用法：和context结合使用，将context和线程关联起来，降低了每个方法传递执行上下文信息的需要。
         便于线程的上下文传输，隐藏参数传递
         注意这里是静态持有上下文的，有两个特性，一个是封闭隐藏，一个是传输性。降低重用性，引入隐晦的类间耦合
         ```    
            public class ApplicationContext implements Serializable
            {
            /** 应用上下文线程变量 */
            private static final ThreadLocal<ApplicationContext> opContextHolder = new ThreadLocal<ApplicationContext>();
            private Object info;
            public static ApplicationContext getContext()
            {
               ApplicationContext context = opContextHolder.get();
               if (null == context)
               {
                     context = new ApplicationContext();
                     opContextHolder.set(context);
               }
               return context;
            }
         ```

      * 本地系统实现
         op系统gateway传值,用做传参工具
         重定向redirect，服务器端在响应第一次请求的时候，让浏览器再向另外一个URL发出请求，从而达到转发的目的。它本质上是两次HTTP请求，对应两个request对象。
         转发forward，客户端浏览器只发出一次请求，Servlet把请求转发给Servlet、HTML、JSP或其它信息资源，由第2个信息资源响应该请求，两个信息资源共享同一个request对象。	
      
         1.将参数放进threadlocal中
         2.在接下来的另一个方法中，取出threadlocal中存的data信息，放进session中
         3.重定向后从session中取出data信息。
         那么这里threadlocal的使用，只不过是为了再后面的方法中不带参数的传递信息，后面方法直接取线程变量，不需要由前传到后。本质上还是从session中取数据。
         重定向后，因为是同一个域名，www.xiaoyuer.com,所以是同一个sessionid,这其实可以直接存session，然后重定向后获取。

         小鱼儿系统
         每次来一个线程就存一次context，每次覆盖一个新的CommonConstant.PAGE_SERVICE_SESSION_KEY, uuid 来存值setContext中存。相当于将信息存在了session中
         ```		
            private GenericResult<Object> invokePageService(OpService opService)
               throws IOException
            {
               String uuid = UUID.randomUUID().toString();
               ApplicationContext context = ApplicationContext.getContext();
               HttpServletRequest request =((ServletRequestAttributes)RequestContextHolder.getRequestAttributes()).getRequest();
               request.getSession().setAttribute(uuid, context);
               request.getSession().setAttribute(CommonConstant.PAGE_SERVICE_SESSION_KEY, uuid);
               LOGGER.info("设置session{},UUID是:{}", request.getSession().getId(),uuid);
               GenericResult<Object> result = new GenericResult<Object>();
               result.setObject(opService.getPageTarget());
               return result;
            }
         ```

         ```
            1.存入线程变量
            private void setContext(Map<String, Object> parameters, OpPartner opPartner, OpService opService,OpPartnerService ops)
            {
               ApplicationContext.getContext().setParameters(parameters);
               ApplicationContext.getContext().setPartnerService(ops);
               ...
            }
            2.存入session
            private GenericResult<Object> invokePageService(OpService opService)throws IOException
            {
               String uuid = UUID.randomUUID().toString();
               ApplicationContext context = ApplicationContext.getContext();
               HttpServletRequest request =((ServletRequestAttributes)RequestContextHolder.getRequestAttributes()).getRequest();
               request.getSession().setAttribute(uuid, context);
               request.getSession().setAttribute(CommonConstant.PAGE_SERVICE_SESSION_KEY, uuid);
               return result;
            }
            3.
            重定向之后  重定向之后，
            uuid = String.valueOf(request.getSession().getAttribute(CommonConstant.PAGE_SERVICE_SESSION_KEY));
            context= (ApplicationContext)request.getSession().getAttribute(uuid);
         ```

3. ​	private static final ThreadLocal   一般都用 private static final 修饰    
   ​	尽量使用static,不然会使线程的ThreadLocalMap产生太多Entry,从而造成内存泄露(知道就行)

      用完remove掉
      ​	使用ThreadLocal最好是每次使用完就调用remove方法，将其删掉，避免先get后set的情况导致业务的错误。
      ​	1内存溢出，ThreadLocal依赖没有释放，无法GC。
      ​	2线程池的某个线程会被反复使用,ThreadLocal的生命周期不等于一次Request的生命周期，造成获取threadLocal内数据异常。

4. 线程变量会有线程挂起？(避免资源的浪费)
      挂起之后，线程的挂起操作实质上就是线程进入"非可执行"状态下，在这个状态下CPU不会分给线程时间片，进入这个状态可以用来暂停一个线程的运行。
      线程挂起后(阻塞状态 cpu不分配运行时间)，可以通过重新唤醒线程来使之恢复运行。  

   （忽略）ThreadLocal无法在父子线程之间传递--内容保存在线程对象中，子线程无法继承父线程的内容
	父线程的概念(创建线程,类似main中开线程)，只是一种逻辑称呼，创建线程的当前线程就是新线程的父线程，新线程的一些资源来自于这个父线程

3. 实现变量的初始化(忽略)
   ```
      ThreadLocal<Integer> iin=new ThreadLocal<Intege r>(){
               public Integer initialValue() {
                     int a=1;
                     return a;
                  }
               };
   ```	

## 高并发
### 基本知识
   
* 并发的三个特性
      1、原子性：
            即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。提供互斥访问，同一时刻只能有一个线程对它进行操作 
         
            常见的原子变量类  
               内部也是用了CAS比较交换原理处理的,核心的方法是compareAndSet,用来比较交换
               常用原子类的3种，1基本类型(atomicInteger,atomicLong，atomicBoolean等) 2数组(AtomicLongArray等) 3引用类型(AtomicReference等),
               
            i++操作不是原子操作，经历了3个操作(读-改-写)， 1.获取变量当前值,  2.为该值加1, 3.写回更新值

      2、可见性：
            是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。

            涉及到本地线程内存和主内存的概念，高速缓存（相当于本地的线程内存/工作内存）中的数据刷新到主存当中
            Java提供了volatile关键字来保证可见性,但是不保证原子性
            通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。

      3、有序性：
            即程序执行的顺序按照代码的先后顺序执行。

### 乐观锁和悲观锁

* 使用场景
         乐观和悲观锁选择的标准：冲突的频率和严重性，
            若冲突很少或者冲突代价低，选乐观，若冲突频繁发生，直到提交数据才通知冲突很不友好，会默认为经常失败。
            若冲突多或冲突代价高，那么就需要使用悲观锁
            
            使用乐观在检测到冲突的时候还是需要面临合并冲突处理的情况，但是一般业务是很难自动合并的，只能扔掉从头开始
            复杂的场景下多用悲观锁，首先保证正确性，其次才是性能
	
* 悲观锁（独占锁或者排他锁）：

		实现原理：利用数据库内部机制提供的锁方法，对更新的数据加锁，并发时一旦一个事务持有了数据库记录的锁，其他的线程将等待，不能再对数据进行更新了
		适用场景：悲观锁是避免冲突，遇到就等；冲突率很高的并发场景下适合用悲观锁
		
		缺点：降低性能，消耗资源
			造成大量的线程被挂起和恢复，CPU频繁切换线程上下文，过多的等待和事务上下文的切换导致缓慢，十分消耗资源，性能不佳，
			悲观锁开销在于加锁本身，需要很复杂的同步与调度机制，即使没有真正发生冲突也增加额外开销
		
         当一条线程抢占了资源后，其他的线程将得不到资源，那么这个时候，CPU就会将这些得不到资源的线程挂起，挂起的线程也会消耗的资源，
         资源只能被一个事务锁持有，只能有一个事务占据资源，其他事务被挂起等待 持有资源的事务提交并释放资源后才能继续，这里的资源也可是数据库连接资源。
		
		使用案例：select ... for update
			容易死锁，因为会一直等到锁可用，两个用户都需要a，b资源，这时候会出现互相等待的场景
			主键查询，所以只会对行加锁（只有通过索引进行检索的时候才会使用行级锁）。如若是非主键查询，要注意不能锁表。
			和update都可以实现悲观锁(一般伴随事务一起使用，数据锁定时间可能会很长)，事物提交是在整个方法执行完才会提交。
			
         select for update 数据库的悲观锁 在高并发长事务情况下容易发生锁表问题,这个是行级锁,等待整个事务提交或者回滚后，释放
         改善方案，使用数据库的乐观锁，表中增加version字段，每次更新+1，数据库根据version操作即可（两个version版本一致才执行操作）

		select * from table_xxx where id='xxx' for update; 
		注意：id字段一定是主键或者唯一索引，不然是锁表


      *************悲观锁场景使用**************
         EXPLAIN
         SELECT * FROM user_account WHERE user_id=1366 FOR UPDATE
         这是通过service的事务id绑定数据库的事务id，实现一个事务中的锁控制（即当前同一个事务中可多次操作该行数据）
         
         使用update  count=count+1  这种事数据库的级别更新是线程安全的。
         for update  主要也是针对的对象的更新控制，select出对象，然后set变化后的对象属性
            
         其中user_id 加上索引 最好是唯一索引就是行锁，不加索引是表锁
            
         ******
            之前的是同时锁住需求者和服务者，两个线程相反锁顺序就会造成交叉的死锁了,原先的互锁会出现问题，ab 和 ba
         ******
         
         考虑支付余额的并发问题(之前是锁住，当前线程可以拿到，其他线程拿不到)，select for update
         被for udpate锁住的数据，另一个线程select  for update 也会阻塞,因为锁被占用了，不能再加写锁了（同理update一样）。但是单独的select是可以的。
      *************悲观锁场景使用**************
      
				
* 乐观锁（非独占锁 ）：
		通过冲突检测和事务回滚来防止并发业务事务中的冲突，提交时才判断锁冲突，导致其他的无效操作。

		实现原理
			乐观锁通常来说根本没有真正的锁(不使用数据库锁和不阻塞线程并发)，只是检查一个标记决定是否真的进行提交，开销要小得多，但发生冲突要撤销整个transaction重做。
         检测到冲突的时候还是需要面临合并冲突处理的情况，但是一般业务是很难自动合并的，只能扔掉从头开始

      缺点：
			只能在提交数据时才能发现业务事务将要失败，某些情况下，发现太迟，代价也大
			可能导致大量的SQL被执行，容易引起数据库性能的瓶颈，且要考虑重入机制，增加开发难度
			
      使用案例
         UPDATE seckill  SET number=number-?,version=version+1 WHERE seckill_id=? AND version = ?
         关键的修改需要加日志，更新版本号只递增不会递减

         使原本一次的 update 操作，必须变为 2 次操作: select 版本号一次；update 一次。


      解决并发失败数量多的场景
         使用重入机制
            不能重入太多次，限制1.按规定时间内重入，入成功为止，过时退出  2.按规定次数重入 ，while(true){} 实现限制重入

         失败的事务处理，更新条数为0后的业务处理问题
            乐观锁中使用版本号，若更新数量为0，表示已经被其他线程修改，这里的补偿操作处理也是乐观锁的重点地方
            要将事务系统回滚以防止不一致的数据进入数据库，即业务事务必须要么被取消，要么解决冲突并重试
            并且请求数量越大，失败的数量会越多。通过重入的机制将请求失败的概率降低(按时间戳或者限制重入次数的办法).

      CAS(Compare and Swap)比较并替换(忽略)
         这就是乐观锁的思想.但是更严谨的应该是使用版本号更新
         synchronized属于悲观锁，悲观的认为程序中的并发情况严重，所以严防死守，CAS属于乐观锁，乐观地认为程序中的并发情况不那么严重，所以让线程不断去重试更新。

         CAS机制中使用了3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。
         更新一个变量的时候，只有当变量的预期值A和内存地址V当中的实际值相同时，才会将内存地址V对应的值修改为B。

      ABA问题（忽略）
         什么事ABA问题？怎么解决？
                  当一个值从A变成B，又更新回A，普通CAS机制会误判通过检测。		利用版本号比较可以有效解决ABA问题。
               缺点；	
                  1） CPU开销过大		在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，循环往复，会给CPU带来很到的压力。
                  2） 不能保证代码块的原子性	CAS机制所保证的知识一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证3个变量共同进行原子性的更新，就不得不使用synchronized了。

         CAS原理并不排斥并发，也不独占资源，只是在线程开始阶段就读入线程共享数据，保存为旧值。其中可能存在ABA问题
               当处理完逻辑，需要更新数据的时候，会进行一次比较，即比较各个线程当前共享的数据是否和旧值保持一致。不一致考虑重试或者放弃。有时候可重试，这样就是一个可重入锁，
               ABA问题，场景两个线程同时判断一个变量，线程2变成了B后又变成了A,但是线程1中无感知，
                  前提1，线程1,2并不是想spring事务一样完成后才提交，外层没事务影响
                  前提2，基于单独的业务字段判断，并且能回退
                  解决，加入不能回退的非业务字段version，只能增。
                  总的来说就是对值变化后还原无感知，导致的更新错误
                  解决，加入不能回退的非业务字段version。


### 分布式锁相关

* 普通锁和分布式锁区别
      如果在集群中，普通锁是相对线程来说的，不同的服务器之间内存不同，自然锁失效。
      普通锁是针对单机多线程中方法调用冲突的问题，可以在单独一块内存中进行解决。可以通过lock和synchronized进行解决
      ReentrantLock 的 lock 和 unlock 要求必须是在同一线程进行，
      而分布式应用中，lock 和 unlock 是两次不相关的请求，因此肯定不是同一线程，因此分布式锁中，无法使用 ReentrantLock。
            
      分布式锁是针对分布式系统中多系统多进程间方法调用冲突的问题，不能在单独的一块内存中进行解决。
      分布式中只能使用分布式锁。将所有锁资源统一到比如redis中

         
* 分布式锁的使用场景：
      若不同系统或是同一个系统的不同主机之间共享了一个或一组资源，访问该资源时，需要互斥来防止彼此干扰来保证一致性，需要使用到分布式锁。
      Java提供的原生锁机制在多机部署场景下失效了
      因为两台机器加的锁不是同一个锁(两个锁在不同的JVM里面)。需保证两台机器加的锁是同一个锁。 Lock或synchronize只能解决单个jvm线程安全问题
      场景1	即同一请求多次操作一个资源 ，也肯能执行定时任务时就会遇到同一任务可能执行多次的情况，
      场景2	不同请求操作统一资源
	
* 分布式锁常用方案：
   整体性能对比：缓存 > Zookeeper > 数据库
   1. 基于Redis的分布式锁(推荐)
         * 使用了Redis的setnx()和expire()操作实现
               * redis的锁实现分布式的简单操作
                     1. setnx(lockkey, 1) 如果返回 0，则说明占位失败；如果返回 1，则说明占位成功
                     2. expire()命令对 lockkey 设置超时时间，为的是避免死锁问题。（这里宕机会出现死锁问题）
                     3. 执行完业务代码后，可以通过 delete 命令删除 key。

               * 使用 redis 的 setnx()、get()和 getset()的改良版(忽略)
                     getSet()，先返回原key的值，然后再设置新值
                     setnx()存在相同key值时，不做任何操作，且没法直接设置Key的失效时间
                     String value = jedis.getSet(key, "李四");

                     1. setnx(lockkey, 当前时间+过期超时时间) ，返回1，取锁成功；返回0，未取到锁，转向 2。
                     2. get(lockkey)获取值 oldExpireTime ，与当前系统时间比较，若小于当前系统时间，则该锁超时，可允许别的请求重新获取，转向 3。
                     3. 计算 newExpireTime=当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值 currentExpireTime。
                     4. 判断 currentExpireTime 与 oldExpireTime 是否相等，若相等，说明当前getset 设置成功，获取到了锁。如果不相等，则这个锁又被别的请求获取走了，当前请求可以直接返回失败，或继续重试。
                     5. 当前线程拿到锁后业务处理，处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处

                     步骤2和4确保是一次原子的时间设置


         * 使用redission实现(忽略)
            Redis官方提供的Java组件
            redisson获取锁，不成功则订阅释放锁的消息，获得消息前阻塞订阅此消息。得到释放通知后再去循环获取锁。后续删除key后会发送消息
            
   2. 基于数据库资源表的分布式锁(不常用)
            效率低，不推荐，线程可能出现死锁
            原理：建表，通过唯一索引保持排他性，加锁时插入一条记录，解锁是删除这条记录。
                  重点字段：唯一的防重id，竞争到锁的持有者id（可实现可重入，异常锁超时存放，也可由该持有者删除）
      
   3. 基于ZooKeeper的分布式锁(不常用)
            使用临时节点，效率高，失效时间可以控制

            Zookeeper实现原理(忽略)
               zk节点唯一的！ 不能重复！节点类型为临时节点
               jvm1创建成功时候，jvm2和jvm3创建节点时候会报错，该节点已经存在。这时候 jvm2和jvm3进行等待。
               jvm1的程序现在执行完毕，执行释放锁。关闭当前会话。临时节点不复存在了并且事件通知Watcher，jvm2和jvm3继续创建。
               设置有效时间，超过时间就删除节点
   
      Redis 实现为去插入一条占位数据，而 ZK 实现为去注册一个临时节点。
      遇到宕机情况时，Redis 需要等到过期时间到了后自动释放锁，而 ZK 因为是临时节点，在宕机时候已经是删除了节点去释放锁	

* 满足的条件(忽略)
         系统是一个分布式系统（关键是分布式，单机的可以使用ReentrantLock或者synchronized代码块来实现）
         共享资源（各个系统访问同一个资源，资源的载体可能是传统关系型数据库或者NoSQL）
         同步访问（即有很多个进程同事访问同一个共享资源）

* 其他   
   使用private static  Lock lock = new ReentrantLock(true);/  系统是放在@service中，因为容器中的实例能保证是单例的，这样并发下lock只有一个实例

   stringRedisTemplate的相关用法
         //将值 value 关联到 key ，并将 key 的过期时间设为 timeout
         public void setEx(String key, String value, long timeout, TimeUnit unit) {
            if(StringUtil.isEmpty(key) || StringUtil.isEmpty(value)) {
               return ;
            }
            stringRedisTemplate.opsForValue().set(key, value, timeout, unit);
         }

         //只有在 key 不存在时设置 key 的值,并设置过期时间
         public boolean setnx(String key, String value, long timeout, TimeUnit unit) {
            boolean setRes = stringRedisTemplate.opsForValue().setIfAbsent(key, value);      //这里类似hasKey的检查+set的效果
            if(setRes) {
               setRes = stringRedisTemplate.expire(key, timeout, unit);
               return setRes;
            }else {
               return false;
            }
         }


### 接口的幂等性

幂等的意味着对同一 URL 的多个请求应该返回同样的结果。

在分布式的接口重试调用考虑接口的幂等性的问题：

* 1、缓存解决方案：非业务性
  	   在要求幂等性协议的接口上，调用方传递一个全局唯一id，server方通过aop拦截参数id，数据缓存中查找，存在返回，不存在就进行业务的处理，处理成功，存入缓存，有失效时间
* 2、数据库解决方案：
  		业务性的去重，比如业务状态判断，建立联合唯一索引
    	数据库的乐观锁

## Mysql相关，数据库相关

1. 读操作可以分成两类：
      * 快照读(snapshot read)
            读取的是记录的可见版本 (有可能是历史版本)，不用加锁。
            简单的select操作，属于快照读，不加锁

      * 当前读(current read)
            读取的是最新版本记录，并且，当前读返回的记录，都会加上锁(基本是X排它锁)，保证其他事务不能并发修改这条记录。
            特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。
            select ... for update;
            insert、update、delete操作

2. 隔离级别

      * READ UNCOMMITTED：脏读
        ​	可以读取未提交的数据，未提交的数据称为脏数据。此时：幻读，不可重复读和脏读均允许；

      * READ COMMITTED：不可重复读
        ​	只能读取已经提交的数据；
        ​	同一个事务中多次执行同一个select, 读取到的数据发生了改变(被其它事务update并且提交)；
        ​	此时：允许幻读和不可重复读，但不允许脏读，这里的RC要求解决脏读；

      * REPEATABLE READ：默认级别
        ​	同一个事务中多次执行同一个select,读取到的数据没有发生改变((一般使用MVCC实现))；
        ​	此时：允许幻读，但不允许不可重复读和脏读，这里的RR要求解决不可重复读；

      * SERIALIZABLE: 幻读，
        ​	同一事务中多次执行同一select, 读取到的数据行发生改变。即行数减少或者增加了(被其它事务delete/insert并且提交)。
        ​	不可重复读和脏读都不允许，所以serializable要求解决幻读

        **不可重复读 和 幻读区别：**

        不可重复读的重点是修改:
        	同样的条件的select, 你读取过的数据, 再次读取出来发现值不一样了

        幻读的重点在于新增或者删除:
        	同样的条件的select, 第1次和第2次读出来的记录数不一样


      RR级别下防止幻读
         快照读：使用MVCC防止幻读
         当前读：使用间隙锁防止幻读


3. 数据库中的锁
		根据锁定对象的不同分为  表级锁 和 行级锁
            InnoDB 行级锁是基于索引实现的，如果查询语句未命中任何索引，那么 InnoDB 会使用表级锁
            InnoDB 行级锁是针对索引加的锁，不针对数据记录，因此即使访问不同行的记录，如果使用到了相同的索引键仍然会现锁冲突
            
            行级锁是一种排他锁，防止其他事务修改此行；在使用以下语句时，Oracle 会自动应用行级锁：
            1.INSERT、UPDATE、DELETE、SELECT … FOR UPDATE [OF columns] [WAIT n | NOWAIT];
            2.SELECT … FOR UPDATE 语句允许用户一次锁定多条记录进行更新
            3.使用COMMIT 或 ROLLBACK 语句释放锁。
			
		从并发事务锁定的关系上看：

			排他锁(写锁、独占锁)
            事务对一个数据加上排他锁后，其他事务不能再对此数据加任何其他类型的锁。
            能读能写，innoDB的增删改会默认加排他锁。典型的是select for update加排他锁

         共享锁(读锁)（忽略）
            可以查看但无法修改和删除数据。获取共享锁的事务，只能读不能改
            是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改(获取数据上的排他锁)，直到已释放所有共享锁。
            如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。
            用法	SELECT ... LOCK IN SHARE MODE;

		数据库行锁住之后，是可以查询的 ，更新不行
         ```
            -- connectionA
               SET autocommit=0;  
               SELECT *FROM require_info WHERE id=520 FOR UPDATE;		//已经加了写锁，排斥其他的加写锁行为
               COMMIT;
            
            -- connectionB
               SELECT * FROM require_info WHERE id=520 						可以  
               UPDATE require_info SET req_title='234' WHERE id=520		阻塞
         ``` 

      GAP锁使用场景(忽略)
           保证两次当前读返回一致的记录，那就需要在第一次当前读与第二次当前读之间，其他事务不会插入新满足条件的记录并提交。
      ​     gap锁本身的作用是防止后续的插入操作，因此gap锁只跟插入相冲突，gap锁之间不冲突，就会发生你这提到的情况。
      ​     gap锁范围，简单来说，就是锁当前记录和上一条记录之间的空白区域。


4. where条件的锁信息,相对于联合索引而言的(忽略)   
      联合索引原则上，应首先定义最唯一的列
      SQL的where条件，均可归纳为3大类：Index Key (First Key & Last Key)，Index Filter，Table Filter。

      ```
         dx_t1_bcd索引上有[b,c,d]  三个索引
         select * from t1 where b >= 2 and b < 8 and c > 1 and d != 4 and e != ‘a’;
      ```

   * Index Key
      ​	确定SQL查询在索引中的连续范围(起始范围+结束范围)的查询条件，需要加上GAP锁；

      ​	一般分为Index First Key和Index Last Key

         * 提取规则：从索引的第一个键值开始，检查其在where条件中是否存在，若存在并且条件是=、>=，则将对应的条件加入Index First Key之中，继续读取索引的下一个键值，使用同样的提取规则；若存在并且条件是>，则将对应的条件加入Index First Key中，同时终止Index First Key的提取；若不存在，同样终止Index First Key的提取。

         * Index First Key正好相反，用于确定索引查询的终止范围。提取规则：从索引的第一个键值开始，检查其在where条件中是否存在，若存在并且条件是=、<=，则将对应条件加入到Index Last Key中，继续提取索引的下一个键值，使用同样的提取规则；若存在并且条件是 < ，则将条件加入到Index Last Key中，同时终止提取；若不存在，同样终止Index Last Key的提取。

      ​	针对上面的SQL，应用这个提取规则，提取出来的Index First Key为(b >= 2, c > 1)。由于c的条件为 >，提取结束，不包括d。

      ​	针对上面的SQL，sql提取出来的Index Last Key为(b < 8)，由于是 < 符号，因此提取b之后结束。

   * Index Filter
      ​	过滤条件，视MySQL版本是否支持ICP，（5.6版本之前）若支持ICP，则不满足Index Filter的记录，不加X锁，否则需要X锁；

      ​	I提取规则：同样从索引列的第一列开始，检查其在where条件中是否存在：若存在并且where条件仅为 =，则跳过第一列继续检查	索引下一列，下一索引列采取与索引第一列同样的提取规则；
      ​	若where条件为 >=、>、<、<= 其中的几种，则跳过索引第一列，将其余where条件中索引相关列全部加入到Index Filter之中；
      ​	若索引第一列的where条件包含 =、>=、>、<、<= 之外的条件，则将此条件以及其余where条件中索引相关列全部加入到Index Filter中；若第一列不包含查询条件，则将所有索引相关条件均加入到Index Filter之中。

   * Table Filter
   ​	   过滤条件，无论是否满足，都需要加X锁。
   ​     提取规则：所有不属于索引列的查询条件，均归为Table Filter之中。
        针对上面的用例SQL，Table Filter就为 e != ‘a’

5. 死锁
      死锁的发生与否，并不在于事务中有多少条SQL语句，死锁的关键在于：两个(或以上)的Session加锁的顺序不一致,存在以相反的顺序加锁。

      场景，常见的就是分次执行1 2和2 1
      死锁的特点:形成等待环

   案例
		代码1
			a.lock();
			b.lock();

		代码2
			b.lock();
			a.lock();

		死锁原因在于用不同的顺序请求了相同的锁(交叉获取),若能保证请求锁的顺序一致，就不会发生死锁(统一先a后b)。
		数据库能再发生死锁的时候，选择一个事务牺牲，释放资源让另一个事务继续下去

   避免死锁的几个常见方法。 
      加锁顺序(线程按一定顺序加锁)
      加锁限时(取锁超时放弃，并释放自己占有的锁，lock.trylock方法实现)
      1.避免一个线程同时获取多个锁。 
      2.避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 
      3.尝试使用定时锁，使用lock.tryLock（timeout）来替代使用内部锁机制。 
      4.对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。

      1.控制访问锁的顺序
      2.trylock，通过timeout获取锁，时间过长就失败
      3.线程转储thread dump分析


6. 索引相关

   1. 普通索引和唯一索引：都可以提高查询效率，后者,值唯一，防重，唯一索引列允许空值。

   2. MySQL InnoDB 默认行级锁，更新操作默认会加行级锁，行级锁是基于索引的，对于没有用索引的操作是不会使用行级锁的，会用表级锁。
      如果操作用到了主键索引会先在主键索引上加锁，然后在其他索引上加锁，否则加锁顺序相反。
      行级锁并不是直接锁记录，而是锁索引，如果一条SQL语句用到了主键索引，mysql会锁住主键索引；如果一条语句操作了非主键索引，mysql会先锁住非主键索引，再锁定主键索引。

   3. innodb对于主键使用了聚簇索引，这是一种数据存储方式，表数据是和主键一起存储，主键索引的叶结点存储行数据。对于普通索引，其叶子节点存储的是主键值。
    
   4. 如果当前id为唯一索引，name为主键索引，在进行索引查找时需要两个步骤，
      第一步，查询到索引id对应的主键，
      第二步，根据主键查询到数据库信息，
      那么这个加锁过程就需要在第一步锁定的数据和第二步锁定的数据分别加上排它锁
    
   5. 复合索引的最左前缀原则
   
   6. 索引失效的场景
      1.当查询的列不是独立的，而是表达式或者函数一部分时，将无法使用该列的索引。
      2.当查询的列需要进行模糊匹配时，索引失效，全表扫描
         模糊匹配，因为%表示全匹配，如果已经全匹配就不需要索引，还不如直接全表扫描。a%不确定
      3.IN可以使用索引， NOT IN 无法使用索引

      where语句中索引独立出现，索引才会起作用，不要放在表达式中(如: 转换函数一般放在值那边，不要放在列那边)，或发生不合适的隐式转换

   7. mysql中的btree索引(忽略)
         其数据文件本身就是索引文件,相比MyISAM ，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data
         保存了完整的数据记录 
         这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引 这被称为聚簇索引(也叫聚集索引), 
         其余的索引为辅助索引 ，辅助索引 data 域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方
         在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引 
         因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂

   8. B+ 树
			B+ 树非叶子节点上是不存储数据的，仅存储键值，不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，
			B+ 树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的。
        
			 b+树结构： 索引管理块--管理块-索引块（叶子块）


7. mysql索引结构，sql优化，limit优化，回表，索引覆盖
		参看链接
		https://blog.csdn.net/mu_wind/article/details/110128016
		https://blog.csdn.net/sinat_14913533/article/details/115537106  
		https://www.cnblogs.com/myseries/p/11265849.html 

		叶子节点(深度为0的节点)才会有data，其他都是索引(附带指针指向了下个节点)

	* 聚集索引 和 普通索引 的区别

         聚集(clustered)索引，也叫聚簇索引，聚集索引clustered index(id), 非聚集索引index(username)

         简单点将就是主键索引 和 非主键索引

			本质区别：表记录的物理排序和与索引的排序是否一致
         类似字典中的拼音就是聚集索引，偏旁部首查汉字，就是非聚集索引。
			
			1.聚集索引：索引顺序和表中数据排序一致(聚集索引的顺序就是数据的物理存储顺序) ,一个表只能有一个聚族索引(因为数据在物理存放时只能有一种排列方式)。 
				
            聚集索引的叶子节点就是对应的数据节点，叶子节点中保存存了索引列和具体的数据
				InnoDB中,把数据data存放在B+树中的叶子节点上(存储了表中行数据)，而B+树的键值就是主键(索引和数据行保存在同一个B-Tree中)。命中叶子节点可直接取出数据，相比非聚簇索引需要第二次查询
				
				InnoDB默认对主键建立聚簇索引
					如果你不指定主键，InnoDB会用一个具有唯一且非空值的索引来代替。
					如果不存在这样的索引，InnoDB会定义一个隐藏的主键，然后对其建立聚簇索引。
				
            
			2.非聚集索引(辅助索引，普通索引，二级索引)： 索引顺序与物理存储顺序不同
				
            除聚集索引外都是非聚集索引，细分：普通索引，唯一索引，全文索引
				
            B+Tree的叶子节点仍然是索引节点，其上的data，不是数据本身，而是数据存放的地址（不是指向行的物理指针，而是行的主键值）。b+中非叶子节点只有键值，叶子节点包含完整数据。
				
            回表二次查询
					查找时，存储引擎需要在二级索引中找到相应的叶子节点，获得行的主键值，然后使用主键去聚簇索引中查找数据行，这需要两次B-Tree查找，检索两次索引

               回表查询	先从普通索引定位主键值，再从聚集索引定位行数据，一般是查询列不在索引覆盖范围情况下		

               查找数据分两个过程，1.通过索引查找到相应的标识符，然后通过这个标识符提取相应的数据(回表)，索引最少要做两次操作。
               但是如果查询中所有返回字段全部在索引中，那就不需要回表了，操作都在索引上完成
                     
				查询过程	扫描两次索引 
					要查询name = C 的数据，其搜索过程如下：
					1.辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。
					2.使用主键在主索引B+树种再检索一次，最终到达叶子节点定位到行数据
					
	* 覆盖索引
			定义：如果一个索引覆盖所有需要查询的字段的值。即索引的叶子节点中已经包含要查询的数据。
			只需要在一棵索引树上就能获取SQL所需的所有列数据，无需回表。
			
         从非主聚簇索引中就能查到的想要数据，而不需要通过回表从主键索引中查询其他列
			尽量不要使用select *
			实现方法是：将被查询的字段，建立到联合索引里去。使用联合索引  一定要使用索引并且带上查询列
			使用场景：Limit分页查询
			
			Using where' 在非索引的列上可能仍然会做全表扫描
				EXPLAIN
				SELECT req_code FROM require_info WHERE req_title='测试鱼食——先发后托6'
				这种使用了where 后面没有索引，但是查询列是单个索引列，这种不走索引覆盖，走的全表

				EXPLAIN
				SELECT user_id FROM require_info 
				这种不加where的直接查询，user_id会走索引覆盖
				索引覆盖，必须要使用索引

				EXPLAIN
				SELECT req_code,req_title FROM require_info
				这种没有使用索引，不走索引覆盖
			
				
		升级联合索引(name, sex)，联合索引也是一棵B+树，不同的是联合索引的键值数量不是1，而是大于等于2.相当于创建了多个索引
			select id,name … where name=‘shenjian’;
			select id,name,sex … where name=‘shenjian’;
			都能够命中索引覆盖，无需回表。


		索引覆盖来优化SQL？
			全表count查询优化
			列查询回表优化				单列索引(name)升级为联合索引(name, sex)，即可避免回表
			分页查询					  将单列索引(name)升级为联合索引(name, sex)，也可以避免回表

	* 大数据量的limit优化思路
			1.尽可能的减少筛选出来的数据，不要扫描大量无用行。想过办法通过索引覆盖查询来查出必要的需要扫描的行，然后再去扫描实际的数据行
			2.避免回表(常发生在查询的列不在索引覆盖范围内)
			3.除了使用覆盖索引，优化查询速度外，我们还可以使用 Redis 缓存，将热点数据进行缓存储存。
			
			方法：1.采取的是限制分页(只查看前几千条数据)和增加缓存(记录上次查询的最大id，带到下次查询中筛选)。
				   2.子查询的分页方式或者JOIN分页方式,用到的是索引覆盖，这两种逻辑一样，功能也一样
					想过办法通过索引覆盖查询来查出必要的需要扫描的行，然后再去扫描实际的数据行
					select orderNo,ctime from `order` order by orderNo limit 2000000,20
					select orderNo,ctime from `order` inner join ( select orderNo from `order` order by orderNo limit 2000000, 20 ) as o using(orderNo);    //using等价于join操作中的on
					分析：inner join的表通过索引覆盖查询直接通过索引找到了需要返回的数据行，然后order表通过与这个派生表进行关联，只扫描20行数据就可以了(第一种跨越大量数据块并取出)
					子查询是在索引上完成的，而普通的查询时在数据文件上完成的
					*****对limit的优化，不是直接使用limit，而是首先获取到offset的id，然后直接使用limit size来获取数据。*****
						
					SELECT * FROM tableName ORDER BY id LIMIT 500000 , 2
					
					-- 基于排序做条件过滤，有一定限制，主键id必须有序，同样适用创建时间等其他字段，但是必须要设置索引
					SELECT * FROM tableName
					WHERE id >= (SELECT id FROM tableName ORDER BY id LIMIT 500000 , 1)
					LIMIT 2;
					时间: 0.274s
					
					SELECT * FROM tableName AS t1
					JOIN (SELECT id FROM tableName ORDER BY id desc LIMIT 500000, 1) AS t2
					WHERE t1.id >= t2.id ORDER BY t1.id  LIMIT 2;
					时间: 0.278s
				
					-- 基于索引覆盖，这种更为高效
					SELECT * FROM tableName AS t1
					JOIN (SELECT id FROM tableName ORDER BY id desc LIMIT 500000, 2) AS t2
					WHERE t1.id = t2.id;


8. 数据库建表的经验：
	1. 主键id设置为bigint类型，int类型最大值才20亿左右
	
   2. 整型数后面的m不是表示的数据长度，而是表示数据在显示时显示的最小长度。
		zerofill(长度不足就补零，补0数量依据m大小)
      总结：int(11)，tinyint(4)，bigint(20)，后面的数字，不代表占用空间容量。
			而代表最小显示位数。这个东西基本没有意义，除非你对字段指定zerofill。
			
	3. 数据库字段在设计非空时候，如果有默认值，代码不插会默认。否则会报错
	   可以为空随便，有默认就默认，没默认就是null

   4. dao的设计
			dao基类的设计：使用通用baseDao<T>设计基类，将常用方法封装，后续子dao继承。

   5. 4.0版本以下，varchar(20)，指的是20字节，如果存放UTF8汉字时，只能存6个（每个汉字3字节） 
      5.0版本以上，varchar(20)，指的是20字符，无论存放的是数字、字母还是UTF8汉字（每个汉字3字节），都可以存放20个，最大大小是65532字节

      字符类型若为gbk，每个字符最多占2个字节
		字符类型若为utf8，每个字符最多占3个字节

      char和varchar
         char是一种固定长度的类型，char(M)类型的数据列里，每个值都占用M个字节
         varchar则是一种可变长度的类型,varchar(M)类型的数据列里，每个值只占用刚好够用的字节再加上一个用来记录其长度的字节（即总长度为L+1字节）
      
      一般情况下货币是decimal(8,2),不严格的用double也够用
      mysql 中double  字段长度 20,2

   6. 设置联合主键，自增的主键不能放在最左边

   7. 库名、表名、字段名：小写，下划线风格，不超过32个字符，禁止拼音英文混用
		禁止使用TEXT、BLOB类型解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能
		必须使用varchar(20)存储手机号解读,不要用外键，影响性能
		建立组合索引，必须把区分度高的字段放在前面解读：能够更加有效的过滤数据

9. 杂记录
      
   * 系统相关(忽略)   
      ```
         #查看数据库中表的主键
         SELECT * FROM INFORMATION_SCHEMA.Columns c WHERE   c.table_schema='db_xiaoyuer' AND c.column_key='PRI'

         #查询自增的返回
         SELECT table_name,AUTO_INCREMENT FROM information_schema.tables WHERE table_schema="db_xiaoyuer" 
      ```

   * 函数相关
      ```
         #REPLACE替换函数，sql字符串的截取
         SELECT SUBSTRING('订单：XQDD-520141119091355验收成功',4)  截取第4个后的所有
         SELECT SUBSTRING_INDEX(SUBSTRING('订单：XQDD-520141119091355验收成功',4),'验',1)  截取第1个 '验' 之前的所有字符。
      ```

   * 重要的sql记录
      1. update select，
            #数据库的update innerjoin select  更新使用select出来的数据
               UPDATE require_info rii 
                  INNER JOIN 
                  (	
                     SELECT ri.id,lrp.date_insert 
                     FROM require_info ri 
                     LEFT JOIN log_require_pay lrp ON lrp.require_info_id=ri.id
                     WHERE ri.status=4 AND ri.date_update IS NULL 
                  )c  	
               ON rii.id=c.id SET rii.date_update=c.date_insert
               inner join 取得是A和B的交集
      
      2. 以id分组，把name字段的值打印在一行(连在一起展示)，逗号分隔
            ```
               select id,group_concat(name) from aa group by id; 

               GROUP_CONCAT  分组拼接
                  SELECT  
                  payer_account_no,user_name,GROUP_CONCAT(DISTINCT pc_name)
                  FROM pp_pay_order 
                  WHERE STATUS=1 
                  AND pay_time BETWEEN '2021-07-01' AND '2021-08-01' 
                  GROUP BY payer_account_no 

                  结果是payer_account_no分组，pc_name逗号拼接起来，类似：通联支付,支付宝,预付交易


                  查询某字段大于2的记录统计
                  having 用来过滤分组(和group by一起使用)，where是过滤行
                  select ... from ... where ... group by ... having ... order by ... limit

                  select address,count(id) from student where 1=1 group by address having count(*)>1

                  HAVING类似于WHERE（唯一的差别是WHERE过滤行，HAVING过滤组），是跟着group by 使用的，以组为查询的维度
                  GROUP BY 合并 取结果集的第一条显示

            ```

   * order by  多列排序， 只有在前一个相同的情况下，才会触发后一个排序
      在union中，只出现在最后一条select之后，并且是针对总结果集排序
      union性能比or要好得多，同样适用case when.并集查询，优化使用union
      

   * between  不仅可以用在时间，也可以用在数字，两端是闭区间
      concat(),拼接，多个参数，逗号分割即可
      between '2019-09-18' AND '2019-09-19' 相当于 '2019-09-18 00:00:00' AND '2019-09-19 00:00:00'


   * Timestamp和Datetime区别：
      只能表示从1970年到2038年的时间，而datetime不受此限制。
      结论：永远使用datetime来表示时间，决不使用timestamp，除非每次更新都要更新的时间，才考虑使用timestamp


   * 其他
      
      * next key锁(lock_mode X)。(简单来说，next key锁有两层含义，一是对当前记录加X锁，防止记录被并发修改，同时锁住记录之前的GAP，防止有新的记录插入到此记录之前。
         锁模式为X lock with no gap

      * 若id列上没有索引，SQL会走聚簇索引的全扫描进行过滤，由于过滤是由MySQL Server层面进行的。因此每条记录，无论是否满足条件，都会被加上X锁。
         但是MySQL做了优化，对于不满足条件的记录，会在判断后放锁，最终持有的，是满足条件的记录上的锁，但是不满足条件的记录上的加锁/放锁动作不会省略。

      * MVCC(多版本并发控制), 通过版本号来减少锁的争用
        事务隔离的实现基于锁机制和并发调度 其中并发调度使用的 MVCC （多版本并发控制），通过保存修改行的旧版本信息来支持并发一致性读和回滚等特性



10. 常见数据库架构

    https://www.cnblogs.com/littlecharacter/p/9084291.html#_lab2_2_0     数据库的架构

    * 常见方案
    ​	   1. 方案一：主备架构，只有主库提供读写服务，备库冗余作故障转移用
    ​	   2. 方案二：双主架构，两个主库同时提供服务，负载均衡
    ​	   3. 方案三：主从架构，一主多从，读写分离
    ​	   4. 方案四：双主+主从架构，看似完美的方案

    * mysql主从，数据库主从
		mysql中的主从复制是基于Binary log实现的
      主从备份的关键是数据库的操作日志：二进制日志 Binarylog，

      * 冷备和热备
         热备份是实时备份，发生倒换也不影响业务；一般是基于binlog实现复制
         冷备份则是周期性备份（如：定时每天凌晨开始备份），发生倒换时，备机的数据不是最新的。
            冷备份指在数据库关闭后,进行备份,适用于所有模式的数据库的定期复制

      * mysql的主从同步的几种方式
         1. 异步复制(默认)
               这种架构数据是异步的，所以有丢失数据库的风险。
               主库将事件写入binlog，提交事务，自身并不知道从库是否接收Binlog处理，不会管备库的同步进度。如果备库落后，主库不幸crash，那么就会导致数据丢失。
               
         2. 全同步复制
               保证数据安全，不丢失数据，损失性能。
               当主库提交事务之后，所有的从库节点必须收到，APPLY并且提交这些事务，然后主库线程才能继续做后续操作。

         3. 半同步复制   MySQL5.7后，已经演变为增强半同步复制(也成为无损复制)。
               性能，功能都介于异步和全同步之间，从5.5开始诞生，目的是为了折中上述两种架构的性能以及优缺点。
               数据零丢失，性能好，mysql5.7诞生。
               这里只是一个收到的反馈，而不是已经完全执行并且提交的反馈，节省时间，总的来说是上面两种的中间状态

      * 主从的作用

         1. 读写分离：适用于读多写少的应用，增加多个从机，提高读的速度，提高程序并发
         2. (主要作用)数据容灾恢复（主从备份）：从机复制主机的数据，相当于数据备份，如果主机数据丢失，那么可以通过从机存储的数据进行恢复。
         3. 高并发、高可用集群实现的基础：在高并发的场景下，就算主机挂了，从机可以进行主从切换，从机自动成为主机对外提供服务。

         主从架构本身是一种数据容错的高可用方案，不是一种处理高并发压力的解决方案，适当的延时是可以接受的
         主从定义：从的存在主要是避免主宕机导致数据丢失。而不是让备机来分担并发压力，所以，主业务建议尽量在主上操作

			主从节点，多个节点同一数据多个备份    针对请求主服务器binlog操作，写入从服务器relay日志中，执行操作，异步复制


      * 主从复制的延迟

         数据同步(从库从主库拉取binlog日志，在从库中就是relay log，再执行一遍)需要时间，这个同步时间内主库和从库的数据会存在不一致的情况。若同步过程中有读请求，那么读到的就是从库中的老数据

         任何写入主服务器的数据，需要在一秒后才能在从服务器中读到。正产复制延迟不会超过500ms
         降低复制延迟的方法：1.写的缓存在客户端侧，2.重要读转发到主服务器上，3.减少延迟	

         核心方案
            主库能处理业务就全放在主库吧（特别是更新后的数据），从库只做灾备，备份，对实时性要求不高的统计报表类工作；
            如果你需要主从的业务时，你可以在中间层加个分布式缓存如redis，缓存组装数据，优先从缓存中读取刚刚写入的数据，
            更改mysql的同步方式，不推荐
               如果是强同步的，随便你怎么读备库都不会有问题，因为在任何情况下数据跟主库都是一样的。
               如果是半同步或者异步，那么在某些场景下会时不时有延迟。如果不能接受就把强一致性读的请求发主库，把弱一致性读的请求发备库。
               可以开启MySQL的 Semi-sync（半同步），从库读取数据的时候就不会有延时，当然这个会影响写入的速度。
         
         mysql5.6版本下 从库是单线程复制 主库是多线程复制， 当遇到执行慢的sql时，就是阻塞后面的同步，该问题可以通过升级5.7，并开启多线程复制来解决,几乎完美解决单线程复制引起的从库延迟
         从库太多，主库binlog同步到多个从库上，io瓶颈 解决方式：减少从库数量，增加io


      
      实际系统主从数据库(忽略)：
         修改的254从库，但是生产是从253主库读取的，
         所以页面读取不到（253主-从，但是254的数据库只是备份用的）254修改后没反应，
         生产读取的是253的库，254只是备份用的库，只能算是展示功能



11. mysql分表相关，分库相关，分库分表相关

      使用场景：当我们使用读写分离、缓存后，数据库的压力还是很大的时候，这就需要使用到数据库拆分。
      分表解决的是单表数据量过大带来的查询效率的下降问题

      分库分表中间件  cobar 和mycat

      * 分表通常分为：垂直拆分、水平拆分；
            垂直拆分
               拆分列，类似业务拆分，更常用是业务分库，将不同业务的数据部署在不同的物理服务器上（推荐使用）
               业务关联用join
               通常是根据业务场景将一个多字段大表拆分成多个少字段小表。带来的问题有，1.分布式事务，2.查询表关联问题
            水平拆分
               拆分行，类似数据拆分，分布式数据库只有在单表数据规模非常庞大的时候才使用(一般少用)
               查询所有数据需要union操作， 也可以使用统一的uuid管理多个分表
               是根据某个分片策略将同一个表的数据分开存到多个相同结构表中。

      *  数据分片  
            数据分片,就是水平切分。多个机器之间通过路由查询。

            缺点， 无法联合查询，必须在各分片上查询，然后应用层合并起来
                  单个分片容易实现acid，但是多个分片从整体很难实现acid，可能某个分片会失败
                  
            核心，是分片键(根据算法分，比如取模和奇偶)，用来定位分片，访问特定的服务器
            
            
            唯一id 可以使用redis的incr命令
            这里的分片不单单指数据库，也可在应用层实现，扩展到缓存，队列等，如redis分片

      * 拆分引入问题
         1.跨节点Join 的问题，需字段路由，常见的跨节点合并排序分页问题
         2.引入分布式事务的问题(水平分支持大数据量存储，应用端改造少)
         3.数据扩容问题
         4.垂直分区会主键冗余，并会引起 Join 操作，可以通过在应用层进行 Join 来解决

         分库后带来的一些join问题
            合并查询等复杂操作，如排序啥，建议都在应用层，将数据中信组装处理。其中排序分页比较复杂，难点在将足够多的数据返回给应用层。一般来说每个数据源都查询需要的条数，然后再筛选。
            1.多次查询，在应用层将原来的单次操作，分成多次数据库操作然后组装(合并查询)。
            2.数据冗余，表的数据冗余，但会涉及到更新的问题，要注意。
            3.使用外部系统，借助外部系统，比如搜索引擎。	

      * 拆分原则
         避免"过度设计"和"过早优化"。分库分表之前，不要为分而分
         1. 尽量不拆分，架构是进化而来，不是一蹴而就。(SOA)
         2. 最大可能的找到最合适的切分维度。
         3. 由于数据库中间件对数据Join 实现的优劣难以把握，而且实现高性能难度极大，业务读取 尽量少使用多表Join -尽量通过数据冗余，分组避免数据垮库多表join。
         4. 尽量避免分布式事务。
         5. 单表拆分到数据1000万以内。
            
	
      * 分库分表的扩容
            分库分表的扩容是避不开的
			   拆表的数量一般是2的n次方,比如根据user_id划分256张表，那么就是user_id%256  这0-255共256张表

            重点，对于第一取模分片之后，后再添加服务器怎么处理数据的迁移。
            *******
               方法1.将所有的映射关系保存在一个独立的数据库中。好处是不用分片计算
               方法2.还有一张方法是 预算最大的服务器数量,比如是32  先搞两台物理机，2*16个db,之后扩容每次增加一倍，这样通过扩容逐渐减小服务器压力
            *******

      * 数据库的路由规则
         目前最简单的映射分片方法是取模,取模是要基于服务器数量来排序的，添加服务器后就会很麻烦。再分片问题
         切分方范围、枚举、时间、取模、哈希、指定等
               
         分表策略，比如citicorder过大   	比如 水平分表的时候，可以使用取模分表。user_id%8  这样就会分成8个表  

         注意分表后的表名问题
            固定hash规则
               分库分表中的常用方法就是取模,分x张表就mod数值x
            映射表法
               将id和对应的表和库建立一个关系表，每次查询关系表，确定数据库和表。这种方法，一般用来以上面的规则为基础，作为配合使用。
            自定义规则，
               自定义函数，确定数据库的访问规则

      缓存一致性(忽略)
         选择主备或者写操作时根据库+表+业务特征生成一个key放到Cache里并设置超时时间（大于等于主从数据同步时间），


      如果同时进行分库和分(忽略)
         这种数据量就很大了，分库还能缓解单库的写操作压力
         一种分库分表的路由策略
            1.中间变量=user_id%(库数量*每个库的表数量)
            2.库=取整(中间变量/每个库的表数量)
            3.表=中间变量%每个库的表数量

13. truncate 、delete 、drop的区别

    * truncate 表和索引所占用的空间会恢复到初始大小(自增初始化),只删除数据

    * delete 不会减少表或索引所占用的空间,只删除数据

    * drop		将表所占用的空间全释放掉,

    truncate 和delete 只删除数据，但是drop删除整个表（结构和数据）

    delete语句为DML（data maintain Language),操作会被放到 rollback segment中,事务提交后才生效。
    truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚

14. 数据库事务测试，自动提交相关,测试用(忽略)

      ```
         show variables like "Innodb_lock_wait_timeout"  查看当前会话锁等待超时时间限制，默认为50S
         set Innodb_lock_wait_timeout = 5; 设置当前会话锁等待超时时间为5S
         show variables like "autocommit"  查看当前会话是否自动提交事务；
         set autocommit = 0   0为非自动提交事务，对应 OFF? 1为自动提交事务，对应为 ON
         commit 提交当前会话的事务
      ```

      start transaction
      ...
      commit;  //rollback
      
      事务回滚，create 和 drop语句无法回退
      SET autocommit = 0; 可以设置是否自动提交

15. 忘记本地数据库密码（忽略）
         1、cmd mysql的bin目录下
            mysqld --skip-grant-tables
         2、重开cmd ，bin目录下mysql，进入
         修改mysql的root密码后，出现Host 'localhost' is not allowed to connect to this MySQL server 错误。

    ​		解决办法：
    ​		C:\Program Files\MySQL\MySQL Server 5.5\my.ini
    ​		在[mysqld]下加下面两行，
    ​		skip-name-resolve
    ​		skip-grant-tables
    ​		重启mysql的windows服务
    ​		
    ​		修改本地数据库的密码，关键的地方是需要关闭进程中的mysql和停止服务中的mysql。

         mysql忘记密码(忽略)
            ```
               GRANT ALL PRIVILEGES ON *.* TO kaifa@"%" IDENTIFIED BY '123123'; 后面的数据库是要加密后存进mysql的

               跳过权限登录linux的mysql
               # mysqld_safe --user=mysql --skip-grant-tables --skip-networking &
               # mysql -u root mysql

               mysql> UPDATE user SET Password=PASSWORD('newpassword') where USER='root';
               mysql> FLUSH PRIVILEGES;
               mysql> quit
            ```

    查看表作为其他表的外键(忽略)
         SELECT
         ke.referenced_table_name parent,
         ke.table_name child,
         ke.REFERENCED_COLUMN_NAME parent_column,
         ke.column_name child_column,
         ke.constraint_name
         FROM
         information_schema.KEY_COLUMN_USAGE ke
         WHERE
         ke.referenced_table_name IS NOT NULL
         AND ke.referenced_table_name = 'Base_Skill_Appli;'
         AND ke.REFERENCED_COLUMN_NAME = 'id'
         AND ke.REFERENCED_TABLE_SCHEMA = 'db_xiaoyuer'
         ORDER BY
         ke.referenced_table_name;

         在5.5中，information_schema 库中增加了三个关于锁的表（MEMORY引擎）；
         innodb_trx ## 当前运行的所有事务
         innodb_locks ## 当前出现的锁
         innodb_lock_waits ## 锁等待的对应关系
      
         SELECT * FROM information_schema.innodb_trx
         SELECT * FROM information_schema.innodb_locks
         SELECT * FROM information_schema.innodb_lock_waits
               
         SHOW ENGINE INNODB STATUS			查看mysql事务处理列表，可查询死锁和事务相关的信息，
         SHOW FULL PROCESSLIST				查看所有mysql进程id



16. configfilter数据库加密(忽略)

      ```
         java -cp druid-1.0.16.jar com.alibaba.druid.filter.config.ConfigTools you_password   1.0.16版本加密默认带有公钥，私钥。1.0.13版本默认不带公钥。
         通过设置<property name="filters" value="config" />
         <property name="connectionProperties" value="config.decrypt=true;config.decrypt.key=$[xye_jdbc_publicKey]" />来解密。
         加密的带公钥的需要传入公钥，未公钥加密的不需要传入公钥。
      ```

      optimize 操作（忽略）
         删除数据时，mysql并不会回收已删除的数据所占据的存储空间，以及索引位。
         而是空在那里，而是等待新的数据来弥补这个空缺，这样就有一个缺少，如果一时半会，没有数据来填补这个空缺，那这样就太浪费资源了。
         所以对于写比较频繁的表，要定期进行optimize，一个月一次，看实际情况而定了。
         注意:在optimize table '表名'运行过程中，MySQL会锁定表。
         
         MySQL执行命令delete语句时，如果包括where条件，并不会真正的把数据从表中删除，而是将数据转换成了碎片，通过下面的命令可以查看表中的碎片数量和索引等信息：
         由于命令optimize会进行锁表操作，所以进行优化时要避开表数据操作时间，避免影响正常业务的进行。
         
         当对MySQL进行大量的增删改操作的时候，很容易产生一些碎片，这些碎片占据着空间，所以可能会出现删除很多数据后，数据文件大小变化不大的现象。当然新插入的数据仍然会利用这些碎片。但过多的碎片，对数据的插入操作是有一定影响的，此时，我们可以通过optimize来对表的优化。可以减少空间与提高I/O性能



17. 最左匹配原则(只针对联合索引)
		联合索引中，最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，该列就可以用作索引
		索引文件以B－Tree格式保存 重点是联合索引的最左边字段(没有就匹配失败，不走该索引)
		index(a,b,c)		相当于创建了多个索引：key(a)、key(a,b)、key(a,b,c)，只能从左到右顺序连贯组合
		where a=3 and c=4    仅使用了a
			
		最左匹配原则都是针对联合索引来说的，in 和 = 都可以乱序，MySQL优化器会将其优化成索引可以匹配的形式
		
		范围查询(>、<、between、like)就会停止匹配。联合索引中使用范围查询(>、<、between、like)的字段后的索引在该条 SQL 中会停止匹配，都不会起作用。
      最左前缀匹配原则，mysql 会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配
      比如
			某表现有索引(a,b,c),现在你有如下语句
			select * from t where a=1 and b>1 and c =1;     #这样a,b可以用到（a,b,c），c不可以 

18. mysql中的重要的日志文件：
      日志方面，主要就是慢查询日志 和 binlog(记录更新数据的所有sql语句)，慢查询在优化中很有用

      * redo log：保证数据可靠的入磁盘
         作用：确保事务的持久性。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。
         生命周期：事务开始之后就产生redo log，在事务执行过程中，就写入redo log文件中，
               当对应事务的脏页写入到磁盘之后，redo log的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。
               使某个事务还没有提交，Innodb存储引擎仍然每秒会将重做日志缓存刷新到重做日志文件。(随着事务的开始，逐步开始的)
   　　			这一点是必须要知道的，因为这可以很好地解释再大的事务的提交（commit）的时间也是很短暂的。

      * undo log:	用于回滚
         作用：保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读
         生命周期：事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性
               当事务提交之后，undo log并不能立马被删除，
   　　			  而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。

      * relay log：主从复制使用
         Mysql 主节点将binlog写入本地，从节点定时请求增量binlog，主节点将binlog同步到从节点。
         从节点单独进程会将binlog 拷贝至本地 relaylog中。从节点定时重放relay log。

      
      * binlog：记录的就是sql语句
         作用：
         　　1，用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。
         　　2，用于数据库的基于时间点的还原。
         生命周期：事务提交的时候，一次性将事务中的sql语句按照一定的格式记录到binlog中。
      
         binlog是用来做point-in-point的恢复和主从复制的，由数据库上层生成，是sql执行的逻辑日志，事务提交完成后进行一次写入。
         mysql内部的xa事务提交 ，以 binlog 的写入与否作为事务提交成功与否的标志
         MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。

         开启mysql的binlog功能，配置文件中开启，
            //配置文件 mysqld.cnf  和 my.cnf，这里5.7版本配置的前者
            //开启binlog功能
            docker exec mysql bash -c "echo 'log-bin=/var/lib/mysql/mysql-bin' >> /etc/mysql/mysql.conf.d/mysqld.cnf"		
            
            //mysql5.7及以上版本可能要加，随机指定一个不能和其他集群中机器重名的字符串，如果只有一台机器，那就可以随便指定了
            docker exec mysql bash -c "echo 'server-id=123454' >> /etc/mysql/mysql.conf.d/mysqld.cnf"						

            *****
               查看binlog日志，转储到指定文件
               // --base64-output=decode-rows -v    转换显示，如果不加，显示是base64编码的内容
               /usr/bin/mysqlbinlog --no-defaults --database=db_linux  --base64-output=decode-rows -v  --start-datetime='2021-11-02 00:00:00' --stop-datetime='2021-11-02 17:00:00'  /var/lib/mysql/mysql-bin.000001 > /usr/local/mysql/ceshi.tx
            *****	

20. 单纯的select * from a,b 是笛卡尔乘积

	   如果对两个表进行关联:select * from a,b where a.id = b.id 意思就变了，等价于：select * from a inner join b on a.id = b.id。即就是内连接。
      
      理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒：效果一样
      where a,b  和 where b，a是一样的

      梯度漏斗   
         select *from t where a = 1 and b = 2 and c = 3; 
         就等于在满足 a = 1 的结果集中过滤掉b = 2 的 再从 a = 1 and b = 2 结果集中过滤掉 c = 3 的，得到最终的结果集越多查询越高效

      关联查询
         子查询		将子查询语句包装成外查询的条件	将select * from a where id>(select id from a where id=1)
         自联结		自己关联自己的表结构				  实际使用场景是省市区   自联结比子查询快

         子查询和join 目前看可以转换
         内连接：(INNER) JOIN，返回两张表都匹配上的行
         join = inner join = from a, b where 过滤条件

         join 后面的 on 标准操作是两表的关联条件，其他无关的条件放where中即可


21. mysql优化
      
		将打算加索引的列设置为 NOT NULL ，否则将导致引擎放弃使用索引而进行全表扫描，且null的列是无法参与到查询中的
		应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描

		几个禁止
			禁止使用SELECT *   会增加无意义的字段消耗，不能有效的利用覆盖索引
			禁止使用属性隐式转换解读：SELECT uid FROM t_user WHERE phone=13800000000 会导致全表扫描，而不能命中phone索引，猜猜为什么？（这个线上问题不止出现过一次）
			禁止在WHERE条件的属性上使用函数或者表达式解读，无法命中索引，如 to_date(create_time) ＞xxxxx

			禁止负向查询，以及%开头的模糊查询解读：
				a）负向查询条件：where子句中，NOT、!=、<>、!<、!>、NOT IN、NOT LIKE等，会导致不走索引，走全表扫描，
				b）%开头的模糊查询，会导致全表扫描	
					like 模糊查询中，右模糊查询（321%）会使用索引，而%321 和%321%会放弃索引而使用全局扫描。
				
			禁止使用OR条件，必须改为IN查询解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费更多的CPU帮助实施查询优化呢
				where 字句有 or 出现还是会遍历全表。

      几种常见的mysql未命中索引的情况
			1.条件中有or，即使有条件带索引也不会使用，除非or条件中的每个列都加上索引
			2.like查询以%开头，索引不会命中。但是以%结尾，索引可以使用。
			3.使用正确的字段类型，强制转换会无法命中索引。(若列类型是字符串，使用引号引用起来，否则不使用索引。)
			4.采用 not in, not exist
			5.链表时候，两个字段的编码不同


      启慢查询日志来找出较慢的SQL
      联表关联时，确保ON和USING中的列上有索引。有一个表的列有就行
      确保任何的GROUP 和 ORDER BY 中的表达式只涉及一个表中的列，这样mysql才可能使用索引来优化


22. CASE WHEN 和 OR 和 union all的转换
		sql优化
			SELECT * FROM mytable t
			WHERE (CASE aPARAM WHEN 'a' THEN t.a = aID WHEN 'b' THEN t.b = aID WHEN 'c' THEN t.c = aID ELSE TRUE END)
			等价于
			SELECT * FROM mytable t
			WHERE ( t.a = aID AND aPARAM = 'a') OR (t.b = aID AND aPARAM = 'b') OR (t.c = aID AND aPARAM = 'c') OR (aPARAM NOT IN ('a','b','c'))

			这里CASE WHEN 可以和OR  或者union all 相互转换，当case when影响到索引使用的使用，可以使用union all 来替换，兼容索引使用
     
22. 使用explain解释，关注三个参数
		type		显示哪种类别，有无使用索引。如果为ALL,说明进行全表扫描
		key		显示mysql实际决定使用的键(索引)。如果没有命中索引，键是NULL
		EXtra		特殊的备注信息
		
      type=const  表示查询结果最多匹配一行(只有在使用了主键和唯一索引情况下，进行常数值比较时查询的type才为const)
      type=ref，表示查找条件列使用了索引而且不为主键和unique。其实，意思就是虽然使用了索引，但该索引列的值并不唯一，有重复。

      mysql中的extra
			using idex						   使用了覆盖索引				避免访问了表的数据行，效率不错
			using where 					   只表示发生了where过滤		过滤条件字段无索引；如果type是all 就是扫全表， 常见的优化方法为，在where过滤属性上添加索引
			using where using index			发生了where过滤，索引覆盖了查询
			extra为null， type为ref  		表明虽然用到了索引，但是没有索引覆盖，产生了回表，常见于，where后面使用索引，但是查询的列回表了
				有点类似Using index condition

      这里可以看explain中的key_len长度， 个位数小差别是mysql的平衡量
         select * from table_name where a = '1' and c = '3' 
         结果显示是，使用到了(a,b,c)索引，但是key_len是一个字段的长度，说明，只有a列走了索引，b和c没走，走了几个字段的索引，看key_len大概。

23. COUNT(*)  COUNT(1)  COUNT(id)比较
      结论是优先使用count(*)
      1、执行速度上：针对一般情况（SQL语句中没有where条件）执行速度上
      count(*)=count(1)>count(主键)>count(其他列)，在没有特殊查询要求推荐使用count(*)来代替其他的count。
      2、执行结果上，count(*)与count(1)以及count(主键)的结果完全相同，即返回表中的所有行数，包含null值；count(其他列)会排除掉该列值为null的记录，返回的值小于或者等于总行数。
      
      count(*)  		对表中行计数，所有行，不管表中列是不是null
      count(colum)	对特定列中有值的计数，忽略null

      对于需要计算和分组的列，建议使用默认值，不要null

24. 格式化
      时间 	DATE_FORMAT(si.date_insert,'%Y-%m-%d %T') AS releaseTime
      数字 	CAST( 120.6 AS DECIMAL(15,2))		
            FORMAT(13625.265,2)	13,625.27     

      case when 语法
         CASE WHEN ro.status = 6 THEN
         CASE ro.status WHEN 6 THEN

      mysql多表联合更新
         UPDATE  t1,t2	set  ... 	WHERE ...

      mysql有很多字符串函数 find_in_set(str1,str2)函数是返回str2中str1所在的位置索引，str2必须以","分割开。

      mysql---同服务器下跨数据库更新（忽略）
            update 
               A数据库.表名,B数据库.表名 
            set 
               B数据库.表名.字段名 = A数据库.表名.字段名 
            where 
            条件（A数据库.表名.id = B数据库.表名.join_shop_id）;

25. mysql-connector  mysql时区
		serverTimezone=UTC   在mysql-connector高版本8.x中会有时区的问题，方案要么配置serverTimezone=Asia/Shanghai,要么换用低版本的mysql-connector
		com.mysql.jdbc.Driver和mysql-connector-java 5一起用。
		com.mysql.cj.jdbc.Driver和mysql-connector-java 8 一起用。	需要配置市区serverTimezone=GMT%2B8
		TIMESTAMPDIFF(MINUTE,'2020-08-27 10:38:00','2020-08-27 11:38:00')  			计算两个时间差

      java.sql.Date		这个是数据库中的类型date，指精确到年月日
      java.util.Date		这个是数据库中的类型dateTime，精确到时分秒，一般开发用这个

26. DISTINCT 和 group by的区别
		DISTINCT 简单用来去重列值(去重)，而group by更多是用来统计(聚合统计)，以某个维度来统计数据
		DISTINCT 用在所有的检测列之前，并且它是作用于所有列，不能部分使用
		
		单纯的去重操作使用distinct(可单列或者多列，多列相当于联合唯一)，速度是快于group by的。
		group by 必须在查询结果中包含一个聚集函数，而distinct不用。 常用来统计数据
		聚合函数 ：AVG、MAX、MIN、SUM、COUNT


27. 批量插入，批量更新,批量操作
      少量多条insert,  insert into table(colum) values (),();
      批量多条insert select		insert into table(colum) select...

      ```
         * 批量插入
            方法1
               <insert id="batchInsert" parameterType="java.util.List">
                  insert into log_user_trade(trade_code,asset_from_code) values
                  <foreach collection="logTrades" item="item" separator=",">
                  (
                     #{item.tradeCode},
                     #{item.assetFromCode}
                  )
                  </foreach>
               </insert>

            方法2，这里dual是mysql中的虚拟表
               <insert id="batchInsert" parameterType="java.util.List">
                  insert into require_withdraw (withdraw_id,date_insert)
                  <foreach collection="list" item="item" index="index" separator="union all">
                     select
                        #{item.withdrawId,jdbcType=INTEGER},
                        #{item.dateInsert,jdbcType=TIMESTAMP}
                     from dual
                  </foreach>
               </insert>
            
         * 批量更新
            <update id="batchUpdateClose" parameterType="java.util.List">
               update require_apply
               <trim prefix="set" suffixOverrides=",">
                  <trim prefix="apply_status=case id" suffix="end,">
                     <foreach collection="applys" item="item">
                        when #{item.id} then #{item.applyStatus}
                     </foreach>
                  </trim>
                  <trim prefix="Status=case id"  suffix="end">
                     <foreach collection="infos" item="item">
                        when #{item.id} then #{item.status}
                     </foreach>
                  </trim>
               </trim>
               where id in 
               <foreach collection="applys" item="item" separator="," open="(" close=")">
                  #{item.id}
               </foreach>
            </update>
               
            上面简化后的sql：
               UPDATE require_apply
               SET apply_status= CASE id  
               WHEN 9289 THEN 2
               WHEN 9290 THEN 4
               END
               WHERE id IN (9289,9290)
      ```	


28. 数据库的迁移
      关键是迁移过程中可能出现的数据变化
      当然最稳就是先停机，再迁移，最后切换。
      实际优化方案	1.正常迁移，	2.记录增量 	3.暂停待迁移的数据写操作，	4.处理完增量，切换规则。

      这里好处是停止写操作的时间只是在处理增量的操作中。减少了停机时间。
      这里就是用日志，记录增量变动，针对变动停机修复。
      1.数据迁移时，记录增量日志，迁移技术后，对增量变化进行处理。
      2.最后可以把要被迁移的数据写暂停，保证增量日志都处理完毕后，再切换规则，放开所有的写，完成迁移工作。

## Nginx相关

      nginx配置文件 和ssl证书     安装OpenSSL  相关的命令生成key和crt文件	https://www.cnblogs.com/chasewade/p/7661290.html

      ssh 秘钥登录 	https://blog.csdn.net/cnclenovo/article/details/37591783

		nginx是按照最全的路径匹配的,如果没有匹配到，则优先按照从左往右，最全域名先匹配
		#nginx是监听的本地ip，所以要改host文件域名，搭配使用，默认http是80端口，https是443端口，浏览器默认不显示端口域名，不得使用已经注册的域名。

1. nginx反向代理原则是，优先匹配全路径，然后再依次匹配后面的路径

   ```
      #requestUrl：https://www.yu.com/ids/login?redirectUrl=http://www.yu.com/XiaoyuerProject/&method=GET
         location /ids {
            proxy_pass https://192.168.6.222:9446/ids;
         }
         location ids/login {
            proxy_pass https://192.168.6.222:9446/ids/login;
         }
         location /login {
            proxy_pass https://192.168.6.222:9446/ids/login;
         }    主站登录优化用
   ```

2. nginx.exe -t 检查配置文件是否正确
      命令行输入 /安装路径/sbin/nginx -t 查看nginx配置是否正确
      命令行输入 /安装路径/sbin/nginx -s reload 重新加载nginx

   windows下nginx用nginx.exe -s stop关


3. nginx配置多域名转发的时候：

      ```
         if ($http_host ~* "^(.*?)\.yu\.com$") {
               set $domain $1;
         }
         location / {
            if ($domain ~* "www") {
               proxy_pass https://192.168.6.222:9443;
            }
            if ($domain ~* "m") {
               proxy_pass https://192.168.6.222:9443;
            }
         }  

         location /ids {
            proxy_pass https://192.168.6.222:9446/ids;
         }
         location /ids/login {
            proxy_pass https://192.168.6.222:9446/ids/login;
         }

   ​      location /ids {
   ​         proxy_pass http://192.168.6.251:8150/ids;
         }
   ​		 
   ​	    #location /login {
   ​	       proxy_pass http://192.168.6.251:8150/ids/login;
         }
   ​	
   ​		#location /ids/login {
   ​	       #proxy_pass http://192.168.6.251:8150/ids/login;
         }

         每次的请求都会先经过系统的nginx转发，主站发起请求-主站nginx转发到251-经过251的nginx转发-到实际运行的服务器上
         nginx和hosts文件的相关:
         http://www.yu.com/xye-open/gateWay/systemGateWay.htm请求先经过本地的host文件解析到本地的请求，在经过nginx转发ip路径到相应服务器，但服务器的请求requesturl还是原地址www.yu.com的
         目前请求先进过host域名解析在经过ningx转发
      ```

4. 正向代理：类似挂vpn从服务端获取数据，	特点：多个客户端
   反向代理：使用负载均衡配多个服务端		特点：多个服务端

5. nginx配置以下三种方式：1.ip	2.端口	3.域名 	
	一个域名只能绑定一个ip，一个ip可以被多个域名绑定。通常ip地址是和dns服务器（根据域名换ip地址）绑定一起的，
	
6. 网址解析步骤
   当用户访问网站时候，第一个交互的组件是dns，将域名解析成ip

	www.baidu.com-->dns服务器---->取得对应的ip地址---->访问百度的服务器,默认的是80端口
	在host文件中，如果配置了ip对应的域名，就会跳过dns解析，进行访问
	
7. nginx请求步骤
	客户端均衡--->nginx--->tomcat  请求经过nginx转发，由tomcat处理，叫反向代理，
	多台tomcat就叫做负载均衡（仅对httpserver如tomcat有效）

8. nginx配置负载均衡：http节点下，添加upstream节点。
    ```
		upstream linuxidc { 
     		 server 10.0.6.108:7080; 			#一个server就是一个虚拟主机
      		 server 10.0.0.85:8980; 
		}
  		2.  将server节点下的location节点中的proxy_pass配置为：http:// + upstream名			称，即“http://linuxidc”.默认的分配策略是轮询
    ```
	高可用：解决高可用的方案就是：添加冗余（备用一主一备）一般用keepalived(集群管理中实现高可用，防止单点故障),不断检查主nginx是否正常，异常就启用备用nginx，主正常，在切换回去。
   一般大公司是可以输入ip地址访问的，但是一些小公司可能运行在同一ip的不同虚拟机上，还是要域名访问才能精确定位

9. nginx配置
	https://m.yu.com   和  https://192.168.6.222:8083  存的cookie是不一样的，
	https://m.yu.com/  nginx 转发后是8443 ，然后登录中的两个域名不一样    RedirectFilter 中 loginUrl
   
	域名转发ip相关
	nginx
	proxy_set_header Host $proxy_host;
      当Host设置为$http_host时，则不改变请求头的值，所以当要转发到bbb.example.com的时候，请求头还是aaa.example.com的Host信息；
      当Host设置为$proxy_host时，则会重新设置请求头为bbb.example.com的Host信息。
      
	   设置 http 请求 header 传给后端服务器节点，如：可实现让代理后端的服务器节点获取访问客户端的真实ip

   ngx_http_upstream_module
      负载均衡模块，可实现网站的负载均衡功能，upstream模块允许Nginx定义一组或多组节点服务器组，使用时可通过proxy_pass代理方式把网站的请求发送到事先定义好的对应Upstream 组 的名字上。
      proxy_pass 指令属于ngx_http_proxy_module模块，此模块可以将请求转发到另一台服务器，在实际的反向代理工作中，会通过 location 功能匹配指定的URI，然后把接收到服务匹配URI的请求通过proyx_pass抛给定义好的 upstream 节点池。
	
	普通的负载均衡软件，如 LVS，其实现的功能只是对请求数据包的转发、传递，从负载均衡下的节点服务器来看，接收到的请求还是来自访问负载均衡器的客户端的真实用户；
	反向代理不一样，反向代理服务器在接收访问用户请求后，会代理用户 重新发起请求代理下的节点服务器，最后把数据返回给客户端用户。在节点服务器看来，访问的节点服务器的客户端用户就是反向代理服务器，而非真实的网站访问用户
   
	nginx中存放静态资源，实现静态资源分离
      localhost:80/image/ 时会访问本机的/usr/local/myImage/image/ 
      location /image/ {
               root   /usr/local/myImage/;
               autoindex on;
            }
      server {
            listen 88;
            server_name localhost;
            root     /home/ubuntu/static/;
      }

      #localhost:80/image/1.jpg
      #也可以移除掉root 和 autoindex 配置，直接在html目录下的image目录下新建一张图片1.jpg。
      http://localhost:8087/image/zxnew.png  实际访问成功

      server {
            listen       8087;
            server_name  localhost;
            location / {
               root   html;											
               index  index.html index.htm;
            }
            location /image/ {
               root   C:/Users/xiaoyuer/Desktop/图片/;
               autoindex on;
            }
         }

   小鱼儿手机页面跨域测试配置
      这里，想要前后端两个项目不跨域，必须使用统一域名和端口，可操作空间就在于，后续的路径转发
      nginx配置不要重复配置，只能有一个生效
      就是说，前端的 http://域名:端口  一致，后面的路径交给 后端或nginx 自己去转发
      server {
        listen 80;
        server_name  m.yu.com;
         if ($http_host ~* "^(.*?)\.yu\.com$") {
            set $domain $1;
         }
		
         location / {
               root   C:/Users/mf/git/xye-phone-html/xye-phone-html/project;
               index  index.html index.htm;
               add_header 'Access-Control-Allow-Origin' *;
            }

         #location / {
            #if ($domain ~* "www") {
               #proxy_pass http://192.168.6.239:8080;
            #}
            #if ($domain ~* "m") {
               #proxy_pass http://192.168.6.239:8082;
            #}
            #if ($domain ~* "gy") {
               #proxy_pass http://192.168.6.239:8081;
            #}
         #}
		
         location /xye-phone {
            proxy_pass http://192.168.6.239:8082/xye-phone;
            proxy_set_header Host $host;
         }
         
         location /xye-phone-html {
            proxy_pass http://192.168.6.239:8082/xye-phone-html;
            proxy_set_header Host $host;	
         }
      }

      #server {
            #listen       80;
            #server_name  m.yu.com;
            #location / {
               #root   C:/Users/mf/git/xye-phone-html/xye-phone-html/project;
               #index  index.html index.htm;
            #add_header 'Access-Control-Allow-Origin' *;
            #}
      #}
	
      #HTTPS server
         server {
            listen       443 ssl;
            server_name    www.yu.com;
            ssl_certificate      D:/nginx-1.11.13/ssl/server.crt;
            ssl_certificate_key  D:/nginx-1.11.13/ssl/server.key;
         
            location = / {
                  root   C:/Users/mf/git/xye-phone-html/xye-phone-html/project;
                  index  index.html index.htm;
            }
         }

10. 域名解析ip(180.97.15.66)->接收66,防火墙打出到250->转到192.168.8.250(keepalived配置的虚拟ip)内部服务器,指定跳到31->31nginx-机器，转发到相应的服务器。

11. 支付都是使用的443端口(可以nginx中转到支付的80，但是原请求都是s端口)

12. nginx可以配置单ip的访问限制。通过域名解析，建立很多子域名，不同业务模块由域名分发到不同的nginx服务器上


## java反射相关	

1. setAccessible(true),没有改变访问权限为public，而是取消java的权限控制检查,accessible的属性默认是false
   
   判断某个B注解是否在A类上
   public boolean isAnnotationPresent(Class<? extends Annotation> annotationClass)

2. 对象复制
   * BeanUtils.copyProperties()
      对象的名称要匹配，不匹配不转，复杂属性类型需一致，否则argument type mismatch，
      常规属性类型的可以不一致，复杂类的属性复制，this.value=value全属性覆盖

   * PropertyUtils的工具类
      提供copyProperties()，并且还提供类型转换的功能，但两者都不支持date类型，这种的速度相对要慢

3. Class.forName().newInstance() 和 new()对象的区别

   * 使用newInstance可以解耦。前提，要先完成类加载并且这个类已连接，由Class.forName()完成。
      newInstance实际上是把new这个方式分解为两步，1.调用class的加载方法加载某个类，2.实例化

   * newInstance: 弱类型。低效率。只能调用无参构造。 new: 强类型。相对高效。能调用任何public构造。
     new: 强类型。相对高效。能调用任何public构造。

   * newInstance()是实现IOC、反射、面对接口编程和依赖倒置等技术方法的必然选择，new只能实现具体类的实例化，不适合于接口编程。 
   * newInstance() 一般用于动态加载类。


4. Field对于到一个成员对象, 这个和类定义是相关的.
   	如果Field拿到的是static, 则get(null)取得当前field的值.
   	如果Field是一个实例成员对象, 那么要传入一个对象实例, 拿到对象实例的实例成员的值. 案例代码如下：	

   	Field f=Counter.class.getField("count"); //拿到Counter类的count 实例域
   	Counter c=new Counter();                 //一个Counter对象实例
   	Long l=(Long) f.get(c);                  //拿到对象实例的 域成员的值

   	field.setAccessible(true); 忽略访问修饰符权限
   	field.get(obj),对象实例换取对象的属性值，拿到对象实例的 域成员的值

5. java.lang.ClassLoader：类装载器类，将类的字节码装载到 Java 虚拟机（JVM）中并为其定义类对象，然后该类才能被使用。
   Proxy类与普通类的唯一区别就是其字节码是由JVM在运行时动态生成的而非预存在于任何一个.class 文件中。

6. ​反射生成Java对象实例
      * 通过类对象调用newInstance()方法，适用于无参构造方法：（常用）
      * 通过类对象的getConstructor()或getDeclaredConstructor()方法获得构造器（Constructor）对象并调用其newInstance()方法创建对象，适用于无参和有参构造方法。
         例如：String.class.getConstructor(new Class[] { String.class, int.class }).newInstance("Hello",10);

      getDeclaredMethod*()     获取的是类自身声明的所有方法，包含public、protected和private方法。
      getMethod*()             获取的是类所有共有方法，包括自身的所有public方法，和从基类继承的、从接口实现的所有public方法。


7. java反射：动态获取类中的信息
        方法，Method[] method=clazz.getDeclaredMethods();
        属性，Field[] field=clazz.getDeclaredFields();
        构造函数，Constructor[] constructor=clazz.getDeclaredConstructors();
        Class 对象的 newInstance()方法来创建该 Class 对象对应类的实例，但是这种方法要求该 Class 对象对应的类有默认的空构造器


   获取Class对象有三种方式，分别对应于前面的三个阶段：
      1.对应于第一个阶段的方法是将字节码文件加载进内存中：
         class.forname("全类名");

      2.第二个阶段已经生成了class类对象，因此方法如下：
         类名.class;

      3.第三个阶段生成了对象，方法如下：
         对象.getClass();

      同一个字节码文件（.class）在一次程序运行过程中只会被加载一次，通过以上三种方法创建的class对象是同一个。

8. 反射本身的field可以直接设置，也可以通过setMethod.invoke()设值

		class.getMethods()//获取所有的共有方法
		class.getDeclareMethods()//只获取本类中的方法，包括私有
		method.invoke(obj,param)

      newInstance构造对象的时候，对象类必须要有一个无参构造
         
      Class.forName 和 ClassLoader加载类的区别
         Class.forName("com.test.mytest.ClassForName"); 后续可以再接着newinstance()创建一个实例       （常用）
         ClassLoader.getSystemClassLoader().loadClass("com.test.mytest.ClassForName");
         
         Class.forName加载类是将类进了初始化，而ClassLoader的loadClass并没有对类进行初始化，只是把类加载到了虚拟机中。
         ClassLoader通过一个类的全限定名来获取描述此类的二进制字节流”，获取到二进制流后放到JVM中。class文件本来就是二进制文件,是字节码文件

      可以动态的执行方法和操作属性
         反射本身的field可以直接设置，也可以通过setMethod.invoke()设值
         Method method=clazz.getdeclaredmethod("add",int.class,int.class);
         method.invike(this,1,1);
         
         Field field=clazz.getDeclaredField("name");
         field.set(this,"Test");
   
         如果属于类的静态属性，那么set和get方法的第一个参数就可直接设置为null；因为静态不需要实例，跟着类走的。


## Job和Quartz相关，job相关，quartz相关

   文档介绍	https://www.w3cschool.cn/quartz_doc/
	
### 基本概念
   1. Quartz的基本组成三要素：：Scheduler、Trigger、JobDetai&Job。
         调度器：Scheduler
            通过管理Trigger和JobDetail来调度、暂停和删除任务。

            Scheduler 的生命期(忽略)
               从 SchedulerFactory 创建它时开始，到 Scheduler 调用shutdown() 方法时结束；
               Scheduler 被创建后，可以增加、删除和列举 Job 和 Trigger，以及执行其它与调度相关的操作（如暂停 Trigger）。
               但是，Scheduler 只有在调用 start() 方法后，才会真正地触发 trigger（即执行 job）。

         任务：JobDetail & Job
             JobDetail：Quartz中需要执行的任务详情，包括了任务的唯一标识和具体要执行的任务，可以通过JobDataMap往任务中传递数据。
             Job：Quartz中具体的任务，包含了执行任务的具体方法。

         触发器：Trigger，
            通过CRON表达式指定任务执行时间，按时自动触发任务执行。
            SimpleTrigger,在具体的时间点执行一次或按指定时间间隔执行多次，
            CronTrigger,按Cron表达式的方式去执行更常用。	“0 0 12 ？* WED“ - 表示”每个星期三下午12:00“。

            cron表达式
               0 0/3 20,23 * * ?” 	每天 20点至20:59 和 23点至 23:59分两个时间段内 每3min一次触发
               cron中 	?  	不指定值，用于处理天和星期天配置的冲突
                     - 	指定时间区间
                     / 	指定时间间隔执行		
               "0 0 */3 * * ? *"      "* * 0/3 * * ? *" 注意* 是匹配任意。这里按时为界线,到了小时点，就按规则执行

   2. Quartz API的关键接口是：
         Scheduler - 与调度程序交互的主要API。
         Job - 你想要调度器执行的任务组件需要实现的接口
         JobDetail - 用于定义作业的实例。
         Trigger（即触发器） - 定义执行给定作业的计划的组件。
         JobBuilder - 用于定义/构建 JobDetail 实例，用于定义作业的实例。
         TriggerBuilder - 用于定义/构建触发器实例。

   4. Spring整合Quartz进行配置遵循下面的步骤：
         1：定义工作任务的Job，通常也是jobdetail        
         2：定义触发器Trigger，并将触发器与工作任务绑定(两种方式)        
         3：定义调度器，并将Trigger注册到Scheduler

   5. 触发的方式
         实现job接口或者继承QuartzJobBean，系统job自动执行。
         在项目中建一个类继承包中的QuartzJobBean抽象类，需重载executeInternal（JobExecutionContext）方法，本质上还是调用了job接口中的execute()方法，

         在设置jobdetail的时候，可以使用QuartzJobBean完成统一的触发入口，这里是各job触发时的调用入口
         也可每个detail都定义实现job统一接口，完成触发也行

   6. 使用Quartz做计划任务时，默认情况下，当前任务总会执行，无论前一个任务是否结束。  
      <property name="concurrent" value="false" /> 可以让Job顺序执行。 true可以让Job并行执行，而不用管上一个Job是否结束。
	   时间梯度，总的来说就是定30,60,100规则穿，记录次数，10job一次，然后每次判断下次的时间点到没到，漏发的再补发一次就可以了

      (忽略)context.getMergedJobDataMap().get(ScheduleConstants.TASK_PROPERTIES));
      可以使用这个属性将当前系统job绑定一个自定义的job，当系统执行的时候，然后通过绑定的job信息，执行对应的逻辑。


   7. job中的重要概念
         * job中的jobdetail 
            
            JobDetail用来绑定Job(多job可绑定同一个jobdetail)，为Job实例提供许多属性：name,group,jobClass,jobDataMap
            每次Scheduler调度执行一个Job的时候，首先会拿到对应的Job，然后创建该Job实例，再去执行Job中的execute()的内容，任务执行结束后，关联的Job对象实例会被释放，且会被JVM GC清除。
            每个任务JobDetail可以绑定多个Trigger，但一个Trigger只能绑定一个任务

            为什么设计成JobDetail + Job，不直接使用Job(忽略)
               可能存在场景：多个任务开多个job执行同一个jobdetail
               JobDetail定义的是任务数据，而真正的执行逻辑是在Job中。
               这是因为任务是有可能并发执行，如果Scheduler直接使用Job，就会存在对同一个Job实例并发访问的问题。
               而JobDetail + Job 方式，Sheduler每次执行，都会根据JobDetail创建一个新的Job实例，这样就可以规避并发访问的问题。

            job中添加参数传递(忽略)
               将参数数据放入JobDataMap，执行job，从JobDataMap中获取参数,实现参数传递
               JobDetail job = newJob(DumbJob.class)
                           .withIdentity("name_myJob", "group_group1")
                           .usingJobData("jobSays", "Hello World!")				#存数据
                           .usingJobData("myFloatValue", 3.141f)
                           .build();
               
               public class DumbJob implements Job {
                  @Override
                  public void execute(JobExecutionContext context) throws JobExecutionException{
                     JobKey key = context.getJobDetail().getKey();
                     JobDataMap dataMap = context.getJobDetail().getJobDataMap();	#取数据map，jobDetail.getJobDataMap() 增加参数
                     String jobSays = dataMap.getString("jobSays");
                     float myFloatValue = dataMap.getFloat("myFloatValue");
                     System.err.println("Instance " + key + " of DumbJob says: " + jobSays + ", and val is: " + myFloatValue);
                  }
               }

		
         * job中的triggers
            主要配置job实现的具体规则,用于触发 Job 的执行	
            job 被创建后，可以保存在Scheduler中，与 Trigger 是独立的，同一个Job可以有多个Trigger，但一个Trigger只能对应一个Job

            JobKey是表明Job身份的一个对象，里面封装了Job的name和group，TriggerKey同理。

            触发原理(忽略)
               job的一个 trigger 被触发后（稍后会讲到），execute() 方法会被 scheduler 的一个工作线程调用(中保存了该的job运行信息)；
               执行 job 的 scheduler 的引用，触发 job 的 trigger 的引用，JobDetail 对象引用，以及一些其它信息。
               
            JobDataMap传参(忽略)
               Trigger 也有一个相关联的 JobDataMap，用于给 Job 传递一些触发相关的参数（不常用）
               JobExecutionContext
                  JobExecutionContext中包含了Quartz运行时的环境以及Job本身的详细数据信息。
                  JobDetail、Trigger都可以使用JobDataMap来设置一些参数或信息，
                  当Schedule调度执行一个Job，执行execute()方法的时候，Job可以通过JobExecutionContext对象获取信息

                  JobExecutionContext是JobDetail中的JobDataMap和Trigger中的JobDataMap的并集，相同的数据后者优先。	
                  获取并集JobDataMap dataMap = context.getMergedJobDataMap(); 

		
         * job中的Scheduler
            Scheduler（调度器）：代表一个Quartz的独立运行容器。
               Trigger和JobDetail可以注册到Scheduler中。Scheduler可以将Trigger绑定到某一JobDetail上，这样当Trigger被触发时，对应的Job就会执行。

            用JobBuilder和TriggerBuilder 创建一个jobdetail和trigger
            ```
               @Configuration
               public class QuartzConfig {
                  #用JobBuilder创建jobDetail
                  @Bean
                  public JobDetail printTimeJobDetail(){
                     return JobBuilder.newJob(DateTimeJob.class)		//PrintTimeJob我们的业务类
                           .withIdentity(“jobName”, “jobGroupName”)			//JobDetail的id
                           .usingJobData("msg", "Hello Quartz")	//可以不配置，JobDataMap传参，可以不串
                           .storeDurably()							//可以不配置，即使没有Trigger关联时，也不需要删除该JobDetail
                           .build();
                  }
                     
                  #可以用TriggerBuilder创建trigger
                  @Bean
                  public Trigger printTimeJobTrigger() {
                     CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule("0/1 * * * * ?");
                     return  (CronTrigger) TriggerBuilder.newTrigger()
                           .forJob(printTimeJobDetail())							//关联上述的JobDetail
                           .withIdentity(“triggerName”, “triggerGroupName”)		// 触发器名,触发器组名
                           .withSchedule(cronScheduleBuilder)
                           .build();
                  }
               }
         
               #scheduler代码加载jobDetail和trigger
               private void startJob1(Scheduler scheduler) throws SchedulerException {
                  //通过JobBuilder构建JobDetail实例
                  JobDetail jobDetail = JobBuilder.newJob(SchedulerQuartzJob1.class).withIdentity("job1", "group1").build();
                  
                  //TriggerBuilder 用于构建触发器实例
                  CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule("0/5 * * * * ?");
                  CronTrigger cronTrigger = TriggerBuilder.newTrigger().withIdentity("job1", "group1").withSchedule(cronScheduleBuilder).build();
                  
                  //配置调度器
                  scheduler.scheduleJob(jobDetail, cronTrigger);
               }

               ```
	
   8. QuartzJobBean原理(忽略)
         QuartzJobBean是实现类job接口，内部调用的时候，实际还是调用的是executeInternal()
            public abstract class QuartzJobBean implements Job {
            @Override
            public final void execute(JobExecutionContext context) throws JobExecutionException {
               try {
                  BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this);
                  MutablePropertyValues pvs = new MutablePropertyValues();
                  pvs.addPropertyValues(context.getScheduler().getContext());
                  pvs.addPropertyValues(context.getMergedJobDataMap());
                  bw.setPropertyValues(pvs, true);
               }
               catch (SchedulerException ex) {
                  throw new JobExecutionException(ex);
               }
               executeInternal(context);
            }
            protected abstract void executeInternal(JobExecutionContext context) throws JobExecutionException;
         }

   9. Spring+Quartz实现Jobdetail有两种方式，xml类型（现在不怎么用xml）	
         本质上，最终是要将Trigger(配置了jobdetail)注册到SchedulerFactoryBean中

         * jobdetail实现两种方式
            1.继承QuartzJobBean类或者实现job接口，
               适合持久化方案，因为只要公用一个job实现类，然后从数据库rest请求路径即可，否则多个job就有多个job实现类。每个job都要一个job实现类，比较乱。
               想要公用一个job实现类，那么必须有个参数配置来选择执行的job，而这里参数需要存起来，自然而然选择数据库方式

            2.MethodInvokingJobDetailFactoryBean指定具体的类和方法
               适合内存方案，内部多个job不需要管理，直接一次将job注入到调度器中即可。
            
         * 选择方式
            如果是要动态更新，可视化管理，那就选持久化方案，使用继承QuartzJobBean类或者实现job接口方式，公用一个job类，通过数据库动态job请求路径
            如果是简单内存后台使用，使用MethodInvokingJobDetailFactoryBean，指定bean和方法名即可，省的多个类都要实现job接口(而且只固定一个实现方法execute,没法将功能聚合起来)，配置繁琐，
      
         * Quartz配置
            在Quartz配置类中，主要配置两个东西：1. JobDetail 2. Trigger
            创建CronTrigger和JobDetail，二者为包含关系:CronTrigger包含JobDetail,

            JobDetail 有两种不同的配置方式：MethodInvokingJobDetailFactoryBean 和 JobDetailFactoryBean
            ​	1.JobDetail用MethodInvokingJobDetailFactoryBean工厂Bean包装普通的Java对象或bean；
            ​	2.JobDetail用JobDetailFactoryBean包装QuartzJobBean的继承子类（即Job类）的实例；

            1.在Spring配置文件中配置JobDetail
               * 一种是不继承任何类或者实现接口，创建普通的Java类，然后自己指定任务的执行方法
                     在xml中配置quartz的配置	JobDetail、Trigger、SchedulerFactoryBean
                     <bean id="firstService" class="com.xye.quartz.firstService"></bean>
                     <bean id="firstTask" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean">  
                        <property name="targetObject" ref="firstService" />  						#指定任务类
                        <property name="targetMethod" value="execute" />  							#指定任务执行的方法
                     </bean>
            
               * 另一种是继承org.springframework.scheduling.quartz.QuartzJobBean类来实现Job任务，并实现里面的抽象方法executeInternal
                     quartz2后废弃了JobDetailBean，使用JobDetailFactoryBean
                     #jobClass定义的任务类，继承QuartzJobBean，实现executeInternal方法；jobDataMap用来给job传递数据;
                     <bean id="firstTaskDetail" class="org.springframework.scheduling.quartz.JobDetailFactoryBean">
                        <property name="jobClass" value="com.xiaoyuer.pay.quartz.service.MyQuartzJobBean" />				#该类继承了QuartzJobBean实现executeInternal方法
                        <property name="jobDataMap">
                           <map><entry key="firstService" value-ref="firstService" /></map>								#可不传
                        </property>
                        <property name="durability" value="true" />															#当没有触发器与之关联时，仍然将job继续保存在Scheduler中
                     </bean>

            2.配置触发器（Trigger）	SimpleTrigger和CronTrigger
               <bean id="firstCronTrigger" class="org.springframework.scheduling.quartz.CronTriggerFactoryBean">
                  <property name="jobDetail" ref="firstTaskDetail" />
                  <property name="cronExpression" value="0/5 * * ? * *" />
               </bean>
                     
               备注：simpleTrigger(忽略)
               <bean id="exampleJobTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean">
                     <property name="jobDetail" ref="firstTaskDetail" />
                     <property name="startDelay" value="10000" />							#延迟触发时间，延迟10秒进行触发
                     <property name="repeatInterval" value="5000" />							#复触发的时间间隔，5秒 
               </bean>
               
               triggerBuilder.startNow(); 有可能是立刻执行
               当使用 Cron 表达式时，StartNow 方法不会起任何效果，Cron 有其自己的执行时间。目前看来 StartNow 应该只适用于 SimpleTrigger 触发器。
                     
            3.配置SchedulerFactoryBean
               <bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean">
                  <property name="triggers">
                     <list><ref bean="firstCronTrigger" /></list>
                  </property>
               </bean>			

   10. 操作quartz的api接口相关，持久化方案适用(忽略)
		   scheduler的几个常用方法，主要实现job的增删改查，暂停恢复
			https://zhoutianyu.blog.csdn.net/article/details/107785587
				创建任务			 scheduler.scheduleJob(jobDetail, trigger);
				删除任务
					//暂停并不调度触发器
					scheduler.pauseTrigger(triggerKey);
					scheduler.unscheduleJob(triggerKey);
					//删除任务
					scheduler.deleteJob(jobKey);
				修改一个任务
					1.暂停任务，2.删除任务，3.创建新任务
					scheduler.pauseTrigger(triggerKey);				//停止触发器
					scheduler.unscheduleJob(triggerKey);			//移除触发器
					scheduler.deleteJob(jobKey);					//删除任务
					scheduler.scheduleJob(jobDetail, trigger);		//创建任务
				暂停任务	
						pauseJob(jobKey)
				恢复任务
					resumeJob(jobKey)
					resumeTrigger(triggerKey)
				触发执行一个JobDetail
					triggerJob(jobKey) 。
					
				修改任务的触发时间
					scheduler.rescheduleJob(triggerKey, trigger);

		   每个任务JobDetail可以绑定多个Trigger，但一个Trigger只能绑定一个任务，这种绑定关系由四种接口来执行
            Scheduler#scheduleJob(JobDetail, Trigger)
               作用是在将任务加入Quartz的同时绑定一个Trigger，Quartz会在加入该任务后自动设置Trigger的JobName与JobGroup为该JobDetail的name与group；

            Scheduler#scheduleJob(Trigger)
               作用是将该Trigger绑定到其JobName与JobGroup指向的任务JobDetail。这时的name与group需要在Trigger绑定前由Quartz的使用者来设置与调用

            Scheduler#rescheduleJob(String, String, Trigger)
               替换一个指定的Trigger, 即解除指定Trigger与任务的绑定，并将新的Trigger与任务绑定，Quartz会自动调整新Trigger的JobName与JobGroup，而旧的Trigger将被移除

            Scheduler#triggerJob(String, String)
               创建一个立即触发的Trigger，并将其与name与group指定的任务绑定
               
         JobDetail有个属性叫durable，表明该任务没有任何trigger绑定时仍保存在Quartz的JobStore中，默认为false。若JobDetail的durable属性为false，则任务将会从Quartz移除。


   11. job分批次发送明细对账(忽略)
         ```
            @Override
            public QueryPage requirePaySynchronized(boolean manual) throws ParseException {
               //筛选需要校验的用户id,获取id和userCode
               SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
               Date beginDate=sdf.parse(begintTime);	
               Date loginDate=sdf.parse(loginTime);
               List<UserAsset> balanceUserIds = userAssetMapper.getBalanceUserIds(loginDate);
               QueryPage page=new QueryPage();
               if(null!=balanceUserIds&&balanceUserIds.size()>0){
                  int pointsDataLimit = 300;//限制条数
                  Integer size = balanceUserIds.size();
                  page.setTotal(size);
                  //判断是否有必要分批
                  if(pointsDataLimit<size){
                     int part = size/pointsDataLimit;//分批数
                     boolean remainFlag = size%pointsDataLimit ==0? true:false;//分批数,整除标记
                     logger.info("共有 ： "+size+"条，！"+" 分为 ："+(part+1)+"批");
                     for (int i = 0; i < part; i++) {
                        List<UserAsset> listPage = balanceUserIds.subList(0, pointsDataLimit);
                        //具体的操作
                        try {
                           //批次间相互隔离开
                           if(i==(part-1) && remainFlag){
                              page=dealUserAccountList(page,listPage,beginDate,"1");
                           }else{
                              page=dealUserAccountList(page,listPage,beginDate,"0");
                           }
                           if(remainFlag){
                              logger.info("第：{}/{}批，job统计用户金额校验 成功",i+1,part);
                           }else{
                              logger.info("第：{}/{}批，job统计用户金额校验 成功",i+1,part+1);
                           }
                        } catch (Exception e) {
                           logger.error("jobUserAccount_validate error");
                        }
                        listPage.clear();	//剔除
                     }
                     if(!balanceUserIds.isEmpty()){
                        page=dealUserAccountList(page,balanceUserIds,beginDate,"1");//表示最后剩下的数据,批量发送
                        logger.info("最后一批，job统计用户金额校验 成功");
                     }
                  }else{
                     page=dealUserAccountList(page,balanceUserIds,beginDate,"1");//不要分页，直接发
                     logger.info("数量小直接发送，job统计用户金额校验 成功");
                  }
               }
               logger.info("-----jobUserAccount_validate 结束-----");
               //尝试从缓存中获取支付端open处理结果
               try {
                  Object total=redisUtil.hGet(PayConstantsRedis.JOB_303, "total");
                  Object success=redisUtil.hGet(PayConstantsRedis.JOB_303, "success");
                  Object fail=redisUtil.hGet(PayConstantsRedis.JOB_303, "fail");//这个是支付端检验处的错误用户个数
                  Integer realFail=Integer.valueOf(fail.toString())+page.getFail();//加上请求支付端失败的个数
                  page.setTotal(Integer.valueOf(total.toString()));
                  page.setSuccess(Integer.valueOf(success.toString()));
                  page.setFail(realFail);
                  redisUtil.delete(PayConstantsRedis.JOB_303);
               } catch (Exception e) {
                  logger.warn("job-{}获取支付端返回信息失败，默认全部失败！");
               }
               return page;
            }
         ```

   12. springtask （忽略）
         @Scheduled 	
         对异常的处理：SpringTask不同，一旦某个任务在执行过程中抛出异常，则整个定时器生命周期就结束，以后永远不会再执行定时器任务。


### quartz的实现方式
   * Quartz 中两种可用的Job存储类型：
      * 1.内存存储(RAMJobStore),简单优先的解决方案
            存储方式：org.quartz.jobStore.class = org.quartz.simpl.RAMJobStore
            RAMJobStore 是quartz默认的配置方式，配置最简单的。不提供就使用默认的配置文件quartz.properties
            内存两种方式加载 Job 信息：1.硬编码到代码中 2.统一xml中加载
            缺点：新增一个任务，暂停一个任务。需要停应用！改XML配置！重新启动！不灵活

            内存RAMJobStore：把job的相关信息存储在内存里，如果用spring配置quartz的job信息的话，所有信息是配置在xml里，当spirng context启动的时候就把xml里的job信息装入内存。
            ```
               <bean name="startQuertz" lazy-init="false" autowire="no" class="org.springframework.scheduling.quartz.SchedulerFactoryBean">
                  <property name="triggers">
                     <list><ref bean="job01Trigger" /></list>
                  </property>
               </bean>
            ```

      * 2.持久化存储(jdbcjobstore)
            存储方式：org.quartz.jobStore.class = org.quartz.impl.jdbcjobstore.JobStoreTX
            需要搭配jdbc进行持久化，数据库要建好对应的表，需加载quartz.properties，内存方案不需要加载
            
            优点：可以持久化job信息，可查询数据库中的job数据信息展示，可动态添加修改job任务，无需重启
                  可单点可做集群，任务多时，可以统一进行管理（停止、暂停、修改任务）。

            数据库：读取配置在数据库里的job初始化信息，并把job通过java序列化到数据库里，实现每个job信息持久化，即使在jvm或者容器挂掉的情况下，也能通过数据库感知到其他job的状态和信息；
            quartz集群各节点之间是通过同一个数据库实例(准确的说是同一个数据库实例的同一套表)来感知彼此的。 
            #quartz job的内容https://www.cnblogs.com/xuxueli/p/4866449.html 官网有相应的数据库的sql
            
            ```
               <bean id="quartzScheduler" lazy-init="false" class="org.springframework.scheduling.quartz.SchedulerFactoryBean"> 
                  <property name="dataSource" ref="dataSource" />
                  <property name="autoStartup" value="true" />
                  <property name="applicationContextSchedulerContextKey"  value="applicationContextKey" />
                  <property name="configLocation" value="classpath:quartz.properties"/>
                  <property name="triggers">
                  <list><ref bean="job02Trigger" /></list>
                  </property>
               </bean>
            ```
      两种本质都是SchedulerFactoryBean配置trigger(绑定一个job)，只是配置trigger方式不一样，内存的多配点，数据库的少配点
       
      JobDetail有两种不同的配置方式：MethodInvokingJobDetailFactoryBean 和 JobDetailFactoryBean



   * 项目落地实现
      * 内存方案，
         * 批量注入，java代码配置
            
            jobdetail有两种不同的配置方式：MethodInvokingJobDetailFactoryBean 和 JobDetailFactoryBean
            //原理：注入了一个SchedulerFactoryBean的bean，然后内部配置了trigger和jobdetail相关
            @Configuration
            public class XyeTriggerSchedulerFactoryBean extends SchedulerFactoryBean{
               private ApplicationContext applicationContext;
               @Override
               public void setApplicationContext(ApplicationContext applicationContext) {
                  this.applicationContext = applicationContext;
               }

               @Override
               protected void registerJobsAndTriggers() throws SchedulerException {
                  XyeSchdulerMethod xyeSchdulerMethod = null;
                  //容器中拿到封装类manualJobService，然后反射获取方法上配置的注解@XyeSchdulerMethod，获取job的配置参数，注入job
                  Class<?> objClass = applicationContext.getType("manualJobService");
                  Method[] methods = objClass.getDeclaredMethods();
                  List<CronTrigger> triggersList = new ArrayList<CronTrigger>();
                  for(Method method : methods) {
                     xyeSchdulerMethod = AnnotationUtils.findAnnotation(method, XyeSchdulerMethod.class);
                     if (xyeSchdulerMethod != null) {
                        String cronExpression = xyeSchdulerMethod.cronExpression();
                        CronTrigger trigger = registerCronJobTrigger(method.getName(),xyeSchdulerMethod);
                        triggersList.add(trigger);
                     }
                  }
                  Trigger[] triggers = (Trigger[]) triggersList.toArray(new Trigger[triggersList.size()]);
                  
                  setTriggers(triggers);						#这里相当于前面的设置trigger，这里之前的代码都是配置trigger的过程。这里是关键点
                  super.registerJobsAndTriggers();			#这里正常执行父方法
               }
               
               //将jobdetail装载到trigger中，启动加载进入内存
               //这里实际上就是返回一个crontrigger，用builder应该也行
      
               //统一的service中管理各个job，通过自定义job注解添加配置信息，然后启动加载进入系统即可。老版的是集中在xml中，新版的是集中代码注入
               //LinKunJobConfig extends SchedulerFactoryBean  实现其中的registerJobsAndTriggers方法  最终setTriggers(triggers); super.registerJobsAndTriggers();
               //生成每个trigger，主要步骤，jobDetailFactory，cronTriggerFactory
               private CronTrigger registerCronJobTrigger(String methodName,XyeSchdulerMethod xyeSchdulerMethod) throws Exception {
                  String cronExpression = xyeSchdulerMethod.cronExpression();
                  int jobType = xyeSchdulerMethod.jobType();
                  String[] arguments = xyeSchdulerMethod.arguments();
                  
                  MethodInvokingJobDetailFactoryBean jobDetailFactory = new MethodInvokingJobDetailFactoryBean();
                  jobDetailFactory.setTargetMethod(methodName);
                  jobDetailFactory.setConcurrent(false);
                  jobDetailFactory.setArguments(arguments);
                  jobDetailFactory.setTargetObject(applicationContext.getBean("manualJobService"));
                  jobDetailFactory.setName(methodName + "_detail_" + xyeSchdulerMethod.jobType());
                  jobDetailFactory.afterPropertiesSet();
                  
                  CronTriggerFactoryBean cronTriggerFactory = new CronTriggerFactoryBean();
                  cronTriggerFactory.setJobDetail(jobDetailFactory.getObject());
                  cronTriggerFactory.setCronExpression(cronExpression);
                  cronTriggerFactory.setName(methodName + "_trigger_" + xyeSchdulerMethod.jobType());
                  cronTriggerFactory.afterPropertiesSet();
                  return cronTriggerFactory.getObject();
               }

         * xml配置，job不可控。
            ```
               #指定业务job
               <bean id="statDayJob" class="com.xiaoyuer.scheduledtimer.StatDayScheduledTimer"/>
               #配置触发规则 
               <bean id="statDayJobTrigger" class="org.springframework.scheduling.quartz.CronTriggerFactoryBean">
                  <property name="jobDetail" ref="statDayJobDetail"/>
                  <property name="cronExpression" value="0 20 06 * * ?"/>
               </bean>
               #组装job内容，指定job指定的具体行为
               <bean id="statDayJobDetail" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean">
                  <property name="targetObject" ref="statDayJob" />
                  <property name="targetMethod" value="excute" />
                  <property name="concurrent" value="false" />
               </bean>
               #加入系统运行
               <bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean">
                  <property name="triggers">
                     <list><ref bean="statDayJobTrigger"/></list>
                  </property>
                  <property name="autoStartup" value="true" />
               </bean>
            ```

      * 实际项目中的持久化
            xye-pay-quartz的系统配置，持久化的配置方案
               配置主要的bean之后，手工代码添加，可以控制job的触发，缺点是需要建表，管理job任务，job动作可控

               @PersistJobDataAfterExecution：保存在JobDataMap传递的参数,执行后保留作业数据
               @DisallowConcurrentExecution: 保证多个任务间不会同时执行.所以在多任务执行时最好加上 
               这里配置了一个jobDetail，所有的任务都需要一个trigger绑定这个jobDetail后才生效，也就是说，系统初始化没有job任务，需要手工添加	

               1.配置job的系统配置
                     <bean name="quartzScheduler" class="org.springframework.scheduling.quartz.SchedulerFactoryBean" >
                        <property name="dataSource" ref ="dataSource" />       
                        <property name="applicationContextSchedulerContextKey" value="applicationContextKey"/>
                        <property name="configLocation" value="classpath:quartz.properties"/>			
                     </bean>

               2.触发类是的类，触发数据库中配置的job，需要指定入口类
                  <bean id="jobDetail" class="org.springframework.scheduling.quartz.JobDetailFactoryBean" >
                     <property name="jobClass">
                        <value>com.xiaoyuer.pay.quartz.service.MyQuartzJobBean</value>      				#继承QuartzJobBean(该类实现了job接口)
                     </property>
                     <property name="durability" value="true" />	
                  </bean>
                  
               3.通过数据库的查询，可以将数据库中的job配置信息展示在页面上
               4.通过scheduler的实现类，操作job的增加，删，改操作。
                  核心还是scheduler设置jobDetail和trigger

                  CronTriggerImpl trigger = new CronTriggerImpl();		#CronTriggerImpl也实现了trigger接口，用triggerBuilder也可以吧
                  trigger.setCronExpression(cronExpression);
                  TriggerKey triggerKey = new TriggerKey(name, group);
                  trigger.setKey(triggerKey);								#设置triggerkey属性，自身属性
                  trigger.setJobName(jobDetail.getKey().getName());		#设置jobName和jobdetail关联上，
                  trigger.setDescription(description);					#设置描述
                  scheduler.addJob(jobDetail, true);						#先detail后trigger,分开设置是因为，一个job可以分批绑定多个trigger，只要jobName, jobGroup和jobdetail一致即可
                  if (scheduler.checkExists(triggerKey)){
                     scheduler.rescheduleJob(triggerKey, trigger);		#存在修改，不存在就新增。还有一种方式是先删除在新增
                  }else{ 
                     scheduler.scheduleJob(trigger);						   #scheduler一共需要设置两个参数jobDetail和trigger	
                  }
               
               关于name和group
                  JobDetail和Trigger都有name和group。
                  name是它们在这个sheduler里面的唯一标识。如果我们要更新一个JobDetail定义，只需要设置一个name相同的JobDetail实例即可。
                  group是一个组织单元，sheduler会提供一些对整组操作的API，比如 scheduler.resumeJobs()。	
                  
                  实际项目中jobdetail就只有一个com.xiaoyuer.pay.quartz.service.MyQuartzJobBean，jobname="jobDetail",job_group是default
                     实际的trigger中，triggername是执行rest路径，triggergroup是net-pay这样的组名
                     调用的时候，就是triggergroup拼接triggername后请求指定地址。

               Scheduler有多个方法操作job，scheduleJob，rescheduleJob，triggerJob，暂不细看，主要用在持久化方案的job控制
                  
               //目前一共三种方式创建trigger了
                  1.triggerBuilder				这个是常用的
                  2.CronTriggerImpl				pay-quartz中用
                  3.CronTriggerFactoryBean		job中用
      
            //xye-pay-quartz相关
               ```
                  //支付中是直接将路径放在库的trigger_name中的，意思是通过绑定参数，封装执行的方法
                  public class MyQuartzJobBean extends QuartzJobBean {
                     //启动job的入口
                     protected void executeInternal(JobExecutionContext jobexecutioncontext) throws JobExecutionException {
                        Trigger trigger = jobexecutioncontext.getTrigger();
                        String triggerName = trigger.getKey().getName();
                        String group = trigger.getKey().getGroup();
                        
                        SimpleService simpleService = getApplicationContext(jobexecutioncontext).getBean("simpleService", SimpleService.class);
                        
                        // 根据Trigger组别调用不同的业务逻辑方法
                        if (StringUtils.equals(group, Scheduler.DEFAULT_GROUP)) {
                              simpleService.defaultMethod(triggerName, group);
                        }else {
                              simpleService.judgeMethod(triggerName, group);//方法向远程调用方法，通过httpclient发送请求到远程
                        }
                     }
                     //获取上下文
                     //获取ApplicationContext
                     private ApplicationContext getApplicationContext(final JobExecutionContext jobexecutioncontext)  {
                        try {
                              return (ApplicationContext)jobexecutioncontext.getScheduler().getContext().get("applicationContextKey");
                        } catch (SchedulerException e) {
                              throw new RuntimeException(e);
                        }
                     }
                  }
               ```



### springboot整合quartz			
	https://blog.csdn.net/u012907049/article/details/73801122  	持久化方案+job界面

	持久化方案，直接属性文件配置完，然后可以直接注入scheduler，然后增删改查
	内存方案，直接配置完就自动触发即可		
	1、springboot集成quartz，应用启动过程中会自动调用schedule的start方法来启动调度器，也就相当于启动了quartz，原因是SchedulerFactoryBean实现了SmartLifecycle接口；
　 2、springboot会自动将我们应用的数据源配置给quartz

	整合demo
		1. 添加maven依赖
			<dependency>
			   <groupId>org.springframework.boot</groupId>
			   <artifactId>spring-boot-starter-quartz</artifactId>
			</dependency>
		
		2.同理启动类上添加 @EnableScheduling注解，开启定时任务
		
		3. 注册为SpringBoot组件，缺点是无法传递参数
			@Component
			public class MyFirstJob {
				public void  hello(){
					System.out.println("MyFirstJob:hello:"+new Date());
				}
			}
			//@description: 继承 QuartzJobBean并实现executeInternal的方法,可以传递参数
			public class MySecondJob extends QuartzJobBean {
				private String name;
				public void setName(String name) {
					this.name = name;
				}
				@Override
				protected void executeInternal(JobExecutionContext context) throws JobExecutionException {
					System.out.println("hello:"+name+":"+new Date());
				}
			}

      4. config配置
         //@description:  JobDetail 的配置有两种方式：MethodInvokingJobDetailFactoryBean 和 JobDetailFactoryBean
         @Configuration
         public class QuartzConfig {
            //使用 MethodInvokingJobDetailFactoryBean,可以配置目标 Bean 的名字和目标方法的名字，这种方式不支持传参 
            @Bean
            MethodInvokingJobDetailFactoryBean methodInvokingJobDetailFactoryBean(){
               MethodInvokingJobDetailFactoryBean bean = new MethodInvokingJobDetailFactoryBean();
               bean.setTargetBeanName("myFirstJob");
               bean.setTargetMethod("hello");
               return bean;
            }
            
            //使用JobDetailFactoryBean 可以配置 JobDetail  
            //这种方式支持传参，将参数封装在 JobDataMap 中进行传递, job中只需要提供属性名和相应的set方法即可
            @Bean
            JobDetailFactoryBean jobDetailFactoryBean(){
               JobDetailFactoryBean bean = new JobDetailFactoryBean();
               JobDataMap map = new JobDataMap();
               map.put("name","ljx");
               bean.setJobDataMap(map);
               bean.setJobClass(MySecondJob.class);			MySecondJob类继承自 QuartzJobBean
               return bean;
            }
            
            //设置JobDetail, setStartTime设置开始时间，
            @Bean
            SimpleTriggerFactoryBean simpleTriggerFactoryBean(){
               SimpleTriggerFactoryBean bean = new SimpleTriggerFactoryBean();
               bean.setJobDetail(methodInvokingJobDetailFactoryBean().getObject());
               bean.setStartTime(new Date());
               bean.setRepeatInterval(2000);			#任务的时间间隔
               bean.setRepeatCount(3);					#任务的循环次数
               return bean;
            }
         
            //主要配置JobDetail和Cron表达式
            @Bean
            CronTriggerFactoryBean cronTriggerFactoryBean(){
               CronTriggerFactoryBean bean = new CronTriggerFactoryBean();
               bean.setJobDetail(jobDetailFactoryBean().getObject());
               bean.setCronExpression("* * * * * ?");
               return bean;
            }
         
            /**
            *  创建schedulerFactory，配置相应的Trigger
               *****这里的setTriggers可以接收多个参数，这里可以批量注册，job数量少就，对应都注册商，多久批量注册*****
               public void setTriggers(Trigger... triggers) {this.triggers = Arrays.asList(triggers);}
               这是内存方案，持久化方案再配置下quartz.properties和datasource即可
            */
            @Bean	
            SchedulerFactoryBean schedulerFactoryBean(){
               SchedulerFactoryBean bean = new SchedulerFactoryBean();
               bean.setTriggers(simpleTriggerFactoryBean().getObject(),cronTriggerFactoryBean().getObject());
               return bean;
            }
         }
   
### xxl-job相关

   官方文档 	         https://www.xuxueli.com/xxl-job/
	xxl-job调用结构图	   https://www.cnblogs.com/wanghongsen/p/12510533.html

   几个定时任务框架对比	 https://www.jianshu.com/p/a23c0d7fe371

   xxl-job是一个开源的分布式定时任务框架，其调度中心和执行器是相互分离，分开部署的，两者通过HTTP协议进行通信。

   设计思想
      将调度行为抽象形成“调度中心”公共平台，而平台自身并不承担业务逻辑，“调度中心”负责发起调度请求。
      将任务抽象成分散的JobHandler，交由“执行器”统一管理，“执行器”负责接收调度请求并执行对应的JobHandler中业务逻辑。
      因此，“调度”和“任务”两部分可以相互解耦，提高系统整体稳定性和扩展性；

   重要系统组成
      调度模块（调度中心）：
         负责管理调度信息，按照调度配置发出调度请求，自身不承担业务代码。调度系统与任务解耦，提高了系统可用性和稳定性，同时调度系统性能不再受限于任务模块；
         支持可视化、简单且动态的管理调度信息，包括任务新建，更新，删除，GLUE开发和任务报警等，所有上述操作都会实时生效，同时支持监控调度结果以及执行日志，支持执行器Failover。
      执行模块（执行器）：
         负责接收调度请求并执行任务逻辑。任务模块专注于任务的执行等操作，开发和维护更加简单和高效；
         接收“调度中心”的执行请求、终止请求和日志请求等。‘

      统一的调度中心，就是xxl-job-admin。
      执行器，一般是bean执行器，分散配置在各自的服务中，在spring的基础上，@XxlJob注入任务(将执行器的name和注册地址作为一对，放入了admin的地址list中) 

      当然可以定义一个通用的httpjobhander，然后配置多个任务附带不同的参数就可以了，也不行直接一个http的请求地址也行
      增加执行器后，admin增加配置，会有心跳检测，间隔一段时间就能连上。

   XXL-JOB最终选择自研调度组件（早期调度组件基于Quartz，2.1.0版本后移除quartz，使用自研调度组件）；一方面是为了精简系统降低冗余依赖，另一方面是为了提供系统的可控度与稳定性；
   XXL-JOB中“调度模块”和“任务模块”完全解耦，调度模块进行任务调度时，将会解析不同的任务参数发起远程调用，调用各自的远程执行器服务。
            这种调用模型类似RPC调用，调度中心提供调用代理的功能，而执行器提供远程服务的功能。
            
   * quartz和xxl-job区别

         都能实现定时功能，区别在于对任务的创建、修改、删除、触发以及监控的操作成本上，
         xxl-job就是quartz的一个增强版，其弥补了quartz不支持并行调度，不支持失败处理策略和动态分片的策略等诸多不足，
         同时其有管理界面，上手比较容易，支持分布式，适用于分布式场景下的使用。 两者相同的是都是通过数据库锁来控制任务不能重复执行。
      
         quartz
            更新使用服务端多点，页面的配置差点，
            直接提供了api，开发人员拥有最大的操作权且更灵活性，然而对于不需要很高灵活性的系统，或调度任务的操控由非开发人员负责的系统，需要额外对api调用做一层封装，隔离 api 操作；
            quartz的调度逻辑和QuartzJobBean耦合在同一个项目中，在调度任务数量逐渐增多，同时调度任务逻辑逐渐加重的情况下，此时调度系统的性能将大大受限于业务；
         xxl-job
            可视化功能做的更好些
            提供这些控制的方式是提供了一个单独的可视化调度中心，这意味着任务的状态控制可以和系统分离，通过更易操作的网页界面的形式，降低了对操控者的门槛。

         
   * 几种运行模式
      * BEAN模式：这个是在项目中写 Java 类，然后在 JobHandler 里填上 @XxlJob 里面的名字，是在执行器端编写的
         
            原理：每个Bean模式任务都是一个Spring的Bean类实例，它被维护在“执行器”项目的Spring容器中。
            任务类需要加“@JobHandler(value=”名称”)”注解，因为“执行器”会根据该注解识别Spring容器中的任务。
            任务类需要继承统一接口“IJobHandler”，任务逻辑在execute方法中开发，
            因为“执行器”在接收到调度中心的调度请求时，将会调用“IJobHandler”的execute方法，执行任务逻辑。
            这个貌似是以前的方式，现在改用@XxlJob
         
            任务以JobHandler方式维护在执行器端；需要结合 "JobHandler" 属性匹配执行器中任务；
            JobHandler：运行模式为 "BEAN模式" 时生效，对应执行器中新开发的JobHandler类“@JobHandler”注解自定义的value值(2.2.0版本已移除@JobHandler，更新使用基于方法的注解 @XxlJob)；
            
            BEAN模式（类形式）
            Bean模式任务，支持基于类的开发方式，每个任务对应一个Java类。
               缺点：1.每个任务需要占用一个Java类，造成类的浪费；
                  2.不支持自动扫描任务并注入到执行器容器，需要手动注入。
            
            BEAN模式（方法形式）
            Bean模式任务，支持基于方法的开发方式，每个任务对应一个方法。
            优点：	1.每个任务只需要开发一个方法，并添加”@XxlJob”注解即可，更加方便、快速。
                  2.支持自动扫描任务并注入到执行器容器。
            缺点：要求Spring容器环境；
            
            代码：
               1、开发一个继承自"com.xxl.job.core.handler.IJobHandler"的JobHandler类，实现其中任务方法。
               2、手动通过如下方式注入到执行器容器。
               ```
               XxlJobExecutor.registJobHandler("demoJobHandler", new DemoJobHandler());
         
            @XxlJob注解的value值对应的是调度中心新建任务配置页面的JobHandler属性的值。
            对应的demo类是 com.xxl.job.executor.service.jobhandler.SampleXxlJob
            
            执行日志：需要通过 "XxlJobHelper.log" 打印执行日志；
               代码：	
               @XxlJob("demoJobHandler")
               public void demoJobHandler() throws Exception {
                  XxlJobHelper.log("XXL-JOB, Hello World.");
               }
            
            ******
               原生内置Bean模式任务
                  为方便用户参考与快速实用，示例执行器内原生提供多个Bean模式任务Handler，可以直接配置实用，如下：
                  demoJobHandler：简单示例任务，任务内部模拟耗时任务逻辑，用户可在线体验Rolling Log等功能；
                  shardingJobHandler：分片示例任务，任务内部模拟处理分片参数，可参考熟悉分片任务；
                  *httpJobHandler：
                     通用HTTP任务Handler；业务方只需要提供HTTP链接等信息即可，不限制语言、平台。
                     示例任务入参如下：
                     url: http://www.xxx.com
                     method: get 或 post
                     data: post-data
                     
                     其中http处理的参数的接收方式和请求传参的方式都是自己定义的
                     代码默认是\n分割取参数，然后实现远程http的调用
                     
                  4、跨平台Http任务
                  这里的执行器还是自己代码中配置的，本质上还是自己配置的执行器，内部使用的http请求，
                  参数使用 String param = XxlJobHelper.getJobParam();接收
                  @XxlJob("httpJobHandler")
                  public void httpJobHandler(){}
                     
                  commandJobHandler：通用命令行任务Handler；业务方只需要提供命令行即可；如 “pwd”命令；
            ******							 
            
      * GLUE模式：支持Java、Shell、Python、PHP、Nodejs、PowerShell，这个时候代码是直接维护在调度中心这边的
         
   * 执行器
      任务触发调度时将会自动发现注册成功的执行器, 实现任务自动发现功能; 每个任务必须绑定一个执行器, 可在 "执行器管理" 进行设置;
      
      执行器实际上是一个内嵌的Server，默认端口9999（配置项：xxl.job.executor.port）。
      在项目启动时，执行器会通过“@JobHandler”识别Spring容器中“Bean模式任务”，以注解的value属性为key管理起来。

      “执行器”接收到“调度中心”的调度请求时，
         如果任务类型为“Bean模式”，将会匹配Spring容器中的“Bean模式任务”，然后调用其execute方法，执行任务逻辑。
         admin中配置的jobhandler中的名称要和 @XxlJob("httpJobHandlergogo")中的value值要一致，这样才能匹配到对应的bean
            
         如果任务类型为“GLUE模式”，将会加载GLue代码，实例化Java对象，注入依赖的Spring服务，然后调用execute方法，执行任务逻辑。
                     
      一次完整的任务调度通讯流程
         1、“调度中心”向“执行器”发送http调度请求: “执行器”中接收请求的服务，实际上是一台内嵌Server，默认端口9999;
         2、“执行器”执行任务逻辑；
         3、“执行器”http回调“调度中心”调度结果: “调度中心”中接收回调的服务，是针对执行器开放一套API服务;
      
   * 任务注册, 任务自动发现
         v1.5版本之后, 任务取消了”任务执行机器”属性, 改为通过任务注册和自动发现的方式, 动态获取远程执行器地址并执行。
         AppName: 
            每个执行器机器集群的唯一标示, 任务注册以 "执行器" 为最小粒度进行注册; 每个任务通过其绑定的执行器可感知对应的执行器机器列表;
         注册表: 
            见"xxl_job_registry"表, "执行器" 在进行任务注册时将会周期性维护一条注册记录，即机器地址和AppName的绑定关系; "调度中心" 从而可以动态感知每个AppName在线的机器列表;
         执行器注册:
            任务注册Beat周期默认30s; 执行器以一倍Beat进行执行器注册, 调度中心以一倍Beat进行动态任务发现; 注册信息的失效时间为三倍Beat; 
         执行器注册摘除：
            执行器销毁时，将会主动上报调度中心并摘除对应的执行器机器信息，提高心跳注册的实时性；	


         为提升系统安全性，调度中心和执行器进行安全性校验，双方AccessToken匹配才允许通讯；
         调度中心和执行器，可通过配置项 “xxl.job.accessToken” 进行AccessToken的设置。
         调度中心和执行器，如果需要正常通讯，只有两种设置；
         设置一：调度中心和执行器，均不设置AccessToken；关闭安全性校验；
         设置二：调度中心和执行器，设置了相同的AccessToken；				
      
   * 后台admin的操作
         添加执行器，
            AppName: 是每个执行器集群的唯一标示AppName, 执行器会周期性以AppName为对象进行自动注册。可通过该配置自动发现注册成功的执行器, 供任务调度时使用;
            注册方式：调度中心获取执行器地址的方式；
               自动注册：执行器自动进行执行器注册，调度中心通过底层注册表可以动态发现执行器机器地址；
               手动录入：人工手动录入执行器的地址信息，多地址逗号分隔，供调度中心使用；
            机器地址："注册方式"为"手动录入"时有效，支持人工维护执行器的地址信息；
            
            添加了新的执行器，online地址没有，就是添加和启动顺序问题，重启即可，不启也没事，一会有心跳会自动检测上
            AppName* 就是执行器配置文件中的	xxl.job.executor.appname=es-xxl-job-sample

         点击“执行一次”按钮，可手动触发一次任务调度，不影响原有调度规则。
         
         此处的启动/停止仅针对任务的后续调度触发行为，不会影响到已经触发的调度任务，如需终止已经触发的调度任务，可查看“4.9 终止运行中的任务”
      
   * 调度中心 RESTful API
      执行任务回调，执行器注册、移除等
      API服务位置：com.xxl.job.core.biz.AdminBiz （ com.xxl.job.admin.controller.JobApiController ）
      API服务请求参考代码：com.xxl.job.adminbiz.AdminBizTest
         
   * 执行器 RESTful API
      心跳检测，忙碌检测，触发、终止任务，查看执行日志
      API服务位置：com.xxl.job.core.biz.ExecutorBiz				
         相关实现类 com.xxl.job.core.biz.impl.ExecutorBizImpl
         相关的通信 使用了netty通信，com.xxl.job.core.server.EmbedServer.EmbedHttpServerHandler
         执行器内嵌netty-http-server提供服务						
         String param = XxlJobHelper.getJobParam();    // 获取参数  本质是在线程变量中存了一个XxlJobContext对象，包含相关的参数信息
         
         说明：触发任务执行
            地址格式：{执行器内嵌服务跟地址}/run
            Header：
               XXL-JOB-ACCESS-TOKEN : {请求令牌}
            请求数据格式如下，放置在 RequestBody 中，JSON格式：
               {
                  "jobId":1,                                  // 任务ID
                  "executorHandler":"demoJobHandler",         // 任务标识
                  "executorParams":"demoJobHandler",          // 任务参数
                  "executorBlockStrategy":"COVER_EARLY",      // 任务阻塞策略，可选值参考 com.xxl.job.core.enums.ExecutorBlockStrategyEnum
                  "executorTimeout":0,                        // 任务超时时间，单位秒，大于零时生效
                  "logId":1,                                  // 本次调度日志ID
                  "logDateTime":1586629003729,                // 本次调度日志时间
                  "glueType":"BEAN",                          // 任务模式，可选值参考 com.xxl.job.core.glue.GlueTypeEnum
                  "glueSource":"xxx",                         // GLUE脚本代码
                  "glueUpdatetime":1586629003727,             // GLUE脚本更新时间，用于判定脚本是否变更以及是否需要刷新
                  "broadcastIndex":0,                         // 分片参数：当前分片
                  "broadcastTotal":0                          // 分片参数：总分片
               }
            响应数据格式：
               {
                  "code": 200,      // 200 表示正常、其他失败
                  "msg": null       // 错误提示消息
               }
               
               //内部代码，根据传参的uri选择操作行为
                  if ("/beat".equals(uri)) {
                     return executorBiz.beat();
                  } else if ("/run".equals(uri)) {
                     TriggerParam triggerParam = GsonTool.fromJson(requestData, TriggerParam.class);
                     return executorBiz.run(triggerParam);
                  } else if ...
      API服务请求参考代码：com.xxl.job.executorbiz.ExecutorBizTest
         
      com.xxl.job.admin.core.scheduler.XxlJobScheduler			配置初始化和启动类	
      com.xxl.job.core.biz.client.ExecutorBizClient  				调度端发送请求，发送类似run请求
      com.xxl.job.core.biz.impl.ExecutorBizImpl					执行器端接收请求，接收类似任务处理的run请求,这里会放进队列queue执行。
      com.xxl.job.executorbiz.ExecutorBizTest						调度的测试类	
      com.xxl.job.admin.core.trigger.XxlJobTrigger#runExecutor	实际任务触发的run调用		
      
      com.xxl.job.admin.core.thread.JobScheduleHelper				循环执行实现任务的触发
         while (!scheduleThreadToStop) {
         //不断的查询job任务触发时间
            if (nowTime > jobInfo.getTriggerNextTime()) {
               //满足触发条件，就触发job，发起run调用
               JobTriggerPoolHelper.trigger(jobInfo.getId(), TriggerTypeEnum.CRON, -1, null, null, null);
            } 
         }

   * 其他
         相关表的操作
            xxl_job_group：执行器信息表，维护任务执行器信息；记录当前的执行器信息
            xxl_job_registry：执行器注册表，维护在线的执行器和调度中心机器地址信息；记录注册信息，
            
         跨语言
            XXL-JOB是一个跨语言的任务调度平台，主要体现在如下几个方面：
            1、RESTful API：调度中心与执行器提供语言无关的 RESTful API 服务，第三方任意语言可据此对接调度中心或者实现执行器。
            2、多任务模式：提供Java、Python、PHP……等十来种任务模式,理论上可扩展任意语言任务模式；
            2、提供基于HTTP的任务Handler（Bean任务，JobHandler=”httpJobHandler”）；业务方只需要提供HTTP链接等相关信息即可，不限制语言、平台；
                        
         失败提醒，默认是邮件提醒(EmailJobAlarm)，需要新增一种告警方式，只需要新增一个实现 “com.xxl.job.admin.core.alarm.JobAlarm”接口的告警实现即可。
         
         避免任务重复执行
            调度密集或者耗时任务可能会导致任务阻塞，集群情况下调度组件小概率情况下会重复触发；
            针对上述情况，可以通过结合 “单机路由策略（如：第一台、一致性哈希）” + “阻塞策略（如：单机串行、丢弃后续调度）” 来规避，最终避免任务重复执行。
            
         日志自动清理
            XXL-JOB日志主要包含如下两部分，均支持日志自动清理，说明如下：
            调度中心日志表数据：可借助配置项 “xxl.job.logretentiondays” 设置日志表数据保存天数，过期日志自动清理；详情可查看上文配置说明；
            执行器日志文件数据：可借助配置项 “xxl.job.executor.logretentiondays” 设置日志文件数据保存天数，过期日志自动清理；详情可查看上文配置说明；

         调度结果丢失处理
            执行器因网络抖动回调失败或宕机等异常情况，会导致任务调度结果丢失。由于调度中心依赖执行器回调来感知调度结果，因此会导致调度日志永远处于 “运行中” 状态。
            针对该问题，调度中心提供内置组件进行处理，逻辑为：调度记录停留在 “运行中” 状态超过10min，且对应执行器心跳注册失败不在线，则将本地调度主动标记失败；

## 单点登录，sso相关

1. sso单点登录流程，小鱼儿主要流程

   * 1.项目启动，AuthenticationFilter中初始化Configuration和AuthInterceptor(鉴权事宜)，并且存入servletContext中
   * 2.初次登录，直接redirect跳转ids_server端，判断cookie中没有登录相关name的cookie value，直接跳转登录页面，登录后，相同的https://www.yu.com/ids/login路径还是跳转登录页面	
   * 3.点击登录，常规的登录次数和密码校验后，一般获取的域名是.yu.com,获取tockenFlag对应的cookie，存在，删除对应的redis值，开始单点登录	
   * 4.生成一个随机数ticketId返回，并且在redis中设置对应的限时值(vId + "%_%" + userInfo.getUserCode())，并将返回的url统一设置成auth认证url，然后在.yu.com下写上tockenFlag对应的cookie值，在session中添加userinfo的信息，
   * 5.认证返回后直接到AuthenticationServlet(只做登录和退出用)，调用之前的Configuration和AuthInterceptor，开始鉴权，见得到的ticket发送到server端验证，验证成功，返回对应的usercode,然后按需，存放相应的redis和cookies值，这里是xyeAuthId，认证后的标记，这样登录就成功了
   * 6.后面的htm请求会经过filter，根据url对应使用三种权限校验，redis中存放的vid相关的信息，每次请求redis没有则清空cookie，存在就刷新redis的缓存时间,

2. 单点登录的主要实现方案：
      1、服务接口的开发
      2、在分布式环境中使用redis实现session共享（缓存唯一token和对应的用户信息）
      3、使用cookie在多个系统中共享。（存放token信息）
      4、拦截器的使用方法（访问校验cookie中的token是否存在或过期）

## Shiro相关（忽略）

* 基本介绍
   *****
      shiro三大核心模块：Subject（用户）、SecurityManager(框架核心)、Realm（Shiro与应用安全数据间的“桥梁”，主要用来权限校验和密码校验）
      重点是shirofilter  securitymanager，和realm(AuthorizingRealm)。
   *****

   https://www.cnblogs.com/yoohot/p/6085830.html   讲解的是shiro的各filter用法
         
   前后端分离项目中,用户的信息及过期权限等信息依然是靠后端存储,以上依然涉及session,只不过是将产生的jsessionid当作token使用,使用redis存储而已.可以考虑使用jwt,彻底是后端无状态化;

   * //登录认证
         @Override
         protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException {
         …
         SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo(loginUser,password,salt,getName());
         return authenticationInfo;  //这里返回的就是shiro中的principal

         //在调用了login方法后,SecurityManager会收到AuthenticationToken,并将其发送给已配置的Realm执行必须的认证检查
         //每个Realm都能在必要时对提交的AuthenticationTokens作出反应
         //所以这一步在调用login(token)方法时,它会走到MyRealm.doGetAuthenticationInfo()

   * //鉴权相关
         doGetAuthorizationInfo方法(封装了对应的权限内容)
         System.out.println("经试验：并不是每次调用接口就会执行，而是调用需要操作码（permission）的接口就会执行");

         可以使用filterChainDefinitionMap配置权限

         权限鉴权
            只有第一次会将信息存入缓存，通过权限过滤，
            其中的路径匹配可以再看看  在org.apache.shiro.authz.permission.WildcardPermission#implies 方法中
            权限的分隔划分，以:划分一个权限的匹配，源码操作

   * shirofilter相关
         shiro强大的自定义访问控制拦截器：AccessControlFilter，
            isAccessAllowed：表示是否允许访问；mappedValue就是[urls]配置中拦截器参数部分，如果允许访问返回true，否则false；
            onAccessDenied：表示当访问拒绝时是否已经处理了；如果返回true表示需要继续处理；如果返回false表示该拦截器实例已经处理了，将直接返回即可。基本上到这就重定向
            onPreHandle：会自动调用这两个方法决定是否继续处理；

         isAccessAllowed和onAccessDenied方法会影响到onPreHandle方法，而onPreHandle方法会影响到preHandle方法，而preHandle方法会达到控制filter链是否执行下去的效果。
         如果使用PathMatchingFilter就是接是onPreHandle方法走。

   * shiro-session
         sessionFactory是创建会话的工厂，根据相应的Subject上下文信息来创建会话；默认提供了SimpleSessionFactory用来创建SimpleSession会话。
         更具需要也可以自钉子新的OnlineSession，搭配自定义 OnlineSessionFactory。实现session的内容增加

         其实这两个都可以使用默认的配置，即不持久化session也不自定义session
         SessionFactory中创建的session实体，Sessiondao操作中是创建了一个sessionid并赋值给当前的session
         顺序是先实例化自定义的session，然后通过sessiondao创建sessionid赋值给对应的session

         kickout使用了cache和Deque队列实现，返回subject.logout退出，然后重定向登录
         WebUtils.issueRedirect(request, response, kickoutUrl);
         deque.push(sessionId);
         cache.put(loginName, deque);// 将用户的sessionId队列缓存

         kickoutSession.setAttribute("kickout", true);设置踢出属性，然后判断当前的session，重定向			
      
         这个也可以
            public RedisCacheManager cacheManager() {
            RedisCacheManager redisCacheManager = new RedisCacheManager();
            redisCacheManager.setRedisManager(redisManager());
            return redisCacheManager;
         }	

   * shiro——rememberme	 
         shiro中使用remenberme中 boolen类型，true会在页面写一个cookie(remenberme)，value是经过加密存放的是用户的信息
         cookieRememberMeManager.setCipherKey(Base64.decode("fCq+/xW488hMTCD+cmJ3aQ=="));这个是内部加密用的key，会用这个key加密principal(SimpleAuthenticationInfo),回写到cookie中

         *****shiro会在每一次访问时都会创建一个subject，这是源码中有，并且绑定了线程变量*****
         DefaultSecurityManager.createSubject在创建subejct的时候就会调用resolvrPrincipals方法，
         这个当前方法内部会先从当前的session中获得的subject的principal，如果没有找到再从cookie找，调用的方法是getRememberedInedtity
         

   * shiro注解配置
         ShiroFilterFactoryBean 中配置相关的拦截配置(但如果接口多，每个接口url都要写一遍太麻烦。所以采用注解的方式，在每个controller方法上加注解。)
         开启Shiro的注解(如@RequiresRoles,@RequiresPermissions),需借助SpringAOP扫描使用Shiro注解的类,并在必要时进行安全逻辑验证
         配置以下两个bean(DefaultAdvisorAutoProxyCreator(可选 depends-on="lifecycleBeanPostProcessor" )和AuthorizationAttributeSourceAdvisor)即可实现此功能
         开启注解 DefaultAdvisorAutoProxyCreator 和 AuthorizationAttributeSourceAdvisor

         必须全部符合（默认不写或者在后面添加logical = Logical.AND）
         @RequiresPermissions(value={“stuMan:find_record_list”,“tea:find_record_list”})
         上面这种情况是默认当前对象必须同时全部拥有指定权限
         符合其中一个即可(logical = Logical.OR)    
         
         @RequiresPermissions  会拦截标签，分发到对应的PermissionAnnotationHandler。
         shiro:hasPermission  是好像直接访问到了AuthorizingRealm中的isPermitted，没有走controller的拦截handler
         真正的拦截规则是AuthorizingRealm.isPermitted>>>>>>>>>>org.apache.shiro.authz.permission.WildcardPermission#implies
               
   * shiro-页面标签
         使用shiro注解，会还是会使用securityManager.isPermitted方法，最终进入AuthorizingRealm.isPermitted方法，执行同controller进入的匹配方法
         延伸到时html页面
            thymeleaf中使用shiro:hasPermission标签控制页面显示需要：
            thymeleaf-extras-shiro
            或者命名空间  xmlns:shiro="http://www.thymeleaf.org/thymeleaf-extras-shiro"
            
            配置一个ShiroDialect的bean
         在freemarker中
            引入shiro-freemarker-tags，页面 使用<@shiro.hasPermission name="权限添加">  
         在jsp中
            在页面顶部引用<%@taglib prefix="shiro" uri="http://shiro.apache.org/tags" %> 标签库，
            页面使用<shiro:hasPermission name="1111">  
      

1. 配置重点ShiroFilterFactoryBean,SecurityManager,Realm

   * 1.shiro.xml中配置ShiroFilterFactoryBean时，在容器中注入filterbean，拦截/*请求，未登录，跳转loginUrl,unauthorizedUrl是没有权限跳转的路径,重点是配置这个ShiroFilterFactoryBean

   * 2.securityManager中继承了多个功能模块（继承链由顶端向下）：
         1、CachingSecurityManager			主要提供缓存支持，管理缓存操作
         2、RealmSecurityManager				可理解为数据处理组件，比如Realm根据用户名数据库，和输入的用户密码比对，验证登录，还有授权数据等）
         3、AuthenticatingSecurityManager	实现接口Authenticator，处理用户登陆验证的 SecurityManager 的 抽象实现，仅仅代理Authenticator
         4、AuthorizingSecurityManager		实现了Authorizer接口的抽象类，该类主要代理Authorizer进行授权
         5、SessionsSecurityManager			实现类SessionManager的方法。代理了SessionManager来处理相关的session操作。
         6、DefaultSecurityManager			默认的SessionManager的具体实现，该实现类会合适的初始化依赖组件。如subjectFactory、SubjectDAO
         7、DefaultWebSecurityManager		WebSecurityManager实现被使用在web应用程序中或者需要http请求的应用程序中（如SOAP，http remoting, etc）

2. 解析ini中的section，最终将每个section解析成为linkedmap使用   map<String,section> 配合scanner解析
   	ini.sections    Map<String, Section> sections
      ```
         <bean id="shiroManager" class="com.sojson.core.shiro.service.impl.ShiroManagerImpl"/>
         <property name="filterChainDefinitions" value="#{shiroManager.loadFilterChainDefinitions()}"/> 
      ```
   ​	在xml中，相当于使用了一个bean实例调用了其方法返回了结果，但是在方法中需要使用ini4j的解析器，解析出String格式的ini信息

3. 在SecurityUtils.getSubject().login(token);获取的时候会获取securityManager实例中的realm，调用realm实例中的认证方法
   

   Subject currentUser = SecurityUtils.getSubject();
   currentUser.getprincipal实际上就是当时 SimpleAuthenticationInfo info = new SimpleAuthenticationInfo(user, password, getName());  中的这个存入的user对象
   
   暂时看是从session中拿到DelegatingSubject.RUN_AS_PRINCIPALS_SESSION_KEY作为key的value，那么还是从session中拿到的缓存的principal信息

   WebUtils.issueRedirect(request, response, loginUrl)
   loginfilter中的返回false后会调用WebUtils.issueRedirect(request, response, loginUrl)跳转到loginUrl

   onlineSessionfilter 可以用来判断是否满足登录，syncOnlineSessionfilter用来接着同步操作
   AuthorizingRealm 中的ispermitted方法可以debug一下

   使用冒号分隔的权限表达式是org.apache.shiro.authz.permission.WildcardPermission 默认支持的实现方式。这里分别代表了 资源类型：操作：资源ID  

4. filterChainDefinitions 从上到下，从左到右，如果有匹配的拦截器就会阻断并返回
      // 参数chainName形如 /admin/list**
      // 参数chainDefinition形如 authc,perms[admin:manage]  每个前置元素表示的是一个filter，默认的和自定义的都会继承AccessControlFilter
      []中的只是作为filter的参数传递进去，并做相应的处理

      // foo[bar, baz]       returned[0] == foo
      returned[1] == bar, baz 
      / 保存用户配置的url与filter之间的映射关系
      applyChainConfig(chainName, filter, chainSpecificFilterConfig);

5. 一些常用的java类
      SimpleAuthorizationInfo		用于存放该用户的权限和角色信息，授权的专用类
      SimpleAuthenticationInfo	用于认证成功后返回的认证信息	

   ​	用户查看	   session中使用的是传递sessionid找到redis中的session信息，在找到session中的用户信息
   ​	踢出功能  	session存在 但是session中设置的status失效

6. 缓存使用
   	如果在项目中并未使用shiro的jsp标签库，那么使用集中式的缓存方案也未尝不妥；但是，如果大量使用shiro的jsp标签库，那么采用本地缓存才是最佳选择。

   ​	 cachemanager和sessionmanager，前者负责缓存实现，比如大量的登录信息和权限信息，后者是服务端的session保存

      shiro 中的session 中 request.getSeesion()与subject.getSeesion()获取session后，对session的操作是相同的。

   ​	 sessionmanager中配置sessiondao（使用缓存管理session），
   ​	 cachemanager，只是返回了一个rediscache， 实现两个接口，manager只返回cache对象，cache对象中封装redis操作，需要配置使用redis来管理rediscache
   ​	 remenberme的cookie秘钥需要使用ase算法生成存进去，subject中能获取principal对象,remenberme标记和认证标记是互斥的
   ​	 shiro中配置缓存，和缓存管理器，都需要实现相应的接口，最终操作的还是cache的对象，使用redis的操作，序列化存放比较多，但是不方便查看
   ​	 使用JedisManager，每次操作从jedispool中获取一个jedis实例操作，
   ​	 实现了cache缓存后，认证的信息会先从缓存中读取，不一定每次都读取realm中的授权方法

      一般通过继承EnterpriseCacheSessionDAO实现sessiondao的操作，这个是用来实现session的持久化。用来自定义session处理的，也可以用默认的
      有doCreate”、“doReadSession”、“doUpdate”和“doDelete”。其中只有doCreate是实现的，其它的都是没有实现的方法。


7. @RequiresPermissions 标签
   		首页和登录成功返回：
      		先访问的是欢迎页中的index.jsp，重定向manage/index,登录拦截求拦截登录，成功后再根据redirectUrl跳转相应的地址，没有则默认数据库中的主页地址
      		filter中的servletrequest可以向下强转为httpservletrequest使用

   ​	shiro中配置的自定义filter一般都需要继承accesscontroller，实现统一的认证授权方法
   ​	捕获subject.login（）方法的异常返回相关的信息
   ​	
   ​	配种filter链的时候，filter后面可以附带参数，后端用mappedValue接收，这里引申出权限的两种方案，

   ​	1、在filter链中带上相应的权限，后端校验使用(subject.isPermitted(permission))使用，这个方法会调用realm的授权方法，在filter中就重定向到无权限页面
   ​	2、使用@RequiresPermissions指定需要的权限，配合realm类

   ​	/manage/** = upmsSessionForceLogout,authc["a","b","c"]

   ​	调用权限认证的入口
   ​	1、subject.hasRole(“admin”) 或 subject.isPermitted(“admin”)：自己去调用这个是否有什么角色或者是否有什么权限的时候；
   ​	2、@RequiresRoles("admin") 或者@RequiresPermissions（）：在方法上加注解的时候；
   ​	3、[@shiro.hasPermission name = "admin"][/@shiro.hasPermission]：在页面上加shiro标签的时候，即进这个页面的时候扫描到有这个标签的时候。

   ​	在配置filter工厂ShiroFilterFactoryBean，会将loginurl等参数set进相应的filter中

   ​	subject.isAuthenticated() 登录成功为true

8. shiro相关（忽略）
		shiro配置的两种方式，代码注入和配置ini文件加载
		配置的几个关键的东西：securityManager,ShiroFilterFactoryBean(配置登录路径,拦截器路径等),自定义Realm（创建时候可能需要加密的方式配置HashedCredentialsMatcher），
		登录之后，存在session中，这样够缓存用户的信息，每次请求都创建一个subject，从缓存中新建的对象
		权限更新后需要更新内存中的权限
		使用shiro的注解需要配置AuthorizationAttributeSourceAdvisor	
		HashedCredentialsMatcher 是密码加密相关
		SecurityUtils.getSubject()是每个请求创建一个Subject, 并保存到ThreadContext的resources（ThreadLocal<Map<Object, Object>>）变量中，也就是一个http请求一个subject,并绑定到当前线程。 

		login的时候，获取一个subject.login(new UsernamePasswordToken(name, userParam.getPassword()));这样在realm中认证的时候，用户的name 和密码信息就会被拿到了
		然后再认证中返回 new SimpleAuthenticationInfo(userName, passwordInDB, ByteSource.Util.bytes(salt),getName());
		最终是通过securitymanager调用AuthenticationInfo info = realm.getAuthenticationInfo(token);实现认证

		可以初始化权限路径，将资源权限加载进来
		  String permission = "perms[" + resources.getResurl()+ "]";
		  filterChainDefinitionMap.put(resources.getResurl(),permission);

		<!-- 过滤链定义，从上向下顺序执行，一般将/**放在最为下边 -->:这是一个坑呢，一不小心代码就不好使了;
		SimpleAuthenticationInfo第一个参数一般传的是userinfo对象，有的也传username，暂时不确定，后续待验证

		首先shiro会先从缓存中获取认证信息(对应getCachedAUthenticationInfo方法),如果没有才会继续从Realm中获取,
		认证的时候 ByteSource credentialsSalt = ByteSource.Util.bytes("vip");
		Object obj = new SimpleHash(hashAlgorithName, password, credentialsSalt, hashIterations);
		会使用SimpleAuthenticationInfo中公用的salt，和token中的密码相对比，一致就校验通过


		除了通用的过滤器外，可以自定义过滤器
		//下面的配置路径 都需要在上面配置 authc 否则访问不到filter
		filterChainDefinitionMap.put("/online","requestURL");

		自定义路径拦截器继承PathMatchingFilter（可以实现用户路径权限的判断）

		filtersMap.put("requestURL", getURLPathMatchingFilter());
		shiroFilterFactoryBean.setFilters(filtersMap);

		相关的配置文件：INI配置文件一般适用于用户少且不需要在运行时动态创建
		INI文件优势 : 简单易懂 , 集成方便.
		INI文件缺点 : 采用硬编码方式把认证授权信息写在INI文件中,可维护性差.
		推荐相关配置放在数据库中动态读取

		常见的realm，我们需要缓存功能,认证功能,授权功能,三大功能 .认证原则:什么样的用户是怎样的角色.拥有什么的权限.
		shiro的默认的拦截器执行是有一定的顺序的，一般全匹配的放最后面，如果匹配到就不继续匹配了，所以把 /放到最前面，则 后面的链接都无法匹配到了。 

		也可以使用一个过滤器，将每个路径都过滤一遍
		感觉实现自己的路径拦截会更灵活点，在项目中配置注解，维护性差

		Subject.login()登录成功后用户的认证信息实际上是保存在HttpSession中的。如果此时Web应用程序部署了多实例，必须要进行Session同步。
		其实在SecurityManager中设置的CacheManager组中都会给Realm使用，即：真正使用CacheManager的组件是Realm。
		首先需要对session进行同步，因为shiro的认证信息是存放在session中的；其次，当前端操作在某个实例上修改了权限时，需要通知后端服务的多个实例重新获取最新的权限数据。

		当用户登录之后就会被缓存在session中，再次访问会根据sessionid渠道对应的pricipals，

		principal, 就是传入的认证信息的第一个参数,传递的类型就是后面获取的值。UserBean user = (UserBean)subject.getPrincipal()
		  public SimpleAuthenticationInfo(Object principal, Object hashedCredentials, ByteSource credentialsSalt, String realmName) {
				this.principals = new SimplePrincipalCollection(principal, realmName);
				this.credentials = hashedCredentials;
				this.credentialsSalt = credentialsSalt;
			}
	
		1.根据页面传的菜单id 查询所有的buttonid，返回所有的button，然后按一定规则展示即可
		2.前台地址先固定，然后后天查出来当前页面可用相关权限，前台直接获取。直接返回按钮或者返回固定的权限列表(页面权限在所属列表中，相当于后台筛选过了，前端直接使用)

		如果按钮配置了路径，就按钮返回显示，没有配，就页面固定，然后根据权限输出

		如果是超级管理员有几百个权限，不可能传到前台去遍历，一定要再查询一遍，(根据模块尽量减小范围，查询后返回)/(遍历所有权限然后过滤？也不可能，都是字符串，没有过滤规则)
		所以目前最好的是更具当前页面级别，范围筛选权限(或者按钮)，前台在判断。		
				
		目录 菜单 按钮
		思路还是后端将权限查出来(全部查出或者根据上级id精细查询，返回给前端判断显示)
			
		根据permssion 判断按钮级别的显示，根据menu_code(归属菜单)判断用户可查看的菜单
		这个应该是查询出所有的权限和路由菜单了，可能数据大点
		如果想做的再精细点，每次查询权限就传入路由菜单或者parentid
			
		https://blog.csdn.net/kity9420/article/details/102530950			自定义权限标签？js组件解析权限列表
				
		******权限设计******
			安全设计(核心就是用户围绕着  角色-权限)
				1.用户的身份认证，即用户登录
				2.用户授权，即操作权限管理。

				权限管理设计一般使用角色来管理(用户-角色-权限)，用户的权限是通过角色来分配的，权限和角色有直接的关系
				登录成功之后，可在登录成功处理器中执行相关自定义操作。

				这里的登录状态，进数据库，保存的是用户名，令牌，和最后登录时间。

				对于页面的按钮显示实现
				后台读取当前用户的所有权限，然后针对当前页面的按钮入新增，newrole,有就返回对应的newrole=true，
				然后页面根据自己的值判断即可，这里传到页面的是对应按钮的判断值。
				传到页面的是按钮的判断值，至于页面，直接写路径和按钮，不必动态读按钮
				系统中使用的，是返回所有可用的button，然后页面根据具体的位置(循环找出合适的)显示按钮
		******权限设计*******

9. shiro的核心匹配机制源码(忽略)
      ```
         public boolean implies(Permission p) {
               if (!(p instanceof WildcardPermission)) {
                  return false;
               } else {
                  WildcardPermission wp = (WildcardPermission)p;
                  List<Set<String>> otherParts = wp.getParts();
                  int i = 0;
                  for(Iterator var5 = otherParts.iterator(); var5.hasNext(); ++i) {
                  同级别比完，拥有的权限短路径，后面全匹配    用的的是a/c  两级权限，访问的是的三级路径,直接过？不是直接过，前面需要都匹配上才行
                     Set<String> otherPart = (Set)var5.next();
                     if (this.getParts().size() - 1 < i) {
                        return true;
                     }
                     //同级别比较
                     Set<String> part = (Set)this.getParts().get(i);
                     if (!part.contains("*") && !part.containsAll(otherPart)) {
                        return false;
                     }
                  }
                  比完之后，拥有的路径长，待匹配的短，那么后一位需要时*
                  while(i < this.getParts().size()) {
                     Set<String> part = (Set)this.getParts().get(i);
                     if (!part.contains("*")) {
                        return false;
                     }
                     ++i;
                  }
                  return true;
               }
            }
      ```

## 日志相关

* 1.日志中的拦截 	UserAccountServiceImpl(97)  日志的行数定位
		使用%line可以输出行号
		使用%file可以输出文件名称
		使用%logger可以输出在获取Logger对象时指定的Class对象的全限定名称

      ```
         <layout class="ch.qos.logback.classic.PatternLayout">
         <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread|%X{invokeNo}] %-5level %logger{36}.%C{1}\(%line\)-%msg%n</pattern>
         </layout>
      ```

* 2.logback中的	
   ```
      private static Logger logger = LoggerFactory.getLogger("test"); 
               <logger name="test" >
               <level value="error" />
               <appender-ref ref="FILE" />
               <appender-ref ref="STDOUT" />
               </logger> 
      会默认寻找 name相符合的log名称，找不到会默认继承根log，打日志
   ```

* 3.lo4j中的日志设置		log4j.logger.packag1= info, packag1    默认packag1包下类和"packag1"的字符串
	protected static final Logger logger = Logger.getLogger(""); 参数只可能是1-字符串，2-*.class类，1就字符串匹配packag1名；2就作为类需要在packag1包下	
		

* 5. logback MDC机制
		作用：扩展logback内置的日志字段，自定义业务数据
		原理：内部持有一个InheritableThreadLocal实例，用于保存context数据。注意操作完要MDC.clear()方法

      在filter中加入MDC变量容器，即可统一处理变量，属于线程变量级别的，用完记得finally中移除相应的变量
      
      如果都是用new Thread方法建立的线程没有问题，因为之后线程会消亡。
      但是如果用ThreadPool线程池的话，线程是可以重用的，如果之前的线程的MDC内容没有清除掉的话，
      再次重线程池中获取到这个线程，会取出之前的数据(脏数据)，会导致一些不可预期的错误，所以当前线程结束后一定要清掉。

      MDC.put   	MDC.remove
      结合类java.lang.ThreadLocal<T>及Thread类可以知道，
      MDC中的put方法其实就是讲键值对放入一个Hashtable对象中，然后赋值给当前线程的ThreadLocal.ThreadLocalMap对象，即threadLocals，这保证了各个线程的在MDC键值对的独立性。
      是的单线程的日志输出具有共享变量

		案例：实际使用中可以用一个filter专门定制自定义的日志字段。也可以个别类中单独处理。只要是一个线程请求即可。
			
		 ```            
         代码实例
            MDC.put("sessionId",sessionId);
            logger.info("test23");
            MDC.remove("sessionId");

         logback.xml中的配置
            <property name="pattern" value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread|%X{invokeNo}|%X{sessionId}] %-5level %logger{36}-%msg%n"/>
            <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
            <File>${logbackpath}/xye.log</File>
            <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
               <FileNamePattern>${logbackpath}/xye.%d{yyyy-MM-dd}-%i.log</FileNamePattern>
               <maxFileSize>500MB</maxFileSize>
            </rollingPolicy>
               <encoder>
                  <pattern>${pattern}</pattern>
               </encoder>
            </appender>
		 ```

      为日志增加变量输出
         MDC.put(STR_USER, accountNo);
         MDC.remove(STR_USER);    这个最终在finally中需要删除掉

      案例 filter中
         @Override
         public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) {
            try {			
               HttpServletRequest req = (HttpServletRequest) request;
               HttpServletResponse res = (HttpServletResponse) response;

               MDC.put("invokeNo", no);
               
               chain.doFilter(req, res);
            } catch (Exception e) {
               logger.error("", e);
            } finally {			
               MDC.remove("invokeNo");
            }
         }

      案例 拦截器中
         preHandle中存放，MDC.put("invokeNo", no);
         postHandle中删除，MDC.remove("invokeNo");

* 6. tomcat中的集中日志文件,主要关注运行catalina.out,和启动的localhost.log

   * localhost_acces log.*.txt   Tomcat存取日志：
		   位于Tomcat log目录下，清晰地记录HTTP服务请求的来源、响应时间、返回的 HTTP 代码等，可用于统计服务的成功数和失败数，也可用于统计接口的响应时间，还可用于统计服务的请求数和吞吐量等。
	
   * catalina.out	Tomcat控制台目志
      	
         即标准输出和标准出错，所有输出到这两个位置的都会进入catalina.out，包含tomcat运行自己输出的日志以及应用里向console输出的日志。
         位于 Tomcat log 目录下，包含 Tomcat是否成功启动、启动所使用的时间，以及应用打印的控制台日志等信息。

   * catalina.{yyyy-MM-dd}.log
         ​tomcat自己运行的一些日志，这些日志还会输出到catalina.out，但是应用向console输出的日志不会输出到catalina.{yyyy-MM-dd}.log。

	* localhost. *. txt	Tomcat 本地日志
		   位于 Tomcat log 目录下，程序异常在没有被捕获时会被一直抛出到容器层，容器处理后记录在这个目志里。
         localhost.{yyyy-MM-dd}.log
         主要是应用初始化(listener, filter, servlet)未处理的异常最后被tomcat捕获而输出的日志，而这些未处理异常最终会导致应用无法启动。

   stdout 输出到控制台，对应linux会输出到catalina.out
		自己配置日志路径	<property name="logbackpath" value="C:/Users/xiaoyuer/Desktop/xye-log"></property>  可行
		C:\Users\xiaoyuer\Desktop\xye-log  不行	

   遇到的日志异常位置：
		tomcat的日志的问题 catalina.out和localhost文件的异常位置
		目前的错误日志问题手动抛出runtimeexcepton是在localhost文件下，jsp页面的错误也在localhost文件下


* 7. logback中   appender是打印器  需要绑定到logger中  而root是根logger
      案例：
         ```
            <logger name="orderValidate" additivity="false">
               <level value="INFO" />
               <appender-ref ref="orderValidate" />
               <appender-ref ref="STDOUT" />      不加控制台不打印，加上打印，不加orderValidate，控制台也不打印，一个appender也不指定，也不会继承root，因为这里已经匹配了相应的logger了
            </logger>
            <root>
               <level value="INFO" />
               <appender-ref ref="STDOUT" />
               <appender-ref ref="ERROR" />
            </root>
         ```
         orderValidate不加STDOUT，控制台是看不到的额，因为指定匹配了logger，没有使用root的
         <logger name="ch.qos.logback" level="ERROR" />  和 root 和日志级别关系    
            root的作用是收集下面所有反馈上来的信息流并根据配置在root中appender进行输出，
            additivity="false"  子Logger不继承父logger的输出，只会在自己的appender里输出，不会在父Logger的appender里输出，不会反馈到root中
            
         <logger name="org.springframework" level="ERROR" />
            这个是单独配置的级别  其中的参数默认覆盖root中的，其他的按照root默认来，会优先匹配logger，匹配不到才使用root的。 

* 8. 这里是工程中以war工程为目录起点，后面可以单独测试下
      <property name="logbackpath" value="../logs/"></property>  作为一个jar包执行的话就是  jar的上级目录
      String path = System.getProperty("user.dir").replace("\\", "/");//获取当前应用所在目录
      path = path.substring(0, path.lastIndexOf("/"));			这里jar目录截取到d盘
      path = path + "/conf/xye-datasource.properties";
      logger.info("datasource-path:{}",path);

* 9. QPS，可以利用tomcat中的access.log中的出现的日志统计到秒的重复次数。(忽略) 
     cat xx.log |grep 'GET /mvc2'|cut -d ' ' -f4|uniq -c|sort -n -r  

10. jar包中日志logger需要和项目中的log.xml匹配，统一logback，不然没有日志	
      成熟的日志框架就 log4j 和logback。   apache的commons logging、slf4j没有日志框架的具体实现，可看做是日志接口框架。
      可以认为 Logback 是slf4j默认的实现
      logback-core,logback-classic,slf4j-api

      logger没有分配级别，那么将从被分配级别的最近的祖先那继承级别。
      rootlogger默认是debug
      logback中的logger有继承机制，配置文件中的additivity为了不继承rootlogger的配置，从而避免输出多分日志
      
      e.printStackTrace()   logger不打印日志,控制台打印 

* 11. 移动端的日志插件(忽略)
   ```
      <script type="text/javascript" src="https://www.w3cways.com/demo/vconsole/vconsole.min.js?v=2.2.0">
      <script>
            var vConsole = new VConsole();
      </script>
      后面直接使用日志打印即可，移动端会有vconsole的显示
      try{
                     
      }catch(err){
         console.log(err)
         console.log(err.message);
      }
   ``` 

* 12. String.format中是%s占位符，log中使用的是{}作为占位符 

* 13. boot中的日志

         问题
            应用中是引入了一个含有logback.xml的jar包，而这个jar包也是使用appclassloader加载的，加载时找到jar包里面的logback.xml，就不会去加载自定义的logback-spring.xml了。

         结论
            SpringBoot首先去查找标准的日志配置文件(logback.xml)，如果找不到在去找拼接Spring的配置的文件(logback-spring.xml)。

         解决方法
            方案一，修改我们的配置文件为logback.xml，这样会首先查找logback.xml
            方案二、避免二方包里面含有logback.xml，这种情况下，无论我们自己的配置是logback-spring.xml还是logback.xml都不会有问题。

## ELK相关
   ELK的适用场景：日志采集器，日志缓冲队列和日志解析器	
	对集群中个节点的日志又不方便通过 Linux 命令行进行聚合查找和统计。通常采用 ELK ( Elasticsearch+Logstash+Kibana ）架构来实现。
	日志缓冲队列是大数据日志处理器系统的核心，连接了日志收集器和日志解析器	

	建立 ELK ( Elasticsearch + Logstash + Kibana ）日志集中分析平台，便于快速搜索、定位日志
			***Elasticsearch***
			ELK
			elasticsearch			日志搜索
			logstash				日志收集汇总分析
			kibana					web界面展示
			三个组件的版本号要一致

### Elasticsearch
		官网 https://www.elastic.co/cn/   
		
		boot中会引入 spring-data-elasticsearch，具体的看官方文档推荐使用的client,We strongly recommend to use the High Level REST Client instead of the TransportClient.
		https://docs.spring.io/spring-data/elasticsearch/docs/4.0.2.RELEASE/reference/html/#reference
		

		官网推荐默认使用的是RestHighLevelClient，添加使用的是indexrequest,搜索使用的是SearchRequest,
		多条件查询使用BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery()，然后.must拼接各种query即可
		最终是sourceBuilder.query(boolQueryBuilder)---> request.source(sourceBuilder)---> client.search(searchRequest)
		查询结构SearchHits hits = response.getHits();然后迭代器迭代后getSourceAsString获取jsonString
		
		其中QueryBuilders是用来生成不同功能的query.
		

   * matchQuery 和 termQuery
         matchQuery：模糊查询，会将搜索词分词，再与目标查询字段进行匹配，若分词中的任意一个词与目标字段匹配上，则可查询到。
         termQuery：精确查询，不会对搜索词进行分词处理，而是作为一个整体与目标字段进行匹配，若完全匹配，则可查询到。

		   如果想通过term查到数据，那么term查询的字段在索引库中就必须有与term查询条件相同的索引词，否则无法查询到结果。
	
   * 分词
		使用ik分词不是万能的，维护词库成本高,直接的做法,就是不分词(7.0版本后使用fieldname.keyword就可以不分词,这里的keyword是=,不是contain)，这里match中的query使用operator.and 也行
		**********
			match
				match中，这里的分词是针对搜索输入的字符串进行分词
			term
				term本身搜索的参数字符串不分词，属性词分词后包含参数串即可
				
				term检索，如果content分词后含有中国这个token，就会检索到
				curl -XPOST http://192.168.1.101:9200/index/fulltext/_search -d’
				{“query” : { “term” : { “content” : “中国” }}}’
		**********

      elasticsearch 里默认的IK分词器是会将每一个中文都进行了分词的切割，所以你直接想查一整个词，或者一整句话是无返回结果的。
		elasticsearch 里默认的分词器(Standard Analyzer)是会将每一个中文都进行了分词的切割，所以你直接想查一整个词  加上.keyword
		
		指定分词三种方式		https://blog.csdn.net/tclzsn7456/article/details/79957221
		elasticsearch 的ik分词器 地址 https://github.com/medcl/elasticsearch-analysis-ik/releases
		关于分词
			实我们只需要关注两个就可以了。
			1、standard				elasticsearch默认
			2、ik分词器				ik_max_word和ik_smart模式
			大家常说ElasticSearch中内置的分词器standard，更确切的说是Lucene内置的，ES是Lucene提供支持的。
			
			直接将里ik分词的解压到plugins\ik 即可,
			
		*****match query中查询，operator：表示单个字段如何匹配查询条件分词，默认是or，设置成and就是全部满足*****
		minimum_should_match：当operator参数设置为or时，该参数用来控制应该匹配的分词的最少数量；
		GET _search
				{
				  "query": {
					"match":{
					  "name":{"query":"保家", "operator": "and"}
				  }
				}

		伪代码是：
		"query":{  
					  "match":{  
						 "eventname":"Microsoft Azure Party"
					  }
				
		“Microsoft Azure Party”，被分析器分词之后，产生三个小写的单词：microsoft，azure和party，

		if (doc.eventname contains "microsoft" and doc.eventname contains "azure" and doc.eventname contains "party") 
		return doc

      分词，使用分词

         在向Elasticsearch中插入数据之前就要给需要的进行分词的属性标记好解析，然后再插入数据即可解决问题！	先分词，再插入数据	
         Elasticsearch的 mapping 一旦新建 只能新增字段 不能修改		
         文档写入的时候会根据字段设置的分词器类型进行分词，如果不指定就是默认的standard分词器。
         写时分词器需要在mapping中指定，而且一旦指定就不能再修改，若要修改必须重建索引。
         读写采用一致的分词器

			CreateIndexRequest 创建索引，确定分词相关，主要是配置index下的settings和mapping结构
			createIndexRequest 搭配XContentBuilder使用可以初始化设置某个字段的分词使用，也是是手工指定字段的分词属性
			
			配置CreateIndexRequest的settings和mapping，其中mapping可以配置相关的field的分词使用，注意是配置某个index下_mapping属性

         查看具体的分词结果
			GET _analyze
				{
				  "text": "hello world, java spark",
				  "analyzer": "standard"
				}
         后台分词配置(配置中配置了field的分词后，查询的时候会按照配置分词分词)
         PUT /user/_mapping
         {
            "properties": {"family":{"type":"text","analyzer": "ik_max_word"}}
         }


   * 高亮搜索
			//千万记得要记得判断是不是为空,不然你匹配的第一个结果没有高亮内容,那么就会报空指针异常
			搜索高亮显示，追加highlight，针对field对应字段拼接pre_tags，post_tags，到html页面显示
			
				 "highlight": {
						"pre_tags" : "<font style=\"color:red;\">",
						"post_tags" : "</font>",
						"fields" : {
							"name" : {}
						}
					}
			
			查询结果出来直接高亮显示到html 
					HighlightField name = next.getHighlightFields().get("name");
					String fragments = name.getFragments()[0].toString();

   * 排序
		   searchSourceBuilder.sort(entry.getKey(), entry.getValue());    依次添加排序规则即可   value 是自带的 SortOrder 枚举类 直接追加sort即可

   * es 初始化配置相关代码段

      ```
         String esHost = "192.168.6.153";
               RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(esHost, 9200, "http")));
               
               //1.索引的settings设置
               Builder settings = Settings.builder()
                     .put("number_of_shards",1)//分片数
                     .put("number_of_replicas",1);//备份数
               
               //2.索引的mapping结构设置
               XContentBuilder mappings = JsonXContent.contentBuilder()
                     .startObject()
                        .startObject("properties")
                           .startObject("req_title")
                              .field("type","text")
                              .field("analyzer", "ik_max_word")
                           .endObject()
                        .endObject()
                     .endObject();
               
               //3.将settings和mappings封装到一个request对象中
               CreateIndexRequest createIndexRequest = new CreateIndexRequest(Es.INDEX_REQUIRE)
                     .settings(settings)
                     .mapping(mappings);
               
               //4.通过client对象连接ES并执行创建索引
               client.indices().create(createIndexRequest, RequestOptions.DEFAULT);
                     
         *******重点********
            queryString  和doc中的field   的分词后几个词语，能找到相等的，就命中。注意这里不是contain关系，而是分词结果存在equals关系
         *******重点********
                     
         查看某个index下的_mapping的配置
         http://localhost:9200/user/_mapping/		
      
      ```



#### Elasticsearch: 权威指南	   处理人类语言，词汇识别，标准分析器          
   官方文档地址	https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html
   es中文社区		https://elasticsearch.cn/  
   
   Elasticsearch 是一个分布式、可扩展、实时的开源的搜索与数据分析引擎，建立在全文搜索引擎库 Apache Lucene 基础之上,。特点在于搜索、分析数据

   Lucene非常复杂，Elasticsearch 是Java编写的，它的内部使用 Lucene 做索引与搜索，通过隐藏 Lucene 的复杂性，使用简单一致的 RESTful API，简化全文检索
      1.一个分布式的实时文档存储，每个字段 可以被索引与搜索
      2.一个分布式实时分析搜索引擎
      3.能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据
            
   它被用作全文检索、结构化搜索(比如查询 30 岁以上的员工)、分析以及这三个功能的组合
   相对数据库等，能实时地做到组合操作，而不经过大型批处理的任务，更加高效。
   功能操作，全文检索、处理同义词、通过相关性给文档评分，从同样的数据中生成分析与聚合数据等
   
   Elasticsearch 是面向文档的，意味着它存储整个对象或文档。
   Elasticsearch 不仅存储文档，而且 索引 每个文档的内容，使之可以被检索。在 Elasticsearch 中，我们对文档进行索引、检索、排序和过滤—?而不是对行列数据。支持复杂全文检索
   文档的序列化格式使用的是json，其中存进去的对象也是json格式
   
   Elasticsearch交互插件
      Sense 是一个 Kibana应用它提供交互式的控制台，通过你的浏览器直接向 Elasticsearch 提交请求。	
      老版本安装Sense是OK的，新版本Kibana已经不需要安装Sense，已经换成了devTools
   
   在 Windows 上面运行 Elasticseach，你应该运行 bin\elasticsearch.bat 而不是 bin\elasticsearch 。
   当ElasticSearch的节点启动后，它会利用多播(multicast)(或者单播，如果用户更改了配置)寻找集群中的其它节点

   Elasticsearch 的潜力，你需要理解以下三个概念：
      映射（Mapping）						描述数据在每个字段内如何存储
      分析（Analysis）					全文是如何处理使之可以被搜索的
      领域特定查询语言（Query DSL）		Elasticsearch 中强大灵活的查询语言
		
   * 核心概念
         1.Cluster：集群。
            ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。
         2.Node：节点。
            形成集群的每个服务器称为节点。
         3.Shard：分片。
            当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。 
            当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。
         4.Replia：副本。
            为提高查询吞吐量或实现高可用性，可以使用分片副本。 
            副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。 
            当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。
         5.全文检索。
            全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。 
            全文索引就是把内容根据词的意义进行分词，然后分别创建索引，
            例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。
                  
         mysql和es的相似概念
            1. 关系型数据库中的数据库（DataBase），等价于ES中的索引（Index） 
            2. 一个数据库下面有N张表（Table），等价于1个索引Index下面有N多类型（Type）， 
            3. 一个数据库表（Table）下的数据由多行（ROW）多列（column，属性）组成，等价于1个Type由多个文档（Document）和多Field组成。 
            4. 在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。与之对应的，在ES中：Mapping定义索引下的Type的字段处理规则，即索引如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等。 
            5. 在数据库中的增insert、删delete、改update、查search操作等价于ES中的增PUT/POST、删Delete、改_update、查GET.



         文档
            倒排索引的检索性能是非常快的，但是在字段值排序时却不是理想的结构。
               在搜索的时候，我们能通过搜索关键词快速得到结果集。
               当排序的时候，我们需要倒排索引里面某个字段值的集合。换句话说，我们需要 转置 倒排索引。
               
            在 Elasticsearch 中，Doc Values 就是一种列式存储结构，默认情况下每个字段的 Doc Values 都是激活的，Doc Values 是在索引时创建的，
            当字段索引时，Elasticsearch 为了能够快速检索，会把字段的值加入倒排索引中，同时它也会存储该字段的 Doc Values。	
               
               
         分布式搜索
            在search 接口返回一个 page 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为 query then fetch
               
            分析器 
               默认是standard分词器
               两个类型不能定义冲突的字段
               如果有两个不同的类型，每个类型都有同名的字段，但映射不同（例如：一个是字符串一个是数字），将会出现什么情况？
               简单回答是，Elasticsearch 不会允许你定义这个映射。当你配置这个映射时，将会出现异常。
            
            
            文档字段和属性
               type	字段的数据类型，例如 string 或 date
               index	字段是否应当被当成全文来搜索（ analyzed ），或被当成一个准确的值（ not_analyzed ），还是完全不可被搜索（ no ）
               analyzer	确定在索引和搜索时全文字段使用的 analyzer
            元数据，_source
               _source 字段存储代表文档体的JSON字符串
               如果没有 _source 字段，部分 update 请求不会生效。
               在一个搜索请求里，你可以通过在请求体中指定 _source 参数，来达到只获取特定的字段的效果
               GET /_search
               {
                  "query":   { "match_all": {}},
                  "_source": [ "title", "created" ]
               }
            元数据，_all
               一个把其它字段值当作一个大字符串来索引的特殊字段。 query_string 查询子句(搜索 ?q=john )在没有指定字段时默认使用 _all 字段
               _all 字段在新应用的探索阶段，当你还不清楚文档的最终结构时是比较有用的
               GET /_search
               {
                  "match": {
                     "_all": "john smith marketing"
                  }
               }
               _all 字段仅仅是一个 经过分词的 string 字段。它使用默认分词器来分析它的值，不管这个值原本所在字段指定的分词器
            元数据,文档标识
               _id 	文档的 ID 字符串
               _type 	文档的类型名
               _index 	文档所在的索引
               _uid	_type 和 _id 连接在一起构造成 type#id
            默认映射
               一个索引中的所有类型共享相同的字段和设置。
               _default_ 映射，是新类型的模板，更加方便地指定通用设置，而不是每次创建新类型时都要重复设置。 除非类型在自己的映射中明确覆盖这些设置
               PUT /my_index
               {
                  "mappings": {
                     "_default_": {
                        "_all": { "enabled":  false }
                     },
                     "blog": {
                        "_all": { "enabled":  true  }
                     }
                  }
               }
            重新索引数据
               从Elasticsearch v2.3.0开始， Reindex API 被引入。它能够对文档重建索引而不需要任何插件或外部工具。
               用新的设置创建新的索引并把文档从旧的索引复制到新的索引。
               
         倒排索引包含一个有序列表，列表包含所有文档出现过的不重复个体，或称为 词项 ，对于每一个词项，包含了它所有曾出现过文档的列表。
         它会保存每一个词项出现过的文档总数， 在对应的文档中一个具体词项出现的总次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度。
         Elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录.


   * es的优点
			1）分布式实时文件存储，可将每一个字段存入索引，使其可以被检索到。 
			2）实时分析的分布式搜索引擎。 
			分布式：索引分拆成多个分片，每个分片可有零个或多个副本。集群中的每个数据节点都可承载一个或多个分片，并且协调和处理各种操作； 负载再平衡和路由在大多数情况下自动完成。 
			3）可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。也可以运行在单台PC上（已测试） 
			4）支持插件机制，分词插件、同步插件、Hadoop插件、可视化插件等。
					
			使用案例	
				1） 2013年初，GitHub抛弃了Solr，采取ElasticSearch 来做PB级的搜索。 “GitHub使用ElasticSearch搜索20TB的数据，包括13亿文件和1300亿行代码”。
				2）维基百科：启动以elasticsearch为基础的核心搜索架构。 
				3）SoundCloud：“SoundCloud使用ElasticSearch为1.8亿用户提供即时而精准的音乐搜索服务”。 
				4）百度：百度目前广泛使用ElasticSearch作为文本数据分析
			
   * 操作介绍
         版本选择7.6.1，7.6.0版本启动有问题
         1.启动es
            启动es，不动了enter继续
            直接输入http://localhost:9200  可以看到相关的es信息	 
         
         2.启动kibana成功  
            http://localhost:5601/
            使用浏览器访问例如：localhost:5601 默认端口，进入首页
            *Discover：日志管理视图 主要进行搜索和查询
            *Visualize：统计视图 构建可视化的图表
            *Dashboard：仪表视图 将构建的图表组合形成图表盘
            Timelion：时间轴视图 随着时间流逝的数据
            APM：性能管理视图 应用程序的性能管理系统
            Canvas：大屏展示图
            Dev Tools： 开发者命令视图 开发工具
            Monitoring：健康视图 请求访问性能预警
            *Management：管理视图 管理工具
			
   * 常用api操作
         #创建索引	PUT /bamboo 
         #删除索引	DELETE /bamboo	
         #添加一条数据
            PUT /bamboo/_doc/1
            {
               "name":"张三",
               "age":18,
               "created":"2018-12-25"
            }					
         #查看数据	GET /bamboo/_doc/100
            
         后端全写
            curl -XGET 'localhost:9200/_count?pretty' -d '				#计算集群中文档的数量
            {
               "query": {
                  "match_all": {}
               }
            }'
            
            这的类似_count，是path，是API 的终端路径。Path 可能包含多个组件，例如：_cluster/stats 和 _nodes/stats/jvm 。
            类似的?pretty，是QUERY_STRING，是任意可选的查询字符串参数 (例如 ?pretty 将格式化地输出 JSON 返回值，使其更容易阅读)
            地址下面的json，是BODY，一个 JSON 格式的请求体 (如果请求需要的话)
            
         缩写是
            GET /_count
            {
               "query": {
                  "match_all": {}
               }
            }
               

   * 索引
      存储数据到 Elasticsearch 的行为叫做索引
      一个 Elasticsearch集群可以 包含多个索引 ，相应的每个索引可以包含多个类型。这些不同的类型存储着多个文档，每个文档又有 多个 属性 。
      
      索引(名词)：索引库，一个索引类似于传统关系数据库中的一个数据库 ，是一个存储关系型文档的地方。 索引 (index) 的复数词为 indices 或 indexes 。
      索引(动词)：索引一个文档就是存储一个文档到一个索引(名词)中以便被检索和查询。类似于SQL中的INSERT(文档已存在时，新文档会替换旧文档)。

      * 倒排索引：
         关系型数据库通过增加一个索引,比如一个B树(B-tree)索引,到指定的列上，以便提升数据检索速度。
         Elasticsearch和Lucene 使用了一个叫做 倒排索引 的结构来达到相同的目的。
            默认一个文档中的每一个属性都是被索引的(有一个倒排索引)和可搜索的。一个没有倒排索引的属性是不能被搜索到的。
         
         PUT /megacorp/employee/1						#这里megacorp表示索引名称，employee表示类型名称，1表示对应数据ID，这个操作就是索引一个雇员文档到索引库中
         {
            "first_name" : "John",
            "last_name" :  "Smith",
            "age" :        25,
            "about" :      "I love to go rock climbing",
            "interests": [ "sports", "music" ]
         }
         
         这里使用POST /megacorp/employee/ 			会自动生成id串

         和原来的正排索引是，找标题，进去找详细字段，倒排就是先找到详细字段，然后关联到具体的文档。
         Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。
         一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表
         
         为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。
         查询出相关性高的靠前
         
         你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。

      * 索引和分片
         一个运行中的 Elasticsearch 实例称为一个节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。
            
         一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。
         
         往 Elasticsearch 添加数据时需要用到 索引 —— 保存相关数据的地方。 索引实际上是指向一个或者多个物理 分片 的 逻辑命名空间 。
         一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。
         索引在默认情况下会被分配5个主分片
         一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。
			
         索引管理
            创建一个索引
               PUT /my_index
               {
                  "settings": {any settings},
                  "mappings": {"type_one": {any mappings}}
               }
               
               禁止自动创建索引	config/elasticsearch.yml 中配置	action.auto_create_index: false
            
            删除索引 
               DELETE /my_index
               DELETE /index_one,index_two
               DELETE /_all，DELETE /*，DELETE /index_*	  一般不要用
               
            索引设置
               number_of_shards	每个索引的主分片数，默认值是 5 。这个配置在索引创建后不能修改。
               number_of_replicas	每个主分片的副本数，默认值是 1 。对于活动的索引库，这个配置可以随时修改。
               
               #创建只有 一个主分片，没有副本的小索引
               PUT /my_temp_index
               {
                  "settings": {
                     "number_of_shards" :   1,
                     "number_of_replicas" : 0
                  }
               }
               
               #update-index-settings API 动态修改副本数：
               PUT /my_temp_index/_settings
               {
                  "number_of_replicas": 1
               }


   * 查询部分
         简单路径查询 和请求体查询(推荐)
            GET /megacorp/employee/1									简单查询
            GET /megacorp/employee/_search								查询所有文档
            GET /megacorp/employee/_search?q=first_name:jane			查询特定条件文档，query
               等价于查询表达式如下
               GET /megacorp/employee/_search
                  {"query" : {"match" : {"last_name" : "Smith"}}}		#使用match_phrase进行精确的匹配短语
                  
               复杂的查询，这里只是用一个过滤器执行一个范围查询，并复用之前的match查询
                  GET /megacorp/employee/_search
                  {
                     "query" : {
                        "bool": {
                           "must": {"match" : {"last_name" : "smith" }},
                           "filter": {"range" : {"age" : { "gt" : 30 }}} 		# gt表示_大于
                        }
                     }
                  }
            GET /_cluster/health										#集群健康 ， 主要是status字段中展示为green、yellow 或者 red 。
               green	所有的主分片和副本分片都正常运行。
               yellow	所有的主分片都正常运行，但不是所有的副本分片都正常运行。
               red	有主分片没能正常运行。
               
            GET /megacorp/employee/1?_source=age,first_name				只查询固定属性的内容,只返回文档一部分
            GET /megacorp/employee/1/_source							只查询_source内容
            POST /website/blog/1/_update								在_source中添加新的字段
               {"doc":{"habbit" : "pinpong"}}
            
            POST /website/pageviews/1/_update?retry_on_conflict=5 
               版本更新，防冲突，指定了失败的重试次数，默认是0
               
               对于增量操作无关顺序的场景(比如递增计数器)，就没必要解决冲突，每次都是+1
               
         最大的差异在于代表 精确值 （它包括 string 字段）的字段和代表 全文 的字段。
         这个区别非常重要——它将搜索引擎和所有其他数据库区别开来。		
					
   * 全文搜索
         Elasticsearch 默认按照相关性得分排序，即每个文档跟查询的匹配程度
         Elasticsearch在全文属性上搜索并返回相关性最强的结果(这种相关性区别于传统关系型数据库，数据库中的一条记录要么匹配要么不匹配)。
         比如搜索rock climbing，最相关的是rock climbing，但是rock albums中的rock也弱相关，也能被搜索出来。这样就是会有两个匹配的文档
         
         精确匹配，在query中使用match_phrase来查询，精确匹配单词或者短语
         
         高亮
            GET /megacorp/employee/_search
            {
               "query" : {"match_phrase" : {"about" : "rock climbing"}},
               "highlight": {"fields" : {"about" : {}}}
            }
         在搜索出来的about内容中，将搜索词rock climbing高亮显示。简单说就是将fields中的某个内容中的搜索词高亮显示
         包含了about属性匹配的文本片段，并以 HTML 标签 <em></em> 封装	

         聚合查询，暂不细看
         
         Elasticsearch 天生就是分布式的，并且在设计时屏蔽了分布式的复杂性，直接使用即可。内部细节暂不细看
			
				
   * 对象和文档
      对象和文档 是可以互相替换的。
      不过，有一个区别： 一个对象仅仅是类似于 hash 、 hashmap 、字典或者关联数组的 JSON 对象，对象中也可以嵌套其他的对象。
      在 Elasticsearch 中，术语 文档 有着特定的含义。它是指最顶层或者根对象, 这个根对象被序列化成 JSON 并存储到 Elasticsearch 中，指定了唯一 ID。
      
      分片扩容啥的，暂不细看
      
      *****我们的应用程序根本不应该关心分片，对于应用程序而言，只需知道文档位于一个索引内。*****
      
		* 文档元数据	
				_index 		文档在哪存放
					名字必须小写，不能以下划线开头，不能包含逗号
				_type		文档表示的对象类别，可在索引中对数据进行逻辑分区
					命名可以是大写或者小写，但是不能以下划线或者句号开头，不应该包含逗号
				_id			文档唯一标识
					和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档
				
		* 文档操作
				文档重复覆盖
				当索引一个文档的时候，文档会被存储到一个主分片中
				Elasticsearch中文档不可改变，不能修改。相反，如果想要更新现有的文档，需要重建索引或者进行替换。会增加了_version 字段值
				在内部，Elasticsearch已将旧文档标记为已删除，并增加一个全新的文档。 尽管你不能再对旧版本的文档进行访问，但它并不会立即消失。当继续索引更多的数据，Elasticsearch 会在后台清理这些已删除文档。
					
				创建新文档对象，
					PUT /megacorp/employee/1?op_type=create						#如果不是新文档会报错
					PUT /megacorp/employee/1/_create							#如果不是新文档会报错
					POST /website/blog/											#这种生成随机id的方式，可以每次创建一个新文档
					
				删除文档
					DELETE /website/blog/123
					和更新操作一样，删除文档不会立即将文档从磁盘中删除，只是将文档标记为已删除状态。随着你不断的索引更多的数据，Elasticsearch 将会在后台清理标记为已删除的文档。
					
				乐观控制文档
					Elasticsearch 是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。
					Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许 顺序是乱的 。
					Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。
					Elasticsearch 使用这个 _version 号来确保变更以正确顺序得到执行
					
					PUT /website/blog/1?version=1 这里也是版本号的冲突控制，
						所有文档的更新或删除 API，都可以接受 version 参数，这允许你在代码中使用乐观的并发控制，这是一种明智的做法。
						
				文档更新，新增字段
					update 请求最简单的一种形式是接收文档的一部分作为 doc 的参数， 它只是与现有的文档进行合并。对象被合并到一起，覆盖现有的字段，增加新的字段。 
					#给文档增加字段，这里是在_source中新增字段
					POST /website/blog/1/_update
					{
					   "doc" : {
						  "tags" : [ "testing" ],
						  "views": 0
					   }
					}

				对象查找
					批量查询，将多个查询请求合并成一个，集中输出
					multi-get 或者 mget API   实现合并查找多个文档。
						每个文档都是单独检索和报告的，即使某个字段没有查出来，也不影响整体的显示
					GET /_mget
					{
					   "docs" : [
						  {
							 "_index" : "website",
							 "_type" :  "blog",
							 "_id" :    2
						  },
						  {
							 "_index" : "website",
							 "_type" :  "pageviews",
							 "_id" :    1,
							 "_source": "views"						#指定要查询的字段
						  }
					   ]
					}
					#可以单独覆盖
					GET /website/blog/_mget
					{
					   "docs" : [
						  { "_id" : 2 },
						  { "_type" : "pageviews", "_id" :   1 }
					   ]
					}
					# 相同_index 和 _type 
					GET /website/blog/_mget
					{"ids" : [ "2", "1" ]}
					
				批量操作
					bulk API 允许在单个步骤中进行多次 create 、 index 、 update 或 delete 请求。 暂不细看
					整个批量请求都需要由接收到请求的节点加载到内存中，因此该请求越大，其他请求所能获得的内存就越少。 
					最佳点 ：通过批量索引典型文档，并不断增加批量大小进行尝试。 当性能开始下降，那么你的批量大小就太大了。
					一个好的办法是开始时将 1,000 到 5,000 个文档作为一个批次, 如果你的文档非常大，那么就减少批量的文档个数。
					
					
				新建、索引和删除 请求都是 写 操作， 必须在主分片上面完成之后才能被复制到相关的副本分片

				主分片执行请求成功后，会将请求并行转发到副本分片上，一旦所有的副本分片都报告成功,主分片向协调节点报告成功，协调节点向客户端报告成功
				在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的
				


   * 其他
      搜索（search） 可以做到：
         在类似于 gender 或者 age 这样的字段上使用结构化查询，join_date 这样的字段上使用排序，就像SQL的结构化查询一样。
         全文检索，找出所有匹配关键字的文档并按照_相关性（relevance）_ 排序后返回结果。
				
      空搜索
         GET /_search			简单地返回集群中所有索引下的所有文档
            返回的文档是按照 _score 降序排列的
            max_score 值是与查询所匹配文档的 _score 的最大值
						
      多索引，多类型查询
         /_search					在所有的索引中搜索所有的类型
         /gb/_search					在 gb 索引中搜索所有的类型
         /gb,us/_search				在 gb 和 us 索引中搜索所有的文档
         /g*,u*/_search				在任何以 g 或者 u 开头的索引中搜索所有的类型
         /gb/user/_search			在 gb 索引中搜索 user 类型
         /gb,us/user,tweet/_search	在 gb 和 us 索引中搜索 user 和 tweet 类型
         /_all/user,tweet/_search	在所有的索引中搜索 user 和 tweet 类型
         
      分页
         默认hits 数组中只有 10 个文档。多了要翻页
            Elasticsearch 接受 from 和 size 参数：
               size	显示应该返回的结果数量，默认是 10
               from	显示应该跳过的初始结果数量，默认是 0
               
               GET /_search?size=5
               GET /_search?size=5&from=5
					
      轻量搜索		不推荐直接向用户暴露查询字符串搜索功能，可读性差。
         1.一种是 “轻量的” 查询字符串 版本，要求在查询字符串中传递所有的参数，
            GET /_all/tweet/_search?q=tweet:elasticsearch									#查询在 tweet 类型中 tweet 字段包含 elasticsearch 单词的所有文档：
         2.请求体查询，是更完整的请求体版本，要求使用 JSON 格式和更丰富的查询表达式作为搜索语言。
            Full-Body Search，大部分参数是通过 Http 请求体而非查询字符串来传递的。
         
               GET /_search?q=+name:john + tweet:mary
               
               + 前缀表示必须与查询条件匹配。类似地， - 前缀表示一定不与查询条件匹配。没有 + 或者 - 的所有其他条件都是可选的——匹配的越多，文档就越相关。
            
            *****_all字段索引*****
            当索引一个文档的时候，Elasticsearch 取出所有字段的值拼接成一个大的字符串，作为 _all 字段进行索引	
            {
               "tweet":    "However did I manage before Elasticsearch?",
               "date":     "2014-09-14",
               "name":     "Mary Jones",
               "user_id":  1
            }
            这就好似增加了一个名叫 _all 的额外字段："However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1"
            除非设置特定字段，否则查询字符串就使用 _all 字段进行搜索。
            _all 字段是 string 类型的。
            
         更多地使用功能全面的 request body 查询API
				
   *  基本查询数据
         Elasticsearch 中的数据可以概括的分为两类：精确值和全文。
         精确值 
            精确值很容易查询。结果是二进制的：要么匹配查询，要么不匹配
         另一方面，全文是指文本数据。意思是模糊匹配
            少对全文类型的域做精确匹配。比如搜索 jump ，会匹配 jumped ， jumps ， jumping 
					
			分析与分析器
				分析包含下面的过程，是分析器执行的工作
				1.首先，将一块文本分成适合于倒排索引的独立的词条 ，
				2.之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall

				
				分析器 实际上是将三个功能封装到了一个包里：字符过滤器、分词器和Token 过滤器。
				es默认使用标准分析器
				
				测试分析器
					GET /_analyze
					{
					  "analyzer": "standard",
					  "text": "Text to analyze"
					}

			使用分词机制
				当我们索引一个文档，它的全文域被分析成词条以用来创建倒排索引。 
				但是，当我们在全文域搜索的时候，我们需要将查询字符串通过相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。

				全文查询：
				当你查询一个全文域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。
				当你查询一个精确值域时，不会分析查询字符串，而是搜索你指定的精确值。	

				查询案例
				GET /_search?q=2014-09-15        # 12 results !    这个默认是_all
				GET /_search?q=date:2014-09-15
				
				date 域包含一个精确值：单独的词条 2014-09-15。
				_all 域是一个全文域，所以分词进程将日期转化为三个词条： 2014， 09， 和 15	

				GET /megacorp/_mapping  也可查看各个字段的映射
				
			映射。string类型全文检索
				默认，string 类型域会被认为包含全文。就是说，它们的值在索引前，会通过一个分析器，针对于这个域的查询在搜索前也会经过一个分析器。
				string 域映射的两个最重要属性是 index 和 analyzer 。默认的解析器是standard 解析器
				index 属性控制怎样索引字符串。它可以是下面三个值：
					analyzed 		首先分析字符串，然后索引它。换句话说，以全文索引这个域。String域默认属性
					not_analyzed	索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。精确值
					no				不索引这个域。这个域不会被搜索到。
				其他简单类型（例如 long ， double ， date 等）也接受 index 参数，但有意义的值只有 no 和 not_analyzed ， 因为它们永远不会被分析。

				映射的cats会分词成cat自动去掉复数形式

			请求体查询
				用来分页
				GET /_search
				{
				  "from": 30,
				  "size": 10
				}
				
				GET /_search
				{
					"query": {
						"match": {
							"first_name": "JohnPost"
						}
					}
				}
				
				复合语句查询
					一个 bool 语句 允许在你需要的时候组合其它语句，无论是 must 匹配、 must_not 匹配还是 should 匹配，同时它可以包含不评分的过滤器（filters）：
					一条复合语句可以将多条语句?—?叶子语句和其它复合语句?—?合并成一个单一的查询语句。
					一条复合语句可以合并任何其它查询语句，包括复合语句。这就意味着，复合语句之间可以互相嵌套，可以表达非常复杂的逻辑。
					{
						"bool": {
							"must":     { "match": { "tweet": "elasticsearch" }},
							"must_not": { "match": { "name":  "mary" }},
							"should":   { "match": { "tweet": "full text" }},
							"filter":   { "range": { "age" : { "gt" : 30 }} }
						}
					}
					
				查询与过滤
					Elasticsearch 使用的查询语言（DSL）拥有一套查询组件.过滤情况（filtering context,简单筛选，y/n）和查询情况（query context，简单筛选+匹配度）
					查询到的相关程度用_score表示，相关性的概念是非常适合全文搜索的情况，因为全文搜索几乎没有完全 “正确” 的答案。
					
					相似的，如果
					
					过滤查询和评分查询
						过滤查询(Filtering query),只是简单的检查包含或者排除,不评分,只过滤情况下的查询.速度很快		将会被缓存	
						评分查询(scoring query),不仅仅要找出匹配的文档，还要计算每个匹配文档的相关性				不会缓存
							
						过滤（filtering）的目标是减少那些需要通过评分查询（scoring queries）进行检查的文档。
						
						filter、filtering query和non-scoring query差不多，单独地不加任何修饰词地使用 "query" 这个词，我们指的是 "scoring query" 。
						
   * 最重要的查询

      * 基本介绍
         match_all 查询简单的匹配所有文档。在没有指定查询方式时，它是默认的查询。
            { "match_all": {}}
            经常与 filter 结合使用—?例如，检索收件箱里的所有邮件。所有邮件被认为具有相同的相关性，所以都将获得分值为 1 的中性 _score。
         match 查询
            是你可用的标准查询，全文搜索还是精确查询
            如果你在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串
               { "match": { "tweet": "About Search" }}
            如果在一个精确值的字段上使用它，例如数字、日期、布尔或者一个 not_analyzed 字符串字段，那么它将会精确匹配给定的值：
               { "match": { "age":    26           }}
               { "match": { "date":   "2014-09-01" }}
            精确值的查询，你可能需要使用 filter 语句来取代 query，因为 filter 将会被缓存。
         multi_match查询
            可以在多个字段上执行相同的 match 查询：
            {
               "multi_match": {
                  "query":    "full text search",
                  "fields":   [ "title", "body" ]
               }
            }
         range 查询
            找出那些落在指定区间内的数字或者时间：
            {
               "range": {"age": {"gte":  20,"lt":   30}}
            }
         term查询 
            查询被用于精确值匹配，可能是数字、时间、布尔或者那些 not_analyzed 的字符串，对于输入的文本不分析 ，将给定的值进行精确查询。
            { "term": { "age":    26           }}
            { "term": { "date":   "2014-09-01" }}
            
            可以用它处理数字（numbers）、布尔值（Booleans）、日期（dates）以及文本（text）。
            
         terms查询
            terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件：
            { "terms": { "tag": [ "search", "full_text", "nosql" ] }}
            terms 查询对于输入的文本不分析。它查询那些精确匹配的值（包括在大小写、重音、空格等方面的差异）。
            用 term 查询词项 Foo 只要在倒排索引中查找 准确词项 
            term 查询只对倒排索引的词项精确匹配，这点很重要，它不会对词的多样性进行处理（如foo或 FOO）
            如term样的底层查询不需要分析阶段，它们对单个词项进行操作。
            
         exists 查询和 missing查询
            类似sql中的IS_NULL (missing) 和 NOT IS_NULL (exists)，常用于某个字段有值的情况和某个字段缺值的情况
            {
               "exists":   {"field":    "title"}
            }
            
         bool 查询(常用),复合过滤器，组合查询
            将多查询组合在一起，成为用户自己想要的布尔查询
            must		文档必须匹配这些条件才能被包含进来。
            must_not	文档必须不匹配这些条件才能被包含进来。
            should		如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。
               返回的文档可能满足should子句的条件.在一个bool查询中,如果没有must或者filter,有一个或者多个should子句,那么只要满足一个就可以返回.
               minimum_should_match参数定义了至少满足几个子句.
            filter		必须匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。
            {
               "bool": {
                  "must":     { "match": { "title": "how to make millions" }},
                  "must_not": { "match": { "tag":   "spam" }},
                  "should": [
                     { "match": { "tag": "starred" }},
                     { "range": { "date": { "gte": "2014-01-01" }}}
                  ]
                  ************备注***************
                  #如果不想因为文档的时间而影响得分，转为不评分过滤
                  "filter": {
                     "range": { "date": { "gte": "2014-01-01" }} 
                  }
                  #通过多个标准来过滤文档，bool 查询本身也可以被用做不评分的查询，简单地将它放置到 filter 语句中并在内部构建布尔逻辑：
                  "filter": {
                        "bool": { 
                           "must": [
                              { "range": { "date": { "gte": "2014-01-01" }}},
                              { "range": { "price": { "lte": 29.99 }}}
                           ],
                           "must_not": [
                              { "term": { "category": "ebooks" }}
                           ]
                        }
                     }
                  ***************************
               }
            }
            bool 查询会为每个文档计算相关度评分 _score ，再将所有匹配的 must 和 should 语句的分数 _score 求和，最后除以must和should语句的总数。
            must_not 语句不会影响评分；它的作用只是将不相关的文档排除。
            *****当没有 must 语句的时候，至少有一个 should 语句必须匹配。*****
            
         constant_score查询,不常用
            它将一个不变的常量评分应用于所有匹配的文档。它被经常用于你只需要执行一个 filter 而没有其它查询。
            {
               "constant_score":   {
                  "filter": {
                     "term": { "category": "ebooks" } 
                  }
               }
            }
         验证查询，分析查询的sql
            GET /megacorp/_validate/query?explain
            {
               "query": {
                  "match" : {
                     "first_name": "smithpost like pingpong"
                  }
               }
            }
            
  
      * ElasticSearch 5.0以后，String字段被拆分成两种新的数据类型: text用于全文搜索，会分词,而keyword用于关键词搜索，不进行分
            精确查询term
            复合过滤器， bool （布尔）过滤器。 
               可以接受多个其他过滤器作为参数，并将这些过滤器结合成各式各样的布尔（逻辑）组合
               term 和 terms 是 包含（contains） 操作，而非 等值（equals）
               
            基于词项的查询	
               用 term 查询词项 Foo 只要在倒排索引中查找 准确词项 
               term 查询只对倒排索引的词项精确匹配，这点很重要，它不会对词的多样性进行处理（如foo或 FOO）
               如term样的底层查询不需要分析阶段，它们对单个词项进行操作。
            基于全文的查询
               match 或 query_string 这样的查询是高层查询，
                  1.如果查询日期(date)或整数(integer) 字段，它们会将查询字符串分别作为日期或整数对待。
                  2.如果查询一个（ not_analyzed ）未分析的精确值字符串字段，它们会将整个查询字符串作为单个词项对待。
                  3.但如果要查询一个（ analyzed ）已分析的全文字段，它们会先将查询字符串传递到一个合适的分析器，然后生成一个供查询的词项列表
               
            match查询分析
               match 查询都应该会是首选的查询方式。是高级全文查询 ，既能处理全文字段，又能处理精确字段。主要场景还是全文搜索
               GET /my_index/my_type/_search
               {
                  "query": {"match": {"title": "QUICK!"}}
               }
               1.检查字段类型 。
                  标题 title 字段是一个 string 类型（ analyzed ）已分析的全文字段，这意味着查询字符串本身也应该被分析。
               2.分析查询字符串 。
                  将查询的字符串 QUICK! 传入标准分析器中，输出的结果是单个项 quick 。因为只有一个单词项，所以 match 查询执行的是单个底层 term 查询。
               3.查找匹配文档 。
                  用 term 查询在倒排索引中查找 quick 然后获取一组包含该项的文档，本例的结果是文档：1、2 和 3 。
               4.为每个文档评分 。
                  用 term 查询计算每个文档相关度评分 _score ，这是种将词频（term frequency，即词 quick 在相关文档的 title 字段中出现的频率）和反向文档频率（inverse document frequency，即词 quick 在所有文档的 title 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式。参见 相关性的介绍 。
               
            提高精度
               GET /my_index/my_type/_search
               {
                  "query": {
                     "match": {
                        "title": {      
                           "query":    "BROWN DOG!",
                           "operator": "and"
                        }
                     }
                  }
               }
               不去匹配 brown OR dog ，而通过匹配 brown AND dog 找到所有文档。
               match 查询还可以接受 operator 操作符作为输入参数，默认情况下该操作符是 or
            控制精度
               match 查询支持 minimum_should_match 最小匹配参数，可以将其设置为一个百分数，指定必须匹配的词项数用来表示一个文档是否相关
         
            多词 match 查询只是简单地将生成的 term 查询包裹在一个 bool 查询中。
            如果使用默认的 or 操作符，每个 term 查询都被当作 should 语句，这样就要求必须至少匹配一条语句。以下两个查询是等价的：
            {"match": { "title": "brown fox"}}
            和
            {
               "bool": {
               "should": [
                  { "term": { "title": "brown" }},
                  { "term": { "title": "fox"   }}
               ]
               }
            }
            如果使用 and 操作符，所有的 term 查询都被当作 must 语句，所以 所有（all） 语句都必须匹配。以下两个查询是等价的：
            {
               "match": {
                  "title": {
                     "query":    "brown fox",
                     "operator": "and"
                  }
               }
            }
            和
            {
               "bool": {
               "must": [
                  { "term": { "title": "brown" }},
                  { "term": { "title": "fox"   }}
               ]
               }
            }
            GET /megacorp/_mapping 查看系统的配置
            
            bool 查询采取 more-matches-is-better 匹配越多越好的方式，所以每条 match 语句的评分结果会被加在一起，从而为每个文档提供最终的分数 _score
            
            评分的介绍
               GET /_search
               {
                  "query": {
                  "bool": {
                     "should": [
                     { "match": { "title":  "War and Peace" }},
                     { "match": { "author": "Leo Tolstoy"   }},
                     { "bool":  {
                        "should": [
                        { "match": { "translator": "Constance Garnett" }},
                        { "match": { "translator": "Louise Maude"      }}
                        ]
                     }}
                     ]
                  }
                  }
               }
               bool 查询运行每个 match 查询，再把评分加在一起，然后将结果与所有匹配的语句数量相乘，最后除以所有的语句数量。
               处于同一层的每条语句具有相同的权重。在前面这个例子中，包含 translator 语句的 bool 查询，只占总评分的三分之一。
               如果将translator语句与title和author 两条语句放入同一层，那么 title 和 author 语句只贡献四分之一评分。
               
               查询结果_score,相关性
               GET /megacorp/employee/_search
               {  
                  "query": {
                     "bool": {  
                     "should": [  
                     { "match": { "last_name": "Smith" }},
                     { "match": { "age": 25 }}
                     ]  
                  }  
                  }
               } 
            dis_max 即分离 最大化查询，暂时没用到。
               只会简单地使用 单个 最佳匹配语句的评分 _score 作为整体评分。
               

            提升单个字段的权重
            使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ，其中 boost 是一个浮点数：
               {
                  "multi_match": {
                     "query":  "Quick brown fox",
                     "fields": [ "*_title", "chapter_title^2" ] 
                  }
               }
               字段的默认boost值为1
            
            短语匹配
               完整精确匹配短语，整体匹配，先分词，再整体匹配
               GET /my_index/my_type/_search
               {
                  "query": {
                     "match_phrase": {
                        "title": "quick brown fox"
                     }
                  }
               }
               等价于：
               "match": {
                  "title": {
                     "query": "quick brown fox",
                     "type":  "phrase"
                  }
               }
               
            一个 match 查询仅仅是看词条是否存在于倒排索引中，而一个 match_phrase 查询是必须计算并比较多个可能重复词项的位置。
            一个简单的 term 查询比一个短语查询大约快 10 倍，比邻近查询(有 slop 的短语 查询)大约快 20 倍。当然，这个代价指的是在搜索时而不是索引时。
                           
            支持es查询，支持通配符与正则表达式查询
            一个多词查询
            GET /my_index/doc/_search
            {
               "query": {
               "match": {
                  "text": "quick fox"
               }
               }
            }
            同内部重写：
            GET /my_index/doc/_search
            {
               "query": {
               "bool": {
                  "should": [
                  {"term": { "text": "quick" }},
                  {"term": { "text": "fox"   }}
                  ]
               }
               }
            }
         查询时权重提升，使用 boost 调节权重，boost 只是影响相关度评分的其中一个因子；它还需要与其他因子相互竞争
         分词器
            standard 分词器使用 Unicode 文本分割算法来寻找单词 之间 的界限，并且输出所有界限之间的内容
            在分词的时候会将一些复数词还原为词根，停顿词 a  and 等省略。查询的时候近义词也能查出来

   *  排序
         搜索结果中通过 _score 参数返回， 默认排序是 _score 降序
         使用的是 filter （过滤），文档将按照随机顺序返回，并且每个文档都会评为零分。
         
         按照字段顺序排序，
            GET /_search
            {
               "query" : {
                  "bool" : {
                     "filter" : { "term" : { "user_id" : 1 }}
                  }
               },
               #这里是多级排序，先按第一个条件排序，依次往后排序
                  { "date":   { "order": "desc" }},
                  { "_score": { "order": "desc" }}
            }
            
         不根据相关性排序，_score和max_score 为null，计算score开销大，这里使用filter+sort根据字段排序
         如果多条查询子句被合并为一条复合查询语句，比如 bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。
            
         检查score的计算分析
            GET /megacorp/employee/2/_explain
            {
               "query": {
                  "match": {"last_name" : "Smith" }
               }
            }
			
   * boot整合es
      Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，其目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。
         
      ElasticSearch7.X已经弃用Transportclient方式，且在8.0版本中完全移除它。现在用RestHighLevelClient
      RestHighLevelClient文档地址			 https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.4/java-rest-high.html
      引入elasticsearch-rest-high-level-client的jar
			
      * 初始化配置
               1.直接注入bean，后续需要注入使用,注入RestHighLevelClient 
                  @Configuration
                  public class ElasticConfig {
                     @Value("${es.host}")
                     public String host;
                     @Value("${es.port}")
                     public int port;
                     @Value("${es.scheme}")				#http,new HttpHost(HOST, 9200, "http")
                     public String scheme;
                  
                     @Bean
                     public RestHighLevelClient getClient(){
                        RestClientBuilder clientBuilder = RestClient.builder(new HttpHost(HOST, 9200, "http"));
                        RestHighLevelClient client = new RestHighLevelClient(clientBuilder);
                        return  client;
                     }
                  }
               
               2.静态懒加载，后续静态加载
               #这里有种，先mq消费，然后今天常量还没加载完成的情况，需要使用标记确认静态常量的加载标记
                  public class Es {
                     public static RestHighLevelClient client;
                     static {
                        HttpHost httpHost = new HttpHost(Constants.URL_ES, 9200, "http");
                        RestClientBuilder restClientBuilder = RestClient.builder(httpHost);
                        client = new RestHighLevelClient(restClientBuilder);
                     }
                  }
						
			
      * 配置索引等结构
               主要的配置就是setting和mapping两个，其他的就是数据的增删改查
               官网的配置CreateIndexRequest中有详细的配置， https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.4/java-rest-high.html
               private static void initRequireIndex(){
                  try {
                     // sit 192.168.6.153 pre 192.168.6.50 prd 192.168.8.45 
                     String esHost = "192.168.6.153";
                     RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(esHost, 9200, "http")));
                     
                     //1.索引的settings设置
                     Builder settings = Settings.builder()
                           .put("number_of_shards",1)//分片数
                           .put("number_of_replicas",1);//备份数
                     //2.索引的mapping结构设置
                     XContentBuilder mappings = JsonXContent.contentBuilder()
                           .startObject()
                              .startObject("properties")
                                 .startObject("req_title")
                                    .field("type","text")
                                    .field("analyzer", "ik_max_word")
                                 .endObject()
                              .endObject()
                           .endObject();
                     //3.将settings和mappings封装到一个request对象中
                     CreateIndexRequest createIndexRequest = new CreateIndexRequest(Es.INDEX_REQUIRE)
                           .settings(settings)
                           .mapping(mappings);
                     
                     //4.通过client对象连接ES并执行创建索引
                     client.indices().create(createIndexRequest, RequestOptions.DEFAULT);
                  } catch (IOException e) {
                  }
               }
               
               初始化索引配置  
                  主要是配置CreateIndexRequest中的setting和mapping
                  ik_max_word分词词库需要安装
                  private static void initRequireIndex(){
               
                     String esHost = "192.168.6.153";
                     RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(new HttpHost(esHost, 9200, "http")));
                     //1.索引的settings设置
                     Builder settings = Settings.builder()
                           .put("number_of_shards",1)//分片数
                           .put("number_of_replicas",1);//备份数
                     
                     //2.索引的mapping结构设置
                     XContentBuilder mappings = JsonXContent.contentBuilder()
                           .startObject()
                              .startObject("properties")
                                 .startObject("req_title")
                                    .field("type","text")
                                    .field("analyzer", "ik_max_word")
                                 .endObject()
                              .endObject()
                           .endObject();
                     
                     //3.将settings和mappings封装到一个request对象中
                     CreateIndexRequest createIndexRequest = new CreateIndexRequest(Es.INDEX_REQUIRE)
                           .settings(settings)
                           .mapping(mappings);
                     
                     //4.通过client对象连接ES并执行创建索引
                     client.indices().create(createIndexRequest, RequestOptions.DEFAULT);
               }
                  
      * 常用操作
         * 新增数据
            使用IndexRequest，方式有三种，
            map 
               Map<String, Object> jsonMap = new HashMap<>();
               jsonMap.put("user", "kimchy");
               IndexRequest indexRequest = new IndexRequest("twitter").id("1").source(jsonMap);
               client.index(indexRequest, RequestOptions.DEFAULT);
            json 目前用
               IndexRequest request = new IndexRequest("twitter");
               request.id(String.valueOf(twitter.getId()));
               request.source(JSON.toJSONString(twitter), XContentType.JSON);
               client.index(request, RequestOptions.DEFAULT);
            XContentBuilder	不看
            
            批量增加
               public void batchAddDoc() throws IOException {
               BulkRequest bulkRequest = new BulkRequest();
               List<IndexRequest> requests = generateRequests();
               for (IndexRequest indexRequest : requests) {
                  bulkRequest.add(indexRequest);
               }
               BulkResponse responses = client.bulk(bulkRequest, RequestOptions.DEFAULT);
            }
					
			* 查看索引是否存在
					public boolean indexExists(String indexName) {
						GetIndexRequest request = new GetIndexRequest();
						request.indices(indexName);
						return rhlClient.indices().exists(request, RequestOptions.DEFAULT);
					}
					
			* 更新数据
					UpdateRequest request = new UpdateRequest("twitter", "1").doc(jsonMap);
					UpdateResponse updateResponse = client.update(request, RequestOptions.DEFAULT);
					map方式同上，也有三种方式
					
			* 删除数据
					DeleteRequest request = new DeleteRequest("twitter", "1");
					DeleteResponse deleteResponse = client.delete(request, RequestOptions.DEFAULT);

			* 查询数据
					1.SearchSourceBuilder
					2.SearchSourceBuilder组装boolQueryBuilder、分页、高亮等配置
					3.SearchRequest.source(SearchSourceBuilder);
					4.client.search(searchRequest, RequestOptions.DEFAULT);
					
					SearchRequest searchRequest = new SearchRequest(indexName);
					SearchSourceBuilder builder = new SearchSourceBuilder();
					
					BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();  #must，filter，must not，should
					boolQueryBuilder.must(QueryBuilders.matchQuery(entry.getKey(), entry.getValue()).operator(Operator.AND)); 	#match默认是or，这里的and没有顺序，分词错开排序也不影响
					boolQueryBuilder.must(QueryBuilders.rangeQuery(entry.getKey()).to(rangeArr[1]));
						
					builder.query(boolQueryBuilder);
					builder.sort(entry.getKey(), entry.getValue());  		#排序  
					builder.from((xyeEs.getPage().getPno()-1)*length);#分页
					builder.size(length);
					
					高亮
					builder.highlighter(new HighlightBuilder()
							.field("reqTitle",10)
							.preTags("<font style=\"color:red;\">")
							.postTags("</font>")
							);
				
					
					searchRequest.source(builder);
					// 获得response
					SearchResponse searchResponse = null;
					searchResponse = Es.client.search(searchRequest, RequestOptions.DEFAULT);
					
					返回中的高亮返回。
					#Map<String, HighlightField> map = item.getHighlightFields()  ;
					HighlightField reqHighLightField = next.getHighlightFields().get("reqTitle");
					if(reqHighLightField != null) {
						obj.put("highLightField", reqHighLightField.fragments()[0].toString());
					}
					
				   //返回结果集
					//T是es中的mapping实体，建议在插入时定义实体类
					List<T> list = new LinkedList<T>();
					SearchHits hits = searchResponse.getHits();
					long count = hits.getTotalHits().value;				#总记录数
					Iterator<SearchHit> iterator = hits.iterator();
					if (count > 0) {
						//查询条数大于0，计算分页
					}
					SearchHits hits = searchResponse.getHits();
					hits.forEach(item -> list.add(JSON.parseObject(item.getSourceAsString(), T.class)));
						
			* 查询api相关
					// Bool Query 用于组合多个叶子或复合查询子句的默认查询
					// must 相当于 与 & =
					// must not 相当于 非 ~ ！=
					// should 相当于 或 | or
					// filter 过滤
					QueryBuilders.termQuery(“key”, “vaule”); // 完全匹配
					QueryBuilders.termsQuery(“key”, “vaule1”, “vaule2”) ; //一次匹配多个值
					QueryBuilders.matchQuery(“key”, “vaule”) //单个匹配, field不支持通配符, 前缀具高级特性
					QueryBuilders.multiMatchQuery(“text”, “field1”, “field2”); //匹配多个字段, field有通配符忒行
					QueryBuilders.matchAllQuery(); // 匹配所有文件
					
					QueryBuilders
						 QueryBuilders.wildcardQuery(“supplierName”,"*"+param+"*")  模糊查询
						
						 QueryBuilders.matchQuery(“supplierName”,param)(最主要查询)
							分词后精确查询，分词之间or关系,有一个分词匹配即匹配
							match查询，会将搜索词分词，再与目标查询字段进行匹配，若分词中的任意一个词与目标字段匹配上，则可查询到。
							先拆词，后比对
							
						 QueryBuilders.matchPhraseQuery(“supplierName”,param)	
							有序连贯分词模糊查询（任意分词任意数量按照原有顺序连续排列组合可以查出，其他不可查出）
							默认使用 match_phrase 时会精确匹配查询的短语，需要全部单词和顺序要完全一样，标点符号除外。
							先经过analyzer拆词，它与match的区别在于，它会严格按传入内容拆出词条结果的内容+顺序，与源字段词条比对，完全一致才会匹配成功，
						 	
						 QueryBuilders.termQuery(“supplierName”,param)
							查询条件不分词精确匹配分词数据，命中一个分词即匹配
							即两个重点：1. 不拆词 2. 与字段拆词结果词条（token）作equal比对
							term query,输入的查询内容是什么，就会按照什么去查询，并不会解析查询内容，对它分词。
							比如i like eating and kuing， param = “i like eating and kuing” 查不出
							
						fieldName.keyword	
							"match": {"message.keyword": "xxx"}全词匹配
								fieldName.keyword决定是否采用es分词数据源，不带keyword即查询text格式数据（分词），带即查询keyword格式数据（不分词）
						
							ES5.0及以后的版本取消了string类型，将原先的string类型拆分为text和keyword两种类型。它们的区别在于text会对字段进行分词处理而keyword则不会
							
							动态映射 和text类型的keyword映射
								当你没有以IndexTemplate等形式为你的索引字段预先指定mapping的话，
								ES就会使用Dynamic Mapping，通过推断你传入的文档中字段的值对字段进行动态映射。比如传入的字段pirce是12，将映射为long类型
								然而对于不满足ip和date格式的普通字符串来说，情况有些不同：ES会将它们映射为text类型，
								但为了保留对这些字段做精确查询以及聚合的能力，又同时对它们做了keyword类型的映射，作为该字段的fields属性写到_mapping中。
								
							 "about" : {
								  "type" : "text",
								  "fields" : {
									"keyword" : {
									  "type" : "keyword",
									  "ignore_above" : 256
									}
								  }
								}
							es的字段常用类型，
							text，keyword，date
							数据类型，long，Integer，double这些类型可以用term精确匹配
							其他类型不常用
							
							*****动态映射 Dynamic Mapping*****
								动态映射时Elasticsearch的一个重要特性: 不需要提前创建index、定义mapping信息和type类型, 你可以 直接向ES中插入文档数据时, 
								ES会根据每个新field可能的数据类型, 自动为其配置type等mapping信息, 这个过程就是动态映射(dynamic mapping).

								通过查看定义某文档的json格式就能猜测到文档结构，我们称之为自动映射
								字段自动检测，当ES在文档中碰到一个以前没见过的字段时，在某个字段第一次出现时，如果之前没有定义过映射，它会利用动态映射来决定该字段的类型，并自动地对该字段添加映射。
								即索引文档前不需要创建索引、类型等信息，在索引的同时会自动完成索引、类型、映射的创建。
								
						这里的查询场景，和 match查询keyword类型一致						
						
						erm是将传入的文本原封不动地（不分词）拿去查询。
						match会对输入进行分词处理后再去查询，部分命中的结果也会按照评分由高到低显示出来。
						match_phrase是按短语查询，只有存在这个短语的文档才会被显示出来。
						也就是说，term和match_phrase都可以用于精确匹配，而match用于模糊匹配。
						
						如果使用term查询，要确保字段是no analyzed的。建索引的时候要注意。否则建索引的时候分词了，存按单个字存的
						使用中发现一个问题，analyzed字段无法使用term
						match_phrase特点
							match_phrase还是分词后去搜的
							目标文档需要包含分词后的所有词
							目标文档还要保持这些词的相对顺序和文档中的一致
							
					term、match、match_phrase的区别       https://www.cnblogs.com/chenmz1995/p/10199147.html	
					term决定查询词不分词，keyword决定文档中的值不分词
						1.字段设置成keyword的时候查询的时候已有的值不会被分词。				term: "a b" 能匹配到文档中的"a b"串
						2.字段设置成text类型的字段会被分词									term: "a b" 不能匹配文档中的"a b"，因为文档内容被分词了

						term查询keyword字段。
							term不会分词。而keyword字段也不分词。需要完全匹配才可。
						term查询text字段。
							text字段会分词，而term不分词，所以term查询的条件必须是text字段分词后的某一个。
							必须使用foobar.keyword来对foobar字段以keyword类型进行精确匹配。
							
						match查询keyword字段
							match会被分词，而keyword不会被分词，match的需要跟keyword的完全匹配可以。
						match查询text字段
							match分词，text也分词，只要match的分词结果和text的分词结果有相同的就匹配
						match_phrase匹配keyword字段。
							这个同上必须跟keywork一致才可以。
						match_phrase匹配text字段。
							match_phrase是分词的，text也是分词的。match_phrase的分词结果必须在text字段分词中都包含，而且顺序必须相同，而且必须都是连续的。
						
						query_string查询，用的不多，暂不看
						1. term&match
							term: 精确查询，对查询的值不分词,直接进倒排索引去匹配。
							match; 模糊查询，对查询的值分词，对分词的结果一一进入倒排索引去匹配
						
						2. text&keyword
							text: 在写入时，对写入的值进行分词，然后一一插入到倒排索引。
							keyword: 在写入时，将整个值插入到倒排索引中，不进行分词。
					组合查询
						QueryBuilders.boolQuery()
									 .must(QueryBuilders.termQuery(“key”, “value2”))
								     .mustNot(QueryBuilders.termQuery(“key”, “value3”))
									 .should(QueryBuilders.termQuery(“key”, “value4”))
									 .filter(QueryBuilders.termQuery(“key”, “value5”));


### kibana	

      Kibana 是用户界面，可对 Elasticsearch 数据进行可视化

		初始需要创建一个index显示
		切换中文，在config/kibana.yml添加i18n.locale: "zh-CN"
		
		GET /_cat/nodes  查看所有节点
		
		在可视化中创建图表保存，然后在仪表盘集中展示图表

		kibana中的console控制台查询
			这是交集查询语法
				GET _search
					{
					   "query": {
                     "bool":{
                        "must":[{"match":{"_index":"user"}},{"match":{"_id":"1366"}}]
                     }
					   }
					}
		
### logstash 
   logstash中，包括了三个阶段: 输入input --> 处理filter（不是必须的） --> 输出output
   
   就是收集日志，过滤日志，输出日志，实际使用中直接收集error.log 的日志
   
   相当于是将收集的日志关联到了elasticsearch上，然后通过kibana查询输出
   
   一般logstash中的filter使用的是grok的语法规则是:%{语法：语义}，这个过滤器暂不看
   所有文本数据都是在Logstash的message字段中中的，我们要在过滤器里操作的数据就是message

   生产(收集的是error.log文件，input使用的是beats组件)
   input 1.从文件中来    2.系统日志方式   3.filebeats方式 

		
   * 样例配置
			input {
				file {
					path => "C:/Users/xiaoyuer/git/xye-soa-pom/logs/xye.log"
				}
			}

         //filter暂不研究
			filter {
            #定义数据的格式，正则解析日志（根据实际需要对日志日志过滤、收集）
            grok {
               match => { "message" => "%{IPV4:clientIP}|%{GREEDYDATA:request}|%{NUMBER:duration}"}
            }
            #根据需要对数据的类型转换
            mutate { convert => { "duration" => "integer" }}
			}

			output {
				elasticsearch {
					hosts => ["localhost:9200"]
					index => "testlogstash"
				}
				stdout { codec => rubydebug }   #用来测试时,验证有没有启动成功
			}

   * filebeats
			使用beats(更新Filebeat配置文件后，启动filebeats)
			启动beats  ./filebeat -e -c filebeat.yml
			
				beats配置文件
					output.logstash:
						hosts: ["192.168.194.6:5044"]					#读取log文件，发送到服务端的logstash中
						
				然后logstash监听beats端口
					input {
					  beats {
						port => 5044  # 设置专用端口用于接收各个来源的日志
					  }
					}
		
         由于logstash只能收集本机日志，故在其他机器上搭建filbeat，将日志发送给logstash
            input {
               beats {
                  port => 5044
               }
            }
            output {
               elasticsearch {
                  hosts => ["http://localhost:9200"]
                  index => "log-%{+YYYY.MM.dd}"
               }
            }
      




## Git,Svn相关

  ssh   原本的rsa的ssh key,可以使用putty将原先的key转换为ppk，然后再使用tortoise git
   
​	git cherry-pick	用于把另一个本地分支的commit修改应用到当前分支。
​	gitreflog中可以查看回退版本的信息，可以挑选分支进行操作
​	在a merge b    b->a,本源合并  

​	git stash 后切换分支后 apply stash 可能有冲突
​	git stash 后 dev有改动 在apply回来会有冲突
​	stash是本地的恢复操作，目前一般使用是：本地stash 切换分支，回来时本地文件没变apply回来
​	
​	eval `ssh-agent`		//启用ssh-agent
​	ssh-add D:/id_rsa_zhanjun.zhanjun	//添加本地的私钥，使用的类似linux，系统的路径要使用反斜杠
​	git branch -r 			//查看远程分支
​	git remote prune origin 	//刷新远程不存在的分支
​	
​	git恢复先前的版本replace
​	reset实现代码回退、还原  包括分支的tag也可以
​	
​	.gitconfig 文件下需要配置[receive]
​	denyCurrentBranch = ignore才能远程提交

​	git的两个仓库之间传输，需要两个仓库独立存在，pull 和push通过url连接，push需要更新后才能执行
​	git提交到本地仓库以后可以revert远程版本实现回退
​	
​	git的基于a点回退，但是a之后又有b点的提交，切换新分支，根据commitId挑选

​	git版本回退：1、在history中的选中某个分支改动，reset（hard），之后再右键team-remote-push，选择分支强制推送，实现回退。
​		         2、右键replacewith 选择相应的tag实现回退，实现的是将tag版本覆盖本地的dev分支，这时候dev会和远程有git变动
​		         3、可以使用checkout挑出tag到本地，单独操作，也可以新建分支，随意

  tag是基于分支存在的，远程有分支才能pull对应的tag
​	版本的上线合并使用tag ，代码暂时保存需要切换其他分支做点事使用stash，stash会暂存当前的工作区内容，然后将工作区内容保持和上次提交相同，做完切换回来再恢复stash即可

​	svn删除恢复
​	找到删除该文件或者文件夹的版本，在Logmessage里右键Revertthechangesfromthisrevision。
​	实时pull一个别人reset之前的操作，那么本地也是有记录的

   1. github 
      创建文件夹，  /dir/aa.txt  就可以创建dir目录

      github提交报错
      Git报错解决：OpenSSL SSL_read: Connection was reset, errno 10054

      桌面 git bash
      git config --global http.sslVerify "false"

   2. cherry pick 的挑选分支commit，需要同源挑选，不然会出现冲突
      git cherry-pick <commit id＞可以选择某个分支中的一个或几个commit(s）来进行操作

   3. git ls-files -v | grep "^[a-z]"   找出   git 中的 assume-unchanged 的文件(忽略)

   4. fork：在github页面，点击fork按钮，将别人的仓库复制一份到自己的仓库。
      clone：直接将github中的仓库克隆到自己本地电脑中						clone别人是无法改动提交的，因为没有权限
      pull request的作用
      在A的仓库中fork项目B （此时我们自己的github就有一个一模一样的仓库B，但是URL不同）将我们修改的代码push到自己github中的仓库B中pull request ，源头url仓库就会收到请求，并决定要不要接受你的代码
      
      两个分支针对同源都做了更改，合并的时候就会出问题
      冲突merge之后，就会标记没有变动，重复merge之后没变化
      merge可以提现时间线，rebase是重新设基会覆盖时间线。
      
      永远不要在所有人都在的公共开发分支上做 rebase 操作。一般情况下在临时分支上是需要 rebase 主分支代码的，而 merge 则主要用在主分支上将临时分支的代码合并过来，然后就可以删除临时分支了。暂时也不使用，先过。

   5. 做一个回退的merge测试，目前回退  更新   pick
      1.A->B  同源
      2.A回退，改动提交
      3.B cherry pick A,这时会有冲突，因为改动已经不是基于B一开始的源头了,B的源头已经改变了。

      同源测试更新是可以的，同源就是在原文件上，没有改动，单一改动，merge过来。
      
   6. reset命令相关
         reset mix  重置index，保留本地改动在working tree
               –mixed:默认参数，可选择不写，保留当前所有代码，包括工作区和暂存区，并将这些代码一并放入工作区，只是HEAD指向发生了变化，指向命令指定的版本。
               会将已经commit的改动放到工作区，	
         hard  重置index,还原清除本地改动
            会丢弃工作区和暂存区的修改，对于未追踪的文件没有影响。
         soft  不用，不实用


## 设计模式

### 单例模式
	单例模式的要点：
			某个类只能有一个实例
				构造器私有化
			类必须自行创建整个实例					 (内存实例化对象)
				含有一个该类的静态变量来保存这个唯一的实例
			类必须自行向整个系统提供这个实例
				对外提供获取该实例对象的方式         (静态方法或者get方法)
	
		饿汉式：直接创建对象
         public class Singleton1 {
            public final static Singleton1 singleton = new Singleton1();
            private Singleton1() {}
         }

         public class Singleton2 {
            public final static Singleton2 INSTANCE;
            static {
               INSTANCE = new Singleton2();
            }
            private Singleton2() {}
         }
		
		第一种形式：饿汉式单例
         public class Singleton {  
            private Singleton(){}  
            private static Singleton instance = new Singleton();  
            public static Singleton getInstance(){  
               return instance;  
            }  
         }  

		第二种形式：懒汉式单例
         public class Singleton {  
            private static Singleton instance = null;  
            private Singleton() {}  
            public static synchronized Singleton getInstance(){  
               if (instance==null) instance＝newSingleton();  
               return instance;  
            }  
         }
		这里使用的是静态方法，也可以使用get方法提供访问吧
	
		枚举法，最简单的单例实现，最简单的单例值枚举
         public enum SingleInstance {
            INSTANCE;
            public void fun1() { 
               // do something
            }
         }
         // 使用
         SingleInstance.INSTANCE.fun1();


## Zookeeper相关

   1. zookeeper=文件系统+监听通知机制，本质是一个精简的文件系统。
         
      用处：可用于服务发现，分布式锁，分布式领导选举，配置管理等。

      监听通知机制
         基于watcher机制，监听znode节点(包括子节点)变化，通知给订阅状态的客户端
			客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。

   2. zookeeper和服务端建立的是长连接，可以定时进行心跳检测
	   zookeeper通常有2n+1台server组成。用作投票机制

   3. 主要的开源框架	两款开源框架ZKClient和Curator。可以用来操作zookeeper
         2.2.0版本改为基于zkclient实现，需增加zkclient的依赖包，
         2.3.0版本增加了基于curator的实现，作为可选实现策略
     
   4. zookeeper结构
         zookeeper中的数据是按照“树”结构进行存储的,本身是一个树型的目录服务，支持变更推送，适合做注册中心	
         Zookeeper提供了一个类似于 Linux 文件系统的树形结构（可认为是轻量级的内存文件系统，但只适合存少量信息），同时提供了对于每个节点的监控与通知机制。
   
         ZooKeeper目录树中每一个节点对应一个Znode。每个Znode维护着一个属性结构，它包含着版本号(dataVersion)，时间戳(ctime,mtime)等状态信息。
         每当Znode的数据改变时，相应的版本号将会增加。
         new ZooKeeper(url,sessionTimeOut,watcher)  三个重要的参数
         临时节点:EPHEMERAL

         znode的结构
            主要属性	
               zxid：		每次的变化都会产生一个唯一的事务zxid。通过zxid，可以确定更新操作的先后顺序(如果zxid1小于zxid2，说明zxid1操作先于zxid2发生)。zxid对于整个zk都是唯一的，即使操作的是不同的znode。
               version:	节点的每次修改，都将使得版本号增加1
               data:		每个znode默认能够存储1M数据。

         ZooKeeper的临时节点不允许拥有子节点
            四种类型的znode：  持久 临时 有序
               -持久化目录节点：客户端与zookeeper断开连接后，该节点依旧存在
               -持久化顺序编号目录节点：断开后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号
               -临时目录节点：断开后，该节点被删除
               -临时顺序编号目录节点：断开后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号
      
   5. windows 下查看zookeeper节点，dubbo节点
			1.zkCli.cmd -server 127.0.0.1:2181 
			2.然后ls /   查看当前目录

		可视化客户端 ZooInspector(Zookeeper Inspector的IP+端口要一次性写对，不然要关掉重新再写才行)
			
   6. ***所有的增删改查操作都是针对znode节点进行的***
			zookeeper的watcher是一次性的，每次处理完状态变化时间后，需要重新注册watcher。
			zkclient其中状态变化主要包括：1.子节点的变化 2.数据变化 3.连接及状态的变化
         
			利用znode的特点和watcher机制，统一管理服务名称和对应的服务器列表信息。
				zookeeper 容错特性			通过事务日志和数据快照来避免因为服务器故障导致的数据丢失，leader和follower服务器都会记录事务日志
				leader 选举特性		      leader 负责数据的读写，而follower只负责数据的读

			Zookeeper 的核心是原子广播,zookeeper使用了ZAB(Zookeeper Atomic Broadcast)协议，保证了leader,follower等server之间的一致性

         Zab协议有两种模式，它们分别是恢复模式和广播模式
            当 leader 崩溃或者 leader 失去大多数的 follower，这时候 zk 进入恢复模式，恢复模式需要重新选举出一个新的 leader，让所有的 server 都恢复到一个正确的状态。
            一旦 leader 已经和多数的 follower 进行了状态同步后，他就可以开始广播消息了，即进入广播状态

            当leader宕机的话，使用 Fast Leader Election 快速选举出新的leader,节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。






   7. zk锁原理(忽略)
         zk基本锁原理：
            利用临时节点与watch机制。每个锁占用一个普通节点 /lock，当需要获取锁时在 /lock 目录下创建一个临时节点，创建成功则表示获取锁成功，失败则 watch /lock 节点，有删除操作后再去争锁。
            临时节点好处在于当进程挂掉后能自动上锁的节点自动删除即取消锁。 缺点：所有取锁失败的进程都监听父节点，很容易发生羊群效应，即当释放锁后所有等待进程一起来创建节点，并发量很大。
         
         zk锁优化原理：
            上锁改为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。
            只有序号最小的可以拥有锁，如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。 步骤：在 /lock 节点下创建一个有序临时节点 (EPHEMERAL_SEQUENTIAL)。判断创建的节点序号是否最小，如果是最小则获取锁成功。不是则取锁失败，然后 watch 序号比本身小的前一个节点。（避免很多线程watch同一个node，导致羊群效应）当取锁失败，设置 watch 后则等待 watch 事件到来后，再次判断是否序号最小。取锁成功则执行代码，最后释放锁（删除该节点）。
         
         优缺点 	
            优点： 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 
            缺点： 性能上可能并没有缓存服务那么高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。

   8. dubbo中主要用了zookeeper的服务动态上下线的功能,注册中心
         Zookeeper 集群是一个基于主从复制的高可用集群，每个服务器承担如下三种角色中的一种
         1. Leader
            1. 一个 Zookeeper 集群同一时间只会有一个实际工作的 Leader，它会发起并维护与各 Follwer及 Observer 间的心跳。
            2. 所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器。只要有超过半数节点（不包括 observeer 节点）写入成功，该写请求就会被提交（类 2PC 协议）。
         2. Follower 
            1. 一个 Zookeeper 集群可能同时存在多个 Follower，它会响应 Leader 的心跳，
            2. Follower 可直接处理并返回客户端的读请求，同时会将写请求转发给 Leader 处理，
            3. 并且负责在 Leader 处理写请求时对请求进行投票。
         3. Observer 
            角色与 Follower 类似，但是无投票权。Zookeeper 需保证高可用和强一致性，为了支持更多的客户端，需要增加更多 Server；Server 增多，投票阶段延迟增大，影响性能；引入 Observer，Observer 不参与投票； Observers 接受客户端的连接，并将写请求转发给 leader 节点； 加入更多 Observer 节点，提高伸缩性，同时不影响吞吐率
         
         

   9. zookeeper异常
         更换dataDir目录
         myid:] - ERROR [main:FileTxnSnapLog@145] - 548494(higes tZxid) > 546747(next log) for type -11
         #dataDir=/tmp/zookeeper
         #D:\zookeeper\zookeeper-3.4.6
         dataDir=D:\zookeeper\zookeeper-3.4.6\data


## RPC相关，服务化相关
	
   常用的rpc调用，1.http restful api方式 2.轻量级消息发送协议  3.消息队列使用  4.dubbo等rpc框架

   1. RPC和http的区别
         RPC的亮点在于将远程调用的细节隐藏起来，使得调用远程服务像调用本地服务一样简单，而实现上面的功能就是代理。
         
         1.从两端的要求上看，RPC要求服务提供方和服务调用方都需要使用相同的技术，如统一hessian或dubbo，而http无需关注语言的实现，只需要遵循rest规范
         2.从传输效率上看，RPC主要是基于TCP/IP协议的，而HTTP服务主要是基于HTTP协议的，HTTP协议是在传输层协议TCP之上的，所以效率来看的话，RPC当然是要更胜一筹啦
               TCP 是传输层协议，HTTP 是应用层协议，前者较后者更底层(数据传输越底层越快)，所以TCP 比 HTTP传输效率高
               相对rpc的调用，HTTP协议虽然简单，但是每次请求服务器都附带堆无用信息，比如User-Agent，Accept-Language这类的浏览器适用的信息
               其实精简的协议， 只需要把要调用方法名和参数发给服务器即可，不需要乱七八糟的额外信息
         3.从服务治理上看，rpc框架，有服务治理，自带负载均衡(http需借助nginx)，使用接口开发

   2. 简单的自定义rpc构建思路（忽略）
         使用工厂模式，将工厂类加入容器，初始化就会返回的代理类添加到容器中，创建接口的代理类，
         将http的请求封装在代理方法中，使用代理类，这样实现了类似本地化的远程调用，
         前提是知道了访问的地址，路径默认是方法名，则这样，调用接口，直接就能找到bean，是实现远程调用

      手写一个rpc框架的实现：（忽略）
         核心就是动态代理一个单独的接口，socket实现参数的传输的接收，同步阻塞等待获取
         可以对一个实现类实现动态代理，也可以只对一个单独的接口实现动态代理，
         这里rpc框架以及mapper使用的就是这种单独接口代理的方法

         rpc的demo中，创建sock连接的时候是阻塞的，在一次连接中，client写一个参数，server接收一个参数，直到接收完成返回
         其中使用new ObjectOutputStream(socket.getOutputStream()) write传参数和new ObjectInputStream(socket.getInputStream()) read 读信息，加入功能所以需要需要封装下
         实现一个注册的map,
         //final修饰的List同Map，都是可以改变内部的值，而不允许指向新的地址。
         private static final HashMap<String, Object> implRegistry = new HashMap<>();         





   3. 服务化好处
        服务化的过程就是拆分的过程，
		  1.单体减压，各个服务之间是独立的，相互解耦的，降低系统负载
        2.系统解耦，各模块独立，上线影响范围小，不需要全团队沟通，不会出现，a功能改动发布影响b的情况，团队分工明确，方便维护
        3.模块公用，代码复用，减少冗余

        服务化之前，rmi或者hessian等工具简单暴露和引用远程服务，配置服务url进行调用
        相对以前的rmi或者hessian等工具，优点：
            1.不用服务url的配置管理
            2.自带负载均衡处理


   4. 互联网中系统拆分：
         水平拆分：从单节点扩展为多个节点，具有一致功能，组成一个服务池，所有节点共同处理大规模请求，简单讲就是做集群
         垂直拆分：就是按照功能进行拆分，专业人干专业事情，产品迭代快，敏捷开发

      可以同时为提供者和消费者，但是最好不要相互依赖。合理的拆分系统，一般相互之间没有直接的相互调用，都是单独的业务系统。
      每个服务都有多个实例在运行，每个实例可以运行在容器化(tomcat)平台内，打到平滑伸缩的效果(根据性能需求独立水平伸缩,大白话就是加机器)
      每个服务都有自己的数据存储，实际上每个服务都应该有自己独享的数据库、缓存、消息队列等资源

   5. 在微服务中的一致性
         保证最终一致性的模式：
            1.查询模式
               提供查询接口，用来输出服务操作执行状态。
               使用查询模式了解被调用服务的处理情况，决定下一步做什么。例如：补偿未完成操作还是回滚已完成操作。
               为了能够实现查询，每个服务操作都需要有唯一的流水号标识，也可使用此次服务操作对应的资源 ID 来标识，例如：请求流水号、订单号等
               
            2.补偿模式
               为了让系统最终达到一致状态而做的努力叫做补偿。
               对于服务化系统中同步调用的操作，若业务操作发起方还没有收到业务操作执行方的 明确返回或者调用超时，可使用查询模式去查，再做相应的操作(类似之前的redis标记)
                  
               补偿操作根据发起形式分为以下几种:
                  自动恢复：程序根据发生不一致的环境，通过继续进行未完成的操作，或者回滚己经完成的操作，来自动达到 致状态。
                  通知运营：如果程序无法自动恢复，并且设计时考虑到了不一致的场景，则可以提供运营功能，通过运营手工进行补偿。
                  技术运营：如果很不巧，系统无法自动回复，又没有运营功能，那么必须通过技术手段来解决，技术手段包括进行数据库变更或者代码变更，这是最糟的 种场景，也是我们在生 中尽 避免的场景
               
            3.异步确保模式
               通过异步的方式进行处理，处理后把结果通过通知系统通知给使用方 。这个方案的最大好处是能够对高并发流量进行消峰.案例是job
            
            4.定期校对模式
               关键是有全局唯一的id
               通过补偿操作来达到最终一致性，但是如何来发现需要补偿的操作呢？
               可以在事后异步地批量校对操作的状态，如果发现不一致的操作，则进行补偿。
            
            5.可靠消息模式 （发送可靠和消费可靠）
               使用消息队列处理可异步处理的操作，发送前存入数据库，未发送成功的定时补发
               幂等处理，因为保证消息可靠发送需要有重试机制，消息就一定会重复，那么我们需要对重复的问题进行处理。
               
               保证幂等性的常用方法：
                  1).使用数据库表唯一键滤重，拒绝重复的请求
                  2).使用分布式锁对请求滤重
                  3).使用状态流转的方向性滤重，通常使用数据库的行级锁实现
               
            6. 缓存一致性模式
                  使用缓存来保证一致性的最佳实践。
                  尽量使用分布式缓存(redis)，而不要使用本地缓存(session 和cookies)。
                  写缓存时数据一定要完整，若缓存数据的一部分有效，另一部分无效，则宁可在需要时回源数据库，也不要把部分数据放入缓存中。
                  读的顺序是先读缓存，后读数据库，写的顺序要先写数据库，后写缓存。 

         几个一致性问题：
            1.下单和扣库存，两个服务之间最终的一致性
            2.同步调用超时，a调用b，a系统是可以得到超时反馈的，但是b不知道，处理情况类似之间的hessian处理，增加缓存补偿，设置超时时间
            3.异步回调超时，补偿做job重新发即可

         tips: 永远不要在本地事务中调用远程服务，在这种场景下如果远程服务出现了问题你，则会拖长事务，导致应用服务器占用太多的数据库连接，让服务器负载迅速攀升，严重情况下，会压垮数据库

   6. 微服务的交互模式：
         1.同步调用模式 				服务1调用服务2，等待返回处理结果
         2.接口异步调用模式			服务1调用服务2，即刻返回受理结果，受理成功，服务2异步处理，处理成功后再通知服务1
         3.消息队列异步处理模式		多应用于非核心链路上负载较高的处理环节中，井且服务的上游不关心下游的处理结果，下游也不需要向上游返回处理结果
         
         两状态同步接口：成功，失败
            使用方---服务1--服务2
            场景1，同步超时发生在使用方调用此同步接口的过程中，这时候是可以异步查询，重试请求的
            场景2，超时发生在内部服务1调用2过程中，使用快速失败原理，同时调用服务2的冲正处理，若成功则回退处理
            原因可能是内部的涉及上下游，超时两端都可能有问题，快速失败比较好
         
         三状态同步接口：成功，失败，处理中
            这里就是尽可能做补偿，两状态就是严格的成功失败
            主要涉及的就是查询接口，重试机制，幂等处理。必要时快速失败

   7. 分布式调用链跟踪系统,链路追踪
         在分布式系统中迅速定位问题时，够跟踪每个请求的调用链，树形结构进行展示调用信息
         TraceID，ParentSpanID，SpanID


   8. 服务化的客户端和服务端
         服务化客户端：
            1、获得可用服务地址列表
            2、确定目标的机器,即发现服务，采用的是完整的类名+版本号作为key去注册的服务列表中查找相应的服务
            3、建立连接，序列化，请求，接收结果，解析结果
            
         服务端：定位服务，一般基于名称和版本号定位，处理返回
         
         在服务化框架中的三个基础的属性，
               interfacename：通过接口生成代理对象，供本地调用
               version：区分的版本号，留个印象即可，可能是分机器调用吧
               group：分组调用，
         
         客户端调用服务端并不是每次都从服务注册中心查找地址，而是把地址缓存在客户端本地，当有变化时主动从服务中心发起通知，告诉客户端可用服务提供者的列表变化

         注册中心的两个基本职能：
            1、聚合地址信息，形成服务地址信息列表
            2、生命周期感知，更新服务地址信息，通过长连接的心跳机制，实现上下线的感知

         服务注册，每个服务将自己的主机、端口、版本和通信协议注册到注册中心，注册中心按照服务名分类组织服务清单，
         同时注册中心还需要以心跳的方式去检测服务是否可用，若不可用，需要从服务清单中排除，达到排除故障的效果。
         
         服务发现，调用者需要向注册中心获取调用清单，实现对具体服务的访问(具体是从清单汇总某种轮询策略调用)。


   9. 四种常见的异步远程通信方式
         1.oneway，只管发送不关心结果，是单向的通知。
         2.callback，是一种被动的方式,执行不在原请求线程中,一般需要创建新线程来执行回调
         3.future,一种主动控制超时、获取结果的方式,能主动控制超时，获取结果，并且执行仍在原请求线程中
         4.可靠异步方式，保证异步请求能在远程被执行，一般用消息中间件完成保证。
            
         **********future的使用场景**********
            底层使用了NIO方式，实现了并行调用。
            需要实现Callable接口
            主线程分别调用服务a,服务b,服务c,最后统一处理三个数据，服务之间没有相互依赖，这种适合futrue模式。
            但是如果服务之间有依赖，那么就必须要等拿到前一个服务结果后再调用下一个服务。
         **********future的使用场景********** 

   10. 接口访问控制，接口安全，接口控制
         * sign验签，基于时间戳防止重放攻击，
         * 限流 熔断
         * RPC 主要指内部服务之间的调用
         * ip限制    控制同一个ip和设备id的请求频率，记录在redis中的，使用incr 和 expire实现

       对接接口安全性   
            1、使用同一的token校验（获取动态口令，优先时间60秒，一般是account-key，先请求key，计入缓存，然后二次请求过来）
            2、增加ip白名单过滤，
            3、使用https证书
            4、增加sign校验

            臣邦的对外接口，即使先根据account-key获取限时token，然后再加入验签拿到统一sign，后续校验有token + ip + sign + 常规业务校验
            限流， (openNo, todayStr-count)组合，限定账号每日请求次数，redis记录日访问次数
            这里的token随便取一个随机串即可，随机串工具类生成即可

   11. 熔断，降级，限流 
         * 熔断机制
               是应对雪崩效应的一种微服务链路保护机制。
               熔断的机制(断路器，故障容错)，是微服务中，调用的服务过载或者出现故障时(后续不断请求阻塞，造成系统down)，自动阻断对服务的访问和调用，转而调用备用方法
               Hystrix 断路器机制
                  会强迫其以后的多个调用快速失败，不再访问远程服务器，从而防止应用程序不断地尝试执行可能会失败的操作
                  Hystrix Command 请求后端服务失败数量超过一定比例(默认 50%), 断路器会切换到开路状态(Open). 这时所有请求会直接失败而不会发送到后端服务。
                  一旦后端服务不可用, 断路器会直接切断请求链, 避免发送大量无效请求影响系统吞吐量, 并且断路器有自我检测并恢复的能力

         * 降级
               降级多是大量请求且不能扩容进行的功能限制，可能针对某功能，也可能根据不同使用者。

         * 限流
               服务调用的限流处理
                  被拒接的请求，直接返回给调用者，也可以进行排队
                  1.0和1开关
                  2.设置固定值(每秒请求次数QPS)，超过请求次数，拒绝请求。
               
               限流的维度，
                  1.根据服务端自身的接口，方法控制，每个负载不一样。
                  2.根据来源做控制，设置不同的限制，根据请求来源不同级别进行不同的流控处理。

   12. 自定义rpc注解（忽略）
         定制@rpcservice注解，带有@service注解，这样启动可以被spring扫描到。然后通过反射创建该类的实例，加入容器管理，
         并建立服务名称和服务实例之间的映射关系，便于从后续rpc请求中获取服务名称，拿到实例，反射调用目标实例
         使用ApplicationContextAware获得上下文，初始化的时候，拿到所有标有@rpcservice的类，并将接口名和实例对象绑定


   13. 服务化系统优化
         1.当服务之间依赖变复杂时，分不清各个服务的启动顺序，这时候需要画出应用间的依赖关系图
         2.当服务调用量越来越大，服务容量问题暴露出来，需要多少机器，加机器的时间
            解决方案
            1.要将服务现在每天的调用量，响应时间，都统计出来，作为容量规划的参考指标。 
            2.其次，要可以动态调整权重，在线上，将某台机器的权重一直加大，并在加大的过程中记录响应时间的变化，直到响应时间到达阀值，记录此时的访问量，再以此访问量乘以机器数反推总容量。

         一般来说调用服务一定是在工作线程(非IO线程)，而反序列化工作取决于具体实现。
         服务框架必须做到要模块化和可配置。配置调用的策略。

       服务化的优化实践
         服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo暂未提供分布式事务支 持。
         服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸 
         不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。

   21. 去中心化
         各服务分拆后，也会拆出各自的数据库(严格的话，服务管理自己的数据库)，其中一些特殊结构或公用的数据，可以放到redis中。
         微服务架构中，更加强调各服务之间进行无事务的调用，对于数据一致性，只要求数据在最后的处理状态是一致的即可。
         若在过程中发现错误，通过补偿机制来进行处理，使得错误数据能够达到最终的一致性。
		
       容错设计
         微服务中需要快速检测出故障并尽快自动恢复服务，因为多节点，一个点出错可能会影响其他节点，可能引起雪崩效应
         比如c-b-a  一致卡在b-a，那么c-b就应该减少调用了，不能一直增加阻塞，减少线程挂起。
         这里一般希望在每个服务中心时间监控和日志记录组件，比如服务状态，断路器状态，吞吐量，网络延迟等关键数据的仪表盘。

   22. 负载均衡
         分布式服务通过内置负载均衡实现高可用，关系型数据库通过主备方式实现高可用

         请求分发    在关卡处可以通过配置禁止一些无效的请求，比如封禁经常作弊的ip地址，这里是初级过滤
         限流	      负载均衡器有限流的算法，对于请求过多的时刻，可以告知用户系统繁忙，稍后再试，从而保证系统持续可用。
         
         常用负载均衡的策略
            随机，轮询，权重

            轮循均衡			   每一次来自网络的请求轮流分配给内部中的服务器
            权重轮循均衡		根据服务器的不同处理能力，给每个服务器分配不同的权值，使其能够接受相应权值数的服务请求。适合在服务集群的机器能力不对等的情况下
            随机均衡
            最少连接数均衡
            处理能力均衡(CPU、内存)

   23. 拆分和集群
         拆分：不同的多台服务器上面部署不同的服务模块，模块之间通过RPC通信和调用，用于拆分业务功能，独立部署，多个服务器共同组成一个整体对外提供服务。
         集群：不同的多台服务器上面部署相同的服务模块，通过分布式调度软件进行统一的调度，用于分流容灾，降低单个服务器的访问压力。

### 序列化相关
      序列化机制的核心作用就是对象状态的保存与重建。字节序列和Java对象之间的转换
      rpc远程传输的类，和内部的属性类都需要序列化

   1. transient关键字只能修饰变量，不再是对象持久化的一部分，不再能被序列化（静态变量也不能）传输

   2. jdk序列化写入不仅是完整的类名，也包含整个类的定义，包含所有被引用的类。性能和效率低，java专用的，不能跨语言

      hessian将对象序列化与语言无关的二进制协议，跨语言，适合传输较小的对象，服务化结构中大量的服务调用是大规模、高并发的短小请求，比较适用
        
   3. 序列化是输出，用out流，反序列化是输入用input流，用到的就是 ByteArrayOutputStream()，ObjectOutputStrea/ByteArrayInputStream,ObjectInputStream;	
   
      序列化
		文本协议，直观、描述性强，容易理解，便于调试，缺点就是冗余数据较多，不适宜传输二进制文件（比如：图片等），解析复杂（需要进行字符串比较）；
		二进制协议，没有冗余字段，传输高效，方便解析（固定长度，并且可以直接比较字节），缺点就是定义的比较死，哪个位置有哪些东西，是什么意义是定义死的，场景单一。
		


   4. protostuff序列化，减小序列化后的数组大小(忽略)
      需要引入的jar有protostuff-collectionschema，protostuff-core，protostuff-runtime，protostuff-api
  
   5.  为什么要用序列化？
         进程通信实现文本图片音频等传输外（二进制序列形式），java对象传输必须序列化

      序列化的注意点
			1.父类实现serializable，所有子类都可以被序列化
			2.子类实现Serializable接口，父类没有，?中的属性不能序列化(不报错，数据会丢失)，但是子类中属性能正确序列化
			3.序列化的属性是对象，这个对象也必须实现序列化接口
			4.序列化过程中对应的序列化id不要变
      
	6. 几种常用的序列化方式，java,hessian,thrift，kryo,protobuf
      
      需要长连接获取高性能，选基于tcp的thrift和dubbo
      需要跨网段，跨防火墙，选基于http协议的hessian

      主流的、常用的序列化框架
		相对来说，二进制存储占用体积小。
			1.json序列化，适用与web与控制层的转换。,可视化强,文本化协议
			2.hessian序列化,HTTP协议的，二进制序列化，跨平台，内部会使用HessianSkeleton(HessianOutput和HessianInput)，将传输对象序列化，不需要手工序列化。
			3.java内置序列化，不支持跨平台，性能低，ObjectOutputStream和ObjectInputStream
			4.XStream，使用json居多，数据传输，xml序列化推荐，XStream,可视化强,文本化协议
			5.Thrift用的也不多。

   7. 序列化相关
        两个进程远程通信时，彼此可以发送各种类型的数据。数据都会以二进制序列的形式在网络上传送。所以需要序列化和反序列化。
        oos = new objectoutputstream(new fileoutputstream("a.txt")).writerObject(obj)  obj需实现序列化
        (person) oos.readerObject() 反序列化

        serializable
        存在唯一的序列化id，没有显示id，默认存在的
        流对象是可以直接读取对象的，ios.readObject(),序列化接受
        序列化过程中，有些属性不想传输，但是也不能static 可以使用tranient关键字	，static无法序列化传输
        网络模块是通过二进制流的方式进行传输的。
		  通过序列化方式将内存对象转为二进制数据(二进制序列化后的数据传输效率高,适合系统之间传输),对象序列化需要实现Serializable接口

    
		

		
      



### Hessian

hessian接口的调用基于http的访问，路径管理还是比较麻烦的，访问单一

#交给DispatcherServlet处理	<servlet>默认加载web-inf下的servlet.xml</servlet>

```
server端
<bean name="/eBankReceiptManagerExpService" class="org.springframework.remoting.caucho.HessianServiceExporter">
    <property name="service" ref="eBankReceiptManagerService" />
    <property name="serviceInterface" value="com.xiaoyuer.pay.service.intf.IEBankReceiptManagerService" />
</bean>
```

	client端
	<bean id="eBankReceiptManagerExtService" class="org.springframework.remoting.caucho.HessianProxyFactoryBean">
	    <property name="serviceUrl" value="${xye.paycore.server}/remoting/eBankReceiptManagerExpService" />
	    <property name="serviceInterface" value="com.xiaoyuer.pay.service.intf.IEBankReceiptManagerService" />
	    <property name="readTimeout" value="30000"/>
	</bean>

hessian实现远程方法的调用:HessianServiceExporter或者HessianProxyFactory，基于二进制的数据传输对象，二进制RPC协议的轻量级远程调用框架，Hessian远程调用框架是构建在Http协议之上的
如果在远程调用时，用到了自定义的实体，必须序列化，在hessian接口调用时候，可设置超时时间，必须要考虑超时情况下，客户端和服务端数据回滚情况，尤其是金融相关问题，两边可以打上tag，实时验证。
	在调用时需要分别配置服务端和客户端的配置，客户端主要是加入容器bean，服务端是拦截路径交给hessian处理转至相应的serviceimpl处理，返回

客户端——>序列化写到输出流——>远程方法(服务器端)——>序列化写到输出流 ——>客户端读取输入流——>输出结果
目前想到的和dubbo的区别是，hessian只能连接单一远程服务，但是dubbo可以有zookeeper提供多个提供者

获取对远程的调用的代理，通过代理服务实现远程服务的调用，创建基于接口的代理给引用	

	在boot 的配置中client 和server的配置文件尽量要分开，放一起会冲突



   //***配置demo***
	
   //消费端
	@Bean
    public HessianProxyFactoryBean tradeManagerExtService(){
		HessianProxyFactoryBean factoryBean = new HessianProxyFactoryBean();
        factoryBean.setServiceUrl(payCoreUrl+"/remoting/tradeManagerExpService.htm");
        factoryBean.setServiceInterface(ITradeManagerService.class);
        factoryBean.setReadTimeout(30000);
        return factoryBean;
    }
	
	//服务端
	@Bean("/remoting/tradeManagerExpService.htm")
	public HessianServiceExporter exportTradeManagerHessian() {
		HessianServiceExporter exporter = new HessianServiceExporter();
		exporter.setService(tradeManagerService);
		exporter.setServiceInterface(ITradeManagerService.class);
		return exporter;
	}	

### Dubbo相关
   Hessian 是基于 HTTP协议的 
	Dubbo 是基于 TCP 协议的，
	而thrift则同时支持两种协议

   传输协议：TCP
      传输方式：NIO异步传输。非阻塞型的io，      
      序列化：Hessian二进制序列化

   dubbo默认的配置	
	使用zookeeper作为注册中心，
	dubbo协议作为传输协议，
	使用netty异步传输作为底层通信框架，
	hessian序列化，
	javassist动态代理

   失败策略，失败自动切换，当出现失败，重试其他服务器，
	轮询机制，随机，按权重设置随机概率

	***dubbo自身底层调用是使用netty异步实现的，消费端默认同步调用返回结果***
 
   升级dubbo的问题，版本升级一定要参考官网的，不能随意使用版本，标准就是官方文档。任何时候都不要用最新的版本试验
		dubbo升级2.7.x,  不要使用最新版，按照官方文档中的推荐2.7.7即可	
		dubbo官方文档						http://dubbo.apache.org/    
		springboot-dubbo官方文档			https://github.com/apache/dubbo-spring-boot-project/


   dubbo的几个重要部分
      Transporter，传输层
         mina, netty(默认), grizzy 
         
      Serialization，序列化
         dubbo, hessian2(默认), java, json 
         
      Dispatcher，消息派发线程池
         all(默认), direct, message, execution, connection 
         
      ThreadPool，线程池
         fixed, cached		
         

1. dubbo超时连接,调整zookeeper的反映连接时间
   	服务端重启检查间断		java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
   	对应的duboo中的源码：	com.alibaba.dubbo.common.Constants   DEFAULT_SESSION_TIMEOUT = 60 * 1000;

2. tomcat启动的时候，dubbo两次加载导致端口占用的问题：autoDeploy="false" deployOnStartup="false"

3. 提供者，消费者，注册中心使用情况
  
   服务者注册服务(服务提供者无状态，任意一台宕掉后，不影响使用)，消费者订阅服务，
	注册中心返回服务提供者地址列表给消费者(二者是长连接)，如果有变更，注册中心将基于长连接推送变更数据给消费者(服务提供者宕机，注册中心将立即推送事件通知消费者)。
	服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心
	
   注册中心宕机后，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表(仍能通过本地缓存通讯) 
   dubbo由于有服务端信息的本地缓存，提供者的地址列表，所以当注册中心挂掉后调用端依然能够工作，也就是说调用端不是强依赖服务端。



4. 短连接的操作步骤是：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接
   长连接的操作步骤是：建立连接——数据传输…（保持连接）…数据传输——关闭连接

5. <dubbo:protocol name="dubbo" port="20880" /> 解析成相应的对象，name目前来看映射的是id属性
   spring的xml中自定义bean 都需要一个实现了BeanDefinitionParser接口的解析parser	

6. 记录一次dubbo线程池耗尽的场景
   dubbo中的接口一次最多处理八个线程，其他的接口就排队执行，线程开启新的事务，还是原来的线程
   
   对于Dubbo集群中的Provider角色，有IO线程池（默认无界）和业务处理线程池（默认200）两个线程池，
   业务的并发比较高，或者某些业务处理变慢，业务线程池就很容易被“打满”，抛出RejectedExecutionException: Thread pool is EXHAUSTED!异常，这是因为Dispatcher(dubbo中的调度器)默认配置是all，这样所有的线程都都在业务池中处理，返回的异常也可能因为线程池满了，导致无法回应消费者，消费者只能等到超时。我们应该配置成message。
   		由于异常处理也需要用业务线程池，当线程池满的时候，可能消费端获取不到异常返回，直到超时应该直接抛出Thread pool is EXHAUSTED异常到消费端。
   这也是为什么我们有时候能在Consumer看到线程池打满异常，有时候看到的确是超时异常
   所以，为了减少在Provider线程池打满时整个系统雪崩的风险，建议将Dispatcher设置成message：

   ```
   <dubbo:protocol name="dubbo" port="8888" threads="500" dispatcher="message" />
   ```

   dispatcher="message"




   配置的时候，soa自身有些线程是不经过业务线程池的，300可以 但是301不行，
   当soa和pc保持连接的时候，配置300就不可以了，因为连接的心跳的线程也占用业务线程池。
   	批量选人回调
   			-- 方案確定
   				1、配置dubbo連接池參數
   				<dubbo:protocol name="dubbo" port="20881" threads="300" dispatcher="message" />
   				2、配置數據庫的連接數量,Druid中的maxActive默认是8															
   				3、調整job時間  延长时间至30秒或者1min，改变主站serverCallBack中soa返回结果，成功即可，不需要重复执行
   				4、修改跳過判断 暂时不考虑，不加入缓存因素
   				数据库锁定，回滚无效

7. dubbo源码相关

      * JavaSPI （忽略）
         ***源码级别，这里的具体serviceloader的代理实现，暂不研究***
         spi Service Provider Interface 服务提供接口 动态加载机制，一种服务发现机制
         在模块装配的时，基于接口从配置文件中装配实现类 SPI的应用之一是可替换的插件机制，
         javaspi就是提供这样的一个机制：为某个接口寻找服务实现的机制。工具类：java.util.ServiceLoader
         就是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要。 同时实现了两个spi接口的类，如果不做限制，都会执行
         重点的思想是使用jar的形式，可插拔

         在dubbo的extensionloader中，只会处理@spi标签的接口，通过loadFile(extensionClasses, SERVICES_DIRECTORY);加载指定的实现类来，完成spi的服务提供

         依赖的api通过dubbo.xml注入容器，并通过zookeeper的地址实现调用，filter过滤属性文件到resource下的属性文件后，通过注入容器，可以直接在xml中使用，也可以通过属性文件的加载读取（加载方式为流读取）。

         ```
            //这里拿到的是实现接口的实现类，具体的实现类在classpath下的META-INF/services/下
            ServiceLoader<ISpiTestService> loads = ServiceLoader.load(ISpiTestService.class);
            for (ISpiTestService iSpiTestService : loads) {
               iSpiTestService.say();
               System.out.println("end-------");
            }
         ```
         spi机制，配置文件发现实现类机制，优点实现三方解耦，缺点会一次性加载全部实现类

      * 解析xml中的dubbo标签
			使用自定义的schema，实现命名空间自定义配置
				1.设计配置属性和JavaBean 
				2.编写XSD文件 													
				3.编写NamespaceHandler和BeanDefinitionParser完成解析工作，串联所有部件 		(Spring提供了默认实现类NamespaceHandlerSupport和AbstractSingleBeanDefinitionParser)
				4.编写spring.handlers和spring.schemas串联起所有部件 						META-INF/spring.handlers和META-INF/spring.schemas  分别定义处理类 和xsd文件    spring默认载入
				<xsi:schemaLocation=" http://blog.csdn.net/cutesource/schema/people  http://blog.csdn.net/cutesource/schema/people.xsd">  分别对应handler处理类和xsd文件
				是通过统一的前者命名关联了handler和xsd解析
		
         dubbo直接使用了BeanDefinitionParser，没有继承AbstractSingleBeanDefinitionParser,将xml中的注解内容parse成bean的实例到容器中
         dubbo:service     其中service就是其中的一个配置类，对应<xsd:element name="service">对应着配置项节点的名称，因此在应用中会用 service 作为节点名来引用这个配置
         dubbo也是实现了InvocationHandler ，最后invoker.invoke(new RpcInvocation(method, args)).recreate(); 这里就开始进入调用远程的服务

         默认情况下如果本地有服务暴露，则引用本地服务
         // 用户指定URL，指定的URL可能是对点对直连地址，也可能是注册中心URL
         按 key=menthodName/value=invoker 缓存起来 

	
	   * 注册模块dubbo-register：
			1.构造器利用客户端创建了对zookeeper的连接，并且添加了自动回复连接的监听器。
			2.注册url就是利用客户端在服务器端创建url的节点，默认为临时节点，客户端与服务端断开，几点自动删除
			3.取消注册的url，就是利用zookeeper客户端删除url节点
			4.订阅url， 功能是服务消费端订阅服务提供方在zookeeper上注册地址.
			5 取消订阅url， 只是去掉url上的注册的监听器
			
			
			使用了zookeeper的注册中心，ZookeeperRegistryFactory,是操作zookeeper的客户端的工厂类，用来创建zookeeper客户端，ZookeeperClient
				
			dubbo://192.168.6.222:20881/com.xiaoyuer.soa.api.service.IRequireService?anyhost=true&application=xye-soa-core-require&default.retries=0&default.service.filter=xyeProviderExceptionFilter&default.timeout=20000&default.token=true&dispatcher=message&dubbo=2.8.4x-SNAPSHOT&generic=false&interface=com.xiaoyuer.soa.api.service.IRequireService&methods=testQueryParams,saveRequireInfo,saveRequire,getRequireInfo&organization=dubbox&owner=programmer&pid=60516&side=provider&threads=300&timestamp=1590393669112
			/dubbo/com.xiaoyuer.soa.api.service.IRequireService/providers/dubbo%3A%2F%2F192.168.6.222%3A20881%2Fcom.xiaoyuer.soa.api.service.IRequireService%3Fanyhost%3Dtrue%26application%3Dxye-soa-core-require%26default.retries%3D0%26default.service.filter%3DxyeProviderExceptionFilter%26default.timeout%3D20000%26default.token%3Dtrue%26dispatcher%3Dmessage%26dubbo%3D2.8.4x-SNAPSHOT%26generic%3Dfalse%26interface%3Dcom.xiaoyuer.soa.api.service.IRequireService%26methods%3DtestQueryParams%2CsaveRequireInfo%2CsaveRequire%2CgetRequireInfo%26organization%3Ddubbox%26owner%3Dprogrammer%26pid%3D60516%26side%3Dprovider%26threads%3D300%26timestamp%3D1590393669112	
			
			
			在zookeeper的服务端创建临时的目录节点，每一级都是节点目录 	使用zkcli客户端可以查看目录节点
			[zk: localhost:2181(CONNECTED) 8] ls /dubbo/com.xiaoyuer.soa.api.service.IRequireChooseService ->[consumers, configurators, routers, providers]
			[zk: localhost:2181(CONNECTED) 10] ls /dubbo/com.xiaoyuer.soa.api.service.IRequireChooseService/providers-> 提供者列表

			zkClient.createPersistent(path, true);	zookeeper递归创建目录
			zookeeper 存储的只是目录节点，每个节点都会有自己对应的值,维护的就是目录节点


		* dubbo的container模块
			默认只会启动dubbo-container-spring的这个container，主要负责jar启动，优雅停机
				因为服务通常不需要Tomcat/JBoss等Web容器的特性，没必要用Web容器去加载服务，一般main加载spring启动即可
				Dubbo是通过JDK的ShutdownHook来完成优雅停机的，所以如果用户使用”kill -9 PID”等强制关闭指令，是不会执行优雅停机的，只有通过”kill PID”时，才会执行。
				
      * dubbo的remoting
         dubbo底层通信模块的实现。实现对请求/应答的各种逻辑处理，包括同步，异步，心跳等逻辑，最底层的通信借助netty或者mina实现

         启动开启netty服务，绑定ip和port，客户端调用使用netty访问对应的地址，这样会进入netty的对应handler
         netty绑定地址，添加处理handler， client端使用netty访问地址，服务端接收到转给了handler处理

		* 原理相关
         创建dubbo代理接口的时候，注解中添加了DubboTransactionContextEditor  事务编辑器
         //实现原理：
         所有的create 和update 都会进缓存，绑定对应的事务txid和当前事务
         两个拦截器；主try包含两个从try，主事务ommit包含三个参与者信息；

         try执行ok，主事务执行confirm(包含了远程两个参与者)（封装的是dubbo 原try方法的调用，但是状态是confirming,会走provider，走confirm）

         dubbo代理接口--->实现类，RpcContext.getContext() 实现隐式传入了2的事务txid，和3共用，然后3中存入缓存的是xid-branch 事务(包含confirm等参与者的信息)， putToCache(transaction)

            1.root进第一拦截器(root事务入库，事务进线程变量),进资源拦截器(创建参与者，branchid自建，将参与者加入根事务，然后更新根事务到数据库，其中事务序列化后存入了content字段)
            2.dubbo代理接口，进第一拦截器(supports,没有事务上下文，直接放行)，进资源拦截器(创建参与者，branchid自建，使用全局事务id，封装的本身的调用record方法，同上，主要是根事务新增了参与者)
            3.dubbo代理接口的实现类，进入第一拦截器(带了上下文[是2中dubbo context中存的xid]，进入provider事务，新建了branch事务，入库，事务进线程)
               进入资源拦截器(创建参与者，branchid自建，使用全局事务id，封装confirm等方法进入上下文，数据库更新branch事务，进缓存[使用的也是2中的xid-事务]，主要是更新参与者)
            4.root的confirm,正常。dubbo的代理confim进第一拦截(supports放过)，第二拦截(不是trying 放过)
            5.dubbo  confirm service，进第一拦截器(带上下文,进provider事务，根据txid从缓存中找到步骤3存的调用上下文，实现事务中的参与者反射调用)，进入第二拦截器(不是trying,放过)

         RpcContext.getContext()

            相当于将在调用前将存放的参数封装传到服务端，是跟着这个方法走的，是一次调用适用
            客户端：RpcContext.getContext().setAttachment("sourceid", "15700007");--A
            调dubbo接口：smsService.smsSend();
            服务端：RpcContext.getContext().getAttachment("sourceid");--B
            一定要注意，调接口时，必须是A直接到B，如果A没有直接到B，而是先到C，再由C到B，那么在B里getAttachment()，就获取不到值了。

            RpcContext对象是绑定在线程临时变量LOCAL上，所以可以通过线程临时变量来获取到RpcContext的相关参数值
            在调用提供者之前，在源码“41处”，会获取当前线程临时变量里的RpcContext对象，再将RpcContext对象里的参数设置到Invocation对象，最后调用doInvoke(Invocation invocation)方法，就会发送参数给提供者。

            这里远离是将设置放在了线程变量中然后添加进dubbo的参数中传过去
            RpcContext.getContext().setAttachment
               这里会消费端将参数传入dubbo参数中，传给服务端invocation.addAttachmentsIfAbsent(attachment);
            invocation.getAttachments();
               这里服务端会从入参中获取加入到RpcContext中，invocation.getAttachments()；RpcContext.getContext().setAttachments(attachments);
            dubbo接口直接从工厂类中获取（使用的还是spring的单例）
            confirm中 dubbo接口调用，会从容器中拿到代理类代理执行。  静态map后面继续使用
            
            
         创建job也很简单，定义一个启动类，
         1.初始化的时候  scheduler.scheduleJob(jobDetail.getObject(), cronTrigger.getObject());
         定义好jobdetail 和触发策略cronTrigger
         2.启动scheduler.start();
            public class DefaultRecoverConfig implements RecoverConfig {
               public static final RecoverConfig INSTANCE = new DefaultRecoverConfig();
            
            }
         recovery job 中
            就是从数据库中读取事务(封装了事务的上下文)到缓存中，然后重新执行
            异常出现的时候，有一个confrim异常
            主order  分支cap 和red
            在cap的confirm中抛异常，会返回消费者拿到异常，然后直接结束，这样 order和cap是confirming，red是trying
            事务信息被持久化到外部的存储器中。事务存储是事务恢复的基础。通过读取外部存储器中的异常事务，定时任务会按照一定频率对事务进行重试，直到事务完成或超过最大重试次数。
            实际分支事务对应的应用服务器也可以重试分支事务，不是必须根事务发起重试，从而一起重试分支事务。	

8. C:\Users\xiaoyuer\.dubbo  下的zookeeper缓存

   <dubbo:protocol name="dubbo" port="20880"/> 使用dubbo协议在20880端口暴露服务
    提供者注册在zookeeper的地址是，192.168.6.85:20880，这样就是通过20880端口对外提供dubbo服务

   只有服务端才需要暴露相应的dubbo端口，客户端只要填写zookeeper地址即可，统一在注册中心找服务

   dubbo已经是20881端口了   linux上还需要端口暴露
	dubbo jar 启动之后就不提供web支持，web是访问不了的	

9. boot中dubbo的启动方式

   		#dubbo在springboot中，不使用内嵌的tomcat启动，jar启动
   		//静态的run方法，使用内嵌的tomcat启动，至于能不能用在无tomcat的jar启动，暂不测试.该配置会默认使用boot自带的tomcat，soa服务工程不需要
   		0.//SpringApplication.run(XyeServiceCoreApplication.class, args);	
   		//jar启动其中中不使用tomcat，启动springboot工程，搭配下面的使用
   		1.new springapplicationbuilder(a.class).web(false).run(args)
   		2.com.alibaba.dubbo.container.Main.main(args);//dubbo的推荐的main函数启动


   	        0.默认内置tomcat的web启动
   	        1.非tomcat启动boot，可以自定义启动参数
   	        2.启动dubbo
   	
   			springapplicationbuilder.sources()在原本的流程中是用来指定外置tomcat的入口，war启动指定入口，main是java启动入口
   			@Override
   			protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
   				return application.sources(XyeServiceCoreApplication.class);
   			}
   			
   			相对于上面的非web启动方式，这个方法也行哦
   			SpringApplication springApplication = new SpringApplication(XyeServiceCoreApplication.class);
   			springApplication.setWebEnvironment(false);
   			springApplication.run(args);
   			com.alibaba.dubbo.container.Main.main(args);

   			springboot启动不错吧
   			@SpringBootApplication  
   			public class Application {  
   			  public static void main(String[] args) {  
   					// SpringApplication.run(Application.class, args);  //方法1静态的run方法，容器tomcat启动
   					// SpringApplication application = new SpringApplication(Application.class);  //方法2启动
   					// application.setShowBanner(false);  
   					// application.run(args);  
   						new SpringApplicationBuilder().showBanner(true).sources(Application.class).run(args);  //方法3启动
   				}  
   			}  

10. 首先在classpath下创建dubbo.properties
    写入dubbo.service.shutdown.wait = 毫秒数
    然后再pom文件里的打包标签里把dubbo.properties加进去
    优雅停机，先停止consumer请求，再执行kill 命令，停止provider端的机器，结果就是55秒后才停机，不能kill -9，

    jar 启动时候可以用java -jar app.jar --spring.profiles.active=dev  来指定运行的环境，目前采用的是pom中指定profile加载的属性文件

11. dubbo注册zookeeper节点相关
				三层节点：根节点，服务名称节点和服务提供者的地址节点
							其中根节点，服务名称节点是持久节点(persistent)，地址节点是临时节点(ephemeral)
			
			
			根节点->服务名称节点->type节点->地址节点
			/dubbo->com.soa.api.IUserService->provider->dubbo://192.168.6.222:20881/com.....
			
			consumers->consumer://192.168.6.222/com...
			
			其中provider、consumers 后续属于新增的一层type节点
				1.dubbo节点，dubbo下一级接口，接口下的provider、consumers、configurators、routers都是持久化节点
				2.provider和consumers下的url子节点都是临时节点。
				
				*******
					消费者从ZK获取provider地址列表后，会在本地缓存一份。后续都是对这个服务列表的本地存储进行更新*******
				*******
					ZK的本质就是为两端提供服务地址的发布/订阅服务，让消费者及时感知最新的服务列表，consumer真正调用provider是通过某种通信协议直接调用，并不依赖ZK。
					即使zk宕机，不影响两端调用，只是让本地缓存的服务列表有可能过时的。
				*******
				
			调用的消费端，从注册中心获取到的是服务提供者的地址列表(负载均衡算法选出一台服务器地址调用)。
			并且服务名称对应节点上有IZKCildListener监听，节点变化，对应的handleChildChange方法执行，更新节点，更新serverlist
				
			只有配置信息更新时，服务消费者才会去zookeeper上获取最新服务地址列表，其他时候使用本地缓存即可。这样服务消费者在服务信息没有变更时，几乎不依赖配置中心，降低配置中心压力。

12. zookeeper挂机，提供者和消费者联系
	   可以的，启动dubbo时，消费者会从zk拉取注册的生产者的地址接口等数据，缓存	在本地。每次调用时，按照本地存储的地址进行调用

      消费者启动获得提供者地址，并缓存本地后，consumer调用provider是直接invoke调用的，可以不依赖zookkeper

13. dubbo的三种运行方式：
        1、使用servlet容器运行(原先使用，现不推荐)
            缺点：浪费资源，内存、端口和管理
        2、自建main方法运行(忽略)
        3、dubbo框架提供的main方法运行 
            com.alibaba.dubbo.container.Main启动类

    dubbo的启动方式
      1、使用Servlet容器（Tomcat、Jetty等）运行
      缺点：增加复杂性（端口、管理） 浪费资源（内存）
      服务容器是一个standalone的启动程序，因为后台服务不需要 Tomcat 或 JBoss 等 Web 容器的功能，如果硬要用 Web 容器去加载服务提供方，增加复杂性，也浪费资源。

      2、自建main方法类来运行（Spring容器）  		
      缺点： Dobbo本身提供的高级特性(优雅停机等)没用上 自已编写启动类可能会有缺陷
      **********
      注意别和boot的main启动搞混了，这种一般就是ClassPathXmlApplicationContext 加载一个context，然后wait实现服务提供,一般测试用
      **********

      3、使用Dubbo框架提供的Main方法类来运行（Spring容器)
      优点：框架本身提供（com.alibaba.dubbo.container.Main） 可实现优雅关机（ShutdownHook）
      配置配在 java 命令的 -D 参数或者 dubbo.properties 中。其他的在dubbo.xml中配置即可，这两个可以结合下配置，目前项目中是xml主要配置
      服务容器只是一个简单的 Main 方法，并加载一个简单的 Spring 容器，用于暴露服务。
      dubbo.spring.config=classpath*:*.xml   这个相当于通过属性文件去导入了，目前项目是直接引入xml到容器中
      运用：生产上dubbo server可以用这种方式部署。
      搭配boot的时候：new SpringApplicationBuilder(XyeServiceCoreApplication.class).web(false).run(args);要以非web方式进行

      //boot 2.3.2
      public static void main(String[] args) {
         new SpringApplicationBuilder(XyeServiceCoreApplication.class).web(WebApplicationType.NONE).run(args);
         Main.main(args);
      }

      boot中继承dubbo
         dubbo-spring-boot-starter 使用这个才能在applicationi.properties中自动配置dubbo属性，否则只引入dubbo的maven，就需要添加dubbo.proerties配置


14. dubbo接口的设计原则
        1、接口的粒度大点，通常是包含完成业务作为一个接口，减少系统之间的交互，dubbo暂未提供分布式事务支持。
        2、不建议使用过于抽象的通用接口，如map query（map），不便于后期的维护
        3、接口都应定义版本号，为后续的不兼容升级提供可能（暂不理解）
        4、必要的接口参数校验
        5、provider上尽量多配置consumer端属性，优先server端，比如超时时间，重试次数，并发数量

15. dubbo的启动检查：
        1、<dubbo:consumer check="false">  关闭所有服务的启动时检查
        2、<dubbo:register check="false">	关闭注册中心启动时检查
        3、	dubbo.reference.check=false，强制改变所有reference的check值，就算配置中有声明，也会被覆盖。
            dubbo.consumer.check=false，是设置check的缺省值，如果配置中有显式的声明，如：<dubbo:reference check="true"/>，不会受影响。
            dubbo.registry.check=false，前面两个都是指订阅成功，但提供者列表是否为空是否报错，如果注册订阅失败时，也允许启动，需使用此选项，将在后台定时重试

16. dubbo直连
      dubbo的直连模式，可以忽略注册中心和监控，一般本地调试用
		绕过注册中心，将以服务接口为单位，忽略注册中心的提供者列表， A接口配置点对点直连，不影响B接口从注册中心获取列表。
		一般三种方式
         1.xml中指定地址(常用)  
         2.java -D中指定地址  
         3.使用-Ddubbo.resolve.file指定映射文件路径     
         
		使用场景
			问题为方便开发测试，经常会在线下共用一个所有服务可用的注册中心，这时，如果一个正在开发中的服务提供者注册，可能会影响消费 者不能正常运行。 
			解决方案 可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务。
			<dubbo:registry address="10.20.153.10:9090" register="false" />

      消费端，在dubbo配置reference时候，指定接口url="dubbo：//localhost：20818" 绕过zookeeper直连提供者（开发测试用）

      dubbo 直连，开发环境可不使用zookeeper,节省启动注册时间，soa关闭再启动不影响消费端使用，步骤如下：
      服务端口SOA   
         1.取消注册 添加 register="false"  
         2.关闭令牌验证   token="false"
            <dubbo:registry protocol="zookeeper" address="${zookeeper.address}" register="false"/>
            <dubbo:provider token="false" timeout="20000" retries="0"/>
      消费端  
         3.替换 spring-dubbo.xml 表达式
            eclipse:   find:interface=\"(.*)\"    replace with:  interface=\"$1\" url="dubbo://localhost:20881"  勾选 regular expressions  

17. dubbo的负载均衡：
		4种策略
			1、随机
			2、轮询
			3、权重,按照活跃数少的优先调用，最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。
			4、hash一致：一致性hash，默认对第一个参数hash，相同参数请求发送统一提供者 

18. dubbo注解配置(忽略)
        EnableDubboConfiguration 是使用dubbo注解的，spring的扫描注解正常配置,不影响
		http://jm.taobao.org/2018/06/13/Provider配置/    			dubbo配置
		http://dubbo.apache.org/zh-cn/blog/dubbo-annotation.html	官方文档
        
        添加扫描，@DubboComponentScan("com.xiaoyuer")
        注册服务
            spring.dubbo.application.name=controller-consumer
            spring.dubbo.registry.address=zookeeper://172.17.0.2:2181
            spring.dubbo.scan=com.gaoxi						扫描dubbo的注解，使用注解
            
            import com.alibaba.dubbo.config.annotation.Service;
            @Service(version = "1.0.0")
            @org.springframework.stereotype.Service

        发现服务
            @Reference(version = "1.0.0")
            spring.dubbo.application.name=controller-consumer # 本服务的名称
            spring.dubbo.registry.address=zookeeper://IP:2182 # zookeeper所在服务器的IP和端口号
            spring.dubbo.scan=com.gaoxi # 引用服务的路径

        代码配置(一般不用)
			@Configuration
			public class Dubboconfig
			    @Bean
                public RegistryConfig registryConfig() {
                    RegistryConfig registryConfig = new RegistryConfig();
                    registryConfig.setAddress(dubboProperties.getAddress());
                    registryConfig.setClient(dubboProperties.getClient());
                    return registryConfig;
                }
			<dependency>
				<groupId>org.apache.dubbo</groupId>
				<artifactId>dubbo-spring-boot-starter</artifactId>
				<version>2.7.3</version>
			</dependency>

   19.  dubbo配置变化
         1.全XML的方式配置
         2.注册地址和协议等通过XML的方式配置,服务和消费可通过注解配置
         3.支持spring-boot-starter的方式，于是就有了dubbo-spring-boot-starter，通过Properties配置

         根据 DUBBO 官方文档，配置DUBBO有 4 种方式，分别是：
            1. XML 配置文件方式			推荐
            2. properties 配置文件方式	dubbo.properties
            3. annotation 配置方式		注解的方式 (忽略)
               <dubbo:annotation package="com.chanshuyi..." /> #扫描注解包路径，多个包逗号分隔，不填package，默认扫描当前applicationcontext中所有类
               @Reference(version="1.0.0")等
            4. API 配置方式			RegistryConfig、ServiceConfig等暂不介绍，(忽略)
            结合boot后，可以直接在application.properties中配置了

         DUBBO 在读取配置时，默认读取resources下的dubbo.properties 文件，但是会优先读取XML文件中的配置(两个都有，优先这个，忽略 properties 里的配置)

         一般用properties配置公共信息，比如可能一个应用要调用多个注册中心的服务，这时候它们的application.name、dubbo.protocol.name等都是相同的。其他情况，还是建议用 XML 配置方式。
   

      xml配置说明
         <dubbo:protocol name="dubbo" port="20881" threads="300" dispatcher="message"/>  #指定使用的协议是dubbo协议，这里是服务端暴露端口
            #使用固定端口暴露服务，而不要使用随机端口 这样在注册中心推送有延迟的情况下，消费者通过缓存列表也能调用到原地址，保证调用成功
            #服务提供者协议、服务的监听端口
            #指定线程池大小，线程模型是线程池和io线程处理，默认是all(全打到线程池)
         
         #提供方的缺省值，主要用来配置超时时间和重试次数
         <dubbo:provider token="false" timeout="20000" retries="0" filter="xyeProviderExceptionFilter"/>
            该标签为<dubbo:service>和<dubbo:protocol>标签的缺省值设置。也可以设置dubbo等
            
         #对应的还有<dubbo:consumer/> 消费方缺省配置
         <dubbo:consumer/>    消费端可以统一配置默认信息
            该标签为<dubbo:reference>标签的缺省值设置。
         
         #指定方法级别的配置，可以service，也可以reference。
         <dubbo:method/> 方法配置，用于ServiceConfig和ReferenceConfig指定方法级的配置信息
            该标签为<dubbo:service>或<dubbo:reference>的子标签，用于控制到方法级
               actives		每服务消费者最大并发调用限制
               executes	每服务每方法最大使用线程数限制，此属性只在 <dubbo:method>作为 <dubbo:service>子标签时有效
            
         以timeout配置为例，
         原则，1.方法级优先，接口级次之，全局配置再次之	
               2.如果级别一样，则消费方优先，提供方次之
               
         服务方提供配置，通过url经由注册中心传递给消费方	
         建议由服务放提供设置超时，因为调用时间由服务方决定。保持消费方配置简约。


         启动检查 check="false" ，关闭检查，默认是true
         默认check="true",启动检查依赖的服务是否可用，不可用抛异常，及时发现问题。
         
         关闭某个服务的启动时检查：(没有提供者时报错) <dubbo:reference interface="com.foo.BarService" check="false" /> 
         关闭所有服务的启动时检查：(没有提供者时报错) <dubbo:consumer check="false" />
         关闭注册中心启动时检查：(注册订阅失败时报错) <dubbo:registry check="false" />
            前面两个都是指订阅成功，但提供者列表是否为空是否报错，如果注册订阅失败时，也允许启动，需使用此选项，将在后台定时重试。
         
         引用缺省是延迟初始化的，只有引用被注入到其它Bean，或被getBean()获取，才会初始化。 如果需要饥饿加载，即没有人引用也立即生成动态代理，可以配置： 
         <dubbo:reference interface="com.foo.BarService" init="true" />  #不常用

      在Provider上尽量多配置Consumer端属性 
		原因如下： 
			1. 作服务的提供者，比服务使用方更清楚服务性能参数，如调用的超时时间，合理的重试次数，等等 
			2. 在Provider配置后，Consumer不配置则会使用Provider的配置值，即Provider配置可以作为Consumer的缺省值。 否则，Consumer会使用Consumer端的全局设置，这对于Provider不可控的，并且往往是不合理的。 
		PS:配置的覆盖规则：
			1)方法级配置别优于接口级别，即小Scope优先 
			2)Consumer端配置 优于 Provider配置 优于 全局配置，最后是Dubbo Hard Code的配置值（见配置文档） 

		Provider上尽量多配置Consumer端的属性，让Provider实现者一开始就思考Provider服务特点、服务质量的问题	
		
		在Provider可以配置的Consumer端属性有：
			1. timeout，方法调用超时
			2. retries，失败重试次数，缺省是2（表示加上第一次调用，会调用3次） 
			3. loadbalance，负载均衡算法（有多个Provider时，如何挑选Provider调用），缺省是随机（random）。 还可以有轮训(roundrobin)、最不活跃优先（leastactive，指从Consumer端并发调用最好的Provider，可以减少的反应慢的Provider的调用，因为反 应更容易累积并发的调用） 
			4. actives，消费者端，最大并发调用限制，即当Consumer对一个服务的并发调用到上限后，新调用会Wait直到超时。 在方法上配置（dubbo:method ）则并发限制针对方法，在接口上配置（dubbo:service），则并发限制针对服务。
		
		Provider上可以配置的Provider端属性有： 
			1. threads，服务线程池大小 
			2. executes，一个服务提供者并行执行请求上限，即当Provider对一个服务的并发调用到上限后，新调用会Wait（Consumer可能到超时）。在方法 上配置（dubbo:method ）则并发限制针对方法，在接口上配置（dubbo:service），则并发限制针对服务。
		
      三种属性配置，常用的是在xml中
         java 				    -Ddubbo.protocol.port=20880
         dubbo.xml			 <dubbo:protocol port:"30880" />
         dubbo.properties	 dubbo.protocol.port=20880
            #不要使用dubbo.properties文件配置，推荐使用对应XML配置 Dubbo2中所有的配置项都可以Spring配置中，并且可以针对单个服务配置。 # 如完全不配置使用Dubbo缺省值，参见Dubbo配置参考手册中的说明。 在Dubbo1中需要在dubbo.properties文件中的配置项，Dubbo2中配置示例如下：

         属性配置的覆盖策略
            1.jvm启动-D参数优先，方便启动覆盖重写
            2.xml次之
            3.properties最后，相当于默认，xml没配，dubbo.properties才会生效。通常用于共享公共配置，比如应用名。		


      <dubbo:registry>中有两个属性  管理服务的注册和订阅,(忽略)
         private Boolean register;	  // 在该注册中心上服务是否暴露
         private Boolean subscribe;	 // 在该注册中心上服务是否引用

   20. dubbo线程模型
         dubbo的线程模型中有两个重要角色 ，ThreadPool（业务线程池）和Dispatcher(调度器，调度io线程处理)

         Dispatcher是dubbo中的调度器，用来决定操作是在IO中执行还是业务线程池执行
         对于Dubbo集群中的Provider角色，有IO线程池（默认无界）和业务处理线程池（默认200）两个线程池，所以当业务的并发比较高，或者某些业务处理变慢，
         业务线程池就很容易被“打满”，抛出“RejectedExecutionException: Thread pool is EXHAUSTED! ”异常。当然，前提是我们每给Provider的线程池配置等待Queue。

         Dispatcher，默认是all，所有消息都发送发线程池
         配置成message，只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在IO线程上执行。

         threadpool 默认 fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省)
            dubbo默认线程池大小200 和tomcat一样,tomcat 可创建的最大线程数默认是200
            dubbo默认线程池是fixed

         协议的消息派发方式，用于指定线程 模型，比如：dubbo协议的all, direct, message, execution, connection等
   
   21. dubbo即是消费者也是提供者

         dubbo的服务者和消费者同时配置时，引入了dubbo-spring-boot-starter 2.7.7 会报application问题，单独引入 dubbo 2.7.7 就没问题
         
         *****既是消费者，也是提供者*****     服务之间，最好不要相互依赖。
         ```
               <?xml version="1.0" encoding="UTF-8"?>
               <beans xmlns="http://www.springframework.org/schema/beans"
                     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                     xmlns:dubbo="http://dubbo.apache.org/schema/dubbo"
                     xsi:schemaLocation="http://www.springframework.org/schema/beans
                     http://www.springframework.org/schema/beans/spring-beans-4.3.xsd
                     http://dubbo.apache.org/schema/dubbo
                     http://dubbo.apache.org/schema/dubbo/dubbo.xsd">

                  <!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 -->
                  <dubbo:application name="XyeIdsServer"  />

                  <!-- 注册dubbo monitor -->
                  <dubbo:monitor protocol="registry"/>

                  <!-- <dubbo:registry protocol="zookeeper" address="192.168.6.237:2181" /> -->
                  <dubbo:registry protocol="zookeeper" address="${zookeeper.address}" />

                  <!-- 消费接口 -->
                  <dubbo:reference check="false" id="requireDepositServiceSoa" interface="com.xiaoyuer.soa.api.service.IRequireDepositService"  />
                  
                  <!-- 用dubbo协议在20880端口暴露服务 -->
                  <dubbo:protocol name="dubbo" port="20883" />

                  <!-- 暴露服务  ids user -->
                  <dubbo:service interface="com.xiaoyuer.ids.api.service.IUserService" ref="userService"/>
               </beans>
         ``` 

   22. dubbo默认服务提供方的IP为内网IP，生产上需要映射成公网ip
         使用dubbo时，提供者注册时显示服务地址ip为[内网IP:20880]，导致其他消费者在外部连接的情况下时，调用dubbo服务失败
         这里将/etc/hosts中的hostname映射为公网ip
         但是系统中貌似没有这种场景，都是走的内网调用

   23. dubbo  qos  qos-server端口冲突(忽略)

         dubbo的重启多次启动导致的端口冲突  qos-server 端口冲突，
         经过网络查找，结果是Root WebApplicationContext 启动了两次，第二次报错，dubbo端口被占用
         qos主要用来对服务动态上下线，目前用不到
         解决办法：在tomcat的server.xml中设置 <Host appBase="webapps" autoDeploy="false" deployOnStartup="false" name="localhost" unpackWARs="false">
   
         Dubbo启动时qos-server can not bind localhost:22222
         问题原因
            consumer启动时qos-server也是使用的默认的22222端口，但是这时候端口已经被provider给占用了，所以才会报错的。
            我们将simple-consumer.xml改成如下即可解决这个问题了。
            一般服务器上的单机运行，端口不会冲突。


         本地解决方案
            	<!--     <dubbo:application name="xye-soa-core-require" />  -->    

               <dubbo:application name="xye-soa-core-require">
                  <dubbo:parameter key="qos.enable" value="false"/>
                  <dubbo:parameter key="qos.port" value="22223"/>
               </dubbo:application>


   24. dubbo_version

         当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。用的暂时不多，后面可以再详细看使用场景
         在低压力时间段，先升级一半提供者为新版本,再将所有消费者升级为新版本,然后将剩下的一半提供者升级为新版本
         同一个接口不同version，不同的实现类。

         多个版本的并存(一般用来实现灰度发布)
               就是一个权限接口类名，对应两个实现类，调用的时候指定对应的版本号即可
               低压力时段，让部分消费者先调用新的提供者实现类，其余的仍然调用老的实现类，在新的实现类运行没有问题的情况下，逐步让所有消费者全部调用成新的实现类。
               
               提供者
                  <dubbo:service interface="com.xxxx.rent.service.IDemoService" ref="iDemoService4" version="2.4.4"/>
                  <dubbo:service interface="com.xxxx.rent.service.IDemoService" ref="iDemoService5" version="2.4.5"/>
               
               消费者
                  <dubbo:reference id="iDemoService5" interface="com.xxxx.rent.service.IDemoService" version="*"/>

         rpc调用原理相关
            一般的服务区分是，interfacename+version+group 
            version的使用场景在于一个大型的分布式系统中，修改更细某个接口，在过渡状态，1.新增方法（多余，重复）2.version区分

         服务升级
            接口不变，直接重发即可。同名起名可能重复点，不好命名。其实这种快点
            接口表动
               1.新增接口
               2.接口参数列表变化，
                  1.全部消费端一起修改，全部重发，动静大，无关消费者躺枪
                  2.版本号，老方法调用原来的，新方法使用新版本服务。对应两个实现类即可，version不一样。新实现类中实现新功能方法即可，老的没动的还是用原来的实现类。
                  3.考虑扩展性使用类似map的结构，但不直观，参数校验复杂。

         版本
            每个接口都应定义版本号，为后续不兼容升级提供可能，如：<dubbo:service interface="com.xxx.XxxService" version="1.0" /> 
            建议使用两位版本号，因为第三位版本号通常表示兼容升级，只有不兼容时才需要变更服务版本。 
            当不兼容时，先升级一半提供者为新版本，再将消费者全部升为新版本，然后将剩下的一半提供者升为新版本。	
                  
         		
         兼容性
            服务接口增加方法，或服务模型增加字段，可向后兼容，删除方法或删除字段，将不兼容，枚举类型新增字段也不兼容，需通过变更版本号升 级。
            各协议的兼容性不同，参见： 服务协议


   25. dubbo 容错，降级 限流

         关注下，基于调度中心，资源调度和服务治理
         容错机制	共六种集群容错机制
            默认的容错机制就是：失败自动切换，重试其它服务器。通常用于读操作。默认2次，可通过 retries="2" 来设置重试次数。

            还有快速失败，失败定时重发等暂不细看
            
         服务降级 代码配置或者dubbo admin配置    目的是为了保证核心服务可用。
            消费方对该服务的方法调用都直接返回null值，不发起远程调用，不抛异常。 
            屏蔽不重要服务不可用时对调用方的影响。

            1.在xml中配置mock属性：在远程调用异常时，服务端直接返回一个固定的字符串(也就是写死的字符串)
            2.创建一个单独的mock类：在远程调用异常时，服务端根据自定义mock业务处理类进行返回)，接口名要注意命名规范：接口名+Mock后缀，mock实现需要保证有无参的构造方法。
            
            使用场景
            不可用或者响应时间太长时，快速返回错误的响应信息
               1) 多个服务之间可能由于服务没有启动或者网络不通，调用中会出现远程调用失败;
               2) 服务请求过大，需要停止部分服务以保证核心业务的正常运行；
                  服务降级，当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。舍车保帅
               
            对一些非核心服务进行人工降级，在大促之前通过降级 开关关闭哪些推荐内容、评价等对主流程没有影响的功能?
            故障降级，比如调用的远程服务挂了，网络故障、或者 RPC服务返回异常。 那么可以直接降级，降级的方案比 如设置默认值、采用兜底数据（系统推荐的行为广告挂 了，可以提前准备静态页面做返回）等等?
            限流降级，在秒杀这种流量比较集中并且流量特别大的 情况下，因为突发访问量特别大可能会导致系统支撑不 了。这个时候可以采用限流来限制访问量。当达到阀值 时，后续的请求被降级，比如进入排队页面，比如跳转 到错误页（活动太火爆，稍后重试等）
            

         服务限流
            <dubbo:service interface="com.foo.BarService" executes="10" /> 也可以精确到方法上

         负载均衡策略
            dubbo默认的负载均默认是随机调用法。
            一共有4种负载均衡策略：
            RandomLoadBalance				   随机调用负载均衡；
            RoundRobinLoadBlance 			轮询调用；
            LeastActiveLoadBlance			最少活跃数调用法，使慢的提供者收到更少请求，就是按性能接收
            ConsistentHashLoadBalance 		一致性Hash算法，相同参数的请求总是发到同一提供者；

   26. dubbo服务超时调用
         默认是服务不成功，重试两次。如果服务端处理时间超过了设定的超时时间，就会有重复请求
            1.对于核心的服务中心，去除dubbo超时重试机制，并重新评估设置超时时间。
            2.业务处理代码必须放在服务端，客户端只做参数验证和服务调用，不涉及业务流程处理

         <dubbo:provider delay="-1" timeout="6000" retries="0"/>  加大超时时间(接口配置>全局配置)

         一个是provide提供的超时参数(推荐服务端配置)，还有一个是consumer提供的超时参数
         服务消费者端设置超时时间，如果在消费者端设置了超时时间，以消费者端为主，即优先级更高。
         
         出现超时异常的时候服务端可不会停下来，你客户端为了保证高可用一顿重试最后结果就是重复请求,需要幂等处理。
         
      
         zookeeper关闭 不影响调用，服务关闭当然失败，新版的dubbo2.7.7 提供者断开后，消费者调用失败，过段时间提供者重启，消费者可以连上

         dubbo超时，这是老的版本配置了
            针对服务端重启，消费者超时，这个在duboo的源码中有个配置，对应的duboo中的源码： com.alibaba.dubbo.common.Constants DEFAULT_SESSION_TIMEOUT = 60 * 1000; 这个时间配置了，需要重新打jar才行
         
   27. dubbo的同步异步调用（忽略）
         DubboInvoker中代码有三种调用方法，分别是：忽略返回值调用、异步调用和同步调用。
         
         在dubbo-demo-consumer.xml中配置调用服务信息，设置为异步调用async="true" ，无需要返回结果，就return="false"，
            <dubbo:reference id="asyncDemoService" check="false" interface="com.alibaba.dubbo.demo.AsyncDemoService">
               <dubbo:method name="sayHello" async="true"/>
            </dubbo:reference>
         
         需要返回结果，就Future<String> future =  RpcContext.getContext().getFuture();
         String hello = future.get(1, TimeUnit.SECONDS);//这个步骤是阻塞的
         
         Dubbo Consumer 端发起调用后，同时通过RpcContext.getContext().getFuture()获取跟返回结果关联的Future对象，然后就可以开始处理其他任务；
         当需要这次异步调用的结果时，可以在任意时刻通过future.get(timeout)来获取。
         
         
         同步调用过程： 阻塞的过程在于先调用了future的get方法
            1.将请求封装为Request对象，并构建DefaultFuture对象，请求ID和Future对应。
            2.通过Netty发送Request对象，并返回DefaultFuture对象。
            3.调用DefaultFuture.get()等待数据回传完成。
            4.服务端处理完成，Netty处理器接收到返回数据，通知到DefaultFuture对象。
            5.get方法返回，获取到返回值。

         
         异步使用案例：将future 返回到 RpcContext.getContext()    future的get方法后置
            fooService.findFoo(fooId);// 此调用会立即返回null
            Future<Foo> fooFuture = RpcContext.getContext().getFuture();// 拿到调用的Future引用，当结果返回后，会被通知和设置到此Future 
            
            barService.findBar(barId);// 此调用会立即返回null
            Future<Bar> barFuture = RpcContext.getContext().getFuture(); // 拿到调用的Future引用，当结果返回后，会被通知和设置到此Future
               
            // 此时findFoo和findBar的请求同时在执行，客户端不需要启动多线程来支持并行，而是借助NIO的非阻塞完成
            // 如果foo已返回，直接拿到返回值，否则线程wait住，等待foo返回后，线程会被notify唤醒
            Foo foo = fooFuture.get(); 
            Bar bar = barFuture.get(); // 同理等待bar返回
               
            //如果foo需要5秒返回，bar需要6秒返回，实际只需等6秒，即可获取到foo和bar，进行接下来的处理。
            
            RpcContext的一次调用生命周期。
               RpcContext.getContext().setAttachment   这个传递参数
               
               因此每发起RPC调用，上下文状态会变化。
               消费端在执行Rpc调用之前，经过Filter处理, 会将信息写入RpcContext.见ConsumerContextFilter，消费者在执行调用之前(AbstractInvoker)，会将RpcContext中的内容写入到invocation，实现参数传递
               服务端在执行调用之前，也会经过Filter处理，将信息写入RpcContext. 见ContextFilter类
               会判断ASYNC_KEY是否为true,如果是，则会向context中写入future对象：

         异步调用（忽略）
            异步处理，Future返回，get获取结果，用的也不多
            如果你只是想异步，完全忽略返回值，可以配置return="false"，以减少Future对象的创建和管理成本： <dubbo:method name="findFoo" async="true" return="false" />
         
   28. dubbo多注册点
         通过注册不同的zookeeper，去找对应不同机器上的服务,相应的提供者和消费者的都需要指定对应的注册中心
         //指定不同的接口，注册不同的zookeeper，(本地测试用，主要是不想本地启动多个服务，去不同机器上找服务)

         <dubbo:registry id="soaRegistry" protocol="zookeeper" address="${zookeeper.address.soa}" />
         <dubbo:registry id="idsRegistry" protocol="zookeeper" address="${zookeeper.address.ids}" />

         <dubbo:service interface="com.xiaoyuer.soa.api.service.IRecruitRequireService" ref="recruitRequireService" registry="soaRegistry"/>
         <dubbo:reference interface="com.xiaoyuer.ids.api.service.IUserService" id="userService" timeout="60000" check="false" registry="idsRegistry"/>

   29. 集群容错
         默认是failover，失败自动切换重试其他服务器。可通过retries="2"来设置重试次数(不含第一次)。
         其他的快速失败等策略暂不用
      负载均衡
         默认是random随机调用
         其他如轮询，最少活跃调用数等暂不用
         
         一致性Hash，相同参数的请求总是发到同一提供者，当某一 台提供者挂时，原本发往该提供者的请求，基于虚拟节点， 平摊到其它提供者，不会引起剧烈变动	
         
      传输协议
         dubbo	默认使用
            Dubbo缺省协议采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数 的情况。 
            Dubbo缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。
         rmi
         hessian
         
      序列化 
         dubbo协议缺省为hessian2， rmi协议缺省为java，http协议 缺省为json
      group
         当一个接口有多种实现的时候，可以使用group，但其实也没必要，放一个里面也行吧
         <dubbo:service group="feedback" interface="com.xxx.IndexService" />
         <dubbo:service group="member" interface="com.xxx.IndexService" />
         
      version	
         当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。
         在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本
         之前整理过了，先不看
         
      结果缓存
         一般不用，调用接口缓存小心使用

   30. dubbo配置类解析，解析配置代码或者xml

      解析类DubboNamespaceHandler，其中定义了xml中标签的解析类,能看到有哪些配置类
		DubboBeanDefinitionParser类根据xml中的内容，加载了dubbo的详细配置，具体的配置类加载。
		所有配置项分为三大类，参见下表中的"作用"一列。 

		服务发现：表示该配置项用于服务的注册与发现，目的是让消费方找到提供方。 			服务注册订阅等
		服务治理：表示该配置项用于治理服务间的关系，或为开发测试提供便利条件。 			令牌验证等
		性能调优：表示该配置项用于调优性能，不同的选项对性能会产生影响。				超时，异步，重试等

		主要是	 com.alibaba.dubbo.config.ServiceConfig 
					com.alibaba.dubbo.config.ReferenceConfig 
					com.alibaba.dubbo.config.ProtocolConfig 
					com.alibaba.dubbo.config.RegistryConfig等
		
		注解API 	com.alibaba.dubbo.config.annotation.Service 
					com.alibaba.dubbo.config.annotation.Reference
				
		上下文API 	com.alibaba.dubbo.rpc.RpcContext
	
		注意：只有group，interface，version是服务的匹配条件，三者决定是不是同一个服务，其它配置项均为调优和治理参数
	
		所有配置最终都将转换为URL表示，并由服务提供方生成，经注册中心传递给消费方，各属性对应URL的参数，参见配置项一览表中的"对应URL参 数"列。 
		URL格式：protocol://username:password@host:port/path?key=value&key=value

   31. dubbo调用的隐式传参
		   //setAttachment设置的KV，在完成下面一次远程调用会被清空。即多次远程调用要多次设置
		服务消费方
			//隐式传参，后面的远程调用都会隐式将这些参数发送到服务器端，类似cookie，用于框架集成，不建议常规业务使用 
			RpcContext.getContext().setAttachment("index", "1"); 
			xxxService.xxx(); // 远程调用
			
		服务提供方
			String index = RpcContext.getContext().getAttachment("index"); // 获取客户端隐式传入的参数，用于框架集成，不建议 常规业务使用//
   
      RpcContext是一个ThreadLocal的临时状态记录器，当接收到RPC请求，或发起RPC请求时，RpcContext的状态都会变化。
	      比如：A调B，B再调C，则B机器上，在B调C之前，RpcContext记录的是A调B的信息，在B调C之后，RpcContext记录的是B调C的信息。

   32. 其他功能点
         并发控制,一般限制服务端
            限制com.foo.BarService的每个方法，服务器端并发执行（或占用线程池线程数）不能超过10个： 
               <dubbo:service interface="com.foo.BarService" executes="10" /> 
            限制com.foo.BarService的sayHello方法，服务器端并发执行（或占用线程池线程数）不能超过10个： 
            <dubbo:service interface="com.foo.BarService"> 
               <dubbo:method name="sayHello" executes="10" />
            </dubbo:service>
            
         连接控制, 一般设置服务端
            限制服务器端接受的连接不能超过10个：（以连接在Server上，所以配置在Provider上） 
            <dubbo:provider protocol="dubbo" accepts="10" /> 
            <dubbo:protocol name="dubbo" accepts="10" /
         
         令牌验证
            主要作用
               防止消费者绕过注册中心访问提供者 
               在注册中心控制权限，以决定要不要下发令牌给消费者 
               注册中心可灵活改变授权方式，而不需修改或升级提供者

            <dubbo:provider interface="com.foo.BarService" token="true" />

            
         优雅停机
            Dubbo是通过JDK的ShutdownHook来完成优雅停机的，所以如果用户使用"kill -9 PID"等强制关闭指令，是不会执行优雅停机的，只有 通过"kill PID"时，才会执行。
            
               服务提供方停止时，先标记为不接收新请求，新请求过来时直接报错，让客户端重试其它机器。 
               然后，检测线程池中的线程是否正在运行，如果有，等待所有线程执行完成，除非超时，则强制关闭。 
               
               服务消费方停止时，不再发起新的调用请求，所有新的调用在客户端即报错。 
               然后，检测有没有请求的响应还没有返回，等待响应返回，除非超时，则强制关闭。
               
               <dubbo:application ...> 
                  <dubbo:parameter key="shutdown.timeout" value="60000" /> 				#单位毫秒
               </dubbo:application>
               
               小鱼儿实际是在类路径下的dubbo.properties 中设置dubbo.service.shutdown.wait=90000
               
         服务容器
            因为是服务功能是独立的程序，不需要tomcat这种web容器，增加复杂性，浪费资源
            服务容器只是一个简单的Main方法，并加载一个简单的Spring容器，用于暴露服务
            
            Spring Container 自动加载META-INF/spring目录下的所有Spring配置。 
            配置：(配在java命令-D参数或者dubbo.properties中) dubbo.spring.config=classpath*:META-INF/spring/*.xml ----配置spring配置加载位置
            
            容器启动 如：(缺省只加载spring) java com.alibaba.dubbo.container.Main   (只是旧版，新版已经移交给apache了)
            soa 启动实例：
            import org.apache.dubbo.container.Main;
            public static void main(String[] args) {
               new SpringApplicationBuilder(XyeServiceCoreApplication.class).web(WebApplicationType.NONE).run(args);
               Main.main(args);
            }

         监控中心
            通过注册中心发现监控中心服务: 
               <dubbo:monitor protocol="registry" /> 
               或： dubbo.properties中dubbo.monitor.protocol=registry
               
            但不注册到注册中心	
               直连监控中心服务: 
               <dubbo:monitor address="dubbo://127.0.0.1:7070/com.alibaba.dubbo.monitor.MonitorService" /> 
               或： <dubbo:monitor address="127.0.0.1:7070" /> 
               或： dubbo.properties 中dubbo.monitor.address=127.0.0.1:7070

   33. zookeeper
            目录节点的动作
               服务提供者启动时 向/dubbo/com.foo.BarService/providers目录下写入自己的URL地址。 
               服务消费者启动时 订阅/dubbo/com.foo.BarService/providers目录下的提供者URL地址。并向/dubbo/com.foo.BarService/consumers目录下写入自己的URL地址。 
               监控中心启动时 订阅/dubbo/com.foo.BarService目录下的所有提供者和消费者URL地址
               
            当提供者出现断电等异常停机时，注册中心能自动删除提供者信息。 
            当注册中心重启时，能自动恢复注册数据，以及订阅请求。 
            当会话过期时，能自动恢复注册数据，以及订阅请求。 
            当设置<dubbo:registry check="false" />时，记录失败注册和订阅请求，后台定时重试
               消费者启动时，没有提供者是否抛异常fast-fail
               

            dubbo中注册中心zookeeper中，默认使用zkclient， 从2.3.0版本开始支持可选curator实现
            <dubbo:registry ... client="zkclient" />
            
            单机配置 
               <dubbo:registry address="zookeeper://10.20.153.10:2181" />
               Or: <dubbo:registry protocol="zookeeper" address="10.20.153.10:2181" />
            这两个都是可以的
               
            Zookeeper集群配置: 
               <dubbo:registry address="zookeeper://10.20.153.10:2181?backup=10.20.153.11:2181,10.20.153.12:2181" /> 
               Or: <dubbo:registry protocol="zookeeper" address="10.20.153.10:2181,10.20.153.11:2181,10.20.153.12:2181" />	
                  

   34. 异常
         服务提供方不应将DAO或SQL等异常抛给消费方，应在服务实现中对消费方不关心的异常进行包装，否则可能出现消费方无法反序列化相应异常。
         调用不要只是因为是Dubbo调用，而把调用Try-Catch起来。Try-Catch应该加上合适的回滚边界上
		
   33. 显示传参
      //消费端
         ```
         @Aspect
         @Component
         public class MethodInterceptor {
            protected final Logger logger = LoggerFactory.getLogger(MethodInterceptor.class);
            @Pointcut("execution(* com.xiaoyuer.soa.api.*.*.*(..))")
            private void dubboSurface(){};
            @Around("dubboSurface()")
            public Object around(ProceedingJoinPoint pj){
               //获取切入点方法需要的参数
               Object[] args = pj.getArgs();
               try {
                     ServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
                     if(requestAttributes != null) {// requestAttributes不为空 且 Rpc参数为空的情况进入
                        HttpServletRequest request = requestAttributes.getRequest();
                        Cookie cookie = CookieUtils.getCookie(request, "loginUser");
                        String no = cookie != null ? Strings.padStart(cookie.getValue(), 8, '0'):"00000000";
                        String sessionId = request.getSession().getId();
                        RpcContext.getContext().setAttachment("sessionId", sessionId);
                        RpcContext.getContext().setAttachment("invokeNo", no);
                     }
                     //调用切入点方法
                     Object res = pj.proceed(args);
                     return res;
               } catch (Throwable throwable) {
                     logger.error("主站dubbo切面类异常");
                     throw new RuntimeException(throwable);
               }
            }
         }
         ```

      //服务端
         ```
            public class XyeProviderExceptionFilter implements Filter {
               private static final Logger LOG = LoggerFactory.getLogger(XyeProviderExceptionFilter.class);
               @Override
               public Result invoke(Invoker<?> invoker, Invocation invocation) throws RpcException {
                  String sessionId = RpcContext.getContext().getAttachment("sessionId");
                  String invokeNo = RpcContext.getContext().getAttachment("invokeNo");
                  MDC.put("sessionId",sessionId);
                  MDC.put("invokeNo",invokeNo);
                  Result result = invoker.invoke(invocation);
                  if(result.hasException()) {
                     LOG.error("dubbo provider Exception:{}",result.getException());
                  }
                  MDC.remove("sessionId");
                  MDC.remove("invokeNo");
                  return result;
               }
            }
         ```



## IO相关，网络相关，流相关，io流相关

   io流 输入(读),输出(写)（相对内存而言  主要是内存和外围设备（打印机，硬盘等））
      电脑存储的原理：二进制--码表--信息

   对应输入和输出，inputstream和outputstream都是通过字节来与终端进行输入输出的。这两个只是定义了最基本的api，具体实现由子类实现
	buffer是与io交互过程中的一个缓冲区，这些缓冲区被反复清空和写入，利用它可以在交互过程中达到一次交互一批信息，而不是1个字节。

1. 转二进制	// 序列化
      ```
         baos = new ByteArrayOutputStream();
         oos = new ObjectOutputStream(baos);
         oos.writeObject(object);
         byte[] bytes = baos.toByteArray();
         return bytes;
      ```

2. 字节大小，字节单位
      一个字节等于8位  1byte = 8bit， int占用4个字节，long 8个字节，
      字节是数据存储的基础单位
      bit（比特）是表示信息的最小单位，是二进制数的一位包含的信息。一个bit是一个0或1，中文叫做一个二进制位
      16进制编码
         每4位二进制对应一位16进制数据
         一个字节包含8位二进制(8bits)，转为2位的16进制

3. IOUtils.copy(fis, toClient)   输入流 和 输出流的转换
      直接实现文件从输入流到输出流的文件写入
      ```
         try {
            toClient = new BufferedOutputStream(response.getOutputStream());
            fis = new BufferedInputStream(new FileInputStream(myfile));
            IOUtils.copy(fis, toClient);  //通过ioutil 对接输入输出流，实现文件下载
            toClient.flush();
         } catch (Exception e) {
            throw new RuntimeException("文件下载失败");
         } finally {
            //关闭流
            IOUtils.closeQuietly(fis);
            IOUtils.closeQuietly(toClient);
         }
      ```

4. 字节流 和 字符流
      流操作分为两种：字节流和字符流（字节流+编码表）
      * 字节流
            字节流读取的时候，读到一个字节就返回一个字节
            字节流可以处理所有类型数据，如：图片，MP3，AVI视频文件
            字节流的抽象基类：操作对象是字节数组,无需编解码，无需缓冲刷新，但需要关闭

      * 字符流
            字符流只能处理字符数据
            字符流使用了字节流读到一个或多个字节时。先去查指定的编码表，将查到的字符返回。
            只要是处理纯文本数据，就要优先考虑使用字符流，除此之外都用字节流。 

5. 流文件的附件下载	
   	页面doc下载
         ```
         windows路径跳转
         @RequestMapping("/ceshidoc")
         public void ceshidoc(HttpServletRequest request, HttpServletResponse response) throws Exception {
            FileInputStream fileInputStream = new FileInputStream(new File("D:/eclipse - pay/file/ceshi.doc"));
            byte[] read = read(fileInputStream);
            String realName="hahahah_"+DateUtil.getCurrentDate("yyyyMMddHHmmss")+".doc";
            if (request.getHeader("user-agent").toLowerCase().contains("msie")) {  
               realName = URLEncoder.encode(realName, "UTF-8"); // IE  
            } else {  
               realName = new String(realName.getBytes("UTF-8"), "ISO-8859-1"); // 非IE  
            } 
         
            response.reset();//设置为没有缓存
            response.setContentType("application/x-download;charset=GBK");
            /*attachment是以附件下载的形式，inline是以线上浏览的形式。当点击“保存”的时候都可以下载，当点击“打开”的时候attachment是在本地机里打开，inline是在浏览器里打开。*/
            response.setHeader("Content-disposition", "attachment; filename=" + realName);		
            OutputStream output = response.getOutputStream();
            byte[] buf = new byte[1024];
            int r = 0;
            ByteArrayInputStream bin = new ByteArrayInputStream(read);
            while ((r = bin.read(buf, 0, buf.length)) != -1) {
               output.write(buf, 0, r);
            }    
            response.getOutputStream().flush();
            response.getOutputStream().close();
         }
         public static byte[] read(InputStream inStream) throws Exception {
            ByteArrayOutputStream outStream=new ByteArrayOutputStream();
            byte[] buffer=new byte[1024];
            int len=0;
            while((len=inStream.read(buffer))!=-1){
               outStream.write(buffer,0,len);
            }
            inStream.close();
            return outStream.toByteArray();
         }
         ```

   ​	 导出excel为什么不能用ajax请求？用表单提交

   ​		因为导出excel，实际上是文件下载，后台需要往前端（浏览器）写文件流（浏览器解析）的。
   ​		而Ajax请求获取数据都是“字符串”，整个交互传输用的都是字符串数据，它没法解析后台返回的文件流，但浏览器可以。
​	 
   ​		ajax，看名字asyn JavaScript and xml，是不能传输2进制数据的，怎么可能导出，直接请求servlet或者action（struts2）或者controller（spring mvc），
   ​		将导出的文件流放入到response的outputStream中，同时设置好header和contentType就可以了

         ajax 其实只是一个javascript中的一个组建 XmlHttpRequest, 他的作用是数据交互, 返回数据是组建内部处理的, 下载是需要浏览器识别http头的,所以肯定只能用iframe
         可以使用表单提交，和地址重定向location.href就行,导出需要http的头，是属于页面级别的。

   ​		Ajax与Form表单提交的区别：
   ​			Ajax提交不会自动刷新页面，需要手动处理。
   ​			Form表单提交在数据提交后会刷新页面，如果是Post提交，点击刷新浏览器会提示 是否再次提交。

6. 使用相对的路径读取文件,类路径加载
   			
      ```
         String privateKeyFile ="filters/xye-ac.properties";
         InputStream resourceAsStream = RequireOrderAllinpayServiceImpl.class.getClassLoader().getResourceAsStream(privateKeyFile);  	

         Thread.currentThread().getContextClassLoader().getResourceAsStream("abc.properties") 	//默认是classpath下
      
      ​		-- 使用文件路径读取
      ​		String configFile ="tlqb/private.key";
      ​		URL classPath = Thread.currentThread().getContextClassLoader().getResource("");
      ​		String proFilePath = classPath.toString();
      ​							
      ​		//移除开通的file:/六个字符
      ​		proFilePath = proFilePath.substring(6); 
      ​							
      ​		//如果为window系统下,则把路径中的路径分隔符替换为window系统的文件路径分隔符
      ​		proFilePath = proFilePath.replace("/", java.io.File.separator);
      ​							
      ​		//兼容处理最后一个字符是否为 window系统的文件路径分隔符,同时建立 properties 文件路径
      ​		//例如返回: D:\workspace\myproject01\WEB-INF\classes\config.properties
      ​		if(!proFilePath.endsWith(java.io.File.separator)){
      ​			proFilePath = proFilePath + java.io.File.separator + configFile;
      ​			} else {
      ​			proFilePath = proFilePath + configFile;
      ​		}
      ​		FileInputStream publicKeyStream = new FileInputStream(proFilePath);
      ​		PublicKey publickey = SecurityUtils.loadPublicKey(publicKeyStream);
      
      ​	 属性文件的读取
   ​			String filename = "com/luhy/test/ReadProperties.properties";  
   ​    		Properties props = new Properties();  
            props.load(ReadProperties.class.getClassLoader().getResourceAsStream(filename));  
            String h = props.getProperty("v"); 
      ```

      此时获得根路径为当前系统应用程序的class路径,如 /D:/sts_pay_workspaces/xye-netpay-plugins/target/classes/
         String oriPath = this.getClass().getClassLoader().getResource("/").getFile();
         if ("\\".equals(File.separator)) {
            oriPath = oriPath.substring(1, oriPath.length());
         } else if ("/".equals(File.separator)) {
            //linux
         } 
      
      获取当前工作目录
         system.getproperty( user.dir )
         Windows:C:\Documents and Settings\当前的xx用户
         Linux: 当前的用户目录，比如/home/xxx

      类加载路径
         Test.class.getClassLoader().getResource("")=Test.class.getResource("/")
         ClassLoader.getResource的path中不能以/开头，path是默认是从ClassPath根目录下进行读取的否则读取为null
         Class.getResource(String path)		path不以’/'开头时，默认是从此类所在的包下取资源；path以’/'开头时，则是从ClassPath根下获取；	
         getResourceAsStream  这个读取的是流文件


7. 流的输出返回	
      
      ```
         Utils.resultJson(response, new JSONObject().toJSONString());
               public static void resultJson(HttpServletResponse response, String jsonString) {
                  PrintWriter pw = null;
                  try {
                     pw = response.getWriter();
                     pw.print(jsonString);
                     pw.flush();
                     pw.close();
                  } catch (IOException e) {
                     e.printStackTrace();
                  } finally {
                     if (null != pw) {pw.close();}
                  }
               }
               
      ```
   ​	@responseBody效果等同于response.getwriter.writer(jsonString)流输出

8. String property = System.getProperty("user.dir");
   	在tomcat中部署显示     D:\apache-tomcat-sso\bin
      在eclipse中显示	       D:\eclipse
      C:\Users\xiaoyuer\git\xye-netpay\xye-netpay-pom\xye-netpay-service/file/  windows系统的路径显示，在项目后的地址为/

9. 编码相关

      两端交互的时候，发送端指定编码，接收方使用固定编码接收即可

      计算机中存储信息的最小单元是一个字节，即8bit们表示的字符范围是0~255个
		ascII,gbk,utf8等可以被看做字典，规定了转化的规则。通常不使用操作系统默认的编码，否则跨环境会出现乱码。

      * 编码转换
         * 流中编码转换
            utf-8系统接收中兴的gbk编码问题，直接在文件流中转即可，看了半天
            BufferedReader in = new BufferedReader(new InputStreamReader(urlCon.getInputStream(),"GBK"));
            StringBuffer strBuff = new StringBuffer();
            String line;
            while ((line = in.readLine()) != null) {
               strBuff.append(line);
            }
            return strBuff.toString();

         * string类的字符字节转换
               String str="海绵宝宝";
               byte[] bytes = str.getBytes("UTF-8");
               String strr = new String(bytes, "UTF-8");

         * json串的转义相关
               String ret = OpenUtil.sendUrl(postUrl);
               JSONObject parseObject = JSONObject.parseObject(ret);
         //		JSONArray array = JSONObject.parseArray(ret);
               
               jsonObject.put("code", "00000");
               jsonObject.put("info", parseObject);
               
               Utils.resultJson(response, jsonObject.toJSONString());
               这里如果直接ret放进json中，就有/的转义符号

         * 请求中转义
            response.setCharacterEncoding("UTF-8");//发送方固定统一编码utf8  然后接收方 的流处理使用统一编码解码即可

            请求路径，特定字段编码
            post请求中，"转码，url中的正常&是不能转义的
            String  withdrawUrl = "/open/withdraw?certNo="+idCardNo +"&callbackUrl="+URLEncoder.encode(callBackUrl,"UTF-8")+"&name="+URLEncoder.encode(user.getUserName(),"UTF-8");

            费劲的转json格式（忽略）
               String param = dataForSign.replaceAll("\"", "%22").replaceAll("}", "%7d").replaceAll("\\{", "%7b").replaceAll(" ", "%20");

            传输带有特殊字符的编码过就可以传后端（忽略）
               [{"amount":"200","orderId":"1"},{"amount":"300","orderId":"2"}]
               %5B%7B%22amount%22%3A%22200%22%2C%22orderId%22%3A%221%22%7D%2C%7B%22amount%22%3A%22300%22%2C%22orderId%22%3A%222%22%7D%5D
            
         * UrlBase64字符编码相关
            ```
               String aa="我是革命一块砖";
               byte[] utf8s = UrlBase64.encode(aa.getBytes(Charset.defaultCharset()));
               String s = new String(utf8s, Charset.defaultCharset());
               byte[] decode = UrlBase64.decode(s.getBytes(Charset.defaultCharset()));
               String s1 = new String(decode, Charset.defaultCharset());
               System.out.println(s1);//最终s1是"我是革命一块砖"
            ```

      * 转码后的乱码
         GBK转成utf8后乱码，gbk字节使用utf8解码：?????δ?? ???????1004
         再将乱码使用utf8编码 得到42 个字节，得到的不是原来的字节了	

         DS205[平台商户未签约支付渠道1004]  gbk的流使用utf8解码，自然是乱码的字符串，然后再用乱码字符串进行utf8编码，本来就不识别的码的字符串，自然转不成正确的字符串
         字符串：平台商户未签约支付渠道1004    gbk 26个字节

         每种字符集都会对它所包含的字符进行相应的数字编码，是字符与字节转换过程中的公式或者映射表。
         如果发送方用utf8编码，接收方用gbk解码，这种转换错误，无法赚回来，但是有的字符集可以，比如iso8859-1
         当错误转码为？，这时候想在还原为字节，即使?对应的字节了，意味着之前的字符还原不了了

         odd=new String(odd.getBytes(),"GBK"); 使用utf8编码在用gbk解码肯定乱码
         把JVM内存中unicode形式的String按encoding制定的编码，
         JVM内部的String，Char都是用unicode存储(没有任何编码)
   

	   * 编码,乱码,字符编码,转码，编辑器乱码（忽略）
         NotePad++
            其实ANSI并不是某一种特定的字符编码，而是在不同的系统中，ANSI表示不同的编码。
            ANSI不是固定的字符编码，不同的系统中，ANSI表示不同的编码。美国系统-ASCII编码，简体中文系统-GBK编码，韩文系统对应EUC-KR编码
         
            *****汉字在电脑显示 实际上计算机只认识编码后的信息，需要根据编码后的信息去字典去找的汉字*****
            这里注意以gbk编码和转为的区别，以是选择当前的编码直接显示(相当于直接去字典找对应编码的字)，转为是转换为另一个字典中的编码

         <META content="text/html; charset=ASCII" http-equiv=Content-Type>
         编码识别正常使用，不识别就默认使用gbk。gb2312没收录的也使用gbk，很可能是因为会去寻找系统默认的编码
         电脑的记事本在打开时候会根据编码自动选择对应编码打开，联通是特殊(默认ANSI保存，编码形同utf8，打开是ut8就会乱码)
         就是说只要编码和解码都是用系统默认就不会有问题
		
		   idea中文件是gbk的  汉字显示正常后再转为utf8格式，再复制到eclipse中


10. tcp三次握手的
		服务器监听请求，客户端发起连接请求（第一次连接），
		请求在路上可能存在丢失的风险，所以当请求到了服务器后如果服务器同意建立连接会给客户端一个回信（第二次连接），告诉它：我已经收到请求，可以连接。
		但是回信也存在一个问题，那就是回信能不能到客户端？它需要客户端给他一个回信说我已经收到批准通知了，如果客户端一直不回复的话意味着客户端没有收到批准通知。因此客户端一收到批准通知就立马回复（第三次握手）：OK老铁我收到你的批准通知了。至此，三次握手结束
		
		TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态；
		1.client发送server，TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号
		2.server接收数据，如果同意连接，则发出确认报文，这个报文也不能携带数据，但是同样要消耗一个序号
		3.client收到确认后，还要向服务器给出确认。此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。
		
		tcp关闭的时候是四次握手，这个不深究了。

12. NAT的最典型应用是：在一个局域网内，只需要一台计算机连接上Internet，就可共享Internet连接，使局域网内其他计算机也可以上网。使用NAT协议，
    局域网内的计算机可以访问Internet上的计算机，但Internet上的计算机无法访问局域网内的计算机。


13. 流操作，写文件，流文件，文件流
		拿到字节，直接输出字节，将字节输入流通过输出流转化

		直接存文本格式，输入流转输出流
			private void zxParseReturnMsg(String msg, HttpServletRequest request, String date) {
				String path = request.getSession().getServletContext().getRealPath("");
				String filePath = path + "/zxPay" + date + ".txt";
				try {
					writeBill(msg, filePath, "UTF-8");
				} catch (Exception e) {}
				Scanner s = null;
				try {
					s = new Scanner(new BufferedReader(new FileReader(filePath)));
					while (s.hasNextLine()) {
						lineString=s.nextLine();
						} catch (FileNotFoundException e) {} finally {  
					 if (s != null) {   
						 s.close();   
					 }   
				}
			}
			
			private void writeBill(String billContext, String filePath, String encoding)throws Exception {
				InputStream bis =null;
				try{
					if(encoding==null||"".equals(encoding)){bis = new ByteArrayInputStream(billContext.getBytes("GBK"));}else{bis = new ByteArrayInputStream(billContext.getBytes(encoding));}
					OutputStream os = new FileOutputStream(filePath);
					IOUtils.copy(bis, os);
					bis.close();
					os.close();
					LOGGER.info("保存文件:"+filePath);
				}catch(IOException e){}
			
			}

		fileAsString = new String(EntityUtils.toByteArray(response.getEntity()),"utf-8"); 这是http请求返回的

		s = new Scanner(new FileInputStream(new File(filePath)),"GBK");// 解析指定编码，读取文件
		s = new Scanner(new BufferedReader(new FileReader(filePath)));
		
		拿到字符串内容后，写文件，直接输出流写字节内容
		 byte[] buffer = new BASE64Decoder().decodeBuffer(base64Code);  
		 FileOutputStream out = new FileOutputStream(targetPath);  
		 out.write(buffer);  
		 out.close(); 
		 log("文件转换完成！  " + targetPath);
		 解压zip就是ZipInputStream 和 FileOutputStream操作


14. java.io操作类主要有4种(前两种属于数据格式，后两种属于传输方式)
			1.基于字节操作的I/O接口：inputSteam 和outputStream
		   2.基于字符操作的I/O接口：writer 和 Reader
			3.基于磁盘操作的I/O接口：file
			4.基于网络操作的I/O接口：socket
			
			
		磁盘和网络传输，最小的存储单元都是字节，不是字符。

		数据持久化或者网络传输都是以字节进行的。
			InputStreamReader是字节到字符的转化桥梁
			OutputStreamWriter完成字符到字节的编码过程
			
			reader类是java的I/O中读字符的父类，inputStream类是读字节的父类
			writer类对应是写字符的父类，outputStream类是写字节的父类
						
		网络优化的几个点
			1.减少网络交互的次数。能用缓存用缓存，能批量处理就批量处理
			2.减少网络传输数据量的大小。文件的话压缩后再传输
			3.尽量减少编码。尽量提前字符转字节。
			
		IO 是面向流的，NIO 是面向缓冲区的
			Selector 类是 NIO 的核心类，Selector 能够检测多个注册的通道上是否有事件发生，如果有事件发生，便获取事件然后针对每个事件进行相应的响应处理。
			在 Java NIO 中，是通过 selector.select()去查询每个通道是否有到达事件，如果没有事件，则一直阻塞在那里。
			Java NIO 实际上就是多路复用 IO。在多路复用 IO模型中，会有一个线程不断去轮询多个 socket 的状态，只有当 socket 真正有读写事件时，才真正调用实际的 IO 读写操作
			
			多路复用 IO 模式，通过一个线程就可以管理多个 socket，只有当socket 真正有读写事件发生才会占用资源来进行实际的读写操作。
			多路复用 IO 为何比非阻塞 IO 模型的效率高是因为在非阻塞 IO 中，不断地询问 socket 状态时通过用户线程去进行的，而在多路复用 IO 中，轮询每个 socket 状态是内核在进行的，这个效率要比用户线程要高的多。		
		

15. 网络传输，网络协议
		TCP/IP 由四个层次组成：网络接口层、网络层、传输层、应用层
		三次握手，四次断开
			TCP 建立连接要进行三次握手，而断开连接要进行四次。这是由于 TCP 的半关闭造成的。因为 TCP 连接是全双工的(即数据可在两个方向上同时传递)所以进行关闭时每个方向上都要单独进行关闭。这个单方向的关闭就叫半关闭。当一方完成它的数据发送任务，就发送一个 FIN 来向另一方通告将要终止这个方向的连接。
			
		HTTPS
         HTTPS是以安全为目标的HTTP通道，是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL。其所用的端口号是 443。 
         简单过程是，建立通信后，服务端发送证书给客户端，客户端用公钥加密传输，服务端私钥解密。
         建立连接获取证书
         1） SSL客户端通过TCP和服务器建立连接之后(443 端口)，并且在一般的 tcp 连接协商（握手）过程中请求证书。即客户端发出一个消息给服务器，这个消息里面包含了自己可实现的算法列表和其它一些需要的消息，SSL 的服务器端会回应一个数据包，这里面确定了这次通信所需要的算法，然后服务器向客户端返回证书。（证书里面包含了服务器信息：域名。申请证书的公司，公共秘钥）。
         证书验证
         2） Client 在收到服务器返回的证书后，判断签发这个证书的公共签发机构，并使用这个机构的公共秘钥确认签名是否有效，客户端还会确保证书中列出的域名就是它正在连接的域名。数据加密和传输
         3） 如果确认证书有效，那么生成对称秘钥并使用服务器的公共秘钥进行加密。然后发送给服务器，服务器使用它的私钥对它进行解密，这样两台计算机可以开始进行对称加密进行通信。
      
      java使用socket通信时，是基于tcp/ip协议通信的


16. 操作流的一些规律(忽略)：
        1、明确源和目的
            源：inputstream		reader
            目的：outputstream	writer
        2.明确数据是否是纯文本数据
            源：是reader	否 inputstream
            目的：是writer	否outputstream
        3、明确具体的设备
            源:
                硬盘：file
                键盘：system.in
                内存：数组
                网络：socket
            目的：
                硬盘：
                控制台：system.out
                内存：
                网络：
        4、是否需要其他的额外功能
            1、是否需要缓冲区： 加上buffer
            2、转换

17. 转换流的使用：
        字节流转换成字符流  	使用指定的charset读取字节并解码为字符
        inputStreamReader isr=new inputstreamreader(inputStream),然后可以作为缓冲流的处理对象
        字节流读取中文字2个字符，会读取两次，一次一个，字符流读一次，

        字符流转换成字节流	使用指定的charset将写入流中的字符编码成字节
        outputStreamWriter  子类filterWriter

        输入源（inputStream）-转换流成字符流-使用缓冲区读取-将读取到的写入到输出地（outPutStream的缓冲区）

        new bufferedreader（new inputstreamreader(system.in，码表)）
        new bufferedwriter（new outputstreamwriter(system.out，码表)）字符流
        
        转换流接收的目前都是字节流，如fileInputStream和fileOutputStream
        outputstreamwriter/inputstreamreader 可以指定编码写/读
        filterwriter/filereader是其子类，使用默认本机的编码
    
        转换流的使用场景：
            1、源或目的对应的设备是字节流，但操作的是文本数据
            2、操作文本涉及到指定编码，必须使用转换流

        字符流的缓冲区：提高读写的效率，一定要有缓冲对象
        bufferedRead：可以读取行readline（） 没有返回null
                先将源中数据读取到缓冲区，再从缓冲区中一个一个读取数据或整行读取
            
        bufferedWriter 特有newline（）
            BufferWriter bw = new BufferWriter（fw） 封装数组提高数组操作数据的效率
            bw.writer("aa");
            bw.flush();
            bw.close(); fw 同时关闭

        bytearrayinputsteam 和bytearrayoutputsteam

18. netty入门学习(忽略)

      长连接，自定义协议，高并发，Netty就是绝配。
      在io模型中一个连接需要开一个线程
	   NIO解决这个问题的方式是数据读写不再以字节为单位，而是以字节块为单位。每次从缓存区读取一块数据

		Netty封装了JDK的NIO(nio，主要使用selector多路复用器来实现)，封装了Java NIO那些复杂的底层细节，抽象出简单方法
		netty是封装java socket nio的。是异步高性能的通信框架。通常作为RPC 框架的高性能基础通信组件    
		建立在客户端和服务端之间的,服务端建立相应的规则，然后运行起来，等待客户端访问或者发送”消息“
		
      传统的Java NIO虽然看起来简单，但是API还是太“低级”了，对编程人员不太方便用，直接使用netty即可

		一个socket对应一个channel？
		服务端
			服务端用ServerBootstrap(netty服务端应用开发的入口)，有两个NioEventLoopGroup；
			有两种通道需要处理， 一种是ServerSocketChannel：用于处理用户连接的accept操作， 另一种是SocketChannel，表示对应客户端连接。
			分别用来用来接收进来的连接和用来处理已经被接收的连接，一旦‘boss’接收到连接，就会把连接信息注册到‘worker’上。
			
			.childHandler(),设置子通道的处理器，也就是SocketChannel的处理器，内部是实际业务开发的”主战场”，具体的业务实现。
			
			Channel都有且仅有一个ChannelPipeline与之对应，Channel包含了ChannelPipeline，ChannelPipeline内部包含了N个handler，每一个handler都是由一个线程去执行；
			channel.pipeline().addLast添加处理的handler
			
			b.option(),.childOption()分别配置ServerSocketChannel的选项 和 子通道也就是SocketChannel选项 
			
			ChannelFuture f = b.bind(port).sync();//绑定端口并启动去接收进来的连接
			f.channel().closeFuture().sync();//这里会一直等待，直到socket被关闭
			
		客户端
			client用Bootstrap，只有一个NioEventLoopGroup。只有一种channel，也就是SocketChannel
			
		网络传输的载体是byte，所有框架都是这个规定，JAVA的NIO提供了ByteBuffer，用来完成这项任务， Netty也提供了叫做ByteBuf
		通过合理的切分微服务变价可结局大缤纷分布式的事务问题
		
		IO多路复用技术通过把多个 IO 的阻塞复用到同一个 select 的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程模型比，I/O 多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源。
		由于 Netty 采用了异步通信模式，一个 IO 线程可以并发处理 N 个客户端连接和读写操作，这从根本上解决了传统同步阻塞 IO 一连接一线程模型，架构的性能、弹性伸缩能力和可靠性都得到了极大的提升



	传统的i/o模式，每个请求都需要独立的线程完成数据read，业务处理，数据write的完整操作问题，容易阻塞
	Netty 的非阻塞 I/O 的实现关键是基于 I/O 复用模型，使用了多路复用器selector，一个 I/O 线程可以并发处理 N 个客户端连接和读写操作，这从根本上解决了传统同步阻塞 I/O 一连接一线程模型，架构的性能、弹性伸缩能力和可靠性都得到了极大的提升。
	传统的 I/O 是面向字节流或字符流的，以流式的方式顺序地从一个 Stream 中读取一个或多个字节, 因此也就不能随意改变读取指针的位置。

	在 NIO 中，抛弃了传统的 I/O 流，而是引入了 Channel 和 Buffer 的概念
      Netty 的线程模型基于主从 Reactor 多线程
      Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。
      当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询(Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel 
	
	
	class java.nio.channels.Selector 是Java 的非阻塞 I/O 实现的关键。它使用了事件通知 API，可在任何的时间检查任意的读操作或者写操作的完成状态，一个单一的线程便可以处理多个并发的连接。统一处理多个socket连接
	
	ChannelFutureListener提供的通知机制消除了手动检查对应的操作是否完成的必要
	Netty 完全是异步和事件驱动的。这里体现监听事件是否完成，
	
	编码解码的事情：这两种方向的转换的原因很简单：网络数据总是一系列的字节
	
	网络数据的基本单位总是字节。
   Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐
	Netty 的 ByteBuffer 替代品是 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。
	bio，传统的一请求一应答通信模型，一请求产生一个线程，交互完成，线程销毁，线程资源不可控，会阻塞
	后来出现了线程池，这是伪异步i/o,采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架，线程资源可控，但是还是会阻塞
	
	Reactor模式：
			connection per thread,早起的tomcat就是这样实现的
			一个线程只能对应一个socket
			改进：采用基于事件驱动的设计，当有事件触发时，才会调用处理器进行数据处理。使用Reactor模式，对线程的数量进行控制，一个线程处理大量的事件。
			
			实际上的Reactor模式，是基于Java NIO的，在他的基础上，抽象出来两个组件——Reactor和Handler两个组件：
		   （1）Reactor：负责响应IO事件，当检测到一个新的事件，将其发送给相应的Handler去处理；新的事件包含连接建立就绪、读就绪、写就绪等。
		   （2）Handler:将自身（handler）与事件绑定，负责事件的处理，完成channel的读入，完成处理业务逻辑后，负责将结果写出channel。

19. reader 和 writer

      reader：
         filereader流对象  可以读取单个字符和数组
         int ch = fr.read()  返回对应的asc码  最后一位再读取 返回-1 (作为流的结束标记)
         char[] a= new char[3]    默认 1024 千字节为单位 abcde#
         int length =  fr.read(a) 将读取指定长度的字符存储到数组中 重复读取会覆盖 返回读取的字符个数
         fw.writer（a，0，length ） a作为两个流之间的中转

      writer： 
         filewriter- 默认新建，存在覆盖   new filewriter("a.txt",true)有续写功能
         filewriter.write("数据写入"); 数据存储到临时存储缓冲区中， 
         filewriter.flush.刷新流的缓冲区，将数据直接写到目的地中
         filewriter.close 内部先flush，流对象最后一定要close


20. http的请求的连接操作
		BufferedInputStream bufferedinputstream = new BufferedInputStream(inputStream);
		InputStreamReader isr = new InputStreamReader(bufferedinputstream, charset);
		BufferedReader in = new BufferedReader(isr);
		StringBuffer buff = new StringBuffer();
		String readLine = null;
		String endLine = SYMBOL_END_LINE;
		while ((readLine = in.readLine()) != null)
		{buff.append(readLine).append(endLine);}
		if (buff.length() > 0)
		{response = buff.substring(0, buff.length() - 1);}
		else
		{response = buff.toString();}

21. NIO
      传统的socket io需要为每个连接创建一个线程，开销巨大
      使用NIO，可用一个含有限数量的线程的线程池，甚至一个线程来为任意数量的连接服务。由于线程数量小于连接数量，所以每个线程进行IO操作的时候就不能阻塞，如果阻塞的话，有些连接就得不到处理，NIO提供了这种非阻塞的能力。
      io：面向流，阻塞, nio：面向缓冲区，非阻塞	
      Java NIO的选择器：一个单独的线程很容易来管理多个通道

    JAVA NIO 和 IO
      各自的使用的api不同
      IO是面向Stream，阻塞的IO
      NIO是面向buffer，非阻塞IO，自带(selector的机制让一个线程管理多个channel变得简单)。

      IO
         服务器端的socket编程，最早的Java是所谓的阻塞IO(Blocking IO)， 想处理多个socket的连接的话需要创建多个线程， 一个线程对应一个。数量一多，效率降低
         *** 一个线程控制一个socket来读写，容易阻塞 ***
      NIO     
         非阻塞IO(NIO：Non-Blocking IO)， 通过多路复用的方式让一个线程去处理多个Socket。	
         *** 一个线程通过一个selector，控制多个socket的读写 ***
         只需要使用少量的线程就可以搞定多个socket了，线程只需要通过Selector去查一下它所管理的socket集合，哪个Socket的数据准备好了，就去处理哪个Socket。

22. 网络带宽
      10M的宽带的带宽是1280KB/s。
      ISP提供的线路带宽使用的单位是比特（bit），而一般下载软件显示的是字节（Byte）（1Byte=8bit），所以要通过换算，才能得实际值。
      1Mb/s=1024Kb/s=1024/8KB/s=128KB/s  
      上行宽带(速度)和下行宽带(速度)是不对称的，一般是下行速度大于上行的速度。我们平时所使用的宽带说多少M，都是指的下行宽带，
      8位是一个字节，8bit=1byte,一位就是二进制数的一个数字。byte类型的数据是8位带符号的二进制数

23. 其他
      (忽略)有时为了让中文字符适应某些特殊要求(如httpheader要求其内容必须为iso8859-1编码),可能会通过将中文字符按照字节方式来编码的情况
         //1.得到三个在ISO8859-1中的字符
            String s_iso88591 = new String("中".getBytes("UTF-8"),"ISO8859-1"),这样,

         //2.字符传递,通过相反的方式接受，得到正确的中文汉字"中"
            Strings_utf8 = new String(s_iso88591.getBytes("ISO8859-1"),"UTF-8")。

24. 文件下载excel的demo    
      @RequestMapping(value="/monthDetailOut/{userId}")
      private void outPutExcle(HttpServletResponse response,HttpServletRequest request,@PathVariable("userId")Integer userId,
            @RequestParam("pno")Integer pno,@RequestParam("type")Integer type,@RequestParam("status")Integer status,@RequestParam("startDate")String startDate,
            @RequestParam(value="endDate",defaultValue="")String endDate) {
         try {
            UserInfo userInfo = userInfoMapper.selectById(userId);
            String dateString=null;
            if(!StringUtil.isBlank_new(startDate)&&!StringUtil.isBlank_new(endDate)){
               dateString=startDate+","+endDate;
            }
            BaseModelInfo<List<IncomeExpenseDetailBean>> userDetails = userPayDetailsService.getUserDetails(pno, type, userInfo.getUserCode(), dateString, status);
            List<IncomeExpenseDetailBean> incomeExpenseDetailList = userDetails.getInfo();
            
            ByteArrayOutputStream byteOut = new ByteArrayOutputStream();//可以捕获内存缓冲区的数据，转换成字节数组
            WritableWorkbook book = Workbook.createWorkbook(byteOut);
            //---------------------------------插入的文字的样式------------------------------
            WritableFont writableFont = new WritableFont(
                  WritableFont.createFont("宋体"), 12, WritableFont.NO_BOLD,
                  false, UnderlineStyle.NO_UNDERLINE, Colour.BLACK);
            WritableCellFormat writableCellFormat = new WritableCellFormat(writableFont);
            writableCellFormat.setVerticalAlignment(VerticalAlignment.CENTRE);// 格式
            writableCellFormat.setBorder(Border.ALL, BorderLineStyle.THIN);
            //--------------------------------插入的文字的样式end------------------------------
            //新建一个sheet---------------
            WritableSheet sheet1 = book.createSheet("收入明细表", 0); 
            //-----------设置单元格长度
            sheet1.setColumnView(0,30);
            sheet1.setColumnView(1,30);
            sheet1.setColumnView(2,30);
            sheet1.setColumnView(3,30);
            sheet1.setColumnView(4,30);
            sheet1.setColumnView(5,30);
            //==========================================2、收入明细表==========================================
            sheet1.addCell(new Label(0, 0, "用户收支明细表"));
            sheet1.addCell(new Label(0, 1, "交易时间"));
            sheet1.addCell(new Label(1, 1, "类型名称|单号" ,writableCellFormat));
            sheet1.addCell(new Label(2, 1, "对方|资金流向" ,writableCellFormat));
            sheet1.addCell(new Label(3, 1, "金额" ,writableCellFormat));
            sheet1.addCell(new Label(4, 1, "状态" ,writableCellFormat));
            sheet1.addCell(new Label(5, 1, "操作" ,writableCellFormat));	
            for (int i = 0; i < incomeExpenseDetailList.size(); i++) {
               IncomeExpenseDetailBean incomeExpenseDetail=incomeExpenseDetailList.get(i);
               if(incomeExpenseDetail.getType()==1){
                  incomeExpenseDetail.setOrderNo("发布的需求"+"|"+incomeExpenseDetail.getOrderNo());
               }else if(incomeExpenseDetail.getType()==2){
                  incomeExpenseDetail.setOrderNo("发布的服务"+"|"+incomeExpenseDetail.getOrderNo());
               }else if(incomeExpenseDetail.getType()==3){
                  incomeExpenseDetail.setOrderNo("报名的需求"+"|"+incomeExpenseDetail.getOrderNo());
               }

               sheet1.addCell(new Label(0, 0+2+i, DateUtil.getDateString(incomeExpenseDetail.getDateUpdate(), "yyyy-MM-dd HH:mm:ss") ,writableCellFormat));
               sheet1.addCell(new Label(1, 0+2+i, incomeExpenseDetail.getOrderNo() ,writableCellFormat));
               sheet1.addCell(new Label(2, 0+2+i, incomeExpenseDetail.getUserName() ,writableCellFormat));
               sheet1.addCell(new Label(3, 0+2+i, incomeExpenseDetail.getAmount().toString() ,writableCellFormat));
               sheet1.addCell(new Label(4, 0+2+i, incomeExpenseDetail.getStatus().toString(),writableCellFormat));
               sheet1.addCell(new Label(5, 0+2+i, "操作暂无",writableCellFormat));
            }
            //将数据写入
            book.write();
            book.close();	

            String realName="收支明细_"+DateUtil.getCurrentDate("yyyyMMddHHmmss")+".xls";
            response.reset();//设置为没有缓存
            response.setContentType("application/x-download;charset=GBK");
            /*attachment是以附件下载的形式，inline是以线上浏览的形式。当点击“保存”的时候都可以下载，当点击“打开”的时候attachment是在本地机里打开，inline是在浏览器里打开。*/
            response.setHeader("Content-disposition", "attachment; filename="
                  + new String(realName.getBytes("utf-8"), "ISO8859-1"));		
            OutputStream output = response.getOutputStream();
            byte[] buf = new byte[1024];
            int r = 0;
            ByteArrayInputStream bin = new ByteArrayInputStream(byteOut.toByteArray());
            while ((r = bin.read(buf, 0, buf.length)) != -1) {
               output.write(buf, 0, r);
               }    
            response.getOutputStream().flush();
            response.getOutputStream().close();
            
         } catch (Exception e) {
            e.printStackTrace();
         }
      }


## java8新特性

1. 函数式接口
      Java8接口引入了默认方法，不管存在多少个默认方法，只要有且只有一个抽象方法（default 默认方法、static 静态方法以及继承 Object 的方法都不算抽象方法），那么它就是函数式接口。
      每个接口只包含一个抽象方法，称为函数式方法
      函数式接口(Functional Interface，@FunctionalInterface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。
      函数式接口可以被隐式转换为 lambda 表达式。以前是采用匿名实现类实现的
         1.一个函数式接口有且只有一个抽象方法。
         2.默认方法不是抽象方法，因为它们已经实现了。
         3.重写了超类Object类中任意一个public方法的方法并不算接口中的抽象方法。


      四大核心函数式接口
         Consumer<T>  	void accept(T t)	消费型接口
         Supplier<T>	 	 T get()			供给型接口
         Function<T, R> 	 R apply(T t)		函数型接口   
         Predicate<T>	 boolean test(T t)	断言型接口

      consumer函数式接口中  void accept(T t); 		
      strings.forEach(a -> System.out.println(a));
      a -> System.out.println(a)		  这里实际上是consumer接口的一个匿名内部类，a是形参，实际list遍历的时候会将实参传入。
      其实定义的就是一个函数接口的匿名实现类，然后真正执行的时候将实参传进去执行

      函数式接口的自动映射
         Lambda 表达式包含类型推导（编译器会自动推导出类型信息，避免了程序员显式地声明）
         只要参数类型、返回类型与 BiConsumer 的 accept() 相同即可。所以在使用函数接口时，名称无关紧要——只要参数类型和返回类型相同。
         Java 会将你的方法映射到接口方法。 要调用方法，可以调用接口的函数式方法名（在本例中为 accept()），而不是你的方法名。


      -- 自定义函数接口接收(忽略)
         方法名称对应有返回值，函数接口方法void类型，也能匹配上
         将Describe 对象的方法引用赋值给 Bb,它没有 show()方法，而是call()方法。但Java 似乎接受用这个看似奇怪的赋值，因为方法引用符合Bb的 call() 方法的签名(参数类型和返回类型)。
         这样可以通过调用 call() 来调用 show()，因为 Java 将 call() 映射到 show()。
         这种将方法映射给函数接口的，操作，代码简单，

         （忽略）未绑定的方法引用是指没有关联对象的普通（非静态）方法。 使用未绑定的引用时，我们必须先提供对象.这种不推荐用。走正规路子
         标准的就是方法签名匹配，然后 类::静态方法+实例::非静态方法，其他野路子不要用。


2. interface中的default方法
		default修饰方法只能在接口中使用，是普通方法，可以直接写方法体。是可使用默认的方法，不满足需求的时候可以覆盖，static可以直接调用
		实现类会继承接口中的default方法

      因为任何接口的实现都会从其父类Object或其它地方获得这些方法的实现。所以Comparator中的equals不算是接口中的抽象方法
         @FunctionalInterface
         public interface Comparator<T> {
            int compare(T o1, T o2);
            boolean equals(Object obj);
            default Comparator<T> reversed() {
               return Collections.reverseOrder(this);
            }
            //...
         }
      
      A extends B implements C 的方法使用(忽略)
		   如果A extends B implements C，B中有b方法，C中也有b方法（被default修饰），那么A会继承B中的b方法而不是C中的b方法

3. Lambda 表达式
      使用lambada的条件，必须只能是函数式接口（概述:接口中只有一个抽象方法）才适用。可用@FunctionalInterface来提前校验函数式接口，编译时提前发现错误。
      Lambda 表达式格式：参数 -> 函数实现主体         箭头右侧是从Lambda返回的表达式。它与单独定义类和采用匿名内部类等价，但代码少。

      jdk8提供的默认的函数式接口基本都在jdk下的java.util.function包下，比如Consumer等 list的forEach就用了这个接口

      Lambda 表达式的基本语法:  参数 -> 方法体
         [1] 当只用一个参数，可以不需要括号 ()。 
         [2] 正常情况使用括号 () 包裹参数。 为了保持一致性，也可以使用括号 () 包裹单个参数，虽然这种情况并不常见。
         [3] 如果没有参数，则必须使用括号 () 表示空参数列表。
         [4] 对于多个参数，将参数列表放在括号 () 中。
         [5]	Lambda 表达式方法体是单行的。该表达式的结果自动成为 Lambda 表达式的返回值，使用return非法。简写方式
            如果在Lambda 表达式中确实需要多行，则必须将这些行放在花括号中。在这种情况下，就需要使用 return。

		格式一： 参数列表 -> 表达式 
		格式二(推荐)： 参数列表 -> {表达式集合}

		传参形式	推荐，省略参数类型，加上括号，主体加上大括号
			1.(People people)-> "男".equals(people.getName())										//完整的参数列表
			2(people) -> "男".equals(people.getName())												//参数类型可以省略
			3.people -> "男".equals(people.getName())												//当传入参数只有一个时，括号可以省略
			4.(People people,People  people1) -> people.getName.equals(people1.getName())			//当传入多个参数，或者不传参数时，括号不能省略
			5.(people, people1) -> people.getName.equals(people1.getName())							//当传入多个参数，或者不传参数时，括号不能省略，多个参数的参数类型也可以省略

		主体形式
			1.（people） -> "男".equals(people.getName())							//当没有花括号时，表达式结果就是返回值,隐含了return
			2.（people） -> {return "男".equals(people.getName())}					//当有花括号时，要显性返回方法要求的返回信息(推荐)
				

		Lambda本身不知道对应哪个函数式接口，编译器会根据上下文（context）环境来适配目标类型(target type)。
		当函数式接口的抽象方法与lambda表达式表达的函数签名相同的时候我们可以使用

      代码实例
         List<person> plist = ...;
         //根据超时天数倒序排序
         Comparator<person> comparator = (t1, t2) -> t1.getAge().compareTo(t2.getAge());
         plist.sort(comparator.reversed());	
         实际就是list.sort(Comparator)操作
         List.sort((o1, o2) -> flag ? a.compareTo(b) : b.compareTo(a));


		特殊情况
			重写了超类Object类中任意一个public方法的方法并不算接口中的抽象方法。			
			比如：Comparator 中就有  int compare(T var1, T var2);  和  boolean equals(Object var1);

   
      本质上，Lambda是一个匿名实现类，是函数式接口的实现，对应的是一个接口类型
      ```
         interface Bb {
            String detailed(String head);
         }

         class Testgo {
            Bb bb= h -> h+"zenme";
         }
      ```


   
4. Java 8 的方法引用
		语法 	类名或对象名 :: 方法名
		类 :: 静态方法
		实例 :: 非静态方法
		
		·静态方法引用 				类名::staticMethod		(args) -> 类名.staticMethod(args)		 Integer::valueOf
		·实例方法引用 				inst::instMethod 		(args) -> inst.instMethod(args)			System out: println
		·构造方法引用 				类名::new				(args) -> new 类名(args)				 User :: new
		·对象方法引用 				类名::instMethod		(inst,args) -> 类名.instMethod(args) 	 User::getName

		本质上是lambda的简写，也是一个接口的匿名实现类,双冒号运算表达式 是 Function<T,R>类型。也是lambda的简写方式
		这个会选择一个函数式接口，将方法体内容作为其函数接口中方法的具体实现，比如Function或者Consumer
		*****方法引用就是将一个方法的方法体实现，映射到一个函数接口的实现中，实质上是函数接口的一个实现类*****

      等价的写法
		//Lambda表达式写法	s -> System.out.println(s); 
		//方法引用写法		System.out::println

		方法签名强制一致
		最好是将方法名称对应方法(参数类型和返回类型),要和对应的函数接口一致。系统有默认的选择(Function或者Consumer)，
		
		这种[方法引用]或者说[双冒号运算]对应的参数类型是Function<T,R> T表示传入类型，R表示返回类型。
		比如表达式person -> person.getAge(); 传入参数是person，返回值是person.getAge()，那么方法引用Person::getAge就对应着Function<Person,Integer>类型。
		https://www.runoob.com/java/java8-method-references.html

		双冒号操作符(方法引用)中的方法需要匹配相应的函数式接口类型再使用
      //因此该静态方法可以使用函数式接口Consumer<T> + 双冒号lambda表达式 的方式进行引用
      Cat cat = new Cat();
      Consumer<Cat> eatMethod = Cat::eat;//静态方法的双冒号表达式是类名::方法名
      eatMethod.accept(cat);
		
		**********选择合适的函数式接口 才是lambda的使用的关键，一定要匹配**********
		由于方法的输入参数类型和输出参数类型相同,所以可以使用一元函数接口
		只需要分析清楚方法的输入参数类型和参数格式，以及返回结果类型，然后由此选择合适的函数式接口，再配合双冒号（"::"）lambda表达式，就可以完成方法引用了。
		https://blog.csdn.net/nrsc272420199/article/details/84718802    方法引用的实际讲解

		如果Lambda要表达的函数方案已经存在于某个方法的实现中，那么则可以通过双冒号来引用该方法作为Lambda的替代者。
		方法引用代替lambda表达式对代码的简化程度远没有lambda表达式代替匿名类的简化程度大， 有时反而增加了代码的理解难度，没必要刻意使用 方法引用
		这里场景基本是单独调用已经存在的某个方法，可以简化使用方法引用，一般情况下用，可读性不好。效果不大。

      使用案例
            ```		
               interface Bb {
                  void call(String s);
               }
               class Describe {
                  void show(String msg) {System.out.println(msg);}
               }
               public static void main(String[] args) {
                  Describe d = new Describe();
                  Bb bb = d::show; 		 	//相当于将show中的方法自动映射一个函数式接口，放在了Bb的匿名实现类中实现,这里默认是 Consumer<String> show = describe::show;
                  bb.call("123");  
               }

               class Go {
                  static void go() {System.out.println("Go::go()");}
               }

               public class RunnableMethodReference {
                  public static void main(String[] args) {
                     //1.基础
                     new Thread(new Runnable() {
                     public void run() {System.out.println("Anonymous");}
                     }).start();
                     //简化lambda
                     new Thread(() -> System.out.println("lambda")).start();
                     //方法引用
                     new Thread(Go::go).start();
                  }
               }
            ```

4. Stream流式编程
   	
   Stream 是对集合（Collection）对象功能的增强，高效的聚合操作（过滤、排序、分组、聚合等），所有 Stream的操作必须以 lambda 表达式为参数

   优点：
		1.流是懒加载的，“延迟计算”，占用内存很少。
		2.集合类的迭代逻辑是调用者负责，通常是for循环，而Stream的迭代是隐含在对Stream的各种操作中，例如map()
      3.内部迭代，是流式编程的一个核心特征。

   操作形式
      组成形式：创建流+中间处理(filter(),map()等操作)+终端处理（只能有一个terminal操作）
      处理链形式：stream of elements -> filter ->sorted -> map -> collect

      流操作的类型有三种：
         1.创建流，
         2.修改流元素（中间操作， Intermediate Operations），
         3.消费流元素（终端操作， Terminal Operations）。这种类型通常意味着收集流元素(通常是汇入一个集合)

         终端处理之后，流关闭，无法再进行中间处理。数据收集主要使用 collect 方法，该方法也属于归约操作
         终端操作，获取流的最终结果，无法再继续往后传递流，是我们在流管道中所做的最后一件事
         collect(Collector)：使用 Collector 收集流元素到结果集合中

   常用案例

      使用Stream.of()将一组元素转化为流
         Stream.of("i ", "am", "a ", "coat ").forEach(System.out::print);
		
      将一个集合生成流
         list.stream()
         map转为流 map.entrySet().stream()
	
   常用操作
      map()
         中间操作 map() 会获取流中的所有元素，并且对流中元素应用操作从而产生新的元素，并将其传递到后续的流中。通常 map() 会获取对象并产生新的对象

      peek()
         相当于是一个调试功能，查看每个元素的转变情况(忽略)
         strings.stream().map(String::toUpperCase).peek(System.out::println).map(e -> e+"1").forEach(System.out::println);
            WO
            WO1
            SHI
            SHI1
            NI
            NI1
         peek() 符合无返回值的 Consumer 函数式接口，所以我们只能观察，无法使用不同的元素来替换流中的对象。
         distinct()：在 Randoms.java 类中的 distinct() 可用于消除流中重复元素。相比创建一个 Set 集合来消除重复，工作量要少。
         filter(Predicate)：过滤操作，保留如下元素：若元素传递给过滤函数产生的结果为true 。

      count()：流中的元素个数(忽略)
	   max(Comparator)：根据所传入的 Comparator 所决定的“最大”元素。
	   min(Comparator)：根据所传入的 Comparator 所决定的“最小”元素。
	   min() 和 max() 的返回类型为 Optional，这需要我们使用 orElse()来解包。
	
      集中类型目前看意义不大(忽略)
         map(Function)：将函数操作应用在输入流的元素中，并将返回值传递到输出流中。
         mapToInt(ToIntFunction)：操作同上，但结果是 IntStream。方法将一个对象流（object stream）转换成为包含整型数字的 IntStream。
         mapToLong(ToLongFunction)：操作同上，但结果是 LongStream。
         mapToDouble(ToDoubleFunction)：操作同上，但结果是 DoubleStream。

      forEach(Consumer)常见如 System.out::println 作为 Consumer 函数。
      forEachOrdered(Consumer)： 保证 forEach 按照原始流顺序操作。

5. 匿名内部类中的变量使用
	int limit = 10;
	Runnable r = () -> {
		for (int i = 0; i < limit; i++) {
			System.out.println(i);
		}
	};
	new Thread(r).start();
	代码仍然可以正常编译，正常运行，那么此时的 limit 变量就是“effectively final”的。
	由于 limit 在接下来的代码中没有被重新赋值，编译器就被欺骗了，想当然地认为 limit 就是一个 final 变量（实际上的最终变量）。
	“effectively final”是一个行为类似于“final”的变量，但没有将其声明为“final”变量，

	TriggerParam triggerParam1 = null;
	triggerParam = triggerQueue.poll(3L, TimeUnit.SECONDS);				#这里和下面的effectively final冲突了
	FutureTask<Boolean> futureTask = new FutureTask<Boolean>(new Callable<Boolean>() {
		@Override
		public Boolean call() throws Exception {
			triggerParam.getExecutorHandler();				#报错，这里上面没有指定static就默认是effectively final，但是上面更新对象引用了，会报错
			return true;
		}
	});

	Thread futureThread = new Thread(futureTask);			#这里再记录一个futuretask的线程启动
	futureThread.start();
	Boolean tempResult = futureTask.get(triggerParam.getExecutorTimeout(), TimeUnit.SECONDS);

6. 匿名内部类，Lambda，访问变量

      匿名内部类，变量的final，局部变量
         Lambda局部内使用外部变量必须是final类型的，但是使用类上的属性，貌似没有问题。方法内的属性需要final，类上的属性不需要
            *****在JDK8之后，匿名内部类引用外部变量时虽然不用显式的用final修饰，但是这个外部变量必须和final一样，不能被修改（这是一个坑）。*****
               即可以对对象属性操作，可以对list操作，但是不能new 对象重新赋值。list是引用对象类型
            解决方案：可以通过定义一个相同类型的变量b，然后将该外部变量赋值给b，匿名内部类引用b就行了，然后就可以继续修改外部变量。
            
         局部内部类和匿名内部类访问的局部变量必须由final修饰，java8开始，可以不加final修饰符，由系统默认添加。java将这个功能称为：Effectively final 功能
         对引用类型(地址传递)来说是引用地址的一致性(地址不变)，可以操作引用类型的值，但是对基本类型(值传递)来说就是值的一致性（数据不变）
         
         主要用的就是list 和string，都能实现
         map foreach 的遍历拼接string， 使用StringBuilder实现;  String 是final，在内部不可更改

         //list操作原理
            list.forEach(System.out::println);
            list.forEach
               default void forEach(Consumer<? super T> action) {
                  Objects.requireNonNull(action);
                  for (T t : this) {
                     action.accept(t);
                  }
               }

         针对基本类型可以
            1）把 limit 变量声明为 static。
            2）把 limit 变量声明为 AtomicInteger。(数字用)
            3）使用数组。

         lambda访问外部变量的规则，
            1.只能引用标记了 final 的外层局部变量，这就是说不能在 lambda 内部修改定义在域外的局部变量，否则会编译错误。
            2.局部变量可以不用声明为 final，但是必须不可被后面的代码修改（即隐性的具有 final 的语义）
            3.不允许声明一个与局部变量同名的参数或者局部变量。
            
            ****这个阶段一般在编译阶段就报错了，以为着可以修改内部的值，但是不能改变地址，不能改变引用*****
            ****只要在lambda中使用的变量都默认是final特性*******
            Lambda表达式引用的局部变量无论是否声明final，均具有final特性！表达式内仅允许对变量引用（引用内部修改除外，比如list增删），禁止修改！
            以下情况均不允许编译通过：
               情况一：修改外部局部变量
               int n = 0;
               src.forEach(item -> {
                  n = 3;
                  dest.add("dest: " + item);
               });
               
               情况二：Lambda使用外部局部变量，变量隐性final！
               int n = 0;
               src.forEach(item -> {
                  dest.add("dest: " + item + n);
               });
               n = 0;
               
               情况三：声明外部局部变量同名参数
               int n = 0;
               src.forEach(n -> {
                  dest.add("dest: " + n);
               });
            *********		
            目前看，只要使用lambda中没有提示外部变量，就可以使用

         * 两种匿名内部类创建方式：
               ```
                  new Thread() {
                           public void run() {
                              for (int i = 0; i < 10; i++) {
                                 System.out.println("aaaaaaaaa");
                              }
                           }
                     }.start();//开启线程

                     new Thread(new Runnable() {
                           public void run() {
                              for (int i = 0; i < 10; i++) {
                                 System.out.println("bbbbbbbbb");
                              }
                           }
                     }).start();//开启线程
               ```

         

7. 常用list，map操作
	//1、正常遍历		list.forEach(item->System.out.println(item));
	//2、根据条件遍历	list.forEach(item->{if("b".equals(item)){System.out.println(item);})

	Map.forEach((key, value) -> {System.out.println(key + "：" + value);});  这个遍历比较方便
	
	创建->转换->集合
	List<Integer> ageList = userInfos.stream().map(userInfo -> userInfo.getAge()).collect(Collectors.toList()); // [10, 20, 10]  可以写map(UserInfo::getAge)
	
	String result = list.stream().collect(Collectors.joining(","));  //逗号拼接
	
	map.entrySet().stream().sorted(Comparator.comparing(Person::getKey)).collect(Collectors.toList())  //排序
	
	//使用map方法获取list数据中的name，这里map就是获取一个新的集合，原集合不变
	List<String> names = list.stream().map(Student::getName).collect(Collectors.toList());

   //list 过滤，产生新集合，原集合无影响
   List<String> arrayList = new ArrayList<String>();
   arrayList.add("1");
   arrayList.add("2");
   arrayList.add("3");
   arrayList.add("4");
   
   List<String> stmtList = arrayList.stream().filter(stmt -> stmt.equals("2") || stmt.equals("3")).collect(Collectors.toList());

   System.out.println(arrayList);
   System.out.println(stmtList);
			
8. Optional（忽略）
	适用场景，连环的属性判空处理
	Optional是为了更优雅的判断null而诞生的,但是并不代表有null的地方一定就要用Optional代替

	判空推荐使用ofNullable
	*****每次的map操作会返回一个optional对象，相当于操作的是optional中的value对象*****，
	连续的map操作，只要有一个就数连续null传递，最后一个orElse收尾返回。
	map(Function)：对Optional中保存的值进行函数运算，并返回新的Optional(可以是任何类型)
	其中如果是空，就会返回一个new Optional<>()，value是null
	目前看只能判断null，但是不能判断''这样的空串,可以使用filter过滤

9. Duration是在Java8中新增的(忽略)
	server.servlet.session.timeout=PT60S				P2DT3M5S
	开头,紧接着’P’,下面所有字母都不区分大小写: D – 天 	H – 小时	M – 分钟 	S – 秒 
	字符’T’是紧跟在时分秒之前的，每个单位都必须由数字开始,且时分秒顺序不能乱			
	
	没有s计算，最短60秒  以分钟分计算单位，		server.servlet.session.timeout=PT5M20S   这种就是5分钟


## Tomcat相关

1. 防止项目加载两次
      ```
         <Host appBase="webroot" autoDeploy="true" name="localhost" unpackWARs="true">
      ```

   本地tomcat运行目录
      ```
         <Context docBase="../webapps/xye-netpay-war" path="/xye-netpay" reloadable="true" source="org.eclipse.jst.jee.server:xye-netpay-war"/>
      ```

   web项目的工程名 properties中web projects setting中修改
   Tomcat中指定URL用UTF-8编码server.xml：
      ```
         <Connector port="8080" protocol="HTTP/1.1"   connectionTimeout="20000"   redirectPort="8443" useBodyEncodingForURI="true" URIEncoding="UTF-8" /> 
      ```

2. 多个tomcat实例运行

   * 复制多个tomcat目录副本,catalina.bat中不需要改动
     修改startup.bat和shutdown.bat文件头下加，实现多个tomcat的运行

      ```
         SET JAVA_HOME=C:\Program Files\Java\jdk1.7.0_17
         SET CATALINA_HOME=D:\apache-tomcat-7.0.55-src
         SET CATALINA_BASE=D:\apache-tomcat-jenkins
      ```

   * 准备多套base_home的目录，公用一个catalina_home
   ​	tomcat 的 catalina_home（安装目录） 和catalina_base（工作目录）

      ```
         set "CATALINA_BASE=%cd%"  
         set "CATALINA_HOME=F:\apache-tomcat-7.0.69"  
         set "EXECUTABLE=%CATALINA_HOME%\bin\catalina.bat"  
         call "%EXECUTABLE%" start 
      ```

3. eclispe部署到root下放入的方案
​		1、部署的路径是webapps下，工程的名字改为root，这种测试中比较快
​		2、解压war工程到root下，运行tomca即可，这个一般linux自动部署方法
​		3、改变工程访问路径 ,server.xml 中 
      ```
         <Context path="/" docBase="XiaoyuerProject" debug="0"  reloadable="false"></Context> 
      ```
      这个和web project setting中设置context root是一致的，倾向后者，在双击tomcat中的modules中也可以设置

4. 项目的部署路径              

   ```
      <Context docBase="D:\apache-tomcat-7.0.70\webapps\XiaoyuerProject" path="" reloadable="true" source="org.eclipse.jst.jee.server:XiaoyuerProject"/>
   ```

   实际访问路径     实际的运用中可以配置图片服务器的虚拟路径
      在图片file写入磁盘的时候，采用的是路径+文件名的方式，路径最后要加上/，写入完整的路径。	

5. tomcat的发布指定内存是在catalina.sh中指定jvm的内存大小，jar执行  就直接在启动命令中追加即可

6. tomcat内存修改    -Xms256M   -Xmx512M   -XX:PermSize=128M   -XX:MaxPermSize=1024M
   		

## Linux相关

### 基本介绍

1. 常用命令
      cd 空格				返回根目录
      cd / 					退回到根路径
      cd..					返回上级目录
      ./startup.sh 		启动tomcat
      pwd					示当前的工作目录（pwd:print working directory）
      
      ​45%跳页
      ls和dir											查看所有文件命令，more 查看具体的文件
            ctrl+shift+f  快速翻页
            CTL + b :上翻
            CTL + f : 下翻
            查找 ?或/   查找下一个  N前翻 n后翻 
            g第一行 G最后一行

      cp 文件 文件路径   复制文件
      cp -r 文件夹 路径  复制文件夹内容到指定路径下,递归复制
      cp						cp -r  递归复制，是相当于windows中的复制+粘贴，而mv相当于windows中的剪切+粘贴，也就是移动文件

      mv abc.txt 1234.txt 						重命名

      mkdir 目录名    创建一个目录, -p 递归创建(即使上级目录不存在，会按目录层级自动创建目录)

      rm -rf * 强制递归删除
      rm -rf ../logs/ 									删除logs以及目录下的所有文件  

      chmod +x *.sh 为sh文件增加可执行权限；
      chmod +x 文件  给出文件的可执行权限,支持通配

      vim 
         /，查询，
         N或n，是向前向后搜索字符串  
         ESC，q退出；q!强制退出；:wq 保存并退出  

      more 可以分页查看文本，不常用

      tar zxvf zhcon.tar.gz
         z代表gzip的压缩包；x代表解压；v代表显示过程信息；f代表后面接的是文件

      tar zxvf
      tar -zxvf apache-rocketmq.tar.gz -C rocketmq/  解压文件到指定的目录
      ​				x : 从 tar 包中把文件提取出来
      ​				z : 表示 tar 包是被 gzip 压缩过的，所以解压时需要用 gunzip 解压
      ​				v : 显示详细信息搜索
      ​				f xxx.tar.gz :  指定被处理的文件是 xxx.tar.gz

      yum install 包名（支持*） ：手动选择y or n  全自动就 -y

      curl 					页面请求返回	
      cat 					是一个文本文件查看和连接工具。查看一个文件的内容,cat后面直接接文件名。

      grep  aa xye.log		字符串查找	找出xye.log中的aa内容
      find  -name xye.log  	文件查找 find  -name "*.log"				查询速度慢
      find / -name dubbo-2.8.4a.jar		查找某个文件

      which   				查看系统命令是否存在，以及执行的到底是哪一个位置的命令。
      whereis 				只能用于可执行文件的搜索，只能搜索二进制文件，说明文件，源代码文件。比which范围广

      :set nu					vi命令下显示行号	

      查看进程 和端口号    kill进程
      ps -ef | grep java	ps 静态的进程统计信息    e：所有   f:完整格式显示   grep  搜索
      ps -ef | grep java   显示所用运行中java进程的状态

      kill -9  pid
      kill -9 java pid 停止特定java进程

      lsof -i :22 		                 查看22端口被哪个进程占用
      netstat -ano | findstr "8080"    pc查看端口占用

		cat命令主要用来查看文件内容，创建文件，文件合并，追加文件内容等功能。
		ps -aux 查看容器的进程


2. 常用操作
      * linux定义、输出环境变量
            #export JAVA_HOME=/usr/java/jdk1.8.0_131 
            MAVEN_HOME=/usr/local/maven3
            export MAVEN_HOME
            export PATH JAVA_HOME CLASSPATH
            echo $PATH

      * linux下配置tomcat的环境变量
            JAVA_HOME=/usr/java/jdk1.8.0_131
            JRE_HOME=${JAVA_HOME}/jre
            PATH=$JAVA_HOME/bin:$PATH
            CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
            
      * linux安装mysql：需和centos的系统版本匹配
            linux下安装 mysql https://www.linuxidc.com/Linux/2016-09/135288.htm
            不行重新yum  clean all 
            yum install mysql-community-server
            service mysqld start 启动
      
            grant 权限 on 数据库.数据表 to '用户' @ '主机名';
      
            //给 xiaogang 分配所有的权限，这时xiaogang就拥有所有权限
            grant all on *.* to 'xiaogang'@'%';
            show grants for current_user() ; 当前登录用户的权限
      
            SHOW GLOBAL VARIABLES LIKE 'port' 查看mysql的端口号
            mysql中的创建密码，password("123123") ，自带加密
      
            mysql中的root用户权限消失，重命名var/lib/mysql，然后重启，重启生成目录，然后将原先的重要数据(ibdata1，和db相关)copy过去即可，再次重启service mysqld restart
      
            问题：
            今天开发中在Centos7中安装MySQL5.6版本后，在表中新建了一个weicheng的账户，并且设置了密码，但是在用weicheng账号登陆mysql发现，如果使用“mysql -uweicheng -p”登陆会报错，即使密码正确也不能登录，最后发现，直接用“mysql -uweicheng”不输入密码也可以登陆。
            后来，查询了资料原因是:应为数据库里面有空用户,通过
            select * from mysql.user where user='';
            查询如果有，然后通过
            use mysql;
            delete from user where user = '';
            删除了多余的空白账户， 然后，通过
            flush privileges;
            重载一次权限表,最后用
            service mysqld restart
            重启mysql服务，问题得到解决，至此mark一下！
            Tip：
            1、一定要记住重启mysql服务，否则不会生效，自己就是因为没有重启msyql导致一直得不到解决！
            2、msyql的用户表在mysql数据库中的user表中，主要字段有host，user，password等，作为mysql用的管理的主要表。
            MYSQL：使用\G参数改变输出结果集的显示方式

      * rocketmq的启动和停止
         	1、rocketmq的启动 
         	    进入rocketMQ解压目录下的bin文件夹 
         	    启动namesrv服务：nohup sh bin/mqnamesrv & 
         	    日志目录：{rocketMQ解压目录}/logs/rocketmqlogs/namesrv.log
         	
         	    启动broker服务：nohup sh bin/mqbroker & 
         	    日志目录：{rocketMQ解压目录}/logs/rocketmqlogs/broker.log 
         	    以上的启动日志可以在启动目录下的nohub.out中看到 
         	
         	2、rocketmq服务关闭
         	    关闭namesrv服务：sh bin/mqshutdown namesrv
         	    关闭broker服务 ：sh bin/mqshutdown broker

      * whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。只能用于查找文件
         	which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置
         			
         	一、查看文件安装路径： 
         		whereis oracle 
         	二、查询运行文件所在路径：
         		which oracle 
         		
         	whereis nginx
         	/usr/nginx/sbin/nginx -t

      

6. linux上的tomcat启动使用的是sit_yunwei的权限操作的，那么运行的java代码也只有运维的权限，不能在/usr/local下创建目录

   ./当前目录 ../上级目录 /根路径
	linxus 中的文件路径一般是/ windows中一般是\\

7. linux网络配置
		默认使用nat连接，主机ping不通虚拟机是因为，v8虚拟网卡没有开启，
		虚拟机需要开放防火墙中的端口，才能对外提供访问，centos7需要yum install iptables-services后才能重启防火墙
			vi /etc/sysconfig/iptables
			systemctl restart iptables
			
		如果发现虚拟机的网卡没有ip地址，/etc/sysconfig/network-scripts/ifcfg-eth0(确认ONBOOT=yes),其中eth0是设备名，实际可能不一样
		ONBOOT是指明在系统启动时是否激活网卡，只有在激活状态的网卡才能去连接网络，进行网络通讯	

      开启防火墙
         ```
            /etc/sysconfig/iptables linux需要在该文件中开放指定的端口才能对外访问
            开启指定的端口后，service iptbales restart更新配置
            nohup是永久执行
            &是指在后台运行
         ```

8. linux和windows文件传输
		1.使用winscp工具
		2.安装xftp后自动集成到xshell的插件中
		3.	rz和sz
			sz  下载(命令格式：  sz filename   下载文件filename || sz file1 file2   下载多个文件 || sz dir/*　　　下载dir目录下所有文件)	　
			rz 上传	(命令格式： 	rz)
			注意：
			1.如果机器上没有安装过 lrzsz 安装包，则无法使用rz和sz命令。
			　可使用yum命令安装：yum install -y lrzsz  或者下载源码进行安装。下载地址：https://ohse.de/uwe/software/lrzsz.html	 

9. yum与rpm，建议使用yum安装rpm包
      Yum是rpm的前端程序，主要目的是设计用来自动解决rpm的依赖关系，
      rpm 只能安装已经下载到本地机器上的rpm 包. yum能在线下载并安装rpm包,能更新系统,且还能自动处理包与包之间的依赖问题,这个是rpm 工具所不具备的		
      yum与rpm的区别：rpm适用于所有环境，而yum要搭建本地yum源才可以使用！yum是上层管理工具，自动解决依赖性，而rpm是底层管理工具。yum 基于 rpm，增加了自动解决依赖关系的方案	
      yum [options] [command] [package …]
      [options] 选项包括-h（帮助），-y（当安装过程提示选择全部为"yes"），-q（不显示安装的过程）
   
10. 安装虚拟机（忽略）
		CentOS 7提供了三种ISO镜像文件：
			DVD ISO 标准安装版，桌面版
			Everything ISO 标准安装版的补充，增加了大量的应用软件
			Minimal ISO 精简版，自带的应用软件最少，生产环境推荐使用			#额 精简版啥都没有，好多需要自己装，只有最小安装(基本功能)
			
		centos7			
		安装虚拟机 https://www.bilibili.com/read/cv7593000
		安装jdk，1.手动安装（下载.tar.gz） 2.yum安装(下载rpm包)
		
		service命令的功能基本都被systemct取代。直接使用systemctl命令即可
		systemctl命令是系统服务管理器指令，它实际上将 service 和 chkconfig 这两个命令组合到一起。
		systemctl是RHEL 7 的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。可以使用它永久性或只在当前会话中启用/禁用服务。
		
		解决CentOS安装VM Tools出现在客户机中装载CD驱动器启动终端，使用tar解压缩安装程序，然后执行vmware-insall.pl安装VMware Tools的问题
			1.进入文件界面，找到左侧“设备”下面的名为“VM Tools“ 的驱动器，进入该CD驱动器中，找到文件名末尾为.tar.gz的文件。
			2.将末尾为.tar.gz的文件拷贝到其他任意的文件夹，我直接拷贝到了桌面上，然后将该文件解压，解压方法有两种，随便选下面中的一种：
			3.直接右键点击该文件，选择“提取到此处”选项。
			4.进入命令行界面，使用tar 指令解压，解压后文件名为vmware-tools-distrib。
			5.进入命令行界面，cd到vmware-tools-distrib或者直接在文件夹处打开终端；然后输入指令： sudo ./vmware-install.pl；
			6.之后输入密码就可以下载VMware Tools了。下载中会出现很多询问的问题，只需要不断按回车以选择默认值即可。
			7.安装完成后的界面如下图所示，出现了红圈中的enjoy字样，就说明安装成功   Enjoy, --the VMware team
		
		
			centos7 安装jdk
				/usr/java 下不能直接复制，要从其他文件中copy过去
				1.删除自带的openjdk
					rpm -qa|grep java			查出列表
					rpm -e --nodeps xxx			依次卸载所有跟openjdk相关的包
					
				2.安装jdk即可，随便是手动解压还是rpm安装
					vi /etc/profile  最下方添加环境变量，保存退出。
					有时需要让 profile 起作用，需要执行 source /etc/profile命令生效
					
					export JAVA_HOME=/usr/java/jdk1.8.0_291-amd64
					export JAVA_BIN=/usr/java/jdk1.8.0_291-amd64/bin
					export PATH=$PATH:$JAVA_HOME/bin
					export CLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
					export JAVA_HOME JAVA_BIN PATH CLASSPATH
					
				3.java -version 检查版本

			服务使用方法
				gpm 是linux中的光标复制功能
			启动gpm服务：
			service gpm start

			运行systemctl enable gpm.servicere 添加到后台服务。
			
			systemctl start [服务文件名]
			systemctl restart [服务文件名]
			systemctl stop [服务文件名]
			systemctl status [服务文件名]
			
			设置开机启动
			systemctl enable [服务文件名]
			systemctl disable [服务文件名]			
			
			
			systemctl命令兼容了service，替代service和chkconfig两个命令。
			**centos7后都是用systemctl命令**
			systemctl start docker.service
			systemctl stop docker.service
			服务名不带后缀默认就是.service单元，其他的单元比如.mount .sockets .device .target都必须指定类型
			
			# CentOS 6
			service mysqld restart
			# CentOS 7
			systemctl restart mysqld
			
			docker 部署mysql
				https://www.cnblogs.com/haoprogrammer/p/11008786.html
				
				常规的pull操作，然后运行镜像到容器
				
				//直接追加文字内容，可以的
				docker exec mysql bash -c "echo 'log-bin=/var/lib/mysql/mysql-bin' >> /etc/mysql/mysql.conf.d/mysqld.cnf"
				docker exec mysql bash -c "echo 'server-id=123454' >> /etc/mysql/mysql.conf.d/mysqld.cnf"
				docker restart mysql
				
				//测试用的mysql5.7版本 直接挂在/etc/mysql，配置文件生成不了，这样可以生成my.cnf配置
				docker run -p 3306:3306 --name mysql_1 -v /usr/local/docker/mysql/logs:/var/log/mysql  -e MYSQL_ROOT_PASSWORD=123456 -it mysql:5.7
				
				docker run -p 13306:3306 --name mysql_1 -v /usr/local/workspace/mysql/conf:/etc/mysql  -e MYSQL_ROOT_PASSWORD=123456 -it mysql:5.7（无配置文件）
				docker run -p 3306:3306 --name mysql_1 -v /usr/local/docker/mysql/logs:/var/log/mysql -v /usr/local/docker/mysql/conf/my.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf -e MYSQL_ROOT_PASSWORD=123456 -it mysql:5.7（无配置文件）

11. linux权限，文件权限，用户权限

      chmod [-cfvR] [--help] [--version] mode file
      mode : 权限设定字串，格式如下 : [ugoa...][[+-=][rwxX]...][,...]
      --help : 显示辅助说明 
      --version : 显示版本
      -c : 若该档案权限确实已经更改，才显示其更改动作 
      -f : 若该档案权限无法被更改也不要显示错误讯息 
      -v : 显示权限变更的详细资料 
      -R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更) 
	
		Linux的文件基本上分为三个属性：可读（r），可写（w），可执行（x）。
		类型后面紧接着的3*3个字符分3组，各指示此文件的读、写、执行权限，对于owner、group、others而言

		| d | rwx | r-x | r-x |
		|文件类型 | 所有者权限|用户组权限 |其他用户权限|

		drwxr-xr-x   2 root root 48 2013-11-27 16:34 test/
		第一个栏位，表示文件的属性，
		第二个栏位，表示文件个数。
		第三个栏位，表示该文件或目录的拥有者
		第四个栏位，表示所属的组（group），每一个使用者都可以拥有一个以上的组，不过大部分的使用者应该都只属于一个组
		第五栏位，表示文件大小
		第六个栏位，表示最后一次修改时间
		第七个栏位，表示文件名

		（以-rwxr-xr-x为例）：　　  
		rwx(Owner)r-x(Group)r-x(Other)　
		这个例子表示的权限是：使用者自己可读，可写，可执行；同一组的用户可读，不可写，可执行；其它用户可读，不可写，可执行。
		另外，有一些程序属性的执行部分不是X,而是S,这表示执行这个程序的使用者，临时可以有和拥有者一样权力的身份来执行该程序。

		egj是yunwei创建，egjjjj是开发创建，在kaifa用户下查看权限，yunwei和kaifa都是同一个组的
		哪个用户创建就属于哪个owner
		drwxrwxr-x.   3 prd_yunwei prd_yunwei  4096 May 12 10:48 egj		yunwei建目录，所有者属于运维，同组有写操作
		drwxr-xr-x.   2 kaifa      prd_yunwei  4096 May 12 10:59 egjjjj		kaifa建目录，所有者属于kaifa，yunwei用户不能写操作(因为同组没有写操作)

		创建权限
		who 
		u 表示“用户（user）”，即文件或目录的所有者。
		g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。
		o 表示“其他（others）用户”。
		a 表示“所有（all）用户”。它是系统默认值。

		+ 添加某个权限。
		– 取消某个权限。
		= 赋予给定权限并取消其他所有权限（如果有的话）。

		chmod ［who］ ［+ | – | =］ ［mode］ 文件名     有对应的数字简化版
		例如：chmod g+r，o+r example使同组和其他用户对文件example 有读权限
		******在配置文件写权限的时候，必须要拥有上层目录的读权限******
		
		groups 查看当前用户所属组
		chmod +rwx file	给file的所有用户增加读写执行权限     	不写用户就默认所有
		
		setfacl命令可以用来细分linux下的文件权限。(忽略)
			chmod命令可以把文件权限分为u,g,o三个组，而setfacl可以对每一个文件或目录设置更精确的文件权限。 比较常用的用法如下：
			setfacl –m u:apache:rwx  file 设置apache用户对file文件的rwx权限 
			setfacl –m g:market:rwx  file  设置market用户组对file文件的rwx权限 
			setfacl –x g:market file   删除market组对file文件的所有权限 
			getfacl  file  查看file文件的权限
						
			setfacl -R -m  g:kaifa:r-x /usr/local/logs   // 赋权给 kaifa 查看日志   -R 递归目录    g: 用户组  u:用户    // 注意给文件w权限时须要给上级文件夹w权限，否则文件有w权限也无法写删
			getfacl  /usr/local/logs     // 查看目录权限 


12. linux中的系统发布，优雅停机
		应用启动后，小校验从而完成服务器上的应用发布。(一般通过检测脚本或者页面。有返回结果即可。)
		优雅停机，在负载均衡上做文章，关闭应用前，把应用从负载均衡中心上移去，然后再优雅关闭应用（结束当前所有请求后关闭），然后进行新应用启动及检查，检查通过后再把应用加到负载均衡上，并对外提供应用。
		灰度发布，指针对新应用，在用户体验上完全感知不到的更新，可能持续时间长，重要的状态需要记录。

13. curl 和 wget
      curl "http://www.baidu.com"  如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地  get
      curl -d "username=user1&password=123" "www.test.com/login"	post
		
      curl模拟的访问请求一般直接在控制台显示，适用结果内容比较少
      而wget则把结果保存为一个文件。适用结果比较多
      wget是个专职的下载利器，简单，专一，极致；而curl可以下载，但是长项不在于下载，而在于模拟提交web数据，POST/GET请求，调试网页等。

    vi和vim	   				
      都是Linux中的编辑器，不同的是vim比较高级，可以视为vi的升级版本。vi使用于文本编辑，但是vim更适用于coding。			
      在vi编辑器中，按u只能撤消上次命令，而在vim里可以无限制的撤消。

15. 查看出网IP
		curl http://ifconfig.me
		curl http://icanhazip.com	
		curl http://cip.cc

		出网ip和本机ip不一样，一个是 221.226.49.186本地  221.226.49.188是pre 
					
		命令行查询(详细):
		UNIX/Linux:#curl cip.cc
		windows环境直接百度ip就有了	

		telnet IP 端口 或者 telnet 域名 端口    telnet就是查看某个端口是否可访问
		ping + ip： 查看某一个ip地址是否能够连通，如： ping 114.80.67.193	用来检查网络是否通畅或者网络连接速度的命令 
	　　telnet ip port ： 查看某一个机器上的某一个端口是否可以访问，如：telnet 114.80.67.193 8080
	　　退出命令： exit---退出dos窗口，q!,wq---Linux下退出vi编辑器
	　　　　　　　ctrl+]，之后在按q ---退出telnet界面
	　　　　　　　quit---退出mysql.......
		系统出网ip是防火墙的出网ip


### shell脚本
   Bash 也是大多数Linux系统默认的Shell。
   一般情况下，并不区分Bourne Shell 和 Bourne Again Shell，所以，像 #!/bin/sh，它同样也可以改为 #!/bin/bash。
   #!/bin/bash					这里#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 程序。
   echo 命令用于向窗口输出文本，还有个类似的功能printf。

   * 运行Shell脚本有两种方法：
			1、作为可执行程序，常用方法
				chmod +x ./test.sh  #使脚本具有执行权限
				./test.sh  #执行脚本

			2、作为解释器参数
				/bin/sh test.sh
				这种方式运行的脚本，不需要在第一行指定解释器信息，写了也没用。
			
			执行当前目录下的sh脚本，一定要写成 ./test.sh，而不是 test.sh，运行其它二进制的程序也一样，
			直接写 test.sh，linux 系统会去 PATH 里寻找有没有叫 test.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 test.sh 是会找不到命令的，要用 ./test.sh 告诉系统说，就在当前目录找。
			
			以 # 开头的行就是注释，会被解释器忽略。
			
   * shell变量
			your_name="runoob.com"		变量赋值，字符串可用单引号，也可用双引号，也可以不用引号。()
										      单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；
										      单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用
										      双引号里可以有变量,双引号里可以出现转义字符
			echo ${your_name}         	#输出展示变量   可省略{}，但是建议加上
			readonly myUrl				设置为只读
			unset variable_name 		删除变量，删除后不能再用，只读不能删
			echo ${#variable_name} 		输出长度
			数组名=(值1 值2 ... 值n)	数组
			
   * shell运算
			val=`expr 2 + 2`
			echo "两数之和为 : $val"
			表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。
			完整的表达式要被 ` ` 包含，注意这个字符不是常用的单引号，在 Esc 键下边。
			
			val=`expr $a + $b`
			echo "a + b : $val"
			
			代码中的 [] 执行基本的算数运算，如：
			result=$[a+b] # 注意等号两边不能有空格
			echo "result 为： $result"
				
			echo -n 不换行输出	
			
			nohup(no hang up,不挂起)，用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。
				nohup 命令，在默认情况下(非重定向时)，会输出一个名叫 nohup.out 的文件到当前目录下，如果当前目录的 nohup.out文件不可写，输出重定向到 $HOME/nohup.out 文件中。
				语法格式： nohup Command [ Arg … ] [　& ]
				
				在后台执行 root 目录下的 runoob.sh 脚本，并重定向输入到 runoob.log 文件
					nohup /root/runoob.sh > runoob.log 2>&1 &
					如果是 > /dev/null 2>&1 &   
						> 代表重定向到哪里
						/dev/null 代表空设备文件，也就是不输出任何信息到终端，说白了就是不显示任何信息
						& 表示等同于的意思，2>&1，表示2的输出重定向等同于1
						标准错误输出重定向等同于标准输出，输出到同一文件中

					2>&1 解释：
					将标准错误 2 重定向到标准输出 &1 ，标准输出 &1 再被重定向输入到 runoob.log 文件中。
					0 – stdin(standard input，标准输入)
					1 – stdout(standard output，标准输出)，">/dev/null"等同于"1>/dev/null",默认是1
					2 – stderr(standard error，标准错误输出)
					
					JAR_HOME_DIR=/usr/local/java_project/webapps/
					APP_NAME=e-manager-pay.jar
					nohup java -Xms512m -Xmx512m -XX:PermSize=256M -XX:MaxPermSize=256M -jar $JAR_HOME_DIR$APP_NAME > /dev/null 2>&1 &
					这里实际grep的名字实际显示出来就是e-manager-pay.jar
					
			关系运算符
				-eq	等于则为真
				-ne	不等于则为真
				-gt	大于则为真
				-ge	大于等于则为真
				-lt	小于则为真
				-le	小于等于则为真
				
			文件运算符
				-b file	检测文件是否是块设备文件，如果是，则返回 true。	[ -b $file ] 返回 false。
				-c file	检测文件是否是字符设备文件，如果是，则返回 true。	[ -c $file ] 返回 false。
				-d file	检测文件是否是目录，如果是，则返回 true。	[ -d $file ] 返回 false。
				-f file	检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。	[ -f $file ] 返回 true。
				-g file	检测文件是否设置了 SGID 位，如果是，则返回 true。	[ -g $file ] 返回 false。
				-k file	检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。	[ -k $file ] 返回 false。
				-p file	检测文件是否是有名管道，如果是，则返回 true。	[ -p $file ] 返回 false。
				-u file	检测文件是否设置了 SUID 位，如果是，则返回 true。	[ -u $file ] 返回 false。
				-r file	检测文件是否可读，如果是，则返回 true。	[ -r $file ] 返回 true。
				-w file	检测文件是否可写，如果是，则返回 true。	[ -w $file ] 返回 true。
				-x file	检测文件是否可执行，如果是，则返回 true。	[ -x $file ] 返回 true。
				-s file	检测文件是否为空（文件大小是否大于0），不为空返回 true。	[ -s $file ] 返回 true。
				-e file	检测文件（包括目录）是否存在，如果是，则返回 true。	[ -e $file ] 返回 true。
				其他检查符：
				
			整数变量表达式
				if [ int1 -eq int2 ]    如果int1等于int2   
				if [ int1 -ne int2 ]    如果不等于    
				if [ int1 -ge int2 ]       如果>=
				if [ int1 -gt int2 ]       如果>
				if [ int1 -le int2 ]       如果<=
				if [ int1 -lt int2 ]       如果<	
			 
			字符串变量表达式
				If  [ $a = $b ]                 如果string1等于string2字符串允许使用赋值号做等号
				if  [ $string1 !=  $string2 ]   如果string1不等于string2       
				if  [ -n $string  ]             如果string 非空(非0），返回0(true)  
				if  [ -z $string  ]             如果string 为空
				if  [ $sting ]                  如果string 非空，返回0 (和-n类似)    
				
			变量符号
				$0	当前脚本的文件名
				$n	传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2。

				$#	传递给脚本或函数的参数个数。
				$*	传递给脚本或函数的所有参数。
				$@	传递给脚本或函数的所有参数。
				　　$* 和 $@ 的区别
				　　	相同点：都是引用所有参数，不被双引号" “包含时，都以”$1" “$2"…"$n” 的形式输出所有参数，
						被双引号" “包含时，”$*" 会将所有的参数作为一个整体；"@" 会将各个参数分开，以换行形式输出所有参数。
						比如，假设在脚本运行时写了三个参数 1、2、3，，则"*"等价于"1 2 3"(传递了一个参数)，而 "@" 等价于 "1" "2" "3"(传递了三个参数)
				$?	上个命令的退出状态，或函数的返回值。
				$$	当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID

			-- if else
				if condition1
				then
					command1
				elif condition2 
				then 
					command2
				else
					commandN
				fi
			-- for循环
				for var in item1 item2 ... itemN; do command1; command2… done;
			-- whlie	
				while condition
				do
					command
				done
				
			-- case
			case ... esac
			case $aNum in
				1)  echo '你选择了 1'
				;;
				*)  echo '你没有输入 1 到 4 之间的数字'
				;;
			esac

			#使用 . 号来引用test1.sh 文件
			. ./test1.sh

			# 或者使用以下包含文件代码
			# source ./test1.sh

			echo "菜鸟教程官网地址：$url"	
				

   * sh执行
		当前目录执行，不加找不到(不在执行程序默认的搜索路径之列)
		./a.sh会用你脚本中第一行的那个#!XXX的shell来执行语句,而sh a.sh则是用sh来执行语句
		sh或是执行脚本，或是切换到sh这个bash里，默认的shell是bash	
		执行脚本的时候是用sh + 脚本名的方式来执行，其实，大部分的时候，简单脚本只要权限设置正确，可以直接执行，不需要sh命令的
				
		以sh执行，那么，不必设定执行权限，也不用写shell文件中的第一行（指定bash路径）。因为方法三 是将hello.sh作为参数传给sh(bash)命令来执行的。这时不是hello.sh自己来执行，而是被调用执行。		
		sh带参数执行
			/usr/local/java_project/bin/java_project.sh stop        #多个参数空格分隔
			
		1、/bin/sh是/bin/bash的软连接，在一般的linux系统当中，使用sh调用执行脚本相当于打开了bash的POSIX标准模式，也就是说 /bin/sh 相当于 /bin/bash --posix
		2、/bin/sh执行过程中，若出现命令执行失败，则会停止执行；/bin/bash执行过程中，若命令执行失败，仍然会继续执行
		
		#!/bin/sh是#!/bin/bash的缩减版
				
		awk是一个强大的文本分析工具，简单来说awk就是把文件逐行读入，（空格，制表符）为默认分隔符将每行切片，切开的部分再进行各种分析处理		
				
		pid =`ps -ef| grep tomcat | grep -v grep | awk '{print $2}'`			#这里命令中的反引号的作用就是先执行命令然后把结果返回作为参数
		grep -v  			这里是查找，排除的意思
		awk '{print $2}'	就是截取第二个字段输出
		
   * linux 特殊的命令符号
			命令中的 |
				利用Linux所提供的管道符“|”将两个命令隔开，管道符左边命令的输出就会作为管道符右边命令的输入。依次向后传递。
			命令> 和 >>
            > 表示stdout标准输出信息重定向输出，覆盖写。
               echo 'World' > test.txt
            >> 表示内容追加写。
               echo 'World' >> test.txt
			命令&& 
				command1 && command2, 即指令1执行成功, 才会执行指令2, 常用来连贯执行
				cd dockertest/ && vi Dockerfile 
			命令|| 
				command1 || command2 指令1执行失败, 才会执行指令2
				例如 cat filename || echo “fail”
				
   * 实际自己测试的shell脚本
			简单的kill进程，然后重启
				#!/bin/bash
				tomcatpid=`ps -ef| grep tomcat | grep -v grep | awk '{print $2}'`
				echo "now pid is ${tomcatpid},trying to stop"
				kill -9 ${tomcatpid}
				echo "trying to reload"
				sh /home/zhanjun/test/tomcats/apache-tomcat-01/bin/startup.sh
				tailf -f /home/zhanjun/test/tomcats/apache-tomcat-01/logs/catalina.out

## 泛型相关

1. Java泛型中的标记符含义： 
      只是为了提高可读性，没有特定意义
      E - Element (在集合中使用，因为集合中存放的是元素)
      T - Type（Java 类）
      K - Key（键）
      V - Value（值）
      N - Number（数值类型）

2. List<String> list = new ArrayList<>();//可以自行泛型推断

3. <? extends T>表示该通配符所代表的类型是 T 类型的子类。
	<? super T>表示该通配符所代表的类型是 T 类型的父类。
	Class<T>代表这个类型所对应的类， Class<?>表示类型不确定的类

4. T 和 <T>的区别
      主要是看是否受Class<T>的影响
      必须要有泛型说明,1.要么自定义<T> T，2.要么跟着Class<E>走。
      ****
         <T> T 	
            表示不限制类型			不受static影响，因为不需要编译期完成指定
            不需要 类上泛型标注，可以使用static修饰
            表示返回值是一个泛型，传递啥就返回啥类型的数据
         T		
            表示编译时候就要限制类型，限制特定类型，编译就指定，使用这个，受static影响，因为编译期需要完成，和static同时期，但是static修饰后又使用T,冲突
            需要类上使用泛型标注，
            代表一个类型,限制传递的参数类型(用于共同操作一个类对象，单独T已经在类上限制了)
      ****

      //泛型常见类型
      ```
         public static <T> T get(String key, Class<T> clazz) {}
         private <T> T getListFisrt(List<T> data) {}			T是代表任意一种类型，<T>是一种形式，可以接受任意类型List参数。这个是不限定类型。
         private T getListFisrt(List<T> data) {}				初始化时已经限定了T的类型，所以getListFirst方法只能接受List<T>类型的参数。这个是限定类型。是编译就要指定的特定的类型
      ```
   
      实例的demo
      ```
         public class Request<E> {
            /**
               * tClass 		方法入参
               * <T>  		声明此方法拥有一个类型T,也声明此方法是一个泛型方法
               * T    		指明该方法返回值为类型T
               * Class<T>  	指明泛型T的具体类型
               */
            public <T> T getObject(Class<T> tClass) throws IllegalAccessException, InstantiationException {
               T t = tClass.newInstance();
               return t;
            }
            
         
            //第一个<T> 表示当前方法的值传入类型可以和类初始化的泛型类型不同
            //也是就是该方法的泛型类可以自定义，不需要跟类初始化的泛型类相同
            // 参数T  第一个表示是泛型，第二个表示是返回是T类型的数据，第三个表示限制参数类型为T
            private <T> T getListFirst(List<T> data) {
               if (data == null || data.size() == 0) {return null;}
               return data.get(0);
            }
            
            //这个只能传E类型的数据
            //这里不能写成T了，要么自定义<T> T，要么跟着类的<E>走
            private E getListFirst2(List<E> data) {
               if (data == null || data.size() == 0) {return null;}
               return data.get(0);
            }
            
            public static void main(String[] args) {
               List<Integer> data = new ArrayList<>();
               List<String> data2 = new ArrayList<>();
               //入参由List<T>的T 决定，因为返回值为<T> T ,所以入参不受 Request<T> 影响
               Integer a = new Request<String>().getListFirst(data);

               //编译出错，入参由Request<T> T的决定，受Request<T>影响
               //new Request<String>().getListFirst2(data);

               //没什么区别
               String aa = new Request<String>().getListFirst(data2);
               String bb = new Request<String>().getListFirst2(data2);
            }
         }
      ```

## 杂知识,杂项，杂记录

1. final相关
      final这类的不可变对象一定是线程安全的。典型的String类就是
      final的方法不能被重写。所以父类中的private方法默认是final的，子类将无法访问、覆盖该方法

      final修饰：
         类：不能继承
         方法：不可重写，实例方法为final，子类无法覆盖该方法；若申明的是final是类方法，则子类无法隐藏
         域：初始化后无法修改，常与static组合创建常量，只赋值一次

2. 类型强转
   public static void main(String[] args) throws ParseException {
         Object a = null;
         String c = "null";
         String b = (String)a;			//null可以转换 不报错
         System.out.println(b);
         System.out.println(c);
	}

3. 常用配置和软件和系统问题,编辑器相关,
   
      * maven 提示可导 但是进不来 java buildpath重新设置下

      * (忽略)SEVERE: Error configuring application listener of class org.springframework.web.context.ContextLoaderListener
         因为web工程中jar最终是要在web-inf的lib下的，没有重新添加maven的buildPath
         1.Deployment Assembly页面，点击Add   2.Jave Build Path Entries  3.添加maven依赖
         
      * ecipse下错误: 找不到或无法加载主类，重新添加buildPath中的add源码即可，maven打包异常不覆盖，也适用

         eclipse创建目录 https://www.cnblogs.com/yuyu666/p/10049954.html

         elipse 两种查看项目地址的方式：1、preference-git-configuration-repository  2、项目右键查看git respository，查看properties。

      * 获取系统的换行符 system.getProperty("line.separator")
	      System.in.read(); 按任意键退出，用来模拟运行状态  

      * 执行一个.bat文件，如若双击，错误闪退，如果是cmd进去执行，错误会显示出来

      * 在notepad++中勾选正则表达式，替换首尾字符，     ^/$->'/'

      * 远程桌面服务开启别人连不上，
	      gpedit.msc -> 计算机配置 -> 管理模板 -> Windows组件 -> 远程桌面服务 -> 远程桌面会话主机 -> 安全 -> 远程(RDP)连接要求使用指定的安全层，启用并把安全层改为RDP。

      * idea文件乱码，先切换到对应的编码，再convert(应该是读取文件内容，并以IDE本身指定的encoding来进行解读)
 



4. 算法加密，加密算法，加密传输

   1. 三种加密算法
         单向加密算法
            只能加密数据，不能解密回原来的明文。
            常会拼接一个salt字符串。密文＝ MDS(MDS(明文) + salt)
            常用：MD5,SHA,HMAC
         
         对称加密算法
            又称为单秘钥加密或密钥加密，指加密和解密使用的是相同的密钥
            加密后的信息可以解密成原值	des(适用数据库密码的加密)
            常用：DES,AES,和PBE
         
         非对称加密算法
            也叫公钥加密，就是指加密和解密使用了不同的密钥，一个公钥，一个私钥，公钥加密数据，私钥解密数据。		
            私钥用来签名，公钥用来验证签名。判断公私钥的正确性，就是ca证书做的事。
            私钥能解开公钥加密的数据，但忽略了一点，私钥加密的数据，同样可以用公钥解密出来（公钥是公开的，那么由服务器-》客户端的就存在安全隐患）
            常用： RSA 和 DH，重点RSA


      用约定好的HASH方式（如md5），把握手消息取HASH值，  然后将“握手消息+握手消息HASH值(签名)”  并一起发送给服务端
      使用加密签名的方式，用于验证握手消息在传输过程中没有被篡改过。这样就完成了一次请求，后续是用公用的随机key使用对称加密算法进行加密即可


   2. 实现md5加密的：（可以用现成，忽略）
         ```
            public static String md5Encode(String inputStr) {  
               MessageDigest md5 = null;  
               try {  
                     md5 = MessageDigest.getInstance("MD5");  
                     byte[] bytes = inputStr.getBytes("UTF-8");  
                     byte[] md5Bytes = md5.digest(bytes);  
                     StringBuffer hexValue = new StringBuffer();  
                     for (int i = 0; i < md5Bytes.length; i++) {  
                        int value = ((int) md5Bytes[i]) & 0xff;  
                        if (value < 16) { hexValue.append("0"); }  
                        hexValue.append(Integer.toHexString(value));  
                     }  
                     return hexValue.toString();  
               } catch (Exception e) {  
                  return "";  
               }    
            }  
            #总的过程是使用md5加密返回16长度的byte[]，使用16进制转换将每个byte转为两位的16进制，这样最终生成的固定长度的32位字符串
         ```

         自定义md5加密工具类(忽略)，实现自定义的转换逻辑，主要是hexDigits数组变化
            private static final String hexDigits[] = { "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "a", "b", "c", "d", "e", "f" };
            //系统中使用的是array[16]作为自定义，然后处理MD5加密后的byte[]。
            private static String byteToHexString(byte b) {
               int n = b;
               if (n < 0)
                  n += 256;
               int d1 = n / 16;
               int d2 = n % 16;
               return hexDigits[d1] + hexDigits[d2];
            }


   3. 常用安全算法
		   彩虹表破解hash算法(穷举存储明文和密文的所有组合)
		   md5 和 sha 都属于hash算法
		
         base64编码	
            任何人只要得到base64编码的内容，便可通过固定的方法，逆向得出编码之前的信息，
            只是一种编码算法，将一组二进制信息编码成可打印的字符，在网上传输展现。
            字节数组，一个字节是8个二进制信息，所以字节数组是二进制信息

         md5 
            md5信息摘要之后(摘要长度是128位),返回一个16个字节的字节数组，将字节数组转成16进制格式的数据(32位)。
            而1个byte转成16进制正好是2位（16进制使用4个bit，一个byte有8个bit），所以MD5算法返回的128bit 转成16进制就正好是 32位

         SHA
            摘要信息长度是160位，常用16进制编码，和base64编码

         数字证书
            要获得数字证书，首先要使用数字证书管理工具，如keytool、openSSL等，然后构建CSR,提交给数字证书认证机构，最终形成数字证书
            
         摘要认证	
            *****数据传输安全，简单说就是验签的过程。(加秘钥验签)*****
               参数排序，加上secret拼接待加签的明文，使用md5等摘要算法，生成密文，传输
            
         签名认证
            使用公私钥
            客户端
               1.排序拼接待加密串，
               2.使用共同的md5等摘要算法生成摘要串，
               3.然后在使用私钥加密生成密文，
               4.然后16进制编码为字符串传输
            服务端
               1.排序拼接待加密串，
               2.使用共同的md5等摘要算法生成摘要串(字节转16进制字符串)，
               3.使用公钥解密传来的密文得到明文串(字节转16进制字符串)
               4.两个字符串一致，即可

         实际项目使用
            * 中信证书加密解密
               加密过程
                  发送方公钥证书		PTNRtest.cer				这个给中信报备了，接收方解密用
                  发送方私钥证书		PTNRtest.key				配合密码加载
                  发送方私钥密码文件	PTNRtest.pwd				一般内容是字符串
                  中信公钥证书		中信公钥.cer				这个和1类似，报备后，用来解密用

                  sign加密的，使用了PTNRtest.pwd，PTNRtest.key，PTNRtest.cer加密   PKCS7Signature.sign()
                  
               解密过程
                  中信返回结果信息，接收方解密验签，使用了中信公钥证书.cer，这个和之前加密流程是相对应的，双方都需要报备证书.cer。		
               
            * 快钱证书
               自己生成   
                  公钥证书cer，用于传给对方验签用，本地加签，对方验签
                  私钥证书pfx，用于本地加签使用，有密码和别名要对应
                  这两个是成对使用的。

               快钱提供公钥证书
                  从快钱网站后台下载的公钥证书cer，到本地，用来验签使用。 是快钱提供的.快钱加签，本地验签。

   4. https相关
         https的传输使用的是证书，证书之间传递相当于加密了一个公用的私钥(用于信息传输),相关的配置是tomcat和nginx的使用
         1.客户端发送自己支持的加密规则给服务器，代表告诉服务器要进行连接了
         2.服务器从中选出一套加密算法和hash算法以及自己的身份信息(地址等)以证书的形式发送给浏览器，证书中包含服务器信息，加密公钥，证书的办法机构
         3.客户端收到网站的证书之后要做下面的事情： 
            1).验证证书的合法性   2).若验证通过证书，浏览器会生成一串随机数，并用证书中的公钥进行加密  3). 用约定好的hash算法计算握手消息，然后用生成的密钥进行加密，然后一起发送给服务器
         
         4.服务器接收到客户端传送来的信息，要求下面的事情： 
            1). 用私钥解析出密码，，用密码解析握手消息，验证hash值是否和浏览器发来的一致   2).使用密钥加密消息，回送
         5.如果计算法hash值一致，握手成功

         访问案例：
            1.小明访问XX，XX将自己的证书给到小明（其实是给到浏览器，小明不会有感知）
            2.浏览器从证书中拿到XX的公钥A
            3.浏览器生成一个只有自己知道的对称密钥B，用公钥A加密，并传给XX（其实是有协商的过程，这里为了便于理解先简化）
            4.XX通过私钥解密，拿到对称密钥B
            5.浏览器、XX 之后的数据通信，都用密钥B进行加密

         ca证书
            ca证书就是对两者之间的传输key进行了加密，确保两者之间的传输key的安全性，这个证书其实就是公钥，认证加密后的公钥，证书中包含了很多信息，最重要的是申请者的公钥，有了这个公钥之后，就可以解密证书，拿到发送方的公钥，然后解密发送方发过来的签名，获取摘要，重新计算摘要
            CA机构在给公钥加密时，用的是一个统一的密钥对，在加密公钥时，用的是其中的私钥，所以ca证书就是ca机构的密钥对中的公钥，ca机构的私钥对发送方的公钥加密。

         
         自定义生成ssl的证书(忽略)
            下载完ssl相关的内容，其中包含了各个web服务器的证书，比如ningx和tomcat相关的
            使用OPENSSL 命令行来生成 KEY+CSR2 个文件，Tomcat，JBoss，Resin 等使用 KEYTOOL 来生成 JKS 和 CSR 文件

         tomcat中加了https安全证书转换的，post请求都要是https请求
            ```
               <Connector SSLEnabled="true" clientAuth="false" keystoreFile="d:\tomcat.keystore" keystorePass="123456" maxThreads="150" port="9443" protocol="org.apache.coyote.http11.Http11Protocol" scheme="https" secure="true" sslProtocol="TLS"/>
            ```

         https相关
            生产上的tomcat是没有配置ssl的，都是nginx端配置的https，所以请求实际上都是在http上存入缓存

            https请求过程
               1.客户端向服务器端发送请求，将客户端的功能和首选项传送给服务器端，包括客端支持的 SSL 本、加密组件列表等
               2.服务器端发送选择的连接参数（从客户端加密组件中筛选出的加密组件内容和压缩方法）以及证书（包含公钥等信息、）给客户端
               3.客户端读取证书中的所有人、有效期等信息并进行校验，然后通过预置的 CA 验证证书合法性，有问题 提示
               4.客户端生成用于数据加密的对称密钥，然后用服务器的公钥进行加密井发送给服务器端
               5.服务器端使用自己的私钥解密数据， 获得用于数据加密的对称密钥
               6.安全的通道建立完毕，后续基于对称加密算法传输数据

            ssl网址		https://myssl.com/   可以查看xiaoyuer.com的https的证书相关

   5. api网关安全(忽略)
         1.基于 Token 的客户端访问控制和安全策略
         2.传输数据和报文加密，到服务端解密，需要在客户端有独立的 SDK 代理包
         3.基于 Https 的传输加密，客户端和服务端数字证书支持
         4.基于 OAuth2.0 的服务安全认证(授权码，客户端，密码模式等）
   

5. java的可变参数
      ```
            public void print(String... args) {
               for (int i = 0; i < args.length; i++) {
                  out.println(args[i]);
               }
            }

            可变参数判空
            public static void checkParamsNotNull(String errorMessage,Object... params) {
            if (params != null && params.length > 0) {
                  for (Object element : params) {
                     if (element == null) {
                        throw new IllegalArgumentException(errorMessage);
                     }
                  }
               }
            }

            1. 多参数入参判空
            public static  Boolean emptyValidate(String... fields){
               boolean flag=true;
               for (String param: fields) {
                  if(StringUtil.isBlank_new(param)){
                     flag=false;
                     break;
                  }
               }
               return flag;
            }


      ```  

6. HttpClient工具
      HttpEntity  消息载体，post或者response中消息的载体
      httpclient.execute(httppost)
      处理返回消息体：EntityUtils.toString(response.getEntity(), "UTF-8");
      post请求：传参
      List<namevaluepair> formparams = new ArrayList<namevaluepair>();

      * 表单提交(http请求默认提交方式)
         DefaultHttpClient client = new DefaultHttpClient();
         client.getParams().setParameter("http.protocol.content-charset","UTF-8");
         HttpPost httpPost = new HttpPost(url);
         JSONObject jsonObject = null;
         try {
            List<NameValuePair> params = new ArrayList<NameValuePair>();
            // 建立一个NameValuePair数组，用于存储欲传送的参数
            for (Entry<String, String> entry : parameters.entrySet()) {    //parameters是jsonobject或map
               if (entry.getValue() != null) {
                  params.add(new BasicNameValuePair(entry.getKey(), entry.getValue()));
               }
            }

            // 添加参数
            httpPost.setEntity(new UrlEncodedFormEntity(params, HTTP.UTF_8));

            // 设置编码
            HttpResponse response = client.execute(httpPost);
            int statusCode = response.getStatusLine().getStatusCode();
            if (statusCode != HttpStatus.SC_OK) {return null;}

            // 返回结果
            String body = EntityUtils.toString(response.getEntity());
            jsonObject = JSONArray.parseObject(body);

      * json格式提交
         案例一
            HttpClient httpClient = null;
            HttpPost httpPost = null;
            JSONObject result = null;
            httpClient = new SSLClient(port);
            httpPost = new HttpPost(url);

            // 设置请求头参数
            if(headerParamMap!=null){
               for (String key : headerParamMap.keySet()) {
                  httpPost.addHeader(key,headerParamMap.get(key));//添加header的参数
               }
            }
            // 在请求头中指定contentType,不设置默认是表单类型
            httpPost.setHeader(new BasicHeader("Content-Type","application/json"));

            // 设置请求体参数, 请求体直接使用raw json
            String bodyString = JSONObject.toJSONString(bodyParamMap);
            StringEntity postingString = new StringEntity(bodyString);
            httpPost.setEntity(postingString);

            HttpResponse response = httpClient.execute(httpPost);
            int statusCode = response.getStatusLine().getStatusCode();
            if (statusCode != HttpStatus.SC_OK) {return null;}
            HttpEntity resEntity = response.getEntity();
            if (resEntity != null) {
               String body = EntityUtils.toString(resEntity, "UTF-8");
               result = JSONArray.parseObject(body);
            }

         httpclient操作
            DefaultHttpClient client = new DefaultHttpClient();
            HttpPost httpPost = new HttpPost(url);
            JSONObject jsonObject = null;
               
            //建立一个NameValuePair数组，用于存储欲传送的参数	
            List<NameValuePair> params = new ArrayList<NameValuePair>();
            
            HttpResponse response = client.execute(httpPost);
            int statusCode = response.getStatusLine().getStatusCode();
            if (statusCode == 200) {
               // Read the response body
               String body = EntityUtils.toString(response.getEntity());
               jsonObject = JSONArray.parseObject(body);
            }

         案例二
            public static String httpPostJson(String url, JSONObject json) throws Exception {
               DefaultHttpClient client = new DefaultHttpClient();
               HttpPost httpPost = new HttpPost(url);
               httpPost.addHeader("Content-Type", "application/json");
               httpPost.setEntity(new StringEntity(json.toString(), "utf-8"));
               HttpResponse response;
               String result;
               try {
                     response = client.execute(httpPost);
                     result = EntityUtils.toString(response.getEntity(), "UTF-8");
               } catch (Exception e) {
                     throw new Exception(e);
               }
               if (response.getStatusLine().getStatusCode() == HttpStatus.SC_OK) {
                     return result;
               }
               return null;
            }
       
      * httpclient 和 HttpURLConnection的区别
         简单来说，HttpClient就是一个增强版的HttpURLConnection
         HttpClient相比较来说简化了HttpURLConnection对象对Session、Cookie的处理。
         HttpClient是Apache提供一个简单的HTTP客户端，不是浏览器，用于发送HTTP请求，接收HTTP响应。但不会缓存服务器的响应。只是关注于如何发送请求、接收响应，以及管理HTTP连接。

         HttpURLConnection是jdk自带的，httpclient是单独引的httpclient的jar包

         * 使用HttpClient：
               NameValuePair nameValuePair1 = new BasicNameValuePair("name", "yang");
               List nameValuePairs = new ArrayList();
               nameValuePairs.add(nameValuePair1);

               String validateURL = "http://10.0.2.2:8080/testhttp1/TestServlet";
               HttpClient client = new DefaultHttpClient();                                     //*1*.创建HttpClient对象
               HttpPost httpPost = new HttpPost(urlString);                                     //*2*.创建请求方式，HttpPost或者HttpGet
               if (nameValuePairs!=null && nameValuePairs.size()!=0) {
                  //把键值对进行编码操作并放入HttpEntity对象中
                  httpPost.setEntity(new UrlEncodedFormEntity(nameValuePairs,HTTP.UTF_8));      //*3*.设置请求体参数(若需头传输使用setHeader()方法或addHeader()方法)
               }
               HttpResponse httpResponse = client.execute(httpPost);                            //*4*.发送请求并等待响应，返回一个HttpResponse。
               // 判断网络连接是否成功
               if (httpResponse.getStatusLine().getStatusCode() != 200) {
                  return false;
               }

               //*5*.调用HttpResponse的getAllHeaders()、getHeaders(String name)等方法获取服务器的响应头；
                     调用HttpResponse的getEntity()方法获取HttpEntity对象，通过该对象获取服务器的响应内容。
               HttpEntity entity = httpResponse.getEntity(); // 获取响应里面的内容            
               inputStream = entity.getContent();  // 得到服务气端发回的响应的内容（都在一个流里面）
               // 得到服务气端发回的响应的内容（都在一个字符串里面）
               // String strResult = EntityUtils.toString(entity); 
         
         * 使用HttpURLConnection：
               String validateURL="http://10.0.2.2:8080/testhttp1/TestServlet?name=yang&pwd=123123";
               try {
                     URL url = new URL(validateUrl); //创建URL对象
                     //返回一个URLConnection对象，它表示到URL所引用的远程对象的连接
                     HttpURLConnection conn = (HttpURLConnection) url.openConnection();
                     conn.setConnectTimeout(5000); //设置连接超时为5秒
                     conn.setRequestMethod("GET"); //设定请求方式
                     conn.connect(); //建立到远程对象的实际连接
                     //返回打开连接读取的输入流
                     DataInputStream dis = new DataInputStream(conn.getInputStream());  
                     //判断是否正常响应数据 
                     if (conn.getResponseCode() != HttpURLConnection.HTTP_OK) {
                        return false;
                     }
                  } catch (Exception e) {
                     e.printStackTrace();
                  } finally {
                     if (conn != null) {
                        conn.disconnect(); //中断连接
                     }
                  }

               if (conn instanceof HttpsURLConnection) {
                  HttpsURLConnection httpsConn = (HttpsURLConnection) conn;
                  httpsConn.setRequestMethod(strMethod);
               } else if (conn instanceof HttpURLConnection) {
                  HttpURLConnection httpConn = (HttpURLConnection) conn;
                  httpConn.setRequestMethod(strMethod);
               }

7. web.xml配置全局错误页面(忽略)
    	调试错误跳转页面出现一个问题,即在web.xml中配置的error-page不起作用，在web.xml中配置代码通常有两种方式：
      1. 根据错误代码
         ```
            <error-page>
               <error-code>404</error-code>
               <location>/404.jsp</location>
            </error-page>
         ```

      2. 根据异常类型（相对优先原则）
         ```
            <error-page exception-type="java.lang.NullPointerException" location="/nullpointer.jsp"/>
         ```

8. 路径的编码
     1.url中有空格等特殊字符的		 2). URL中有中文的    需要编码
     2.编码要只对参数编码,不要对整个url进行编码,因为如果对整个url编码的话会把url中的"/","&"等字符也进行编码了
     3.使用URLEncoder.encode("str","charset");方法编码
     示例:String url = “http://localhost:7080/test?condition=”+URLEncoder.encode("工厂","utf-8")；

     js中encodeURI()是Javascript中真正用来对URL编码的函数。

     ```
     encodeURI	排除ASCII字母、数字、~!@#$&*()=:/,;?+'			需要编码整个URL，然后需要使用这个URL，那么用encodeUR
     encodeURIComponent	排除ASCII字母、数字、~!*()'	编码URL中的参数的时候，使用encodeURIComponent，参数后面的/是需要编码
     encodeURIComponent比encodeURI编码的范围更大。
     ```

9. 创建数组
      1. int[] arr=new int[6];
      2. int[] x={1,2,3,4};
      3. int[] y= new int[]{1,2,3,4,5};

10. 成员变量和局部变量的区别
     * 成员变量
         1. 定义在类里，方法之外的变量。
         2. 成员变量可以不显式初始化，它们可以由系统设定默认值；
         3. 成员变量在所在类被实例化后，存在堆内存中

     *  局部变量：
         1. 定义在方法体内部的变量。
         2. 局部变量没有默认值，所以必须设定初始赋值。
         3. 局部变量在所在方法调用时，存在栈内存空间中。

11. 加载类的几种方式：
      new StaticCode();//生成了一个匿名的对象（过后不能再使用），加载类  
      StaticCode.show();//通过类名调用静态方法，加载类  
      StaticCode s = new StaticCode();//生成了一个对象，加载类  
      StaticCode s = null;//没有生成类，与StaticCode类没有关系，不加载类 

     代码中的//在编译后的class文件中，是空行，依然占用行

12. 字符串比较，字符串相等判断

      String a=new String("nihao")，左开栈右开堆，常量相加是，先加，再在常量池中找，没有则创建；变量相加是先开空间再拼接
      
      关于，创建对象的堆，栈和常量池
      String str = "abc"创建对象的过程
         1 首先在常量池中查找是否存在内容为"abc"字符串对象
         2 如果不存在则在常量池中创建"abc"，并让str引用该对象
         3 如果存在则直接让str引用该对象
         
      String str = new String("abc")创建实例的过程
         1 首先在堆中（不是常量池）创建一个指定的对象"abc"，并让str引用指向该对象
         2 在字符串常量池中查看，是否存在内容为"abc"字符串对象
         3 若存在，则将new出来的字符串对象与字符串常量池中的对象联系起来
         4 若不存在，则在字符串常量池中创建一个内容为"abc"的字符串对象，并将堆中的对象与之联系起来
         
         String str1 = "abc"; String str2 = "ab" + "c"; str1==str2是ture
         String str1 = "abc"; String str2 = "ab"; String str3 = str2 + "c"; str1==str3是false
         
         小结：字符串若变量相加，是先开空间，再拼接；若常量相加，是先加，然后再常量池中找，存在返回，不存在创建

      String.format("%s%s ", fieldName) 字符串的拼接问题，%s代表一个站位符


    字符串比较  String s4 = s3+",world!";  s3+=",world!"; 类似  变量引用会开新地址


    String是不可变类，字符串的连接操作(如a+b+c)总是通过生成新的Stirng对象来进行的

13. 金额转换，数据转换，类型转换，分元转换，元分转换，分转元，元转分，分元转换，BigDecimal处理
  
      double的保留位数和四舍五入处理,BigDecimal处理金额计算
         public static double setDifScare(double arg) {
            BigDecimal bl = new BigDecimal(arg).setScale(2, BigDecimal.ROUND_HALF_UP);
            return bl.doubleValue();
         }

         BigDecimal.ROUND_HALF_UP 这个就是 以前的四舍五入
         元转分时，32.6589这种直接取点后两位，3265 其余的直接忽略  新老都是这样处理的
   
		BigDecimal bigDecimalLong = new BigDecimal(3269874l);
		
      几种方式的精度问题
         BigDecimal bigDecimal23 = new BigDecimal(3.3d); //这个精确会有问题
         BigDecimal bigDecimal23 = new BigDecimal(String.valueOf(3.3d));    
         BigDecimal bigDecimal25 = BigDecimal.valueOf(3.3d);//推荐使用
		
         类似Double.valueOf(21604435.94d) 会有科学计数法原因

      统一使用bigdecimal转换
         long longValue = bigDecimalDouble.multiply(new BigDecimal(100)).longValue();	//元转分
         double doubleValue = bigDecimalLong.divide(new BigDecimal(100)).doubleValue();//分转元

		   OrderCodeUtil 老版的就是   完全的String 在那判断.和0的index，然后拼接后返回(没有运算的概念)。不用重复造轮子。正规点

14. 按照字节截取字符串
     ```
         /**
         * 	src：byte源数组
         srcPos：截取源byte数组起始位置（0位置有效）
         dest,：byte目的数组（截取后存放的数组）
         destPos：截取后存放的数组起始位置（0位置有效）
         length：截取的数据长度
         * @param a
         * @param b 起始长度
         * @param c	字节长度
         * @return
         * @throws UnsupportedEncodingException
         */
         public static String getByteString(String a,int b,int c) throws UnsupportedEncodingException{
               byte[] bytes = a.getBytes("GBK");
               byte aaa[]=new byte[1024];
               System.arraycopy(bytes, b, aaa, 0, c);
               String string = new String(aaa, "GBK").trim();
               return string;
         }
     ```

     字节长度转换
         注意文件长度是字节长度，不是位长度
         byte[] bytes = string.getBytes("GBK");
         StringBuilder sb = new StringBuilder(minLength);	//执行字节长度
         for (int i = bytes.length; i < minLength; i++) {sb.append(padChar);}

     

15. 生产jar替换(为什么不直接发呢，忽略)
     测试prd更改，直接将com.xiaoyuer.pay.web.controller.gateway.WithdrawController.class（在对应工程的target下的jar）的文件夹放到tomcat下的classes下，
     虽然依赖的jar中有这个类，但是会优先使用classes下的文件。

16. 路径匹配
      AntPathMatcher路径的匹配工具类,boolean result = new AntPathMatcher().match(patternPath, requestPath) 校验路径是否匹配
      拦截器的通配符的写法
         ?：匹配一个字符
         *：匹配零个或多个字符
         **：匹配零个或多个路径

      ant 风格资源地址匹配：
         ?: 匹配文件名中的一个字符
         *：匹配文件名中任意字符	
         **：匹配多层路径

17. classpath: 和classpath*:的区别。
		1.classpath：只会到你的classes路径中查找找文件。
		2.classpath*：不仅会到classes路径，还包括jar文件中(classes路径)进行查找。所以即使直接的class下没有路径也不会启动报错，但是使用上面的会启动报错


18. VerifyImage.java  VerifyResult.java  图形验证
      1、使用的是随机生成Stringcode
      2、使用图片流将String转成相应的图片输出，使用的jar多为java.awt包下的内容
      3、生成的图片中的String存到缓存中，用于校验输入
      4、使用servlet配置在web.xml中生成和返回
      5、相关的代码已经上传github

19. 配置path的作用(忽略)
      1.程序的执行需要使用外部指令javac，但是javac指令仅仅能在JDK安装目录下的bin目录下运行，因此java程序只能写入bin目录。这样bin目录会很乱，不易管理。 
      2. 程序开发过程中，不能将源代码写入JDK的安装目录，因此需要将源程序保存到任意位置的指定目录(英文目录)，所以需要使javac指令在任意目录下可以运行。


       cmd 不识别mysql命令的时候，需要在环境变量中的path中添加mysql对应的bin目录 

20. 构造xml，xml解析 

      * 构造XML
         ```
            Document document = DocumentHelper.createDocument();
            Element root = document.addElement("root");
            root.addElement("user").addText(ids[1]);
            root.addElement("sessionIdentifier").addText(ids[0]);
            response.setContentType("application/xml;charset=UTF-8");
            response.getWriter().write(document.asXML());
            response.getWriter().flush();
         ```

      * xml解析
         xml 和java对象关系比较近的是xstream, 是java 对象和xml之间的一个双向转换器。
         xml解析，XStream用的比较常用，//把xml为转换为实体对象 对于嵌套的xml内容使用内部类接收，别整dom xpath（复杂的xml处理模型
         xml 和 json都是数据交换的一种规范。
         web service中 xml用的比较多

         对于xml解析的实用类，XStream用的比较常用，
         把xml为转换为实体对象 对于嵌套的xml内容使用内部类接收

         * dom解析xml
            DocumentBuilderFactory解析xml用 工具类的级别
            DocumentBuilder builder = factory.newDocumentBuilder();
            Document doc = builder.parse(f);
            解析xml文件
            使用tagname 和nameitem相关解析，没有节点灵活

         * xstream解析xml
            
            xstream: xml和java对象之间的转换工具类   
            XStream.alias("INFO", InfoReq.class);	相当于将类路径的标签转换为自定义的标签，节点重命名
            xstream.aliasField("author", Blog.class, "writer");     不使用别名，还是要使用具体的class
            
            使用隐式集合:	xstream.addImplicitCollection(Person.class, "list");
            <list><element><element/></list>   变为 <element><element/>
               
            缺点：在定义别名中的下划线“_”转换为xml后会变成“__”这个符号。

            解析list
            ```
               <?xml version="1.0" encoding="utf-8"?>
               <ROOT>
                  <RSP_CODE>00000</RSP_CODE>
                  <LIST>
                     <ROW><USER_ID></USER_ID></ROW>
                     <ROW><USER_ID></USER_ID></ROW>
                  </LIST>
                  <SIGN_INFO>AAA==</SIGN_INFO>
               </ROOT>
         
               public static Object getObjectListFromXML(String xml, Class clazz,Class clazzt){
                  XStream xStreamForResponseData = new XStream();
                  xStreamForResponseData.alias("xml", clazz);
                  xStreamForResponseData.alias("ROW", clazzt);
                  xStreamForResponseData.ignoreUnknownElements();//忽略掉一些新增字段
                  return xStreamForResponseData.fromXML(xml);
               }

            ```


21. 代码测试：（忽略）
      1. 主函数中测试，这种只支持一些常规类的创建对象测试，无法注入interface，mapper，applicationContext等，无法进行接口等测试
      2. junit测试，入口属于代码接口测试，在项目中添加library中添加junit，即可进行单元测试
            相关maven依赖		
            ```
               <dependency>
                  <groupId>junit</groupId>
                  <artifactId>junit</artifactId>
                  <version>3.8.1</version>
                  <scope>test</scope>
               </dependency>
            ```

            在springboot中：
            ```
               @RunWith(SpringJUnit4ClassRunner.class) // SpringJUnit支持，由此引入Spring-Test框架支持！ 
               @SpringBootTest(classes = XyeServiceCoreApplication.class) // 指定我们SpringBoot工程的Application启动类
               @WebAppConfiguration      //调用javaWEB的组件，比如自动注入ServletContext Bean等，指定加载 ApplicationContext是一个WebApplicationContext
               public class TalentsSearchServiceImplTest {
                  @Autowired
                  ITalentsSearchService  talentsSearchService;
                  @Test
                  public void testSelectUsersInfoBycellPhones() {
                     BaseModelInfo<usersInfoBean> selectUsersInfoBycellPhones = 	  talentsSearchService.selectUsersInfoBycellPhones(1, 202, "18862241316");
                     }
               }
            ```

            ​在普通的ssm工程中：在这里是在war工程中进行测试，也因为各个jar的引用集中，mybattis也是集成在这里。不引入spring-mvc.xml的原因是这里是接口测试，无需controller相关，引入spring.xml是获得容器中的service，引入spring-mybatis.xml是因为接口中需要mapper等，否则接口注入失败
            
            ```
               @RunWith(SpringJUnit4ClassRunner.class)
               @ContextConfiguration(locations = { "classpath:spring.xml","classpath:spring-mybatis.xml" }) /////单个的文件读取@ContextConfiguration("/applicationContext.xml")  
               public class UserInfoServiceImplTest {
                  @Autowired
                  IUserInfoService userInfoService;
                  @Test
                  public void test() {
                     UserInfo selectById = userInfoService.selectById(203);
                  }
               }
            ```
      3. postman测试，这种入口是路径测试
      ​		在spring 接受参数是@pathvariable和@requestparam
      ​		在springboot jboos中节后参数是@pathparam，@queryparam和@requestparam   @path标签和这些标签的匹配比较严格
            最初的soa测试是引入的jersy框架测试的
            
         postman测试
            表单提交 	x-www-form-urlencoded，后台常规request方式获取
            json提交，后台使用@RequestBody接收

         其他的接口测试工具
            APIPOST        https://www.apipost.cn/


22. jdk升级的tomcat问题(忽略):
			Tomcat 从7升级到8的时候出现了 java .lang.IllegalArgumentException: An inval id domain [.xxx.com] was specified for this cookie 
			在 tomcat context.xml中配置 <CookieProcessor className="org.apache.tomcat.util.http.LegacyCookieProcessor" />

         修改配置文件 context.xml ，指定 CookieProcessor 为 org.apache.tomcat.util.http.LegacyCookieProcessor，具体配置如下：
         <CookieProcessor className="org.apache.tomcat.util.http.LegacyCookieProcessor" /> 
         <Context>

         SpringBoot 内嵌 Tomcat 的解决方式
         在 springboot 启动类中增加内嵌 Tomcat 的配置 Bean，如下代码：
            //tomcat版本升级，An invalid domain [] was specified for this cookie
            @Bean
            public WebServerFactoryCustomizer<TomcatServletWebServerFactory> cookieProcessorCustomizer() {
               // lambda写法,暂时不用
         //		return (factory) -> factory.addContextCustomizers((context) -> context.setCookieProcessor(new LegacyCookieProcessor()));

               return new WebServerFactoryCustomizer<TomcatServletWebServerFactory>() {
                  @Override
                  public void customize(TomcatServletWebServerFactory factory) {
                     factory.addContextCustomizers(new TomcatContextCustomizer() {
                        @Override
                        public void customize(Context context) {
                           context.setCookieProcessor(new LegacyCookieProcessor());
                        }
                     });
                  }
               };
            }

23. split的特殊字符串
      首先java doc里已经说明, split的参数是reg, 即正则表达式, 如果用"|"分割, 则需使用"\\|"
      用* 分隔字符串运行将抛出java.util.regex.PatternSyntaxException异常，用加号 + 也是如此, 因此也应加入"\\"
      如果字符串中包含"\",首先这个字符串中的"\"需要转义, 即为"\\", 用split时需要写成split("\\\\"),;

24. 正则表达式：
		.（点号）也是一个正则表达式，它匹配任何一个字符
		.*	任意字符匹配多次	a*   多个字符匹配多次  依次匹配
		.*?	任意字符匹配一次	a*?
		\s+	匹配任意多个上面的字符。另因为反斜杠在Java里是转义字符，所以在Java里，我们要这么用\\s+
		. 匹配除换行符以外的任意字符

		\w 匹配字母或数字或下划线或汉字 等价于 '[^A-Za-z0-9_]'。
		\s 匹配任意的空白符
		\d 匹配数字
		\b 匹配单词的开始或结束
		^ 匹配字符串的开始
		$ 匹配字符串的结束

		^\d+(\.\d+)?

		1,^ 定义了以什么开始
		2,\d+ 匹配一个或多个数字
		? 设置括号内的选项是可选的
		\. 匹配 "."
		可以匹配的实例："5", "1.5" 和 "2.21"。

		特殊字符必须转义之后才能当做字符串
		java中\\代表一个\

		^匹配输入字符串的开始位置，除非在方括号表达式中使用，此时它表示不接受该字符集合。要匹配 ^ 字符本身，请使用 \^。
		
      正则表达式：
         字符串 String 的 split 方法，传入的分隔字符串是正则表达式！部分关键字（比如.[]()\|等）需要转义. |
         "a.ab.abc".split("\\."); // 结果为["a", "ab", "abc"]
         "a|ab|abc".split("\\|"); // 结果为["a", "ab", "abc"]


25. 图像处理ocr(忽略)
      图像预处理：灰度灰，二值化，膨胀，轮廓提取，取矩形，获取坐标，原图分割图片，根据坐标排序，二值化分割图片，降噪
      ocr：循环识别，结果过滤

26. HTTP 请求报文由 3 部分组成（ 请求行+请求头+请求体 ）
      HTTP 的响应报文也由三部分组成（ 响应行+响应头+响应体 ）
      web项目的标准结构：java+resource+webapp.这个默认的结构，一般不会去自己定义

27. 项目路径
      request.getRequestURI() /jqueryWeb/resources/request.jsp
      request.getRequestURL() http://localhost:8080/jqueryWeb/resources/request.jsp
      request.getContextPath()/jqueryWeb
      request.getServletPath()/resources/request.jsp			获取项目名后的路径 
      注： resources为WebContext下的目录名
      jqueryWeb 为工程名
      httpServletRequest.getRequestURI()== getContextPath() + getServletPath() + getPathInfo()


      路径操作
         String getServerName()：获取服务器名，localhost；
         String getServerPort()：获取服务器端口号，8080；
         String getContextPath()：获取项目名，/Example；
         String getServletPath()：获取Servlet路径，/AServlet；
         String getQueryString()：获取参数部分，即问号后面的部分：username=zhangsan
         String getRequestURI()：获取请求URI，等于项目名+Servlet路径：/Example/AServlet
         String getRequestURL()：获取请求URL，等于不包含参数的整个请求路径：http://localhost:8080/Example/AServlet 。





28. 工具类相关，常用工具类
      java操作excel的两种方式： poi和jxl
      文件功能  apachecommonsIo的fileUtils操作工具类，写到本地服务器或者上云
      AntPathMatcher是URLs匹配工具类

      ResourceUtils，IOUtils	
      Kaptcha  一个可配置的实用验证码生成工具
      联系我们中的地图  使用的是百度地图的开放api
      Pattern类  url路径匹配工具类
      Hutool 工具类可以留意下，常用的工具类封装

      比较常用的工具类
         apache commons ，google guava, joda time，fastjson
         HttpClient，工具类现在已 经从 Apache Commons 移到 Apache HttpComponents 中，并且包名被改为 org apache.http
         PropertyUtils 其和 BeanUtils 功能几乎一致 不同的是 BeanUtil 在对Bean 赋值时会进行自动类型转化，只要属性名相同，类型会尝试转换，而 PropertyUtil会报错
         httpClient 是使用HttpEntity来现的 常用的几个 httpEntity:UrlEncodedFormEntity,MultipartFormEntity,StringEntiry

      * 图片和base64的转换
         https://blog.csdn.net/qq_38508087/article/details/84671522   图片转base64

         转换之后的格式是：data:image/jpg;base64,/9j/4RTJRXhpZgAATU0AKgAAAAgAEAEAAAMAAAABC9AA...

         //测试图片链接转base64
         private static void NetImageToBase64(String netImagePath) {
            String url="https://upload.chinaz.com/picmap/202011091027589801_4.jpg";
            final ByteArrayOutputStream data = new ByteArrayOutputStream();
            try {
                  // 创建URL
                  URL url = new URL(netImagePath);
                  final byte[] by = new byte[1024];
                  // 创建链接
                  final HttpURLConnection conn = (HttpURLConnection) url.openConnection();
                  conn.setRequestMethod("GET");
                  conn.setConnectTimeout(5000);
                  new Thread(new Runnable() {
                     @Override
                     public void run() {
                        try {
                              InputStream is = conn.getInputStream();
                              // 将内容读取内存中
                              int len = -1;
                              while ((len = is.read(by)) != -1) {
                                 data.write(by, 0, len);
                              }
                              // 对字节数组Base64编码
                              BASE64Encoder encoder = new BASE64Encoder();
                              String strNetImageToBase64 = encoder.encode(data.toByteArray());
                              System.out.println("网络图片转换Base64:" + strNetImageToBase64);
                              // 关闭流
                              is.close();
                        } catch (IOException e) {
                              e.printStackTrace();
                        }
                     }
                  }).start();
            } catch (IOException e) {
                  e.printStackTrace();
            }
         }

         /**
         * base64编码字符串转换为图片
         * @param imgStr base64编码字符串
         * @param path 图片路径
         * @return
         */
         public static boolean base64StrToImage(String imgStr, String path) {
            if (imgStr == null)
            return false;
            BASE64Decoder decoder = new BASE64Decoder();
            try {
               // 解密
               byte[] b = decoder.decodeBuffer(imgStr);
               // 处理数据
               for (int i = 0; i < b.length; ++i) {
                  if (b[i] < 0) {
                     b[i] += 256;
                  }
               }
               //文件夹不存在则自动创建
               File tempFile = new File(path);
               if (!tempFile.getParentFile().exists()) {
                  tempFile.getParentFile().mkdirs();
               }
               OutputStream out = new FileOutputStream(tempFile);
               out.write(b);
               out.flush();
               out.close();
               return true;
            } catch (Exception e) {
               return false;
            }
         }


      * 获取用户的ip
         ```
            /** 
               * 获取用户真实IP地址，不使用request.getRemoteAddr()的原因是有可能用户使用了代理软件方式避免真实IP地址, 
               * 可是，如果通过了多级反向代理的话，X-Forwarded-For的值并不止一个，而是一串IP值 
               * @return ip
               */
               public static String getUserIp(HttpServletRequest request) {
                  String ip = request.getHeader("x-forwarded-for"); 
                  if (ip != null && ip.length() != 0 && !"unknown".equalsIgnoreCase(ip)) {  
                        // 多次反向代理后会有多个ip值，第一个ip才是真实ip
                        if( ip.indexOf(",")!=-1 ){
                           ip = ip.split(",")[0];
                        }
                  }  
                  if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) {  
                        ip = request.getHeader("Proxy-Client-IP");  
                  }  
                  if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) {  
                        ip = request.getHeader("WL-Proxy-Client-IP");  
                  }  
                  if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) {  
                        ip = request.getHeader("HTTP_CLIENT_IP");  
                  }  
                  if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) {  
                        ip = request.getHeader("HTTP_X_FORWARDED_FOR");  
                  }  
                  if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) {  
                        ip = request.getHeader("X-Real-IP");  
                  }  
                  if (ip == null || ip.length() == 0 || "unknown".equalsIgnoreCase(ip)) {  
                        ip = request.getRemoteAddr();  
                  } 
                  return ip;  
               }
         ```


      * 时间转换，date相关,date转换
            ```
               * 获取月份头尾
                  LocalDateTime.of(LocalDate.from(LocalDateTime.now().with(TemporalAdjusters.firstDayOfMonth())), LocalTime.MIN);
                  本月最后一天23：59：59
                  LocalDateTime.of(LocalDate.from(LocalDateTime.now().with(TemporalAdjusters.lastDayOfMonth())), LocalTime.MAX);

               * 转为date
                  LocalDateTime of = LocalDateTime.of(LocalDate.from(LocalDateTime.now().with(TemporalAdjusters.lastDayOfMonth())), LocalTime.MAX);
                  Date date = Date.from( of.atZone( ZoneId.systemDefault()).toInstant());


               * Date和localDate的转换
                  public class DateUtils {
                     public static Date asDate(LocalDate localDate) {
                        return Date.from(localDate.atStartOfDay().atZone(ZoneId.systemDefault()).toInstant());
                     }
                     public static Date asDate(LocalDateTime localDateTime) {
                        return Date.from(localDateTime.atZone(ZoneId.systemDefault()).toInstant());
                     }
                     public static LocalDate asLocalDate(Date date) {
                        return Instant.ofEpochMilli(date.getTime()).atZone(ZoneId.systemDefault()).toLocalDate();
                     }
                     public static LocalDateTime asLocalDateTime(Date date) {
                        return Instant.ofEpochMilli(date.getTime()).atZone(ZoneId.systemDefault()).toLocalDateTime();
                     }
                  }

               * 获取当天的起止时间
                  SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
                  String beginTimes = new SimpleDateFormat("yyyy-MM-dd" + " 00:00:00").format(new Date());
                  String endTimes = new SimpleDateFormat("yyyy-MM-dd" + " 23:59:59").format(new Date());

                  Date beginTime = sdf.parse(beginTimes);
                  Date endTime = sdf.parse(endTimes);

               * 获取指定日期的起止时间
                  SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMdd HH:mm:ss");
                  Date beginTimed = sdf.parse("20211231" + " 00:00:00");
                  Date endTimed = sdf.parse("20211231" + " 23:59:59");

                  Calendar calb = Calendar.getInstance();
                  Calendar cale = Calendar.getInstance();
                  calb.setTime(beginTimed);
                  cale.setTime(endTimed);
                  calb.add(Calendar.DATE, 1);//时间加一天
                  cale.add(Calendar.DATE, 1);

            ```

29. restful相关
      rest:根据三个单词缩写，资源(标识定位资源)+表现(获取资源后json等形式展示出来)+状态转换(增删改查等操作)
		简单参是用@pathvariable,复杂的用@requestBody接收json绑定转换为java对象(多个参数可以用json传递)

      非rest url http://.../queryitem?id=001&type=t01
      rest url   http://.../item/001
      实现rest访问 前端控制器需要配置/拦截
      配置/ ,那么静态资源js也会由前端控制器拦截，所以找不到，
      在springmvc.xml中添加静态资源解析即可，<mvc:resources location='/js/'  mapping='/js/**'>

30. jdk版本切换要点(忽略)
      cmd  echo %path% 输出系统的环境变量
      更换java_home
      删除path中的变量C:\Program Files (x86)\Common Files\Oracle\Java\javapath;
      删除C:\ProgramData\Oracle\Java，将Java文件直接删除
      然后更换system32中的三个java文件对应版本即可
      最多还要修改下注册表	
      HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment
      HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Development Kit

      JDK，开发java程序用的开发包，JDK里面有java的运行环境(JRE)，包括client和server端的。需要配置环境变量
      JRE，运行java程序的环境，JVM，JRE里面只有client运行环境，安装过程中，会自动添加PATH。

31. 自增键
    1、uuid   2、redis.incr  3、date+random   4、mysql自增

32. Random rand = newRandom();：
	   rand.nextInt(100);
	   这行代码将生成范围[0, 100) 之间的随机数

33. hashcode和equals方法
      equals 方法  原本是 == 比较两个对象的地址是否相等(即，是否是同一个对象)
      hashCode() 作用是获取对象的散列码，进而确定该对象在散列表中的位置。
      1、两对象相等，hashCode()值一定相同；
      2、两个对象hashCode()相等，它们并不一定相等。
      3、hashCode()不相等，两个对象一定不equals()

      自定义类要重写equals方法来进行等值比较，(不重写equals，比较的是对象的引用是否指向同一块内存地址)
      自定义类要重写compareTo方法来进行不同对象大小的比较，
      自定义类重写hashcode(),方便类存入hashmap等数据时，相等对象必须具有相同的hashcode()

      hashCode存值原理
         hashcode是用于散列数据的快速存取,如利用HashSet/HashMap等存储数据时，根据存储对象的hashcode值来进行判断是否相同的

         集合添加新元素时，先调用元素的hashCode方法，定位存储位置。
         如果这个位置上没有元素，它就可以直接存储在这个位置上，不用再进行任何比较了；
         如果这个位置上已经有元素了，就调用它的equals方法与新元素进行比较，相同的话就不存了，不相同就表示发生冲突了，冲突后散列表特殊处理，最终将新元素保存在适当位置。

34. 如果一个类，没有定义构造函数，那么系统默认有一个无参构造函数
	 如果你自定义了构造函数(无论有参还是无参)，那么编译器就不会创建无参构造函数。若还需无参构造，就必须显式的声明一个

35. 回调的思想
		类A的a()方法调用类B的b()方法
		类B的b()方法执行完毕主动调用类A的callback()方法
		
		目前就是a调用b方法，并把实现callback接口的实例传过去，b方法调用完直接用实例回调方法即可
		a调用了b的方法，b开始执行，b执行完了，再调用a的方法，这就是回调。

36. lombok(忽略)
		1.引入lombokjar 2.ide中安装lombok插件	3.使用@data注解
		不过一般不建议用，需要别人强制安装插件。

37. 引用复制
		UserInfo userInfo = new UserInfo();		userInfo.setNickName("ceshi1");
		UserInfo userInfo2 = new UserInfo();	userInfo2=userInfo;
		userInfo2.setNickName("ceshi2");
		这种引用传递要注意,最终会将userInfo的信息也变更了
		obj.sonObj    这种类中包含属性类的情况，也属于引用复制，改变新的，也会改变老的值属性 要注意

38. 网银支付，网银网页跳转
		后端生成自动跳转的Html表单串(包含验签信息),将生成的html写到浏览器中完成自动跳转打开银联支付页面；
		
39. ajax回来的请求只能是数据，跳页面还的事表单或者
      ```
         function payError(jSon) {
            var redirectLink;
            redirectLink = jSon.redirectLink;
            $("#form1").before("<form id='payError' method='post' action='#'><input type='hidden' name='resultPayOrderId' id='resultPayOrderId' value=''/>" +
            $("#terminalCode").val(jSon.terminalCode);
            $("#payError").attr("action", redirectLink);
            $("#payError").submit();
            
         }
         
         window.location.href ="/xye-open/rechargetGateWay/showInside.htm?refresh="+Math.random();
      ```
 
40. 前后端分离相关，前后分离相关
         https://www.cnblogs.com/z00377750/p/9136385.html    前后端分离相关介绍

         前端只利用Json来和后端进行交互，后端不返回页面，只返回Json数据。前后端之间完全通过public API约定
         未分离，先后端取数据后渲染页面。需要等后端返回数据才能处理(页面跳转，值在requrest中)，/分离后，先html后向后台取数据，这个可以自己模拟Jason数据不需要等
      
         前后端分离的情况下，基本使用json交互，没有什么服务端跳转或者客户端跳转之类，也没有mvc视图配置。使用@RestController相对@responseBody更加的方便，
         这和mvc模式不一样，原:通过视图名称找到对应视图，将数据模型渲染展示
         
         rest
         一般还是只是用Get和Post请求，使用接口名字来区分，所以，对于Rest规范，只需要记得传递数据只使用JSON，而不是后端去渲染模板，从而实现前后端的完全分离。
         前后端约定接口&数据&参数
         前后端并行开发（无强依赖，可前后端并行开发，如果需求变更，只要接口&参数不变，就不用两边都修改代码，开发效率高）
         
         大多数都是单独请求后台数据，使用json传输数据，而不是一个大而全的HTTP请求把整个页面包括动+静全部返回过来
      
         浏览器发送请求
            1.直接到达html页面（前端控制路由与渲染页面，整个项目开发的权重前移）
            2.html页面负责调用服务端接口产生数据（通过ajax等等，后台返回json格式数据，json数据格式因为简洁高效而取代xml）
            3.填充html，展现动态效果，在页面上进行解析并操作DOM。

41. 注解相关
		Annatation(注解)是一个接口，程序可通过反射来获取指定程序中元素的 Annotation对象，然后通过该Annotation 对象来获取注解中的元数据信息。
      注解中的@Target 常用的类型 指定用在什么上面，根据实际需求定
			TYPE,		      用于描述类、接口(包括注解类型) 或enum声明 
			FIELD,	      用于描述域 
			METHOD,		   用于描述方法
			PARAMETER,	   用于描述参数
		一般@Retention(RetentionPolicy.RUNTIME)都是运行期
       
42. 使用TimeUnit类来实现线程的sleep也可以(忽略)
      TimeUnit.MINUTES.sleep(4);  // sleeping for 1 minutes
      ```
            Calendar calendar = Calendar.getInstance();
            calendar.setTime(new Date());
            calendar.set(Calendar.HOUR_OF_DAY, 0);
            calendar.set(Calendar.MINUTE, 0);
            calendar.set(Calendar.SECOND, 0);
            Date zero = calendar.getTime();
               
            Calendar.HOUR_OF_DAY是24小时制
            Calendar.HOUR是12小时制
            所以下面方法是结果是不同的
            calendar.set(Calendar.HOUR_OF_DAY, 23); ?输出日期?2017-04-13 23:07:02
            calendar.set(Calendar.HOUR, 23); ?输出日期2017-04-13 11:07:02
         ```

43. 静态初始化和bean加载的顺序问题
	   es的client静态初始化，mq启动消费和常量类的属性加载顺序引起的异常。之前留下的一个记录，先消费消息，然后加载常量类，出现错误

44. 小鱼儿系统的设计相关
		open传参方法，这里更像一个网关，负责将请求转发到具体的业务处理。API Gateway 负责请求转发、合成和协议转换。
		请求地址:	/xye-open/gateWay/systemGateWay.htm|Params:partner=XYE_ADMIN2&query_type=userBankcardsInfo&service=op_common_query_serv...&sign=055e5bb5dbda93f43d82...&sign_type=md5&userId=131718|Spend:8

		systemGateWay--->1.url参数转map,2.验证op_service,3.验证op_partner，并且验证两者关系,4.验证sign
		--->在线程变量中存入先关的参数信息（parametersMap,opservice,oppartner 和partner_service）
		--->调用opservice的执行方法(IOpService ops = (IOpService)SpringContextUtil.getBean(opService.getSystemTarget());if (ops.execute()))
		-->ApplicationContext.getContext().getParameters();    从线程变量中拿到变量开始执行方法.那么相当于是使用的map传递。然后根据 业务组装各自的参数dto
		-->层级返回
		
	   request->防火墙->负载均衡->转发到服务器

45. 小程序登录相关信息返回
		   String url = "https://api.weixin.qq.com/sns/jscode2session?appid=" + appId + "&secret=" + appSecrect + "&js_code=" + wmpcode + "&grant_type=authorization_code";
         resultStr = HttpClientUtil.httpGet(url);	

46. Websocket 电签  上传图片(忽略)
      Websocket之前， long poll 和 ajax轮询。  服务端能主动调用客户端
      程序设计中，这种设计叫做回调
      1.前段开启连接，需要带上userid。前端会将session传过去，通过参数绑定唯一的userid
      2.后端配置WebSocketServer，通过onOpen将session和userid绑定。WebSocketServer中奖session和userid绑定。
      3.发送消息就调用对应的server发消息

      打开websocket连接开启,生成二维码，扫码跳转地址，打开另一个websocket连接(和第一个同参数，但是不同session)，签名发送消息，前端接收。

      引入jquery.qrcode.min.js插件，然后直接调用即可
      $('#qrcode').qrcode({
            render: 'canvas', //table,canvas方式
            width: 120, //宽度
            height:120, //高度
            text: "http://www.runoob.com" //二维码内容
         });

      手写功能：
      jSignature.js  然后var $sigdiv = $("#signature").jSignature({'UndoButton':true});初始化插件即可
      
      
      函数放script中可以直接执行，比如alert 和 bind等
      $(document).ready(function() {}

      List tempList = upload.parseRequest(request);
      InputStream is = item.getInputStream();
      然后outstream输出即可
      

47. javap签名 (忽略)
      javap -s a.class 这里常出现的问题就是jar版本不一致导致的方法变更
      Boolean 比较特殊, 对应的是 Z ， Long 对应J
   　 引用数据类型：比较麻烦点，以“L”开头，以“；”结束，中间对应的是该类型的路径
         如：String ： Ljava/lang/String；
             Object： Ljava/lang/Object；
         自定义类 Cat  对应  package com.duicky;
                  Cat ： Lcom/duicky/Cat；
   　 数组表示：数组表示的时候以“[” 为标志，一个“[”表示一维数组
         如: int [] ：[I
             Long[][]  ： [[J
             Object[][][] ： [[[Ljava/lang/Object；


48.  return null;  和	return;
      一个是有返回值的return  还有一个是void的返回结束
      测试controller 中的return；   遇到return; 直接终止执行

      Java字符串用\\表示\


49. int i=0;
    int i=i++;  最后值为0			
    JVM 在处理 i = i++; 时 , 会建立一个临时变量来接收 i++ 的值 , 然后返回这个临时变量的值 ,
    返回的值再被等号左边的变量接收了 , 这样就是说 i 虽然自增了但是又被赋值了0 , 这样输出的结果自然就是 0 了
    不妨我们用 temp 临时变量来接收 i++ 的值 , 来看一下结果 :
    int i = 0;
    int temp = i++; //temp的值是 : 0

50. JSONArray 可以直接添加obj
      JSONArray 和 list  转 jsonstring是一样的
      JSONObject parameters     parameters.entrySet()      和map类似

51. 其他前端框架相关 
      管理后台就上 vue axios elementui
      小程序app就上 uni-app

      微信小程序
         使用 微信开发者工具 开发小程序
         文档地址，https://developers.weixin.qq.com/miniprogram/dev/devtools/devtools.html

52. 枚举类信息覆盖之后会影响其他的(一般作为常量的集中，不做更新)		

53. java程序和虚拟机
      程序开始执行时Java虚拟机才运行，程序结束时Java虚拟机就停止。
      同一台机器上运行三个java程序，就会有三个运行中的Java虚拟机。
      只要Java虚拟机中还有普通的线程在执行，Java虚拟机就不会停止
   
      session是放在tomcat中(整个tomcat运行的程序，运行在jvm中)，服务化soa有没有对应的session中，除非集成了第三方的缓存

54. 前端杂记，前端相关(忽略)

      1. var name=$(this).parent().siblings('input').eq(1).val(); 获取同级第二个input元素
         ​ftl中的注释用<#-- -->  <!--  -->注释不能嵌套用
         ​	
         ​	<c:forEach var="addType" items="${addSers}" varStatus="addVsType">
         ​	varStatus.index 从0开始的索引
         ​	varStatus.count	从1开始的计数
            ​	
         ​	<#if user??>
         ​	${(user.name)!""}

      2. data:{"reqAppointInfo":JSON.stringify(dataForm),"userId":$('#userId').val()},如果走的ajax data数据，没有传就拿不到，和表单提交不一样

      3. 将jquery的相关js引入到classpath下的js文件夹下，
            最简洁的ajax使用,ajax和js是不允许跨域请求的
            ```
            #页面引入 <script  src="js/jquery-1.8.3.min.js">，即可使用。
                  $.ajax({
                           type: "post",
                           url: "http://192.168.1.102:8081/addTalents",
                           data: {"a":"nihao"},
                           dataType: "json",
                           success: function(data){} 
                        });
            ```

      4. js前段编码
            encodeURI()，用来encode整个URL，不会对下列字符进行编码：+ : / ; ?&。它只会对汉语等特殊字符进行编码。针对访问路径，
            url = 'www.xxx.com/aaa/bbb.do?parm1=罗'
               
            encodeURIComponent ()，用来enode URL中想要传输的字符串，它会对所有url敏感字符进行encode。针对参数param
            url = 'www.xxx.com/aaa/bbb.do?parm1=www.xxx.com/ccc/ddd?param=abcd'	

55. 自动拆箱装箱
      JVM会自动维护八种基本类型的常量池，int常量池中初始化-128~127的范围，所以当为Integer i=127时，在自动装箱过程中是取自常量池中的数值，而当Integer i=128时，128不在常量池范围内，所以在自动装箱过程中需new 128，所以地址不一样。
      包装类的==运算在没有遇到算数运算的情况下不会自动拆箱，而且他们的equals()方法不会处理数据转型的关系，比如g=(a+b)
			







## Springcloud相关
   https://www.springcloud.cc/		Spring Cloud中文网

   cloud中比较重要的组件
      (1)spring-cloud-netflix-Eureka 注册中心
      (2)spring-cloud-netflix-hystrix RPC保护组件
      (3)spring-cloud-netflix-ribbon 客户端负载均衡组件
      (4)spring-cloud-netflix-zuul 内部网关组件
      (5)spring-cloud-config 配置中心


   b站看的尚硅谷周阳 cloud

   lcn事务,暂不看
   spring-boot-starter-actuator    boot监控相关，用的也不多

   Cloud Server不需要注册到注册中心，要False掉。只有生产者Provider和消费者Consumer需要通过需要注册。
   设置与Eureka Server交互的地址，查询服务和注册服务都需要依赖这个地址。默认是
   eureka.client.serviceUrl.defaultZone=http://localhost:8800/eureka/
   
   #eureka.client.registerWithEureka ：表示是否将自己注册到Eureka Server，默认为true。注册中心不用注册
   #eureka.client.fetchRegistry ：表示是否从Eureka Server获取注册信息，默认为true。因为这是一个单点的Eureka Server，不需要同步其他的Eureka Server节点的数据，故而设为false。

1. ribbon和feign的区别（转）
      spring cloud的Netflix中提供了两个组件实现软负载均衡调用：ribbon和feign。

      Ribbon
      是一个基于 HTTP 和 TCP 客户端的负载均衡器
      它可以在客户端配置 ribbonServerList（服务端列表），然后轮询请求以实现均衡负载。

      Feign
      Spring Cloud Netflix 的微服务都是以 HTTP 接口的形式暴露的，所以可以用 Apache 的 HttpClient 或 Spring 的 RestTemplate 去调用，
      而 Feign 是一个使用起来更加方便的 HTTP 客戶端，使用起来就像是调用自身工程的方法，而感觉不到是调用远程方法。

      String path = request.getSession().getServletContext().getRealPath("");    #直接砸到了/usr/local/tomcat/webapps/xye-netpay

      @SpringCloudApplication包含了
      @SpringBootApplication
      @EnableDiscoveryClient
      @EnableCircuitBreaker
      hystrix 继承的方式?
      
      cloud 问题记录  https://blog.csdn.net/uotail/article/details/84673347

2. Nginx和Ribbon应用场景的区别：
		服务器端负载均衡 Nginx
		nginx 是客户端所有请求统一交给 nginx，由 nginx 进行实现负载均衡请求转发，属于服务器端负载均衡。
		既请求由 nginx 服务器端进行转发。

		客户端负载均衡 Ribbon
		Ribbon 是从 eureka 注册中心服务器端上获取服务注册信息列表，缓存到本地，然后在本地实现轮询负载均衡策略。
		既在客户端实现负载均衡。

		Nginx 适合于服务器端实现负载均衡 比如 Tomcat ，
		Ribbon 适合与在微服务中 RPC 远程调用实现本地服务负载均衡，比如 Dubbo、SpringCloud 中都是采用本地负载均衡。

3. cloud和dubbo等框架的区别
		之前的dubbo，链路，治理，等框架都只是解决了微服务中的某一个问题，没有成体系。
		而cloud是解决微服务架构实施的综合性解决框架。相当于一个组装机，不用像之前一样，自由度高的配置零件。
		之前的类似自己组装电脑，cloud相当于品牌机各个原装组件都配置好了。是整套的实现框架。

3. 版本号相关
      因为cloud下有很多个子项目，为避免cloud版本号和子项目版本号混淆，没有采用版本号方式，而是通过命名的方式。

4. spring cloud eureka  
      使用Netflix Eureka来实现服务的注册与发现。包含客户端和服务端。
		
      Eureka服务端称为服务注册中心  
			Eureka server 用来做注册中心，
			如果是集群部署，当集群中有分片出现故障时。允许分片在故障期继续提供服务发现和注册，当故障分片恢复运行时，集群中的其他分片会把他们的状态再同步回来。

			eureka server的高可用实际就是将自己作为服务向其他注册中心注册自己，这样实现一组互相注册的服务注册中心，实现服务清单的同步，达到高可用。暂不细看
			服务注册中心之间互相注册为服务。当服务提供者发送注册请求到一个服务注册中心时，它会将该请求转发给集群中相连的其他注册中心，从而实现注册中心之间的服务同步。

		Eureka客户端主要处理服务的注册与发现
			客户端服务通过注解和参数配置的方式，嵌入在客户端应用程序代码中，Eureka客户端向注册中心注册自身提供的服务，
			并周期性的发送心跳来更新服务租约。也支持服务信息缓存本地并周期性刷新服务状态。

      * 服务治理的基础框架的三个核心
         * 服务注册中心		
            eureka提供的服务端，用于服务的注册和发现。eureka-server	
            
         * 服务提供者			
            通过rest请求注册到eureka server上，同时带上了自身服务的一些元数据信息。
            eureka server接收到请求后，将元数据信息存储在一个双层结构的map中，第一层key是服务名，第二层key是具体服务的实例名。
            通过心跳维持和注册中心的连接。这里叫服务续约。
               
            #Renew频率。默认是30秒，也就是每30秒会向Eureka Server发起Renew操作，用来通知Eureka Server自己还活着
            eureka.instance.leaseRenewalIntervalInSeconds	
            
            #服务失效时间。默认是90秒，也就是如果Eureka Server在90秒内没有接收到来自Provider的Renew操作，就会将其剔除。
            #注册中心也有定时任务会自动清理，进行失效剔除。
            eureka.instance.leaseExpirationDurationInSeconds

         * 服务消费者			
            消费者从注册中心获取服务列表，消费方式，ribbon或者feign
            eureka server会维护一份只读的服务清单来返回给客户端，同时该缓存清单会每隔30秒更新一次(时间可以更改)。
            eureka.client.fetch-registry要确保是true不能是false，默认也是true，从Eureka Server获取注册信息
            
            当调用失败的情况下，客户端需要有容错机制，比如重试、断路器等。
					
      * eureka的自我保护机制（忽略）
         作用是避免因网络分区故障导致服务不可用的问题。
         机制开启后，不会从注册列表中剔除因长时间没收到心跳导致租期过期的服务，而是等待修复，直到心跳恢复正常之后，它自动退出自我保护模式。
         
         为什么会有自我保护机制？
            Eureka服务端为了防止Eureka客户端本身是可以正常访问的，但是由于网路通信故障等原因，造成Eureka服务端失去于客户端的连接，从而形成的不可用。
            因为网络通信是可能恢复的，但是Eureka客户端只会在启动时才去服务端注册。如果因为网络的原因而剔除了客户端，将造成客户端无法再注册到服务端。
            不让其过期，但这些服务也并不是永远不会过期。Eureka在启动完成后，每隔60秒会检查一次服务健康状态，如果这些被保护起来失效的服务过一段时间后（默认90秒）还是没有恢复，就会把这些服务剔除

         例如，两个客户端实例 C1 和 C2 的连通性是良好的，但是由于网络故障，
         C2 未能及时向 Eureka 发送心跳续约，这时候 Eureka 不能简单的将 C2 从注册表中剔除。
         因为如果剔除了，C1 就无法从 Eureka 服务器中获取 C2 注册的服务，但是这时候 C2 服务是可用的。
         建议开启，关闭之后注册中心中不可用的实例被及时的剔除。

      eureka配置详解
         #eureka.client.serviceUrl.defaultZone  指定注册中心
         
      跨平台
         eureka通信机制使用了http的rest接口实现。
         默认情况下，eureka使用jersey和xstream配合json作为server与client之间的通信协议。


5. hystrix 服务容错保护
		具备服务降级，服务熔断、服务监控等功能，避免因个别的服务出现异常，而引起级联故障蔓延
			
      具体实现
         1.boot引入spring-cloud-starter-netflix-hystrix
         2.启动类加上@EnableHystrix
         3.在需要监控的方法A上加上@HystrixCommand(fallbackMethod="backfix")，注意fallbackMethod的函数要和本方法一致。包括入参和返回
            这样A中调用出现异常，直接返回fallbackMethod的结果。断路器作用。
            fallback是一种服务降级的操作
            降级逻辑不是一个依赖网络请求的处理，而是一个能稳定返回结果的处理逻辑。
            可以再fallbackmethod中实现方法的参数中增加throwable e，就可以获取触发服务降级的具体异常内容了。
               public  String fallbackmethod(String name,Throwable e){
                  System.out.println("ceshi backfix"+e);
                  return ">>>>>>>>>>>>>>>>>>>>>ceshifail>>>>>>>>>>>>>>>>>>>>>";
               }
					
			Fallback相当于是降级操作. 对于查询操作, 我们可以实现一个fallback方法, 当请求后端服务出现异常的时候, 可以使用fallback方法返回的值. 
			fallback方法的返回值一般是设置的默认值或者来自缓存.
			
			Hystrix支持两种方式定义HystrixCommand
            一种是将类继承自HystrixCommand类，并重写run方法；
               属性配置：通过setter对象来对请求命令的属性进行设置。暂时用的少
               
            另一种是在方法头上写注解@HystrixCommand的方式，使用注解的方式代码会比较清晰，将Hystrix代码和业务代码隔离开。
               属性配置：通过使用@HystrixCommand中的commandProperties来设置。搭配@HystrixProperty

			HystrixCommand       用在依赖的服务返回单个操作结果的时候
				可以用ignoreException参数，排除降级异常，除了hystrixbadrequestexception异常外。
				-- Hystrix的参数配置
				@HystrixCommand(fallbackMethod = "error", 
								commandProperties = { @HystrixProperty(name = "execution.isolation.thread.timeoutInMilliseconds", value = "4000") })
				fallback.enable	用来设置服务的降级策略是否可用，设置为false，那么请求失败或拒绝发生，将不会调用HystrixCommand.getFallback()来服务降级。
			
			HystrixObservableCommand		用在依赖的服务返回多个操作结果的时候
			
		Hystrix仪表盘暂不看
		
		ribbon的超时和hystrix超时是连个概念。后者需要大于前者，否则hystrix命令超时后，该命令直接熔断，重试机制就没有意义。
		
		feign.hystrix.enabled 设置hystix的开关，主要涉及异常时，服务的降级。
		
		指令命令配置
			hystrix.command.<commandKey>最为配置前缀，commandKey默认采用feign客户端中的方法名作为表示，重名的方法公用该配置。
			
		feign中的hystrix配置，服务降级，服务容错
			1.服务降级类HelloServiceImpl 是feign客户端定义接口的降级实现类，其中每个重写方法的实现逻辑都可以用来定义相应的服务降级逻辑。
			
			2.在接口中@FeignClient(name="helloservice" fallback="HelloServiceImpl.class")      #指定对应服务降级实现类
				@RequestMapping("/hello")
				String hello()

6. cloud zuul 网关

      * 优点：	
         1.统一入口，屏蔽内部服务细节
         2.整合注册中心，自动化服务实例维护和负载均衡的路由转发
         3.校验前置，将权限校验和业务逻辑解耦。

         zuul默认会将通过以服务名作为contextpath的方式来创建路由映射。
         好处，提供一套过滤机制，经过校验，将请求路由到具体的微服务接口，这样服务应用可以更专注业务开放。
      
      * 整合步骤
         1.boot中引入spring-cloud-starter-netflix-zuul
         2.启动类加上@EnableZuulProxy
         3.配置路由规则，
            1.传统的路由规则
               zuul.routes.api-a-url.path=/api-aa-url/**
               zuul.routes.api-a-url.url=http://localhost:8700/
               
               其中api-a-url为路由名，一组path和url映射的路由名要一致
               path映射具体的url
               缺点在于要维护url和path的关系映射。
               
            2.面向服务路由(推荐)，zuul整合eureka
               boot中引入spring-cloud-starter-netflix-eureka-client,注册到注册中心
               本身这个jar，附带了，hystrix，ribbon等jar
                  
               zuul.routes.api-a-url.path=/api-aa-url/**
               zuul.routes.api-a-url.service-id=producer
               eureka.client.serviceUrl.defaultZone=http://localhost:8800/eureka/
         
               path映射具体的服务，url交给eureka的服务发现机制去自动维护，交给ribbon去负载均衡。
            
               如果没有整合eureka服务治理框架,就需要手工维护转发的地址，但是一般不这样，了解下即可
                  ribbon.eureka.enabled=false
                  producer.ribbon.listOfService=url1,url2

               ***更为简洁的配置：zuul.routes.<serviceId>=<path>***  

               原理：
                  这网关api也是eureka服务治理下的一个普通的微服务应用，除了将自己注册到注册中心，也订阅了其他的serviceid服务清单，
                  这样当请求来时，能根据path对应的serviceId,找到服务清单，在根据ribbon去负载均衡调用服务实例。

      * 核心功能：请求过滤	ZuulFilter
            前置网关服务做非业务性的校验，不要等到转发后再过滤，过滤要提前到网关层
            设置前置过滤器，继承抽象类ZuulFilter，并实现4个抽象函数
               @Service
               public class ZuulServiceImpl extends ZuulFilter {
                  @Override
                  public String filterType() {return "pre";}
                  @Override
                  public int filterOrder() {return 0;}
                  @Override
                  public boolean shouldFilter() {return true;}
                  @Override
                  public Object run() throws ZuulException {
                     RequestContext currentContext = RequestContext.getCurrentContext();
                     HttpServletRequest request = currentContext.getRequest();
                     String name = request.getParameter("name");
                     if(name==null || !name.equals("ckck")){
                        currentContext.setSendZuulResponse(false);				#不进行路由
                        currentContext.setResponseStatusCode(404);				#返回具体的错误码
                        //currentContext.setResponseBody("zenmeshuoa laotie");	#对返回的body进行编辑
                        return  null;
                     }
                     return null;
                  }
               }

            重要参数说明
               filterType：	过滤器类型，决定过滤器在哪个生命周期执行，这里pre(主要类型)，代表会在请求被路由之前执行
                  其他的routing,将请求转发到具体服务实例，请求结果返回后，阶段完成
                  post,可以对请求结果进行加工转换
                  error，上述3个阶段发生异常时触发，该阶段最终还是流转到post阶段执行，将最终结果返回客户端。
               filterOrder：	执行顺序，场景：多个过滤器场景	，数值越小，优先级越高
               shouldFilter：	是否执行该过滤器，这里指定true，后面可以用函数执行范围
                  还可以用zuul.<>simpleclasssname>.<filterType>.disable=true 来禁用某个过滤器
                  
               run:			过滤器的具体逻辑，
                  拦截路由转发，或者在请求路由返回结果之后，对处理结果做一些加工等
         
            可以上面的直接@service或者@component加入容器，也可以下面的单独配置
            @Bean
            public ZuulFilter zuulFilter(){return  new ZuulServiceImpl(); }
            
            这的path路径匹配是ant风格，
               ？，支持单个字符
               *，支出多个字符
               **，支持多个字符，支持多级目录
               
            另外，多路径优先级和忽略路径暂不细看
            #路由忽略 	zuul.ignored-patterns=/**/hello/**
            #路由前缀 	zuul.prefix		
            foward实现本地跳转
               zuul.routes.api-a-url.path=/api-aa-url/**
               zuul.routes.api-a-url.url=forward:/local
               这样/api-aa-url/hello 就转到了本地网关的/local/hello上处理了

            zuul.routes.<route>.retryable=false   设置重试开关		路由转发时间超时可以发起重试

7. cloud sleuth 链路追踪
		
      * 基本介绍
            从调用日志中看到，[applicationName,traceId,spanId,false]
               applicationName，	表示应用名称，在属性文件中配置
               traceId，			标识一条请求链路，一条请求链路中包含一个traceId 和多个spanId
               spanId，			表示一个基本的工作单元，可以用来记录span的时间延迟
               false，				表示是否要将该信息输出到zipkin等服务中来收集和展示
                  
            实际案例：[producer,dea7756506cabdd3,3604e4eeabe25f51,true]
            其中traceId和spanId是sleuth链路追踪的核心，一次服务请求链路的调用过程中，保持并传递同一个traceId,串联起整个请求链路
            
            在requestHeader的参数中
               header参数：x-b3-traceid=fb287ae05e268fa4				同traceId，全链路唯一id
               header参数：x-b3-spanid=2793597d4883a690				同spanId，一个基本的工作单元
               header参数：x-b3-parentspanid=fb287ae05e268fa4			标识当前工作元所属的上一个工作单元，rootspan(请求链路的第一个工作单元)的该值为空
               header参数：x-b3-sampled=1								是否被抽样输出的标志 1表示需要被输出，反之为0
            
            1.创建两个客户端都要加上@EnableDiscoveryClient，实现a-b调用，其中a可以既是提供者有时消费者。
            2.两个都引入spring-cloud-starter-sleuth
               
            可以整合elk，作为日志信息的记录和查询，但是这种日志分析，相对有点乏力，不直观
            elk中数据分析缺少对调用链中出现的延迟过高的瓶颈源，或对分布式系统做延迟监控等与实践消耗相关的需求，elk有点乏力。
		
	   * 引入zipkin
            官网：https://zipkin.io/            
            zipkin可以集成dubbo，也可以集成cloud 	  https://www.cnblogs.com/jmcui/p/10940372.html		

            官方文档的执行
               curl -sSL https://zipkin.io/quickstart.sh | bash -s
               java -jar zipkin.jar									#启动zipkin工程，可视化管理页面就可以访问了。

            实际下载路径
               curl -fL -o 'zipkin.jar' 'https://repo1.maven.org/maven2/io/zipkin/zipkin-server/2.23.2/zipkin-server-2.23.2-exec.jar'
               
            实际访问地址，http://localhost:9411	
               
            Spring Boot 2.0之后，使用EnableZipkinServer创建自定义的zipkin服务器已经被废弃，将无法启动。
            也就是说原来通过@EnableZipkinServer或@EnableZipkinStreamServer的路子，启动SpringBootApplication自建Zipkin Server是不行了。需直接使用编译好的jar 包
         
            收集服务器上请求链路的跟踪数据，通过提供的rest api接口监控系统，及时发现系统中出现的延迟高问题，并找出系统性能瓶颈的根源。
            默认情况下，zipkin的信息是存在内存中的，但是是可以存到外部存储中的。可以扩展转为外部mysql存储
            主要是引入zipkin-storage-mysql相关依赖，然后配置。
            
            其中被监控的链路系统需要引入spring-cloud-sleuth-zipkin的maven配置
            zipkin后台页面查询的json节点信息
               {
                  "traceId": "3dcfd9dbf0a28ba8",
                  "parentId": "3dcfd9dbf0a28ba8",
                  "id": "4b1f49b158024db7",
                  "kind": "SERVER",
                  "name": "get /give",
                  "timestamp": 1626079982046935,
                  "duration": 2634190,
                  "localEndpoint": {
                     "serviceName": "producerb"
                  },
                  "remoteEndpoint": {
                     "ipv4": "192.168.6.222",
                     "port": 64418
                  },
                  "tags": {
                     "http.method": "GET",
                     "http.path": "/give",
                     "mvc.controller.class": "MessageController",
                     "mvc.controller.method": "give"
                  },
                  "shared": true
               }
				 

8. 其他组件
      cloud config 配置中心	
            详细配置暂时不看。跳过。连接github那边有点问题，暂不看。
            
      cloud bus   消息总线
         就是在我们需要把一个操作散发到所有后端相关服务器的时候，就可以选择使用cloud bus了
         https://blog.csdn.net/BuquTianya/article/details/78698755
         暂不细看
         
      cloud stream	消息驱动微服务
         主要用来坐后续的mq切换，降低mq迁移的成本。
         Spring Cloud Stream 是一个用来为微服务应用构建消息驱动能力的框架。
         有效简化开发人员对消息中间件的使用复杂度，目前 Spring Cloud Stream 只支持 RabbitMQ 和 Kafka 的自动化配置。
         Stream屏蔽了底层MQ的区别，可以很好的实现切换。目前主流的有ActiveMQ、RocketMQ、RabbitMQ、Kafka，Stream支持的有RabbitMQ和Kafka。
            
         核心在于内部的绑定器binder，作为中间层，隔离了应用程序和消息中间件的具体细节实现，
         通过向应用程序暴露统一的channel通道，使得升级更换其他消息中间件时，只要更换对应的绑定器，而不需要修改任何boot的引用逻辑。
      
9. 发现服务以及消费服务

      发现服务由eureka的客户端完成，服务的消费任务由ribbon完成。
      
      ribbon是一个基于http和tcp的客户端负载均衡器，
      它可以在通过客户端中配置ribbonserverlist服务端列表去轮询访问以达到负载均衡的作用。

      简单总结功能：在eureka服务发现的基础上，实现了一套对服务实例的选择策略，从而实现对服务的消费。
      
         ribbon实现负载均衡
            是一个工具类框架，不像注册中心，配置中心，api网关那样需要独立部署。

            维护一个下挂的可用服务端清单，通过心跳检测来剔除故障的服务端节点以保证清单中都是可以正常访问的节点，
            当收到请求时，通过负载均衡算法从清单中取出一台服务端的地址，进行转发。
            
            客户端负载均衡和服务端负载均很最大不同在于服务清单的存储位置。
            客户端负载均衡中，所有客户端节点都维护自己要访问的服务端清单(来自于服务注册中心，也需要心跳维护服务端清单)
            
            restTemplate使用
               get请求中
                  getForEntity()	返回对象是responseEntity，包含了状态码，头信息等。
                  getForObject()	当不需要关注请求响应除body外的其他内容时，比较好用
                  
               post请求中
                  postForEntity()
                  postForObject()
                  postForLocation()	会返回新资源的uri
                  
               restTemplate可以配置一定的重试机制
            
               被@LoadBalanced注解修饰的RestTemplate对向外发起http请求时，会被LoadBalancerInterceptor类的intercept函数所拦截。
               由于我们在使用RestTemplate时才用了服务名作为host，所以直接从httpRequest的uri对象中通过gethost()就可以拿到服务名，然后调用execute函数去根据服务名来选择实例并发起实际的请求
               实现以服务名为host的uri请求到host:post形式的实际访问地址。说白就是从拦截器中获取服务名对应的实例，拿到具体的地址，发起调用。
               通过重写geturi的方式，来请求服务的实际地址

         当spring cloud的应用中同时引入ribbon和eureka依赖时，会触发eureka中实现的对ribbon的自动化配置。 
         
         spring cloud eureka实现的服务治理机制强调了cap原理中的ap，即可用性和可靠性。
         和zk强调的cp(一致、可靠)不太一样。
         
         ribbon两种配置方式
            1.全局配置方式，			ribbon.<key>=<value>
            2.指定客户端的配置方式 		<client>.ribbon.<key>=<value>
				
5. cloud_demo

   demo样例  https://blog.csdn.net/chengyuqiang/article/details/90645498

   1. 注册中心demo，启动cloud eurekademo   
			1.父pom中 
				<dependencyManagement>
				<dependencies>
					<dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-dependencies</artifactId>
						<version>${spring-cloud.version}</version>
						<type>pom</type>
						<scope>import</scope>
					</dependency>
				</dependencies>
			</dependencyManagement>
			2.子工程中
			 <dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
				</dependency>
				
			3.启动类加上@EnableEurekaServer 启动注册中心功能
			
			问题版本不兼容导致的 javax.servlet找不到
			查询兼容版本
			cloud 的 Hoxton.SR5   - boot 2.3.x 可以了 
			
			
	2. 启动provider
			1.boot中引入spring-cloud-starter-netflix-eureka-client	可以看做是服务注册中心的客户端
			2.引入spring-boot-starter-web，服务生产者提供的具体服务，目前看是rest方式
			3.启动类加上@EnableEurekaClient，表明自己是一个eurekaclient，
			
         属性文件中
            #服务名称
            spring.application.name=producer
            
            #启动服务发现的功能，开启了才能调用其它服务
            spring.cloud.config.discovery.enabled=true

            #发现的服务的名字--对应注测中心的服务名字
            spring.cloud.config.discovery.serviceId=register-server
			
	3. 启动consumer
			Spring cloud两种服务调用方式，
			一种是ribbon+restTemplate，本次demo采用，pom文件中增加了spring-cloud-starter-netflix-ribbon依赖
			另一种是feign。

			@EnableDiscoveryClient和@EnableEurekaClient
				相同点：都是能够让注册中心能够发现，扫描到改服务。
				不同点：@EnableEurekaClient只适用于Eureka作为注册中心，
						@EnableDiscoveryClient 可以是其他注册中心。
						
				从Spring Cloud Edgware开始，@EnableDiscoveryClient 或@EnableEurekaClient 可省略。只需加上相关依赖，并进行相应配置，即可将微服务注册到服务发现组件上。
			
			1.引入spring-cloud-starter-netflix-eureka-client
				引入spring-cloud-starter-netflix-ribbon
				
			2.启动类加上注解@EnableDiscoveryClient，
				注册bean  
				
				@LoadBalanced //使用负载均衡机制，实现面向服务的接口调用
				@Bean
				public RestTemplate restTemplate(){
					return new RestTemplate();
				}
				调用通过
					restTemplate.getForObject("http://producer/get?name="+name, String.class);			#注意这里producer是服务名，不是一个具体的地址。和dubbo的不太一样。
					
				负载均衡
					当consumer通过restTemplate调用producer的get接口时，因为用ribbon进行了负载均衡，会轮流的调用producer：8700和8701 两个端口的get接口。
			
	* 使用feign搭建工程
			
         feignClient已经默认使用了ribbon，兼容了负载均衡的配置。
         
         spring cloud feign的客户端负载均衡是通过ribbon实现的额。可以直接使用ribbon.<key>=<value>方式来设置ribbon的参数。
         
         @FeignClient用于通知Feign组件对该接口进行代理(不需要编写接口实现)，使用者可直接通过@Autowired注入。 
         Spring Cloud应用在启动时，Feign会扫描标有@FeignClient注解的接口，生成代理，并注册到Spring容器中。生成代理时Feign会为每个接口方法创建一个RequetTemplate对象，该对象封装了HTTP请求需要的全部信息，请求参数名、请求方法等信息都是在这个过程中确定的，Feign的模板化就体现在这里。

         参数和method请求
            feign作为消费方去消费服务
               1.get方式消费服务提供者：
                  在所有参数前加上@RequestParam注解。method可加，可不加，为了简单，个人觉得直接加上。
               2.post方式消费服务提供者
                  在所有参数前加上@RequestParam注解。method加上Post.
                  使用@RequestBody注解（有且至多一个，多了报错），其他的参数前，必须使用@RequstParam。
               
               注:1.在服务消费者中，使用feign消费服务时，如果参数前什么也不写，那么默认是由@RequestBody指明的。这时用的是post
                  2.@RequestBody注解的参数，需要POST方式才能传递数据。
                  3.采用POST进行feign调用，接口参数前，最多只能有一个参数是@RequestBody指明的，其余的参数必须使用@RequestParam指明。
                  4.@RequstParam("name"),这个变量名说是可加，可不加（接受参数名字和传递过来变量名字一致时候），
                     mvc中校验不严格，但是在cloud中必须要加上否则会报错：RequestParam.value() was empty on parameter 0

                  5.@RequestBody,一般用在feign的消费者和提供者之间传递复杂参数（对象和map）。底层原理：处理HttpEntity传递过来的数据，一般用来处理非Content-Type: application/x-www-form-urlencoded编码格式的数据
                  6.	@RequestBody注解用来接收字符串类型参数，有以下注意点
                     @RequestBody 注解在服务端和客户端都需要使用
                     参数名和参数类型在服务端和客户端需要保持一致
      
         1.boot中引入spring-cloud-starter-netflix-eureka-server 和spring-cloud-starter-openfeign
         2.启动类加上@EnableFeignClients 和 @EnableDiscoveryClient
         3.配置调用关联
            @FeignClient("producer")										#这里是关联的服务名
            public interface IHelloService {
               @RequestMapping("get")										#这里是对应服务下的rest请求地址
               String testgo(@RequestParam("name") String name);			#这里需要加上注解限定传参
            }
         
         4.在controller中注入IHelloService，直接调用服务。
         
         5.这里对服务的调用的api实际上可以单独拉一个jar工程引用，实现rest接口定义的复用。
      
         feign的继承，这个比较简洁点，就是变动的维护性 和dubbo的接口差不多。
         使用feign的继承属性，可以进一步将公用的接口抽象到一个jar中，然后提供者和消费者分别依赖jar，实现代码复用
         
         抽离出来的接口中
            @RequestMapping("/refactor")
            public interface HelloService {
               @RequestMapping(value = "/hello4", method = RequestMethod.GET)
         
         提供者实现接口，
            @RestController
            public class RefactorHelloController implements HelloService {
               @Override
               public String hello(@RequestParam("name") String name) {
                  return "Hello " + name;
               }
               
            实现类中不再包含以往会定义的请求映射注解@RequestMapping，在重写的时候会自动将注解带过来，
            增加@RestController注解来标明是一个REST接口类

         消费者创建子接口，添加@FeignClient注解来绑定服务，实现注入，调用服务
            @FeignClient(value = "HELLO-SERVICE")
               public interface RefactorHelloService extends com.didispace.service.HelloService {
            }
         


## Docker相关

   相关文档网址
		docker官网：http://www.docker.com
		docker中文网站：https://www.docker-cn.com/
		Docker Hub 仓库官网: https://hub.docker.com/
		http://www.dockerinfo.net/document  文档介绍


	总结docker容器就是：一个镜像格式，一系列标准的操作，一个执行环境
	
	查看docker toolbox的ip地址（即decker服务器的localhost或者ip地址）：$ docker-machine ip default
   docker toolbox创建了一个本地虚拟机，拥有自己的网络接口和ip地址
	docker程序是docker守护进程的客户端程序
	
	开始：
	从下往上，由基础镜像往上，最终生成一个可写容器，创建dockerfile时，每条run指令都会创建一个镜像层，指令成功，镜像层提交
	
	创建一个可交互的容器， 这样就创建了新容器，直接root进入了容器操作，/bin/bash就是交互的shell，交互式容器
		sudo docker run -i -t ubuntu /bin/bash  	也可以使用--name来给容器命名，跟在run后面 创建交互式的容器
		exit 										   退出容器(容器停止运行)，回到宿主机命令提示符，容器仍然存在，只有在指定的/bin/bash命令运行时，容器才运行。
		docker ps -a 								查看当前系统中的容器列表，不带-a查看运行中的
		sudo docker start test_linux			重新启动容器
		sudo docker attach test_linux			重新附着到容器的会话上
		sudo docker run --name daemon_dave ubuntu /bin/sh -c "while ture; do echo hello world; sleep 1; done" 创建守护式容器，-d 创建守护容器
		sudo docker logs -f daemon_dave 		查看容器日志
		sudo docker top test_linux				查看容器中的进程
		docker exec 								容器内启动新进程
		docker stop test_linux					停止容器
		docker rm test_linux						删除容器
		docker images								查看镜像
		docker pull fedora:20 					从远程拉取镜像
		docker rmi									删除镜像
		
		docker build -t="jamtur01/static_web"  基于dockerfile(vim编写)创建一个镜像，其中设置了仓库和名称
		下载war挂在到数据卷中，然后启动带有tomcat的容器，最后会将数据卷中的war挂载到tomcat对应的容器执行
		启用微服务，在dockerfile中的 配置容器启动后执行的命令，ENTRYPOINT ["java","-jar","/app.jar"],打成镜像，配置端口，直接run即可

	 	
1. 基础概念
		镜像是容器生命周期中的构建或者打包阶段，容器是启动或执行阶段
		
		docker推荐单个容器只运行一个应用程序或者进程，形成一个分布式应用程序模型，也是一个内部互联的容器。
		
		Docker仓库管理镜像与Git非常相似。
		我们要配置的应用运行环境就是镜像，本地没有镜像时候，需要从远程仓库pull，启动这个镜像时，就会有一份镜像被复制并运行在容器中。
		如果我们改动了容器里面的内容，也可以将其 commit 保存成新的镜像。

		docker在linux中的主目录是，/var/lib/docker/，主要配置都在这里面
			/var/lib/docker/image/overlay2/repositories.json  这里是镜像的Repositories，存放着pull镜像的一些元信息

      docker的核心组件：
         1、docker的客户端和服务端，	 连接方式：命令行工具或api调用
         2、docker镜像					   相当于容器的“源代码”，生命周期中的构建或者打包阶段
         3、registry						   相当于远程/私有库 										官方的使用的是docker hub
         4、docker容器					   启动或者执行阶段
	
	* Docker 有三大核心概念：仓库、镜像、容器
		* 1.仓库(repository)
			默认是Docker Hub，公共的镜像仓库
				  
			docker仓库有共用的docker hub或者私有的仓库
			docker hub中有两种类型的仓库，
				用户仓库	由dokcer用户创建，由用户名/仓库名组成,比如，jam/tomcat
				顶层仓库	由docker内部人管理，就是官方的，只包含仓库名 /tomcat
			
			私有仓库一般两种
				1.dockerhub上的私人仓库
				2.防火墙后面的运行的自己的register、
			
				私有仓库创建步骤
					1.docker  pull  registry 
					2.创建一个主机目录，用来挂载目录
						cd /usr/local/
						mkdir docker_registry 
						
				运行仓库容器
				3.docker run -d -p 5000:5000 --name=localRegister --restart=always --privileged=true  -v /home/zhanjun/test/dockertest/docker_registry:/var/lib/registry  docker.io/registry
				 1.commit到本地		docker commit -a "xiaoyuer" -m "open镜像"  075d5f9bd402  sit_open_tomcat:v1
				 2.打tag			docker tag  sit_open_tomcat:v1 192.168.6.138:5000/sit_open_tomcat:v1
				 3.推送远程			docker push  192.168.6.138:5000/sit_open_tomcat:v1 // 推送到远程docker仓库
					push过程中出现https问题
					1. vim  /etc/docker/daemon.json    增加一个daemon.json文件
					{ "insecure-registries":["192.168.1.100:5000"] }  wq
				
					2.重启docker服务
					systemctl daemon-reload
					systemctl restart docker
				仓库路径查看 目录查看路径 http://192.168.175.132:5000/v2/_catalog            ip:port/v2/_catalog
		
		* 2.像(image)
			只读不可修改
			镜像的两个特征
				镜像是分层（Layer）的：即一个镜像可以多个中间层组成，多个镜像可以共享同一中间层，我们也可以通过在镜像添加多一层来生成一个新的镜像(类似dockerfile中的起始from)。
				镜像是只读的（read-only）：镜像在构建完成之后，便不可以再修改，而上面我们所说的添加一层构建新的镜像，这中间实际是通过创建一个临时的容器，在容器上增加或删除文件，从而形成新的镜像，因为容器是可以动态改变的。
	
			虚悬镜像
				Docker镜像名由仓库名和标签组成，但有时候我们会看到仓库名和标签皆为<none>的镜像，我们称为这种镜像为虚悬镜像
				虚悬镜像一般是当我们使用docker pull拉取最新镜像时，生成的新的镜像，所以仓库名和标签给了新的镜像，旧的镜像仓库和标签则被取消，成为虚悬镜像。

				
			docker将镜像保存到宿主集中，docker在文件系统内部创建了一个容器，该容器拥有自己的网络、ip地址，以及一个用来和宿主机进行通信的桥接网络接口。
	
			镜像分层框架+顶层读写系统
				docker构建了一个镜像栈，在最顶层添加了一个读写层，该读写层+下面的镜像层+一些配置数据=一个容器
				每个镜像层都是只读的
				顶层的读写文件系统，docker的读写程序就是在这个读写层中进行的
				容器的这种镜像分层框架，方便快速构建和运行
			
			docker镜像一般最好指定标签tag，标明是哪个版本的镜像，image:tag，默认拉取最新的latest版本
				
				
			一般构建是基于一个基础镜像构建新镜像，从0开始的构建，一般不用
			构建镜像的两种方式	
					1.使用docker commit构建镜像
						docker commit -a "作者" -m "说明"  容器id  镜像名称:版本号
						docker commit提交的只是创建容器的镜像与容器当前状态之间有差异的部分，这样很轻量
						
						步骤1.进入容器修改内容 
						步骤2.exit退出后直接commit对应的容器，这里提交的是容器，将修改提交为一个新镜像
						
						关于commit命令，其实就是使用docker run -i -t XXX /bin/bash 进入容器的交互界面，进行各种操作后，
						再将这个容器通过提交命令提交上去来达到目的。
						
						$docker ps -l命令获得安装完命令之后容器的id如698***
						$docker commit 698 pika/py_ubuntu ? ?#把这个容器保存为镜像py_ubuntu
				
						****制作镜像步骤*****
							 0.进入到容器中，更改后exit
								docker exec -it mytomcat /bin/bash    // 登录容器内部
							 1.commit到本地
								docker commit -a "xiaoyuer" -m "open镜像"  075d5f9bd402  sit_open_tomcat:v1
							 2.打tag
								docker tag  sit_open_tomcat:v1 192.168.6.138:5000/sit_open_tomcat:v1
							 3.推送远程
								docker push  192.168.6.138:5000/sit_open_tomcat:v1 // 推送到远程docker仓库
				
						删除镜像中的无用的目录
							挂载情况，挂载目录无法直接删除。
								作镜像的时候做好吧webapps下的删除赶紧，不然webapp中会有残留的，主机中也不好在webapp中删除全部，有其他的项目包在。挂载目录要精确点
								取消挂载再看，当前是两次commit覆盖版本
						
							如过目录没有挂载，直接删除即可。
				
					2.使用Dockerfile构建镜像(实际推荐)
						相比commit制作的优点
							核心概念是可重复和可移植
							根据指令生成定制的image(制作步骤清晰，易于维护)。commit制作image是黑盒子
							当需要定制额外的需求时，只需在Dockerfile上添加或者修改指令，重新生成image即可，省去了敲命令的麻烦。
							Dockerfile包含创建镜像所需要的全部指令。基于在Dockerfile中的指令，我们可以使用Docker build命令来创建镜像。通过减少镜像和容器的创建过程来简化部署。

					docker build  -t ImageName:TagName dir    通过dockerfile构建镜像
						-f :指定要使用的Dockerfile路径；    #目前没用到
						--tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。
					
						实际上 Dockerfile 的文件名并不要求必须为Dockerfile ，而且并不要求必须位于上下文目录（Context）中，
						同在dockertest目录下，指定某个文件作为 Dockerfile
						docker build -t gogo:v1 -f testFile 		/home/zhanjun/test/dockertest/   这样也能生成
						docker build -t gogo2:v1 -f ../test2file 	/home/zhanjun/test/
						
					docker build -t centostest:v1 /home/zhanjun/test/dockertest
						dockertest目录下有Dockerfilewen文件，
							FROM centos
							MAINTAINER zhanjun
							RUN mkdir -p /usr/local/docker/
							RUN echo "ceshiwenben" > /usr/local/docker/test.text	
							
					dockerfile编写
						Dockerfile的指令是不区分大小写的(但命名约定为全部大写)，使用#作为注释，每一行只支持一条指令，每条指令可以携带多个参数。
						Dockerfile的指令根据作用可以分为两种，构建指令和设置指令。(命令的顺序就是被执行的顺序)
						构建指令用于构建image，其指定的操作不会在运行image的容器上执行；
						设置指令用于设置image的属性，其指定的操作将在运行image的容器中执行。
									
						基于DSL语法构建dockerfile
							1.docker从基础镜像运行一个容器
							2.执行一条指令，对容器做出修改
							3.执行类似docker commit的操作，提交一个新的镜像层
							4.docker再基于刚提交的镜像运行一个新容器
							5.指定dockerfile中下条指令，知道所有指令执行完毕
							
						常用的dockerfile指令
							所有Dockerfile都必须以FROM命令开始，
							FROM <image>:<tag>				//用来指定基础镜像,必须指定在其他指令的前面
							MAINTAINER <author name>		//指定维护者的姓名和联系方式

							RUN 	<command> 或 RUN ["executable", "param1", "param2"]。	
									前者将在shell终端中运行命令，即 /bin/sh -c ；常用
									后者则使用 exec 执行。(exec()系列函数使用新的进程映像替换当前进程映像)
									指定使用其它终端可以通过第二种方式实现，例如 RUN ["/bin/bash", "-c", "echo hello"]。			
								
									RUN指令会在新创建的镜像上添加新的层面，接下来提交的结果用在Dockerfile的下一条指令中。
									每条RUN指令将在当前镜像基础上执行指定命令，每条run指令都会创建一个新的镜像层，如果该指令执行成功，并将此镜像层提交，之后继续执行下一条指令。当命令较长时可以使用\来换行。
									也就是说RUN命令会在上面FROM指定的镜像里执行任何命令，然后提交(commit)结果，提交的镜像会在后面继续用到。
									run指令等价于
									docker run image command
									docker commit container_id
								
							ADD 	<src> <dest>		复制文件指令，附带解压功能   将构建目录下的文件复制到容器目录下，  
										dest 结尾 /就是目录，否则是文件
										可以将本地配置文件直接添加到容器中
							COPY 	纯复制功能
							CMD 	提供了容器默认的执行命令。默认是关键 
										Dockerfile只允许使用一次CMD指令。多个只有最后一个指令生效。
								
								
									RUN是构建镜像时运行的命令，cmd是容器启动时运行的命令，有点类似docker run 指定的命令。
									也就是容器启动以后，默认的执行的命令。
									
									支持三种格式
									CMD ["executable","param1","param2"] 使用 exec 执行，推荐方式；
									CMD command param1 param2 在 /bin/sh 中执行，提供给需要交互的应用；
									CMD ["param1","param2"] 提供给 ENTRYPOINT 的默认参数；
									
									docker run 指令会覆盖cmd的命令
										如果我们在run时指定了命令或者有entrypoint，那么cmd就会被覆盖。仍然是上面的image。
										如果docker run没有指定任何的执行命令或者dockerfile里面也没有entrypoint，那么，就会使用cmd指定的默认的执行命令执行。
										
										因为cmd的角色定位就是默认，如果你不额外指定，那么就执行cmd的命令，反之指定了，那么就不会执行cmd，也就是cmd会被覆盖。
							ENTRYPOINT	配置给容器一个可执行的命令，Docker只允许一个ENTRYPOINT
									ENTRYPOINT ["executable", "param1", "param2"] 
									ENTRYPOINT command param1 param2 					#这种方式，没法接受参数
								
									entrypoint才是真正的容器启动以后要执行命令。
									用于设定容器启动时第一个运行的命令及其参数，是容器的“入口”	
									
							EXPOSE <port> [<port>...]   指定容器在运行时监听的端口。
									如果镜像的使用者关心容器公有映射了哪个公有端口，他们可以在运行镜像时通过-p参数设置，否则，Docker会自动为容器分配端口。
									切勿在Dockerfile映射公有端口。
									端口映射是docker比较重要的一个功能，原因在于我们每次运行容器的时候容器的IP地址不能指定而是在桥接网卡的地址范围内随机生成的。
									宿主机器的IP地址是固定的，我们可以将容器的端口的映射到宿主机器上的一个端口，免去每次访问容器中的某个服务时都要查看容器的IP的地址。
									对于一个运行的容器，可以使用docker port 查看该端口号在宿主机器上的映射端口。	
												
									在Dockerfile中你有能力映射私有和公有端口，但是你永远不要通过Dockerfile映射公有端口。
									通过映射公有端口到主机上，你将只能运行一个容器化应用程序实例。
												
							VOLUME	授权访问从容器内到主机上的目录。用于containers之间共享数据，容器数据持久化
									指定挂载点，使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以共享给其他容器使用。
									容器使用的是AUFS，这种文件系统不能持久化数据，当容器关闭后，所有的更改都会丢失。
									
							WORKDIR	指定 RUN、 CMD与 ENTRYPOINT命令的工作目录。			 	WORKDIR /path/to/workdir
							ENV		设置环境变量。它们使用键值对，增加运行程序的灵活性。	ENV <key> <value>
							USER	指定什么用户运行镜像  docker run -u可以覆盖
									
									
						*****在使用DockerFile制作镜像时，需要修改配置文件文本时候，通过 sed命令可实现对镜像中文件的修改。*****
							ADD 也可以使用ADD  将一个编辑好的文件复制过去覆盖，这样更快点
							COPY 也可以类似功能			
						
						ENTRYPOINT 和 CMD 的区别	https://blog.csdn.net/u010900754/article/details/78526443
							一般还是会用entrypoint的中括号形式作为docker 容器启动以后的默认执行命令，里面放的是不变的部分，
							可变部分比如命令参数可以使用cmd的形式提供默认版本，也就是run里面没有任何参数时使用的默认参数。
							如果我们想用默认参数，就直接run，否则想用其他参数，就run里面加参数。

						构建错误的场景：
							由于构建过程中会产生许多中间镜像，所以，我们可以基于最后一个成功的镜像运行一个容器，然后在该容器中调试下一步的构建指令，找出到底哪里出错了。
							一旦解决了问题，就可以退出容器，然后修改Dockerfile，尝试重新构建。
							docker run -it 7fcc8dc893fc /bin/bash    找到最新一条成功的镜像调试，修改dockerfile后重新构建
								Step 3/4 : RUN mkdir -p /usr/local/docker/
								 ---> Using cache
								 ---> 7fcc8dc893fc
								Step 4/4 : RUN ec123ho "ceshiwenben" > /usr/local/docker/test.text
								 ---> Running in 505a2be18ebc
								/bin/sh: ec123ho: command not found
								The command '/bin/sh -c ec123ho "ceshiwenben" > /usr/local/docker/test.text' returned a non-zero code: 127		
				
	   * 3. 容器(container)
			可以动态修改
			类似一个轻量级的沙箱,容器是从镜像创建应用运行实例, 可以将其启动、开始、停止、删除、而这些容器都是相互隔离

			容器的三种运行模式
				1.运行后退出							$ docker run centos echo "hellowrold"
				2.常驻内存，就是守护进程的模式			$ docker run -d -p 80:80 nginx
				3.交互式								$ docker run -it centos /bin/bash
			
		
2. 安装docker
			++++++++++CentOS 6 安装docker，官网已经废弃了6版本的更新++++++++++
				安装docker
				安装docker可以有两种方式：一种是使用curl获得docker的安装脚本进行安装，还有一种是使用yum包管理器来安装docker。
					1.安装epel源，	yum install epel-release
						#Error: Cannot retrieve repository metadata (repomd.xml) for repository: epel. Please verify its path and try againxml
						#修改epel.repo文件，修改mirrorlist=https://*****这行，将https修改成http便可正常使用
					2.yum -y install docker-io
					错误 No package docker-io available
					
					*****直接用下载源安装 	yum install https://get.docker.com/rpm/1.7.1/centos-6/RPMS/x86_64/docker-engine-1.7.1-1.el6.x86_64.rpm  成功
					service docker start		docker 启动				redhat6或者7 命令不太一样	自启动另行配置
					docker run hello-world		入门的demo展示
					docker info					查看信息
					
					linux内核升级，步骤网上有，更新错误就yum update/yum update nss   尼玛真费劲，官方已经不维护了，废弃的centos6版本，各种安装包找不到
						官网已经停止centos的升级，换新地址rpm -Uvh http://ftp.iij.ad.jp/pub/linux/centos-vault/centos/6.9/centosplus/x86_64/Packages/kernel-2.6.32-754.el6.centos.plus.x86_64.rpm
						RHEL/CentOS 第三方软件源  http://rpm.pbone.net/index.php3/stat/4/idpl/15285241/dir/redhat_el_6/com/epel-release-6-5.noarch.rpm.html
								elrepo-release-6-8.el6.elrepo.noarch.rpm    注意是elrepo 不是epel
						 cat  /etc/redhat-release	查看linux版本  	uname -a
			++++++++++CentOS 6 安装docker++++++++++	
			
			
			++++++++++CentOS 7 安装docker++++++++++	
			
				安装docker
				准备环境
					Docker 要求 CentOS 系统的内核版本高于 3.10 
					确保所有的yum更新  yum update     包括像nss等，全部更新完事。
					遇到yum中的host不能识别问题，直接将网络配置移除后重新添加使用NAT模式。
					看网络通不通使用curl www.baidu.com 看下即可
				安装开始	
					1.yum install docker
					2.docker version  	 看到已经安装成功
					3.systemctl  start docker.service    	启动docker服务
					4.systemctl  enable docker.service	 	设置开机启动
					5.docker pull centos					远程拉取镜像测试
					6.docker images 						查看到镜像已经拉取成功
					7.docker run -i -t centos /bin/bash		启动容器，看到了bash提示符。在docker命令中，使用了 “-i 捕获标准输入输出”和 “-t 分配一个终端或控制台”选项。这里centos是镜像image的名称
						进入到容器内部了，相当于一个centos环境，
						这里就算安装成功了，每个镜像有对应的id，使用docker ps -a 查看linux版本
			++++++++++CentOS 7 安装docker++++++++++	
		
3. docker run指令详细
      docker run [OPTIONS] IMAGE [COMMAND] [ARG...]	用镜像启动一个容器
      #docker run -itd -h "docker_pc_tomcat" --add-host docker_pc_tomcat:192.168.6.139 --net=none -m 1500M  -v /usr/local/webapps/XiaoyuerProject:/usr/local/tomcat/webapps/ -v /usr/local/logs/pc:/usr/local/tomcat/logs  -v /etc/localtime:/etc/localtime  --name docker_pc_tomcat  192.168.6.138:5000/xye-tomcat:v1;sudo /usr/local/bin/pipework br0 docker_pc_tomcat 192.168.6.139/24@192.168.6.1
      
         --name: 		为容器指定名称，否则是数字字母串，可读性差 名称唯一
         -p: 			指定端口映射，格式为：主机(宿主)端口:容器端口 左边映射到右边。公开端口是为了从外面访问容器web程序
         -i 				选项指示 docker 要在容器上打开一个标准的输入接口
         -t 				指示 docker 要创建一个伪 tty 终端，连接容器的标准输入接口，之后用户就可以通过终端进行输入
                     ***
                        一般-it表示一个交互式的任务,进入命令交互界面
                        Docker中系统镜像的缺省命令是 bash，不加的话，bash命令执行了自动会退出。
                        这是因为如果没有衔接输入流，本身就会马上结束。
                        加-it 后docker命令会为容器分配一个伪终端，并接管其stdin/stdout支持交互操作，这时候bash命令不会自动退出
                        /bin/bash   创建bash会话，就可以运行其他的命令
                     ***
         
         --restart="no"	指定容器停止后的重启策略:
                        no：容器退出时不重启
                        on-failure：容器故障退出（返回值非零）时重启
                        always：容器退出时总是重启
         --add-host		在启动容器时，向容器的/etc/hosts文件添加一个host:ip的映射
         -h				指定容器的主机名，也是/etc/hosts中添加一个映射主机地址的hostName
         -d 				作为后台守护式容器运行
         -v				挂载目录	本地路径:容器路径,将本地目录和容器目录中的文件修改实时同步
                     
                     将容器中目录内容实时同步到主机目录中，相当于容器内容在主机中持久化,这样主机中也可以查看容器中目录，比如logs日志
                     通过挂载的方式，将项目代码同步到容器中的webapps中
                     
                     为docker提供持久数据和共享数据，对卷修改会直接生效，绕过镜像，当提交或者创建镜像时，卷不被包含在内。
                     卷可以容器共享，容器停止，卷内容依旧存在
                     好处是，这样可以不把应用和代码构建到镜像中，这样代码和运行环境分离开来，不用每次改动重构镜像
                     可以追加ro和wo来指定读写状态
                     直接修改主机中挂载目录的内容，容器内容显示也生效
                     
                     docker run -it -v /宿主机目录:/容器目录 镜像名 /bin/bash
                        都用绝对目录，容器目录必须绝对路径，宿主目录不存在会自动创建，宿主目录建议使用绝对路径(否则是相对/var/lib/docker/volumes/路径)
                        容器销毁了，新建的挂载目录不会消失
                        挂载宿主机已存在目录后，在容器内对其进行操作，报“Permission denied” run 增加--privileged=true参数
                        好处：更新内容，然后重新运行 run 命令即可。这样就将环境和代码分离，镜像就是运行环境，运行环境只要不改变，就不用重新生成镜像。否则如果将内容打包到镜像中的话，任何对代码的修改都必须重新生成镜像。
         
         --net			指定容器运行的网络
                     docker一共有4中网络模式：
                        1：bridge模式，--net=bridge(默认)。
                           这是dokcer网络的默认设置。安装完docker，系统会自动添加一个供docker使用的网桥docker0，我们创建一个新的容器时，容器通过DHCP获取一个与docker0同网段的IP地址。并默认连接到docker0网桥，以此实现容器与宿主机的网络互通。如下：
                        2：host模式，--net=host。
                           这个模式下创建出来的容器，将不拥有自己独立的Network Namespace，即没有独立的网络环境。它使用宿主机的ip和端口。
                        3：container模式，--net=container:NAME_or_ID。
                           这个模式就是指定一个已有的容器，共享该容器的IP和端口。除了网络方面两个容器共享，其他的如文件系统，进程等还是隔离开的。
                        4：none模式，--net=none。
                           这个模式下，dokcer不为容器进行任何网络配置。需要我们自己为容器添加网卡，配置IP。
                           因此，若想使用pipework配置docker容器的ip地址，必须要在none模式下才可以。
                     
                     --net=none      配合pipework自定义配置网络的ip
                     /usr/local/bin/pipework br0 docker_ids_tomcat 192.168.6.148/24@192.168.6.1	#pipework为添加新的网卡，并将它们连接到br0上，配置了192.168.6.148 ip
      
                     192.168.6.148/24 表示网络 192.168.6.148 网络位为24 位。
                     /24 这是一种简化的子网掩码表示法。它表示子网掩码为 255.255.255.0。	也就是说 /24 表示 IP 地址中的前 24 位是网络位。
      
                     pipework介绍 http://lsword.github.io/2014/05/22.html
                     命令：pipework br_name container_name ip/netmask@vlan(gateway) 
                     pipework安装：
                        # wget https://github.com/jpetazzo/pipework/archive/master.zip
                        # unzip pipework-master.zip
                        # cp pipework-master/pipework? /usr/local/bin/
                        # chmod +x /usr/local/bin/pipework

                        创建none模式的容器，为其分配IP。
                        #ip a show docker0

                        #docker run -idt --name test --net=none resin
                        #pipework docker0 test 172.17.42.100/16@172.17.42.1
                        #docker attach test
                        以上操作给新建的test容器分配了一个172.17.42.100的IP地址


4. docker网络配置
      相关介绍， https://blog.51cto.com/u_13362895/2130375
            
      docker内部联网
         安装docker时候，会创建一个新的网络接口docker0,每个容器都会在这个接口上分配一个ip地址，
         
         docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
         link/ether 02:42:c3:a6:fe:cc brd ff:ff:ff:ff:ff:ff
         inet 172.17.0.1/16 scope global docker0
            valid_lft forever preferred_lft forever
         inet6 fe80::42:c3ff:fea6:fecc/64 scope link 
            valid_lft forever preferred_lft forever

         接口本身地址172.17.0.1是docker网络的网关地址，也是所有容器的网关地址
         
         docker0是一个虚拟的以太网桥，用于连接容器和本地宿主网络
         docker每创建一个容器就会创建一组互联的网络接口，这组接口一端作为容器里的eth0接口，另一端统一命名为类似verhec6a这种名字，作为宿主机的一个端口。
         这个veth接口一端插在docker0的网桥上，另一端插到容器里。通过把每个veth*接口绑定到docker0网桥，docker创建一个由主机和所有docker容器共享的虚拟子网
            
         docker0 的IP地址的范围是  172.16. - 172.30.    docker默认会使用172.17.x.x作为子网地址，
         docker0-多个veth*接到一端接docker0，另一端接到容器(容器中显示的是eth0)，这里的ip分配是随机的，不能固定。	

         本地容器之间私有网络互联
            Docker 默认的桥接网卡是 docker0。它只会在本机桥接所有的容器网卡，
            举例来说容器的虚拟网卡在主机上看一般叫做 veth* 而Docker0只是把所有这些网卡桥接在一起
            
         docker 网络注意iptables 和NAT的配置
         容器默认无法访问，从宿主网络与容器通信时，必须明确指定打开的端口。
			
5. docker初次部署tomat镜像
      docker run -itd   --name tomcat_test -p 8080:8080   -v /home/zhanjun/test/dockertest/tomcat_test/log:/usr/local/tomcat/logs tomcat 
         这里主机访问8080就转到了容器的地址中
         如果将容器80端口映射到主机的49161端口，那么在主机上浏览 localhost:49161 就会看到容器的对应内容
         
      问题1.启动container的时候出现iptables: No chain/target/match by that name
         重启docker，service docker restart
            systemctl restart  docker
            原理：
            使用的centos7服务器，在部署docker的过程中，因端口问题有启停firewalld服务，在centos7里使用firewalld代替了iptables。在启动firewalld之后，iptables还会被使用，属于引用的关系。
            所以在docker run的时候，iptables list里没有docker chain，重启docker engine服务后会被加入到iptables list里面。
      
      问题2.初次启动访问是404问题，
         1.进入容器
         2. rm -rf webapps  			强制删除空文件夹
         3. mv webapps.dist webapps  把webapps.dist文件 当做新的webapps文件
      
      问题3.没有vi等相关命令
         apt-get包管理应用软件
         1.apt-get update 			同步 /etc/apt/sources.list 和 /etc/apt/sources.list.d 中列出的源的索引，这样才能获取到最新的软件包
         2.apt-get install vim
      
      问题4.没有ll命令
         1.vim  ~/.bashrc 
         2.增加 alias ll='ls -l'    wq退出
         3.source ~/.bashrc  刷新
      
      问题5.没有权限查看挂在目录
            docker运行一个容器后，将主机中当前目录下的文件夹挂载到容器的文件夹后
            进入到docker容器内对应的挂载目录中，运行命令ls后提示：ls: cannot open directory .: Permission denied
            在docker run一个容器时，加上  --privileged=true	
		
6. docker相关指令
      docker images 									查看宿主机上的镜像，Docker镜像保存在/var/lib/docker目录下
      docker pull image_name							拉取docker镜像
      docker push  192.168.6.138:5000/xye-tomcat:v1 	推送到仓库
         docker push [OPTIONS] NAME[:TAG]
         
         默认是推到dockerhub  需要指定user/repo
         这里push的实际上是一个tag
         
      docker ps 										列出正在运行的容器，(-a,显示所有的容器，包括未运行的)，(-n x,显示最后x个容器，包括运行和停止的)，-l:显示最近创建的容器，-q:只显示容器编号。
      docker start container_name/id					重新启动容器
         docker start docker_pc_tomcat&&sudo /usr/local/bin/pipework br0 docker_pc_tomcat 192.168.6.139/24@192.168.6.1  // 对网络有要求需要重新指定，再次启动容器，无须其它参数
         
      docker stop container_id1，container_id2		停止容器，name or id  最好一致
      docker exec -it container_name/id /bin/bash    	登录容器内部，容器的id或name
      docker rmi  image_name/image_id 				删除docker镜像
      docker search nginx								查找Docker Hub上的nginx镜像
      docker image prune								清除所有的虚悬镜像，删除所有未被tag标记和未被容器使用的镜像，(-a删除所有未被容器使用的镜像)
         docker rm ： 删除一个或多个 容器			
            docker rm [OPTIONS] CONTAINER [CONTAINER...]
               -f	通过 SIGKILL 信号删除一个正在运行的容器
               -l	移除容器间的网络，而非容器本身
               -v	删除与容器映射的目录
         docker rmi ： 删除一个或多个 镜像
         docker prune ： 用来删除不再使用的 docker 对象
      
      docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]		打tag
            docker tag  xye-tomcat:v1 192.168.6.138:5000/xye-tomcat:v1 // 打tag  必须符合 192.168.6.138:5000/xxxx
            
            docker tag : 标记本地镜像，将其归入某一仓库。
            docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]
               #没有地址默认dockerhub，如果是私有库一定要指定地址，username一般可写可不写这里是dockerhub的accountname
            docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
               
            必须先打tag才能push，
            docker push 私有库地址/镜像名:镜像tag
            必须先打tag, tag的server地址是私有仓库地址. 然后再push

            tag不指定仓库地址默认的是dockerhub地址(传 到dockerhub还需要指定accountname)，如果要传到私有库 一定要指定仓库地址
            公有库 tag  [USERNAME/]NAME[:TAG]  		REGISTRYHOST可以省略
            私有库 tag  [REGISTRYHOST/]NAME[:TAG]	username可以省略    
               192.168.175.132:5000/zhanjun/centostest   这种就是在仓库下多了一层目录
               
            不指定tag，默认是latest版本
            打了新的 TAG 虽然会多了一条记录，但是从 IMAGE_ID 是相同的，可以得知他们是同一个镜像
            类比：在git中，同一个代码项目，可以有多个不同的 tag
      
      docker pull [OPTIONS] NAME[:TAG|@DIGEST]
         要拉取镜像，需要指定Docker Registry的URL和端口号，默认是Docker Hub，另外还需要指定仓库名和标签，仓库名和标签唯一确定一个镜像，而标签是可能省略，如果省略，则默认使用latest作为标签名，而仓库名则由作者名和软件名组成。
      
      docker commit -a "xiaoyuer" -m "open镜像"  075d5f9bd402  sit_open_tomcat:v1			//提交镜像
      docker tag  sit_open_tomcat:v1 192.168.6.138:5000/sit_open_tomcat:v1				//镜像打tag
      docker push  192.168.6.138:5000/sit_open_tomcat:v1 									//推送到远程docker仓库
      
      docker inspect docker_pc_tomcat 				// 查看容器元数据信息	其中Mounts是挂载模块，Cmd是初始启动模块
      docker stats 容器id  							// 查看容器内存使用情况  docker stats -a 查看所有容器情况
      docker logs
         docker logs [OPTIONS] CONTAINER
         
         docker logs -f -t --tail 100 xye-soa
         
         -f : 跟踪日志输出
         --since :显示某个开始时间的所有日志
         -t : 显示时间戳
         --tail :仅列出最新N条容器日志
      docker top docker_pc_tomcat						查看容器内进程
      docker history image_id							查看镜像创建的历史，镜像分层框架
      systemctl status docker							查看docker状态，启动异常时排查原因 
   
      **********
         运行java项目
         获取tomcat服务器上的war，运行一个java程序。需要构建一个有两个步骤的docker管道
         1.一个镜像从url拉取指定的war文件并将其保存到卷里
         2.一个含有tomcat服务器的镜像运行这些下载的war文件
      **********

      docker运行nginx为什么要使用 daemon off
         Docker 容器启动时，默认会把容器内部第一个进程，也就是pid=1的程序，作为docker容器是否正在运行的依据，如果 docker 容器pid=1的进程挂了，那么docker容器便会直接退出。
         Docker未执行自定义的CMD之前，nginx的pid是1，执行到CMD之后，nginx就在后台运行，bash或sh脚本的pid变成了1。
         所以一旦执行完自定义CMD，nginx容器也就退出了。
      
      docker服务的编排，暂时不看  Docker Compose,k8s等
      docker api   目前没用到

      k8s调试注意点(忽略)
         数据库base_config中的
            url_web_host_newadmin  	http://admin1.xiaoyuer.net -> https://admin2.xiaoyuer.net  登录更新
            
            url_web_host			https://www1.xiaoyuer.net  -> https://www2.xiaoyuer.net
            url_web_host_for_admin	http://www1.xiaoyuer.net -> http://www2.xiaoyuer.net
            url_web_hosts			https://www1.xiaoyuer.net ->https://www2.xiaoyuer.net
            
         启动后第一次发需求访问会超时；容器内部是可以相互访问的
         内部环境manager可以访问外面的soa
         
         
         查看当前运行的所有机器	 	kubectl get pod -nsit
         替换文件名进入容器		kubectl exec -it tomcat-xye-manager-cf9bf4887-24dpr -nsit /bin/bash
         查看容器的ip 			kubectl get pod -nsit -owide

         192.168.6.135 root 123456

         k8s的容器中locale环境配置不生效，是没有删除之前的镜像，使用了旧的镜像。
         upstream xye-ids_upstream {
               server 192.168.6.136:30812 weight=1 max_fails=2 fail_timeout=60s;
               server 192.168.6.137:30812 weight=1 max_fails=2 fail_timeout=60s;
            }


         k8s+dubbo(忽略)	
            https://blog.csdn.net/weixin_41715077/article/details/89385413	 		核心点和原理图
            https://blog.csdn.net/xujiamin0022016/article/details/107288208/		实际操作

         幸运的是dubbo提供了自动义指定注册ip的配置，我们可以在部署开发联调服务时指定ip地址为宿主机的ip地址。
         env:
                  - name:  APOLLO_META
                  value: http://172.31.205.22:8080
                  - name: DUBBO_IP_TO_REGISTRY
                  value: 172.31.205.23
                  - name: DUBBO_PORT_TO_REGISTRY
                  value: "30011"
                  - name: DUBBO_PORT_TO_BIND
                  value: "30011"

         有考虑过，但后期 K8S 节点扩容后，集群外服务往集群内的一些网络权限变更就得重新搞，一个服务就得开一个策略，假设有一百个服务，扩容一台节点，就得加一百条策略.
         针对外部访问不到容器内的服务
            目前是这样解决的，在容器部署模板里的 env 部分定义一个变量，并声明 valueFrom.fieldRef.fieldPath 的值为 status.hostIP，
            这样容器启动后就可以通过环境变量获取到宿主机的 IP 地址，将注册地址改为主机地址，外部 zookeeper 对主机地址可见即可
            env:
                  - name: DUBBO_IP_TO_REGISTRY
                     valueFrom:
                     fieldRef:
                        fieldPath: status.hostIP
                  - name: DUBBO_PORT_TO_REGISTRY
                     value: "20920"	



## 系统优化方案，系统监控相关，性能调优相关
	
   1. 静态化页面
	      效率最高、消耗最少的就是纯静态化的html页面。对于一些更新频率不频繁而又被大量访问的页面，我们就可以做静态化处理，来避免大规模的数据库访问。
	
   2. 对数据库分库分表，读写分离
         可以解决容量和性能问题。
         读写分离：就是将数据库分成读库和写库，再通过主从来属性同步数据库。
         分库分表：分为水平切分和垂直切分。水平切分就是将一个特大的表拆分成多个小的表。垂直切分则是根据业务的不同来切分。
         关于数据库的优化，在另一篇文章中有记录。

      通过缓存加速数据库的访问。

   3. 使用CDN加速全国用户的静态文件访问
         这里简单记录一下CDN原理：将数据内容缓存到附近的机房，用户访问时先从最近的机房获取数据，减少网络访问的路径，提高用户访问网站的响应速度和网站的可用性。解决网络宽带小、用户访问量大、网点分布不均等问题。
         cdn部署在网络提供商的机房，使请求近地区机房获取数据；而反向代理提供在网站的中心机房

         加速使用cdn
            目前主要用来缓存静态数据
            其实就是一种网络缓存技术
            cdn作用是把用户需要的内容分发到离用户近的节点，使用户就近获取所需内容。
            cdn系统分为cdn源站，cdn节点。源站提供节点使用的数据源头，而节点部署在距离最终用户比较近的地方。

   4. 反向代理（可归类为5）
         Nginx反向代理的作用：
         1.保护网络安全：任何的请求都必须先经过代理服务器，可以过滤一些非安全请求。
         2.通过配置缓存功能加速web请求。可以缓存服务器上的某些静态资源，减轻服务器的负载压力。
         3.实现负载均衡：充当负载均衡服务器均衡的分发请求，平衡集群中各个服务器的负载压力。

   5. 集群：多台服务器部署相同应用构成一个群体，通过负载均衡设备共同对外提供服务，这里使用了失效转移机制

      首页是不应该访问数据库的，一般从缓存服务器或者搜索引擎服务器获取数据
 
   6. 网站的伸缩性
         1、根据功能进行物理分离实现伸缩，即服务化，多个机器不同服务
         2、单一功能通过集群实现伸缩，多个机器同服务

   7. 网站性能，安全防护，网站解析
        
         服务降级
            对于调用超时的非核心服务，超时次数超过阈值，先跳过调用，过时间点再重试调用
            增加系统调用白名单，系统压力过大，丢弃非核心的服务，保证核心服务正常
            服务开关，高峰时屏蔽一些非核心链路的调用
            
         负载过高：		用备用机器扩容
         依赖宕机		数据无法写入，先写在本地，依赖恢复后，脚本数据还原
         ddos攻击		针对攻击类型，流量清洗，通过开关开启验证码来验证，启动流控机制。借助公网控制大量的用户计算机，同一时刻攻击一个主机。针对服务器ip的
         物理设备损坏	更换新机器
         网卡流量占满	修改负载均衡策略，流量牵引到其他机房。 	

         服务器防火墙中，只开启使用的端口，比如网站web服务的80端口、数据库的3306端口、SSH服务的22端口等。关闭不必要的服务或端口

         实际ddos防护
            高防ip 180.188.20.5
            高防ip拦截了快钱的请求,快钱、建周 回调失败问题确认（高防平台拦截，原因TLS1.0版本太老）
            dnspod 可以配置购买的域名转到哪个ip
            dnspod 配置www.xiaoyuer.com转到 美橙高防ip，美橙后台配置来源域名转到服务器ip
         
         域名解析ip(180.97.15.66)->接收66,防火墙打出到250->转到192.168.8.250(keepalived配置的虚拟ip)内部服务器,指定跳到31->31nginx-机器，转发到相应的服务器。
         
         https://help.aliyun.com/knowledge_detail/43742.html			个别请求绕过防火墙和高防ip

         高防ip拦截异常处理，更换为https://makersxye20141118.xiaoyuer.com/xye-netpay/callback/quickServerCallBack.htm，该路径没有经过高防，dnspod直接转发到服务器。


   8. 了解系统总体架构，明确压力方向。具体的接口或者是模块的使用率。

         业务给出总的访问量（各系统比重不一样），即总的pv,uv后，再推导出每个独立的系统，接口上的流量，评估机器数量，网络带宽和技术实现方式
         压测最关键的两个值，一个是qps，一个是rt。可预留curl地址，检测远程服务是否可用

         关键业务数据量分析，一天多少量，缓存数据量
         了解系统响应速度，吞吐量，tps,qps等指标需求。

         系统层面影响的只有三个，CPU 、内存和 I/O。  
         使用top查看，关键看进程id,虚拟内存分配 和物理内存，cpu占有，一般关心物理内存的占用，虚拟内存不用关心
         ps -aux|grep java 相比 -ef 能看物理内存占用情况
         
         常见的攻击方式
            xss  	 	由页面输入脚本引起，接收转义即可
            crsf  		利用cookie引起，使用token或者在cookie中设置httpOnly
            sql注入		使用预编译语句，用占位符将参数语句当做普通字符串
         
         释放不必要的引用： ThreadLocal 使用完记得释放以防止内存泄漏，各种 stream用完也记得close
         新生代不能设置过小，过小会经常minor GC，

   9. 大型电商网站架构案例和技术架构【推荐】
         https://blog.csdn.net/jianai0602/article/details/80346837 


   10. 性能排查，异常定位，性能测试

         使用top 和 ps 先查个大概 ，然后看具体进程的原因

         dump出来的线程ID（nid）是十六进制的，而我们用TOP命令看到的线程ID是十进制的，所 以要用printf命令转换一下进制。然后用十六进制的ID去dump里找到对应的线程。
         注意的是，要支持的2万的QPS必须是峰值，而不能是平均值，比如一天当中有23个小时QPS不足1万，只有一个小时的QPS达到了2万，我们的系统也要支持2万的QPS。
         visualvm  threaddump 分析

         内存分析工具
            测试在虚拟机上使用root用户是可以的，jstack 和 jstat 相关命令 
            jstat   	查看类加载，内存管理，gc情况
            jstack  	打印异常进程的堆栈信息 dump线程快照，定位死锁，超时问题
            jmap        查看堆内存使用情况，可生成JVM堆的转储快照，dump文件较大，消耗大量资源
         
            jps   		查看进程和运行的主类，cmd下测试可用
            jstack
               jstack用于生成java虚拟机当前时刻的线程快照。注意是当前时刻
               线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。
                  
               jstack命令的语法格式： jstack  <pid>。可以用jps查看java进程id。
               jstack 31177 > /home/tengfei.fangtf/dump17
               在实际运行中，往往一次 dump的信息，还不足以确认问题。建议产生三次 dump信息，如果每次dump都指向同一个问题，我们才确定问题的典型性。
            
            **********现在用VisualVM 比较多点，直接线程的客户端，比较好用(直接继承了上面的命令)**********
            目前连接远程，有两种方式，1.服务器上启动jstatd  2.服务器上配置jmx。然后客户端连接
            连接后续有个调优的可以试试


         系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间
   ​			QPS（TPS）：每秒钟request/事务 数量
   ​			并发数： 系统同时处理的request/事务数
   ​			响应时间：  一般取平均响应时间
   ​			QPS（TPS）= 并发数/平均响应时间

         在jmeter中的吞吐量是每秒处理的请求数



   11. QPS递增大多是由于读放大造成的压力
         无状态前端机器不足以承载请求流量，需要进行水平扩展，一般QPS是千级。 
         然后是关系型数据库无法承载读取或写入峰值，需要数据库横向扩展或引入nosql，一般是千到万级。 
         之后是单机nosql无法承载，需要nosql横向扩展，一般是十万到百万QPS。 
         最后是难以单纯横向扩展nosql，比如微博就引入多级缓存架构，这种架构一般可以应对百万到千万对nosql的访问QPS。 

         所以QPS和业务场景和设计相关性很大，比如可以通过浏览器本地缓存，用缓存做热点数据查询，写事务MQ异步处理等方式提升QPS。

   12. DNSPod中 nginx转发

## Jvm相关，虚拟机相关

   1. 基础的概念

      * java堆(heap)：
         是jvm中所管理内存中最大的一块，是所有线程共享的内存区域，jvm启动时创建。用来存放对象实例。
         所有对象的实例都要在堆上分配
         java堆是垃圾收集器管理的主要区域，因此也被称为gc堆。现在的收集器基本采用的是分代手机算法（根据对象存活周期的不同，将内存划分成为几个区域），所有java堆中还可以细分为：新生代（MinorGC）和老年代（MajorGC）
         目前主流的虚拟机都是按照可扩展来实现的（通过-Xmx 和-Xms控制），如果堆中没有内存完成实例分配，并且堆也无法再扩展时，将抛出OutOfMemoryError异常
         将堆的最小值-Xms参数和最大值-Xmx参数设置一样可避免堆自动扩展，增加无效的fullgc，浪费时间，即不可扩展
         
         堆中有eden，survivor，和old三个区域，非堆中有code cache和 perm gen

      * java虚拟机栈
         是线程私有的，生命周期与线程相同
         常见的两种异常情况：
            1.当线程请求的栈深度大于jvm允许的深度，抛出stackoverflowError	递归调用会出错
            2.如果jvm动态扩展时无法申请到足够的内存时会抛出outOfMemoryError   经常的编译class文件的内存不够
         使用-Xss配置栈内存容量，可以减少栈内存容量
         定义大量的本地变量，可增加次方法帧中本地变量表的长度
         
         是虚拟机中局部变量的表部分，所需的内存空间在编译时期完成分配，方法运行时期不会改变大小，存放了编译器可知的各种基本数据类型，对象引用和returnadress类型（指向了一条字节码指令的地址）

         本地方法栈(stack)：
            虚拟机栈为虚拟机执行java方法（也就是字节码）服务
            本地方法栈为虚拟机使用到的native方法服务

         每个线程都有自己的程序计数器，栈，本地变量(这些本地变量存储在线程的栈中)
         虚拟机只会直接对Java栈执行两种操作，以帧为单位的压栈和出栈。生命周期和线程相同

         栈中的方法调用
             ***栈的特性——后进先出。***
            方法调用，对应一个栈帧在虚拟机中从入栈到出栈（栈的特性——后进先出。压栈，弹出）
            java栈总是和线程关联一起，每创建一个线程，jvm就会创建对应一个java栈，这个线程执行的多个方法，每运行一个方法就创建一个栈帧(包含参数，方法名，内部变量，操作栈和方法返回值等信息)
            每一个方法执行完成，弹出栈帧作为方法的返回值，并清除该栈帧，java栈的栈顶的栈帧就是房钱正在执行的方，方法的到调用就是创建活动栈帧，后面逐层出栈。
            栈是非线程共享，堆是线程共享的。

            当线程执行一个方法时，就会随之创建一个对应的栈帧，并将建立的栈帧压栈。当方法执行完毕之后，便会将栈帧出栈。因此可知，线程当前执行的方法所对应的栈帧必定位于Java栈的顶部。
             一般的for是调用完方法就出栈了
           
            当前正运行的函数在数据区必须在栈顶
            函数的嵌套调用在很大程度上由栈的大小决定，栈越大，函数可以支持的嵌套调用次数就越多，
           

      * 方法区：
         存放class相关信息和常量池相关，如类名、访问修饰符、常量池、字段描述、方法描述
         各线程共享的内存区域，用于存放已被jvm加载的类信息、常量、静态变量、即是编译器编译后的代码等数据。别名Non-Heap(非堆)。区别于堆内存heap
         可选择固定大小或可扩展，该区域内存回收目标主要是针对常量池的回收和对类型的卸载，
         当方法区无法满足内存分配需求时，将抛出outOfMemoryError		PermGen space
         MaxPermSize，最大方法区容量
         1.8之后移除，-XX:PermSize，使用元空间(Metaspace)代替
         -XX:permSize 和 -XX:MaxPermSize 配置大小
         方法区用于存放 Class 相关信息，需要足够大内存的方法区用于保证动态生成的Class可以加载入内存

            方法区（永久代）： 永久代的回收有两种：常量池中的常量，无用的类信息，常量的回收很简单，没有引用了就可以被回收。
            对于无用的类进行回收，必须保证 3 点：
               1. 类的所有实例都已经被回收
               2. 加载类的 ClassLoader 已经被回收
               3. 类对象的 Class 对象没有被引用（即没有通过反射引用该类的地

         方法区就是java堆中的永久区。

   
         
      * 程序计数器(忽略)

         具备线程隔离的特性，每个线程工作时都有属于自己的独立计数器
         当前线程执行的字节码行号指示器
         jvm的多线程是通过线程轮流切换并分配处理器执行时间的的方式实现的，任何时刻，一个处理器只会执行一条线程指令。
         为了切换后恢复正确的执行位置，每条线程需要独立的程序计数器。这类内存区域称为“线程私有”的内存

         当被挂起的线程重新运行，它要想从被挂起的地方继续执行，就必须知道它上次执行到哪个位置，
         在JVM中，通过程序计数器来记录某个线程的字节码执行位置。
      
      * 直接内存 	direct memory (忽略)
         分配不会受到java堆大小的限制，配置内存容易忽略这块，导致超过物理内存限制，从而导致动态扩展时出现OutOfMemoryError异常
         比如大量的nio操作

      * 	常量池
            Java中的常量池，实际上分为两种形态：静态常量池和运行时常量池。
               * 静态常量池
                  即*.class文件中的常量池，class文件中的常量池不仅仅包含字符串(数字)字面量，还包含类、方法的信息，占用class文件绝大部分空间。

               * 运行时常量池
                  是jvm虚拟机在完成类装载操作后，将class文件中的常量池载入到内存中，并保存在方法区中，我们常说的常量池，就是指方法区中的运行时常量池。
                  运行时常量池中的常量，基本来源于各个class文件中的常量池。
                  属于方法区的一部分，在类加载之后，存放到方法区的运行时常量池中
                        
                  字符串常量池（编译期间产生）是运行时常量池（可以动态扩展）的一部分，运行时常量池只有一个

            程序运行时，除非手动向常量池中添加常量(比如调用intern方法)，否则jvm不会自动添加常量到常量池。

      堆和方法区是线程共享的，Java栈和本地方法栈是线程私有的

   2. 内存溢出
         * java堆的溢出
            内存映像分析工具分析
            1、内存泄漏			泄漏对象垃圾收集器无法自动回收，
            2、内存溢出			内存中对象必须存活，调节虚拟机的堆参数的配置
         
            内存溢出就是你要求分配的内存超出了系统能给你的，系统不能满足需求，于是产生溢出。

            Java内存泄漏就是没有及时清理内存垃圾，导致系统无法再给你提供内存资源（内存资源耗尽）。
               1.Java内存泄露是说程序逻辑问题,造成申请的内存无法释放.这样的话无论多少内存,早晚都会被占用光的. 最简单的例子就是死循环了.
               2.Java内存泄漏是指在堆上分配的内存没有被释放，从而失去对其控制。这样会造成程序能使用的内存越来越少。

         * 栈内存溢出：
            每个线程分配到的栈容量越大，可以建立的线程数量自然越少，建立线程时就越容易吧剩下的内存耗尽，
            如果是建立过多线程导致的内存溢出，在不能减少线程数量情况下，只能通过减少最大堆和减少栈容量来换取更多的线程。

         
         * 常见内存溢出：	
            * 第一种OutOfMemoryError： PermGen space   永久区内存溢出(非堆内存溢出) 
               永久保存区域是存放class信息和meta信息，分配了后，jvm是不会去回收的
               发生这种问题的原意是程序中使用了大量的jar或class，使java虚拟机装载类的空间不够，与Permanent Generation space有关。
               
               解决这类问题有以下两种办法：
                     1. 增加java虚拟机中的XX:PermSize和XX:MaxPermSize参数的大小，前者是初始永久保存区域大小，后者是是最大永久保存区域大小。
                        如针对tomcat6.0，在catalina.sh 或catalina.bat文件中一系列环境变量名说明结束处（大约在70行左右） 增加一行：JAVA_OPTS=" -XX:PermSize=64M -XX:MaxPermSize=128m"
                     2. 清理应用程序中web-inf/lib下的jar，如果tomcat部署了多个应用，很多应用都使用了相同的jar，可以将共同的jar移到tomcat共同的lib下，减少类的重复加载。

            * 第二种OutOfMemoryError：  Java heap space 堆内存溢出 Xms 和Xmx
               发生这种问题的原因是java虚拟机创建的对象太多，在进行垃圾回收之间，虚拟机分配的到堆内存空间已经用满了，与Heap space有关。
               解决这类问题有两种思路：
                     1. 检查程序，看是否有死循环或不必要地重复创建大量对象。找到原因后，修改程序和算法。
                     2. 增加Java虚拟机中Xms（初始堆大小）和Xmx（最大堆大小）参数的大小。如：set JAVA_OPTS= -Xms256m -Xmx1024m
               
               内存配置默认是物理内存的1/64;JVM最大分配的内存由-Xmx指定，默认是物理内存的1/4。
               默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制;空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小
            
         * 虚拟机栈和本地方法栈溢出
               StackOverflowError ：线程请求的战深度大于虚拟机所允许的最大深度 循环递归会触发这种 OOM   	常见递归方法操作      纵向无法分配，即无法分配新的栈帧
                  当一个方法执行完毕  这个栈帧才会从栈顶pop掉，递归会一直向栈里push栈帧 ，而这个java栈是有一定的长度或深度的,当栈满了,无法再进行push的时候会出现异常

               OutOfMemoryError ：虚拟机在扩展棋时无法申请到足够的内存空间，一般可以通过不停地创建线程触发这种 QOM             横向无法分配，无法建立新的线程
                  
                  堆内存+方法区内存+线程栈内存(程序计数器忽略不及)，那么不论堆内存和方法区内存占的内存空间有多小，线程栈内存的大小总是有限制的，
                  由于java虚拟机会为每个线程分配一块内存，不断创建线程的方法，为每个线程分配的内存越大，如果线程数量足够多，当再申请新的内存时，则无内存可用，也会抛出OOM

               Java 堆溢出： 在创建大量对象并且对象生命周期都很长的情况下，会引发OutOfMemoryError
               方法区溢出： 方法区存放 class 等元数据信息，如果产生大量的类（使用CGLIB ），那么就会引发此内存溢出，即 OutOfMemoryError:PermGen space， 运行期间，jvm不会在主程序运行期间清理。一般发生在启动阶段
	
         栈内存溢出(StackOverflowError)：程序所要求的栈深度过大导致。
         
         堆内存溢出(OutOfMemoryError:java heap space)： 分清 内存泄露还是 内存容量不足。泄露则看对象如何被 GC Root 引用。不足则通过 调大 -Xms，-Xmx参数。
         
         持久带内存溢出(OutOfMemoryError: PermGen space)：Class对象未被释放，Class对象占用信息过多，有过多的Class对象。

            (永久代的作用是存储每个类的信息，如类加载器引用、运行池常量池、字段数据、方法数据、方法代码、方法字节码等)
            无法创建本地线程：总容量不变，堆内存，非堆内存设置过大，会导致能给线程的内存不足。

            这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGen space中，它和存放类实例(Instance)的Heap区域不同,GC(Garbage Collection)不会在主程序执行期对PermGen space进行清理，所以假设你的应用中有非常多CLASS的话,就非常可能出现PermGen space错误。
            这样的错误常见在webserver对JSP进行pre compile的时候。
            假设你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。

         减少内存解决内存溢出：
         内存-堆-方法区/栈容量=线程数，创建多个线程，容易引起内存溢出
         情景，不减少线程数情况下，减少最大堆和栈容量来换取更多的线程

         内存溢出 out of memory，是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。
		   内存泄露 memory leak，是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。
               当一个对象已经不需要再使用本该被回收时，另外一个正在使用的对象持有它的引用从而导致它不能被回收，导致本该被回收的对象不能被回收而停留在堆内存中，这就产生了内存泄漏。
               无用可达

         内存泄漏场景：
            (1) 长生命周期的对象持有短生命周期对象的引用
            这是内存泄露最常见的场景，也是代码设计中经常出现的问题。
            例如：在全局静态map中缓存局部变量，且没有清空操作，随着时间的推移，这个map会越来越大，造成内存泄露。
            (2) 修改HashSet中对象的参数值，且参数是计算哈希值的字段
            当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段，否则对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了，
            在这种情况下，即使在contains方法使用该对象的当前引用作为参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致无法从HashSet集合中删除当前对象，造成内存泄露。

            (3) 机器的连接数和关闭时间设置
            长时间开启非常耗费资源的连接，也会造成内存泄露。


   3. 对象访问（句柄访问，指针访问）
         hotspot采用的是直接指针访问：直接保存的是对象实例的地址，对象实例中保存了类型数据的指针。其中实例在堆中，对象类型数据在方法区中。
         根据对象引用能定位到堆中的对象

         栈随线程生灭，方法或者线程结束，其内存自然就回收了，但是java堆和方法区不一样。
         堆中存放的是java中的对象实例，已经dead(属于对象是否存活)的对象需要被回收

         对象的访问 
            我们使用的HotSpot虚拟机 的对象访问方式是指针访问的方式
            Object obj=new object()
            Object obj存在栈的本地变量表中，作为refernce类型数据出现
            new object() 在堆中开辟了空间存放实例
            相应的对象类型数据(对象类型，父类，接口，方法)的地址信息，存放在方法区中
            指针的访问方式，refernce中直接存储的就是对象的地址，访问到相应实例，然后对象的类型数据要去方法区中找

   4. jvm判断对象是否存活，
         使用根搜索算法，基本思路是通过一系列名为gc roots 的对象作为起始点，从该节点向下搜索，走过的路径成为引用链，
         当一个对象到gc roots没有任何引用链相连（即从gc roots到这个对象不可达），则对象不可用将被判定为可回收对象

   6. java中，可作为gc roots对象包括：（忽略）
         1、虚拟机栈（栈帧中的本地变量表）中的引用对象
         2、方法区中的类静态属性引用对象
         3、方法区中的常量引用对象
         4、本地方法栈中的native方法的引用的对象

   7. finalize()：(忽略)
         对象进行根搜索就发现没有与gc roots相连接的引用链，需要看对象是否覆盖finalize(),没有覆盖或者该方法已经被jvm调用过，则对象没必要执行finalize()。
         对象死亡需要经过两次至少两次的标记，一次就是gc roots的不相连接，执行finalize()会对f-queue队列中的对象第二次标记，只要重新与引用链上的任何一个对象建立关联，那么将移除即将回收的集合，即执行finalize()，是对象的自我拯救方法，对象依然可以存活，在被gc时自我拯救，且对象只能执行一次该方法。

   8. 内存分配与回收策略
         对象内存分配，往大方向上讲，就是在堆上分配，对象主要分配在新生代的eden区上
         1、对象优先在eden分配
               当Eden区内存不够的时候就会触发MinorGC，对新生代区进行一次垃圾回收。
         2、大对象直接进入老年代
               指的是需要大量连续内存空间的java对象，典型的是很长的字符串和数组，比如byte[]，
         3、长期存活的对象将进入老年代
               默认gc后年龄计数器+1，达到15进入老年代，参数可配置
         4、动态对象年龄判断
               当年龄大于survivor空间的一半，大于等于该年龄的直接进入老年代
         5、空间分配担保
               发生minor GC,jvm会检测室之前晋升到老年代的平均值大小，再根据老年代的剩余空间大小，够就开启担保（避免频繁full gc），允许就执行minor gc，不允许就full gc，不够直接full gc，如果担保失败，也进行full gc。
            
         虽然java的垃圾回收机制仅是回收堆区的资源，而对于非堆区无效，这种只能凭借开发人员自身的约束来解决。（堆区有java回收机制、非堆区开发人员能够很好的解决），
         配置堆区：-Xms 、-Xmx、-XX:newSize、-XX:MaxnewSize、-Xmn
         配置非堆区：-XX:PermSize、-XX:MaxPermSize


         https://www.jianshu.com/p/4b4519f97c92     配置总结
	
         其默认空间(即-Xms)是物理内存的1/64，最大空间(-Xmx)是物理内存的1/4。
         如果内存剩余不到40％，JVM就会增大堆到Xmx设置的值，内存剩余超过70％，JVM就会减小堆到Xms设置的值。
         所以服务器的Xmx和Xms设置一般应该设置相同避免每次GC后都要调整虚拟机堆的大小
         

         -Xms 为jvm启动时分配的内存，大点，程序会启动的快点，比如-Xms200m，表示分配200M
                           
         -Xmx 为jvm运行过程中分配的最大内存，比如-Xms500m，表示jvm进程最多只能够占用500M内存

         -Xss 为jvm启动的每个线程分配的内存大小，默认JDK1.4中是256K，JDK1.5+中是1M








   9. jvm的性能监控域处理工具   
         jps：显示指定系统所有的hotspot jvm进程 常用的是使用-l查看完整的类路径和jar包的位置
         jstat：手机hotspot jvm各方面（类装载，内存，垃圾收集）的运行数据				运行期定位jvm性能首选
         jinfo： 显示虚拟机的配置信息
         jmap：生成jvm的内存转储快照（heapdump文件），java内存映像工具
         jhat：分析heapdump文件，会建立一个http/htm服务器，让用户可在浏览器上查看分析结果
         jstack：显示jvm的线程快照  可定位线程长时间未响应的原因
         jconsole 和jvisualVM 可视化监控工具，目前主推的是jvisualVM这款的功能

   10. jvm的类加载机制：jvm把描述类的数据从class文件加载到内存，并对数据进行校验，转换解析和初始化，最终形成可被jvm直接使用的java类型，	
         对于静态字段，只有直接定义这个字段的类才会初始化，因此通过其子类来引用父类中的定义的静态字段，只会触发父类的初始化而不会触发子类的初始化
         静态代码块static{}用来初始化信息
         
         加载的几个阶段：加载，验证，准备，解析，初始化，使用，卸载，前5个就是类的加载过程，2-4属于连接
         * 加载：1.获取类的二进制字节流，2.将其代表的静态存储结构转化为方法区的运行时数据结构，3.在堆中生成代表该类的class对象，作为方法区这些数据的访问入口。
         * 验证：确保class文件的字节流中包含的信息符合当前虚拟机的要求，并不会危害jvm自身安全
         * 准备：在方法区中，为类变量(被static修饰的变量，不包括实例变量)分配内存并设置类变量的初始值，实例变量将会在对象实例化时随着对象一起分配在java堆中，
         * 解析：jvm将常量池内存内的符号引用替换为直接引用的过程
         * 初始化：真正开始执行类中定义的java程序代码(或者说是字节码)


         (忽略)类加载器的有趣味解读，总结就是先加载类，再实例化一个类的对象
            在 Java 中，Class<T> 和 ClassLoader 是造物之始。万物皆是“某类T” 的存在物，
            而“某类T” 是“万类之类 Class<T>” 的存在物，类别也是一种存在物，存在物即 Object。实例 t -> 类别 T -> 所有类别的抽象 Class<T> -> Object。
            要创造类别 T 的实例，先通过某种方式(ClassLoader)找到该物的“种子”(Class<T> 对象)，然后通过该种子来创造具体的物 t。
            要生成一个 Integer 对象，先找到 Class<Integer> , 然后 newInstance 出 Integer 的实例。
            java本身并不实现，会到META-INF下去寻找相应接口名文件中的实现类去实现  

         
         class文件是一组以8位字节为基础单位的二进制流,jvm只要识别是class文件就可以执行了，不用关心class的来源是什么语言
	      class文件中，各数据严格按照顺序紧凑排列，其中只有两种数据类型：无符号数和表，class本质上是一张表，按序解析
	      虚拟机把描述类的数据从class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可被虚拟机直接使用的java类型
	

         符号引用(忽略)
            在编译的时候一个每个java类都会被编译成一个class文件，但在编译的时候虚拟机并不知道所引用类的地址，多以就用符号引用来代替，而在这个解析阶段就是为了把这个符号引用转化成为真正的地址的阶段。



   11. 双亲委派机制(防止重复加载同一个.class)
      **双亲委派机制，就是一个类只能被一个类加载器加载。**

         类加载器一般是三层的classLoader，是上下子父级关系，不是继承关系，只是调用逻辑关系
            BootstrapclassLoader:
               主要负责加载核心的类库(java.lang.*等)，$JAVA_HOME$/jre/lib，		负责加载 JAVA_HOME\lib 目录中的，
               加载jvm自身工作需要的类，没有字符加载器，jvm控制，别人访问不到
            ExtClassLoader：
               主要负责加载jre/lib/ext目录下的一些扩展的jar。		 					负责加载 JAVA_HOME\lib\ext 目录中的，
               除了System.getProperty("java.ext.dirs")目录下是由其加载，其他都由AppClassLoader来加载
            AppClassLoader：
               主要负责加载应用程序的主函数类，										负责加载用户路径（classpath）上的类库
               是上个加载器的子类，加载classpath目录下的类。
               是自实现类加载器的父加载器

            实际测试 可以遍历this.getClass.getClassLoader 的parent即可，就能看出链	

         *****
            向上递归检查，向下递归加载
            原则：1.自底向上检查类是否已经装载  2.自顶向下尝试加载类
            三层的classLoader负责的范围也不一样，这样依次检查，依次加载(在自己的范围内)。
            逐层往上(接待就返回，没有就抛给上层，直到有一级接待或者上级返回没有接待且不应由上级接待，那么本层就会加载)
         *****

         自底向上，挨个检查是否已经加载了指定类，如果已经载入，那么直接返回该类的实例的引用
         如果bootstapclassloader也未加载成功该类，那么会抛出异常。然后自顶向下挨个尝试加载。直到customclassloader,如果还未能加载，就抛出ClassNotFoundException给调用者。

         类请求递归委派到顶层，放父加载无法完成，子类才会去加载。这里双亲就是指的是父类。
         特殊场景：jdbc的spi加载，因为BootstrapclassLoader加载范围，只能委托子类加载实现。
         
         这种设计有个好处是，如果有人想替换系统级别的类：String.java。篡改它的实现，但是在这种机制下这些系统的类已经被Bootstrap classLoader加载过了，所以并不会再去加载，从一定程度上防止了危险代码的植入

         jvm表示一个类是否是同一个类的条件 1.完整类名是否一致，2.加载该类的classloader实例是否是同一个
      
         jvm显式加载class文件到内存
            1.this.getClass.getClassLoader.loadClass()	2.Class.forName()
            
         类加载的双亲委派模型实现逻辑：
            先检查是否已经被加载过，若没有加载则调用父加载器的loadclass()方法，若父加载器为空则默认使用启动类加载器作为父加载器。
            如果父类加载失败，则在抛出classNotFoundException异常后，再调用自己的findclass()方法进行加载
            即所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求(他的搜索范围中没有找到所需要的类时)，子加载器才会尝试自己加载
            好处是某个class，再程序的各种类加载环境中都是同一个类
         




   12. 垃圾回收的算法

         分代算法
            新生代中：主要是用来存放新生的对象。一般占据堆的1/3空间。由于频繁创建对象，所以新生代会频繁触发MinorGC进行垃圾回收。
               对象优先在Eden区分配
               Eden区：Java新对象的出生地(如果新创建的对象占用内存很大，则直接分配到老年代)。
               ServivorTo：保留了一次MinorGC过程中的幸存者。
               ServivorFrom：上一次GC的幸存者，作为这一次GC的被扫描者。
               默认Eden与单个Survivor默认比例是8:1
               
            老年代的对象比较稳定，所以MajorGC不会频繁执行。
            在进行MajorGC前一般都先进行了一次MinorGC，使得有新生代的对象晋身入老年代，导致空间不够用时才触发。当无法找到足够大的连续空间分配给新创建的较大对象时也会提前触发一次MajorGC进行垃圾回收腾出空间。
         
            * 新生代中的算法:停止-复制
               将 Eden 区和一个 Survivor 中仍然存活的对象拷贝到另一个 Survivor中
               年轻代可以分为 3 个区域：Eden 区（表示内存首次分配的区域，）
               和两个存活（Survivor 0 、Survivor 1,并且两个servivor区中必须有一个是空白的）内存分配过程为，最终gc后存活的就进入老年代，
               
               需要明确一点，“停 止（Stop-the-world）”的意义是在回收内存时，需要暂停其他所有线程的执行。
               这个是很低效的，现在的各种新生代收集器越来越优化这一点，但仍然只是将停止的时间变短，并未彻底取消停止。
               
               一次minorgc，将Eden区和一个Survivor中仍然存活的对象拷贝到另一个Survivor中，永远有一个survivor space是空的，放不下就直接进入老年代

            * 老年代中的算法：标记-整理
               如果使用停止-复制算法，则相当低效。一般，老年代用的算法是标记-整理算法，即：标记出仍然存活的对象（存在引用的），将所有存活的对象向一端移动，以保证内存的连续

               JVM 每次只会使用 Eden 和其中的一块 Survivor 区域来为对象服务，所以无论什么时候，总是有一块Survivor区域是空闲着的,复制完就清空。存活区默认切换15次就会进入老年代了。


         Survivor的存在意义，就是减少被送到老年代的对象，进而减少Full GC的发生，Survivor的预筛选保证，只有经历16次Minor GC还能在新生代中存活的对象，才会被送到老年代。


         minor GC触发条件：当eden区没有足够的空间进行分配的时候
         full gc 的触发条件：
            1.当老年代满了的时候就需要对老年代进行垃圾回收，老年代的垃圾回收称作Full GC
            2.发生minor gc时，虚拟机会检测之前每次晋升到老年代中的平均大小是否大于老年代的剩余空间大小，若大于，则改为直接进行一次full gc

         1. 调用System.gc时，系统建议执行Full GC，但是不必然执行
         2. 老年代空间不足
         3. 方法区空间不足
         4. 通过Minor GC后进入老年代的平均大小大于老年代的可用内存
         5. 由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小。


   13. 定义类就是定义类中的成员，包括成员变量（属性）和成员函数（行为）
         成员变量属于类，在堆中
         局部变量属于方法，在栈中
         new 一个对象的过程：
            Java中所有对象的存储空间都是在堆中分配的，但是这个对象的引用却是在堆栈中分配,也就是说在建立一个对象时从两个地方都分配内存，
            在堆中分配的内存实际建立这个对象，而在栈中分配的内存只是一个指向这个堆对象的指针(引用变量)而已。 
            在堆中分配的内存，由Java虚拟机的自动垃圾回收器来管理。 

   14. 运行期的优化(忽略)		
         区别是：即时编译生成机器相关的中间码，可重复执行缓存效率高。解释执行直接执行字节码，重复执行需要重复解释。
         被即时编译器编译的热点代码有两个特点：1被多次调用的方法。2被多次调用的循环体。
         
         java体系中的三种编译方式：前端编译、即时编译（JIT编译）、静态提前编译（AOT编译）,目前主流的是前端编译+JIT编译方式的运作
         1、前端编译
               把Java源码文件（.java）编译成Class文件(.class)的过程；
               也即把满足Java语言规范的程序转化为满足JVM规范所要求格式的功能；

            优点：
               这阶段的优化是指程序编码方面的；
               许多Java语法新特性（"语法糖"：泛型、内部类等等），是靠前端编译器实现的,而不是依赖虚拟机；
               编译成的Class文件可以直接给JVM解释器解释执行，省去编译时间，加快启动速度；

            缺点：
               对代码运行效率几乎没有任何优化措施；
               解释执行效率较低，所以需要结合下面的JIT编译；    
               前端编译器：Oracle javac、Eclipse JDT中的增量式编译器（ECJ）等;
            
         2、后端编译/即时(JIT)编译
            通过Java虚拟机（JVM）内置的即时编译器（Just In Time Compiler，JIT编译器）；在运行时把Class文件字节码编译成本地机器码的过程；            

            优点：
               通过在运行时收集监控信息，把"热点代码"（Hot Spot Code）编译成与本地平台相关的机器码，并进行各种层次的优化；
               可以大大提高执行效率；
            
            缺点：
               收集监控信息影响程序运行；
               编译过程占用程序运行时间（如使得启动速度变慢）；
               编译机器码占用内存；
               JIT编译器：HotSpot虚拟机的C1、C2编译器等；

            另外，JIT编译速度及编译结果的优劣，是衡量一个JVM性能的很重要指标；
            所以对程序运行性能优化集中到这个阶段；
         
         3、静态提前编译（Ahead Of Time，AOT编译）
            程序运行前，直接把Java源码文件（.java）编译成本地机器码的过程；

            优点：
               编译不占用运行时间，可以做一些较耗时的优化，并可加快程序启动；
               把编译的本地机器码保存磁盘，不占用内存，并可多次使用；

            缺点：
               因为Java语言的动态性（如反射）带来了额外的复杂性，影响了静态编译代码的质量；
               一般静态编译不如JIT编译的质量，这种方式用得比较少；
               静态提前编译器（AOT编译器）：JAOTC、GCJ、Excelsior JET、ART (Android Runtime)等；
         
         
         在 JVM 中有三个非常重要的编译器，它们分别是：前端编译器、JIT 编译器、AOT 编译器。
         前端编译器，最常见的就是我们的 javac 编译器，其将 Java 源代码编译为 Java 字节码文件。（纯文本编辑器打开 Demo.class 文件，我们会发现是一连串的 16 进制二进制流。）
         JIT 即时编译器，最常见的是 HotSpot 虚拟机中的 Client Compiler 和 Server Compiler，其将 Java 字节码编译为本地机器代码。
         而 AOT 编译器则能将源代码直接编译为本地机器码。
         
         这三种编译器的编译速度和编译质量如下：
            编译速度上，解释执行 > AOT 编译器 > JIT 编译器。
            编译质量上，JIT 编译器 > AOT 编译器 > 解释执行。
         而在 JVM 中，通过这几种不同方式的配合，使得 JVM 的编译质量和运行速度达到最优的状态。
         
         
         javac的编译过程：1解析与填充符号表过程 2.插入式注解处理器的注解处理过程 3、分析与字节码生成过程
         
         编译阶段的填充符号表：类之间是会互相引用的，但在编译阶段，我们无法确定其具体的地址，所以我们会使用一个符号来替代。
                           即对抽象的类或接口进行符号填充。等到类加载阶段，JVM 会将符号替换成具体的内存地址。
         
         
         字节码生成是javac编译过程的最后一个阶段，该阶段，把前面各步骤生成的信息转化为字节码写到磁盘中，编译器还进行了少量的代码添加和转换工作
         
         当源代码转化为字节码之后，其实要运行程序，有两种选择。
         一种是使用 Java 解释器解释执行字节码，
         另一种则是使用 JIT 编译器将字节码转化为本地机器代码。
         

         无论什么语言写的代码，最终都是通过机器码运行的。 编译过后的字节码文件就是.class格式的文件
         因为机器码的运行效率肯定是高于 Java 解释器的，

         前者启动速度快但运行速度慢，而后者启动速度慢但运行速度快。
         因为解释器不需要像 JIT 编译器一样，将所有字节码都转化为机器码，自然就少去了优化的时间。
         而当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。
         而我们知道，机器码的运行效率肯定是高于 Java 解释器的。
         所以一个节约内存，一个提高效率，现在一般是二者并存使用的，因为解释器可以作为编译器激进优化的一个逃生门。
         
         
         现在的的即时编译器一般是两种，为 Client Compiler 和Server Compiler。简称C1和C2编译器，这两种不同的编译器衍生出两种不同的编译模式，
         我们分别称之为：
         C1 编译模式：优化相对保守，编译速度快
         C2 编译模式：会根据性能监控信息，激进优化，编译质量好，耗时长
         
         
         目前是和解释器混搭的模式，工作模式都是可以配置的

15. 虚拟机字节码执行引擎(忽略)
		栈帧：是用于支持jvm方法调用和执行的数据结构，是stack中的栈元素，(存储了局部变量表，操作数栈，动态连接，方法返回地址等信息),方法调用到执行完成，对应栈帧在jvm中入栈到出栈的过程
		编译时期，栈帧需要多发的局部变量表、多深的操作数栈已经完全确定，因此一个栈帧分配多少内存，不会受到运行期变量数据影响。
		在方法的调用链中，只有栈顶的栈帧是有效的，称为当前栈帧，对应所关联的方法称为当前方法，执行引擎所运行的所有字节码指令，都只针对当前栈帧进行操作
		局部变量：表示一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。由于是建立在线程的堆栈上，是线程私有的数据
		
		类变量有两次赋初始值的过程，一次是准备阶段，赋予系统初始值；另一次是在初始化阶段，赋予程序员定义的初始值。所有即使没有人为赋值，也还是会有一个确定的初始值。
		但是局部变量是必须初始化赋值的
		
		虚拟机把操作数栈作为它的工作区——大多数指令都要从这里弹出数据，执行运算，然后把结果压回操作数栈。比如，iadd指令就要从操作数栈中弹出两个整数，执行加法运算，其结果又压回到操作数栈中
			
		执行引擎在执行java代码时候可能有解释器执行或者通过即时编译器产生的本地代码执行编译器
		所有执行引擎是一致的，输入的是字节码文件，处理过程是子界面解析的等效过程，输出的是执行结果